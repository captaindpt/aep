‰¢idÙ@fe92b604e213013098231e84a841d171c25c58f04bf325ae24c503518016f96e¢tsËAÚóõ©ü¨trace_idÙ$9af2d0e6-34d4-4852-8f4f-794b1242f419­parent_run_idÙ$b8239853-019a-441e-9b76-8657f5f810c8¦run_idÙ$9af2d0e6-34d4-4852-8f4f-794b1242f419ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚAmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Name two ChatModel parameters that help avoid exceeding rate limits. \nContext: 1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.\n2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information.\n\n3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\n\nIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\n\nThe size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.\n\n## Advanced topics\n \n### Rate-limiting\n\nMany chat model providers impose a limit on the number of requests that can be made in a given time period.\n\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n\nSome important things to note:\n\n- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can\'t be supported on these.\n- Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. `langchain-openai`, `langchain-anthropic`, etc.), they\'re not enforced on models in `langchain-community`.\n\nChat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model head to the their respective [API reference](https://python.langchain.com/api_reference/) for that model.\n\n## Tool calling\n\nChat models can call [tools](/docs/concepts/tools) to perform tasks such as fetching data from a database, making API requests, or running custom code. Please\nsee the [tool calling](/docs/concepts/tool_calling) guide for more information.\n\nMany of the key methods of chat models operate on [messages](/docs/concepts/messages) as input and return messages as output.\n\nChat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the [standard parameters](#standard-parameters) section for more details.\n\n:::note\nIn documentation, we will often use the terms "LLM" and "Chat Model" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@d7a2388cb5dadfb038857d65275b12ac5c013e34eced956749da6e8dc38a8b63¢tsËAÚóõ—pŸ¨focus_msÍR§payload‚¤role©assistant§contentÙÓTwo ChatModel parameters that help avoid exceeding rate limits are `rate_limiter`, which controls the rate of requests, and `max_retries`, which determines the number of retries after hitting a rate limit error.ªfocus_kind¬exec_latency¨query_id¤Q056‰¢idÙ@8369a2e07aaf590ff0e9dea431523b04a3f6414e642fd75beac571ae4b5ee568¢tsËAÚóõ—}²¨trace_idÙ$b8239853-019a-441e-9b76-8657f5f810c8­parent_run_idÙ$ef043d87-651c-4a7d-a2e6-37f4d9aa25d7¦run_idÙ$b8239853-019a-441e-9b76-8657f5f810c8ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙÓTwo ChatModel parameters that help avoid exceeding rate limits are `rate_limiter`, which controls the rate of requests, and `max_retries`, which determines the number of retries after hitting a rate limit error.¨query_id¤Q056ªfocus_kind¶chain_execution_result‰¢idÙ@6e91c59988bd625e05199483fe8a50b6384f6e5d0d806d7a9bd334c9d55d61c0¢tsËAÚóõ—‰×¨trace_idÙ$ef043d87-651c-4a7d-a2e6-37f4d9aa25d7­parent_run_idÀ¦run_idÙ$ef043d87-651c-4a7d-a2e6-37f4d9aa25d7ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙDName two ChatModel parameters that help avoid exceeding rate limits.§context•‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËSome important things to note:

- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration fo...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËMany of the key methods of chat models operate on [messages](/docs/concepts/messages) as input and return messages as output.

Chat models offer a standard set of parameters that can be used to config...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÙÓTwo ChatModel parameters that help avoid exceeding rate limits are `rate_limiter`, which controls the rate of requests, and `max_retries`, which determines the number of retries after hitting a rate limit error.¨query_id¤Q056ªfocus_kind¶chain_execution_result‹¢idÙ@4fbc56ad9b253c61c435f7a2f35b74e74c16f64d800af89e4232ef7e1fbecd3e¢tsËAÚóõ—˜¯¨trace_id¤Q057­parent_run_idÀ¦run_idÙ$9ed34a55-2a09-4d3b-95ef-43d4fb7700ecªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ4How do you specify a custom UUID for a Runnable run?¨query_id¤Q057§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q057ªfocus_kind¯chain_execution‹¢idÙ@03d1d35a718964245e37e69facdee739637fb159443278378ebcdad49f9eb8a8¢tsËAÚóõ—­ã¨trace_id¤Q057­parent_run_idÙ$9ed34a55-2a09-4d3b-95ef-43d4fb7700ec¦run_idÙ$cf484551-40db-49f0-b979-6f908394108cªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ4How do you specify a custom UUID for a Runnable run?§context¦answer ¨query_id¤Q057¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q057®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:04f2d0cd-cc10-545f-5fd2-36256f21a73dªfocus_kind¯chain_execution‰¢idÙ@9d4880e3a88c5a446447c7f26c15b36487d237edda81b4d84fdc308eb500959f¢tsËAÚóõ®ææ¨trace_idÙ$cf484551-40db-49f0-b979-6f908394108c­parent_run_idÙ$9ed34a55-2a09-4d3b-95ef-43d4fb7700ec¦run_idÙ$cf484551-40db-49f0-b979-6f908394108cªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË:::note Related
* [How-to trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)
:::

### Setting run id

:::note
This is an advanced feature that is unnece...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

* `configurable_fields`: This method allows you to configure specific **attribute...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË### Setting custom run name, tags, and metadata

The `run_name`, `tags`, and `metadata` attributes of the `RunnableConfig` dictionary can be used to set custom values for the run name, tags, and metad...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q057ªfocus_kind¶chain_execution_result‹¢idÙ@9955e4ee57091238ee7ca371216a5ddf3e8f58487de5b2fb88757da3313355b8¢tsËAÚóõ®îl¨trace_id¤Q057­parent_run_idÙ$9ed34a55-2a09-4d3b-95ef-43d4fb7700ec¦run_idÙ$b551dca4-1937-4e5f-bbe9-bb6da2684c69ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ4How do you specify a custom UUID for a Runnable run?§context•‚´page_content_summaryÙË:::note Related
* [How-to trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)
:::

### Setting run id

:::note
This is an advanced feature that is unnece...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

* `configurable_fields`: This method allows you to configure specific **attribute...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË### Setting custom run name, tags, and metadata

The `run_name`, `tags`, and `metadata` attributes of the `RunnableConfig` dictionary can be used to set custom values for the run name, tags, and metad...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answer ¨query_id¤Q057¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q057®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:3cb1ed50-2a95-1add-0773-c01d67e63affªfocus_kind¯chain_execution‹¢idÙ@aff5294001f9c06b21f593084bf51aa3b2835d7698aea3d41ac917d2447f949f¢tsËAÚóõ®ó¨trace_id¤Q057­parent_run_idÙ$b551dca4-1937-4e5f-bbe9-bb6da2684c69¦run_idÙ$e4784895-d524-4a38-888e-2555ae74a151ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ4How do you specify a custom UUID for a Runnable run?§contextÚN:::note Related
* [How-to trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)
:::

### Setting run id

:::note
This is an advanced feature that is unnecessary for most users.
:::

You may need to set a custom `run_id` for a given run, in case you want 
to reference it later or correlate it with other systems.

The `run_id` MUST be a valid UUID string and **unique** for each run. It is used to identify
the parent run, sub-class will get their own unique run ids automatically.

To set a custom `run_id`, you can pass it as a key-value pair in the `config` dictionary when invoking the Runnable:

```python
import uuid

run_id = uuid.uuid4()

some_runnable.invoke(
   some_input, 
   config={
      'run_id': run_id
   }
)

# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
and is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.
:::

Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.

To simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)](/docs/concepts/lcel) to compose
multiple Runnables and you need to add custom processing logic in one of the steps.

There are two ways to create a custom Runnable from a function:

* `RunnableLambda`: Use this for simple transformations where streaming is not required.
* `RunnableGenerator`: use this for more complex transformations when streaming is needed.

See the [How to run custom functions](/docs/how_to/functions) guide for more information on how to use `RunnableLambda` and `RunnableGenerator`.

:::important
Users should not try to subclass Runnables to create a new custom Runnable. It is
much more complex and error-prone than simply using `RunnableLambda` or `RunnableGenerator`.
:::

## Configurable runnables

To simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

* `configurable_fields`: This method allows you to configure specific **attributes** in a Runnable. For example, the `temperature` attribute of a chat model.
* `configurable_alternatives`: This method enables you to specify **alternative** Runnables that can be run during runtime. For example, you could specify a list of different chat models that can be used.

See the [How to configure runtime chain internals](/docs/how_to/configure) guide for more information on how to configure runtime chain internals.

### Setting custom run name, tags, and metadata

The `run_name`, `tags`, and `metadata` attributes of the `RunnableConfig` dictionary can be used to set custom values for the run name, tags, and metadata for a given Runnable.

The `run_name` is a string that can be used to set a custom name for the run. This name will be used in logs and other places to identify the run. It is not inherited by sub-calls.

The `tags` and `metadata` attributes are lists and dictionaries, respectively, that can be used to set custom tags and metadata for the run. These values are inherited by sub-calls.

Using these attributes can be useful for tracking and debugging runs, as they will be surfaced in [LangSmith](https://docs.smith.langchain.com/) as trace attributes that you can
filter and search on.

The attributes will also be propagated to [callbacks](/docs/concepts/callbacks), and will appear in streaming APIs like [astream_events](/docs/concepts/streaming) as part of each event in the stream.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q057®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:3cb1ed50-2a95-1add-0773-c01d67e63aff­checkpoint_nsÙ-generate:3cb1ed50-2a95-1add-0773-c01d67e63aff¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@ed5d92fd75aaaec3e0a5ae58eeb8734e0cee0d3dbaf5a7e584be3c6ec4f0c4e1¢tsËAÚóõ®úv¨trace_idÙ$e4784895-d524-4a38-888e-2555ae74a151­parent_run_idÙ$b551dca4-1937-4e5f-bbe9-bb6da2684c69¦run_idÙ$e4784895-d524-4a38-888e-2555ae74a151ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ>messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: How do you specify a custom UUID for a Runnable run? \nContext: :::note Related\n* [How-to trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\n:::\n\n### Setting run id\n\n:::note\nThis is an advanced feature that is unnecessary for most users.\n:::\n\nYou may need to set a custom `run_id` for a given run, in case you want \nto reference it later or correlate it with other systems.\n\nThe `run_id` MUST be a valid UUID string and **unique** for each run. It is used to identify\nthe parent run, sub-class will get their own unique run ids automatically.\n\nTo set a custom `run_id`, you can pass it as a key-value pair in the `config` dictionary when invoking the Runnable:\n\n```python\nimport uuid\n\nrun_id = uuid.uuid4()\n\nsome_runnable.invoke(\n   some_input, \n   config={\n      \'run_id\': run_id\n   }\n)\n\n# Do something with the run_id\n```\n\n### Setting recursion limit\n\n:::note\nThis is an advanced feature that is unnecessary for most users.\n:::\n\n## Configurable runnables\n\n:::note\nThis is an advanced feature that is unnecessary for most users.\n\nIt helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)\nand is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.\n:::\n\nSometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.\n\nTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:\n\n## Creating a runnable from a function {#custom-runnables}\n\nYou may need to create a custom Runnable that runs arbitrary logic. This is especially\nuseful if using [LangChain Expression Language (LCEL)](/docs/concepts/lcel) to compose\nmultiple Runnables and you need to add custom processing logic in one of the steps.\n\nThere are two ways to create a custom Runnable from a function:\n\n* `RunnableLambda`: Use this for simple transformations where streaming is not required.\n* `RunnableGenerator`: use this for more complex transformations when streaming is needed.\n\nSee the [How to run custom functions](/docs/how_to/functions) guide for more information on how to use `RunnableLambda` and `RunnableGenerator`.\n\n:::important\nUsers should not try to subclass Runnables to create a new custom Runnable. It is\nmuch more complex and error-prone than simply using `RunnableLambda` or `RunnableGenerator`.\n:::\n\n## Configurable runnables\n\nTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:\n\n* `configurable_fields`: This method allows you to configure specific **attributes** in a Runnable. For example, the `temperature` attribute of a chat model.\n* `configurable_alternatives`: This method enables you to specify **alternative** Runnables that can be run during runtime. For example, you could specify a list of different chat models that can be used.\n\nSee the [How to configure runtime chain internals](/docs/how_to/configure) guide for more information on how to configure runtime chain internals.\n\n### Setting custom run name, tags, and metadata\n\nThe `run_name`, `tags`, and `metadata` attributes of the `RunnableConfig` dictionary can be used to set custom values for the run name, tags, and metadata for a given Runnable.\n\nThe `run_name` is a string that can be used to set a custom name for the run. This name will be used in logs and other places to identify the run. It is not inherited by sub-calls.\n\nThe `tags` and `metadata` attributes are lists and dictionaries, respectively, that can be used to set custom tags and metadata for the run. These values are inherited by sub-calls.\n\nUsing these attributes can be useful for tracking and debugging runs, as they will be surfaced in [LangSmith](https://docs.smith.langchain.com/) as trace attributes that you can\nfilter and search on.\n\nThe attributes will also be propagated to [callbacks](/docs/concepts/callbacks), and will appear in streaming APIs like [astream_events](/docs/concepts/streaming) as part of each event in the stream. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@e6c462bca5257fd59b57dc0e55a2b420b436a756adfa902f7d0af0eded1791fb¢tsËAÚóö?v¨focus_msÍÒ§payload‚¤role©assistant§contentÚrTo specify a custom UUID for a Runnable run, you need to pass it as a key-value pair in the `config` dictionary when invoking the Runnable. The `run_id` must be a valid UUID string and unique for each run. For example, you can generate a UUID using `uuid.uuid4()` and include it in the invocation like this: `some_runnable.invoke(some_input, config={'run_id': run_id})`.ªfocus_kind¬exec_latency¨query_id¤Q057‰¢idÙ@a1d58e0e32879020942ca8ac6761116f43a1cd039b97757d0e4a12d1df52ae46¢tsËAÚóö?š¨trace_idÙ$b551dca4-1937-4e5f-bbe9-bb6da2684c69­parent_run_idÙ$9ed34a55-2a09-4d3b-95ef-43d4fb7700ec¦run_idÙ$b551dca4-1937-4e5f-bbe9-bb6da2684c69ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚrTo specify a custom UUID for a Runnable run, you need to pass it as a key-value pair in the `config` dictionary when invoking the Runnable. The `run_id` must be a valid UUID string and unique for each run. For example, you can generate a UUID using `uuid.uuid4()` and include it in the invocation like this: `some_runnable.invoke(some_input, config={'run_id': run_id})`.¨query_id¤Q057ªfocus_kind¶chain_execution_result‰¢idÙ@edd06dbc0dc18cc41d4b196ecf11313b990d287a31e0eefe09ff094876204a69¢tsËAÚóö?¥v¨trace_idÙ$9ed34a55-2a09-4d3b-95ef-43d4fb7700ec­parent_run_idÀ¦run_idÙ$9ed34a55-2a09-4d3b-95ef-43d4fb7700ecªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ4How do you specify a custom UUID for a Runnable run?§context•‚´page_content_summaryÙË:::note Related
* [How-to trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)
:::

### Setting run id

:::note
This is an advanced feature that is unnece...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

* `configurable_fields`: This method allows you to configure specific **attribute...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË### Setting custom run name, tags, and metadata

The `run_name`, `tags`, and `metadata` attributes of the `RunnableConfig` dictionary can be used to set custom values for the run name, tags, and metad...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚrTo specify a custom UUID for a Runnable run, you need to pass it as a key-value pair in the `config` dictionary when invoking the Runnable. The `run_id` must be a valid UUID string and unique for each run. For example, you can generate a UUID using `uuid.uuid4()` and include it in the invocation like this: `some_runnable.invoke(some_input, config={'run_id': run_id})`.¨query_id¤Q057ªfocus_kind¶chain_execution_result‹¢idÙ@33b6f7ac0af529ad766c54fbf1193e3557bb6f43fc498e13193b970f9ff4e022¢tsËAÚóö?­X¨trace_id¤Q058­parent_run_idÀ¦run_idÙ$6f26f3be-3c69-4986-ae7a-9108484bab1dªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙICompare `batch` and `batch_as_completed` when processing multiple inputs.¨query_id¤Q058§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q058ªfocus_kind¯chain_execution‹¢idÙ@deb774eddd789fb5617a0db912bb3454abd90470582dbdf372061e9d12c1b9e8¢tsËAÚóö?µV¨trace_id¤Q058­parent_run_idÙ$6f26f3be-3c69-4986-ae7a-9108484bab1d¦run_idÙ$febcf4c6-2cdc-43b8-a366-9ba70eb89f5bªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙICompare `batch` and `batch_as_completed` when processing multiple inputs.§context¦answer ¨query_id¤Q058¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q058®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:d5ecd4ba-2496-755e-bf4a-889d52326784ªfocus_kind¯chain_execution‰¢idÙ@e260b878dad8a55460cac5e94fbbe399dbd2f69867ac0a51f70f8563e6849353¢tsËAÚóöPÀj¨trace_idÙ$febcf4c6-2cdc-43b8-a366-9ba70eb89f5b­parent_run_idÙ$6f26f3be-3c69-4986-ae7a-9108484bab1d¦run_idÙ$febcf4c6-2cdc-43b8-a366-9ba70eb89f5bªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for user...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¨query_id¤Q058ªfocus_kind¶chain_execution_result‹¢idÙ@6fd8590d223343c0a661503ceaab56df155ef95893e5a2eff53fcb96c497374b¢tsËAÚóöPÇC¨trace_id¤Q058­parent_run_idÙ$6f26f3be-3c69-4986-ae7a-9108484bab1d¦run_idÙ$6a945d32-d842-4b8b-9e9e-7d0b3b670283ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙICompare `batch` and `batch_as_completed` when processing multiple inputs.§context•‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for user...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¦answer ¨query_id¤Q058¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q058®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:ade05d39-c438-4db7-b284-6b94ba5c45cdªfocus_kind¯chain_execution‹¢idÙ@2b0c67c8c53c6dbf0b8c232ac783f58c92ced0df4bd730245e78d8b546fabea9¢tsËAÚóöPËu¨trace_id¤Q058­parent_run_idÙ$6a945d32-d842-4b8b-9e9e-7d0b3b670283¦run_idÙ$8ae1020b-6fc3-4868-b523-66bbf309502aªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙICompare `batch` and `batch_as_completed` when processing multiple inputs.§contextÚÁ<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to process multiple inputs in parallel.

Using these methods can significantly improve performance when needing to process multiple independent inputs, as the
processing can be done in parallel instead of sequentially.

The two batching options are:

* `batch`: Process multiple inputs in parallel, returning results in the same order as the inputs.
* `batch_as_completed`: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.

The default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution.

Some Runnables may provide their own implementations of `batch` and `batch_as_completed` that are optimized for their specific use case (e.g.,
rely on a `batch` API provided by a model provider).

:::note
The async versions of `abatch` and `abatch_as_completed` relies on asyncio's [gather](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather) and [as_completed](https://docs.python.org/3/library/asyncio-task.html#asyncio.as_completed) functions to run the `ainvoke` method in parallel.
:::

:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency` attribute in the `RunnableConfig` dictionary. See the [RunnableConfig](/docs/concepts/runnables/#runnableconfig) for more information.

Chat Models also have a built-in [rate limiter](/docs/concepts/chat_models#rate-limiting) that can be used to control the rate at which requests are made.
:::

### Asynchronous support
<span data-heading-keywords="async-api"></span>

Runnables expose an asynchronous API, allowing them to be called using the `await` syntax in Python. Asynchronous methods can be identified by the "a" prefix (e.g., `ainvoke`, `abatch`, `astream`, `abatch_as_completed`).

Please refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.

## Streaming APIs
<span data-heading-keywords="streaming-api"></span>

## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.
* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.
* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.
* Inspected: Schematic information about Runnable's input, output, and configuration can be accessed.
* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.

Please review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.

<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.
- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q058®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:ade05d39-c438-4db7-b284-6b94ba5c45cd­checkpoint_nsÙ-generate:ade05d39-c438-4db7-b284-6b94ba5c45cd¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@a4daea5425cfccd9b78334be03616582ea659747f1d96c0610a1190b88c55cd9¢tsËAÚóöPÒ$¨trace_idÙ$8ae1020b-6fc3-4868-b523-66bbf309502a­parent_run_idÙ$6a945d32-d842-4b8b-9e9e-7d0b3b670283¦run_idÙ$8ae1020b-6fc3-4868-b523-66bbf309502aªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ¤messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Compare `batch` and `batch_as_completed` when processing multiple inputs. \nContext: <a id="batch"></a>\n### Optimized parallel execution (batch)\n<span data-heading-keywords="batch"></span>\n\nLangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to process multiple inputs in parallel.\n\nUsing these methods can significantly improve performance when needing to process multiple independent inputs, as the\nprocessing can be done in parallel instead of sequentially.\n\nThe two batching options are:\n\n* `batch`: Process multiple inputs in parallel, returning results in the same order as the inputs.\n* `batch_as_completed`: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.\n\nThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution.\n\nSome Runnables may provide their own implementations of `batch` and `batch_as_completed` that are optimized for their specific use case (e.g.,\nrely on a `batch` API provided by a model provider).\n\n:::note\nThe async versions of `abatch` and `abatch_as_completed` relies on asyncio\'s [gather](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather) and [as_completed](https://docs.python.org/3/library/asyncio-task.html#asyncio.as_completed) functions to run the `ainvoke` method in parallel.\n:::\n\n:::tip\nWhen processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency` attribute in the `RunnableConfig` dictionary. See the [RunnableConfig](/docs/concepts/runnables/#runnableconfig) for more information.\n\nChat Models also have a built-in [rate limiter](/docs/concepts/chat_models#rate-limiting) that can be used to control the rate at which requests are made.\n:::\n\n### Asynchronous support\n<span data-heading-keywords="async-api"></span>\n\nRunnables expose an asynchronous API, allowing them to be called using the `await` syntax in Python. Asynchronous methods can be identified by the "a" prefix (e.g., `ainvoke`, `abatch`, `astream`, `abatch_as_completed`).\n\nPlease refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.\n\n## Streaming APIs\n<span data-heading-keywords="streaming-api"></span>\n\n## Overview of runnable interface\n\nThe Runnable way defines a standard interface that allows a Runnable component to be:\n\n* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.\n* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.\n* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.\n* Inspected: Schematic information about Runnable\'s input, output, and configuration can be accessed.\n* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.\n\nPlease review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.\n\n<a id="batch"></a>\n### Optimized parallel execution (batch)\n<span data-heading-keywords="batch"></span>\n\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@697ff2c0c344236215ea65a33b698307b6be0e423a7bb2be3f322a42c405c98b¢tsËAÚóöÎ†j¨focus_msÍ«§payload‚¤role©assistant§contentÚöThe `batch` method processes multiple inputs in parallel and returns results in the same order as the inputs, while `batch_as_completed` also processes inputs in parallel but returns results as they complete, which may be out of order. Both methods improve performance by allowing parallel execution, but the choice between them depends on whether maintaining input order is important. Users can control the maximum number of parallel calls with the `max_concurrency` attribute in the `RunnableConfig`.ªfocus_kind¬exec_latency¨query_id¤Q058‰¢idÙ@a6c92ea6647f7699b6fe0e4c498bc0bafa0c26f6a065af8154a609caedfa4cf3¢tsËAÚóöÎ—¨trace_idÙ$6a945d32-d842-4b8b-9e9e-7d0b3b670283­parent_run_idÙ$6f26f3be-3c69-4986-ae7a-9108484bab1d¦run_idÙ$6a945d32-d842-4b8b-9e9e-7d0b3b670283ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚöThe `batch` method processes multiple inputs in parallel and returns results in the same order as the inputs, while `batch_as_completed` also processes inputs in parallel but returns results as they complete, which may be out of order. Both methods improve performance by allowing parallel execution, but the choice between them depends on whether maintaining input order is important. Users can control the maximum number of parallel calls with the `max_concurrency` attribute in the `RunnableConfig`.¨query_id¤Q058ªfocus_kind¶chain_execution_result‰¢idÙ@4285fe6fc10939b2a586d94f56c4177170dd447dce8b45c243011ee871f5406c¢tsËAÚóöÎk¨trace_idÙ$6f26f3be-3c69-4986-ae7a-9108484bab1d­parent_run_idÀ¦run_idÙ$6f26f3be-3c69-4986-ae7a-9108484bab1dªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙICompare `batch` and `batch_as_completed` when processing multiple inputs.§context•‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for user...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¦answerÚöThe `batch` method processes multiple inputs in parallel and returns results in the same order as the inputs, while `batch_as_completed` also processes inputs in parallel but returns results as they complete, which may be out of order. Both methods improve performance by allowing parallel execution, but the choice between them depends on whether maintaining input order is important. Users can control the maximum number of parallel calls with the `max_concurrency` attribute in the `RunnableConfig`.¨query_id¤Q058ªfocus_kind¶chain_execution_result‹¢idÙ@92db61da3cea98524a44ab30c9e8695944f1f4a5b9f48a84ef46b2ad137d00ab¢tsËAÚóöÎ¥¹¨trace_id¤Q059­parent_run_idÀ¦run_idÙ$af5ade7a-6af2-4c1a-896f-a889edc03dc2ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙIHow can LangChain assist with evaluating generated answers automatically?¨query_id¤Q059§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q059ªfocus_kind¯chain_execution‹¢idÙ@64319193e114893e7e377af1079788629970e187b5fc3eb9ffe0387a464a1538¢tsËAÚóöÎ®æ¨trace_id¤Q059­parent_run_idÙ$af5ade7a-6af2-4c1a-896f-a889edc03dc2¦run_idÙ$3c4416a1-da51-46bf-bc29-51eb9796881cªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙIHow can LangChain assist with evaluating generated answers automatically?§context¦answer ¨query_id¤Q059¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q059®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:fa0a690c-0e93-4062-e9eb-752411212b9aªfocus_kind¯chain_execution‰¢idÙ@a21a41f484670ce7f2ecf5fcebf46d2cc61d698c1c735b5fef29368b093b2d8a¢tsËAÚóöÙp¨trace_idÙ$3c4416a1-da51-46bf-bc29-51eb9796881c­parent_run_idÙ$af5ade7a-6af2-4c1a-896f-a889edc03dc2¦run_idÙ$3c4416a1-da51-46bf-bc29-51eb9796881cªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing t...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/evaluation.mdx‚´page_content_summaryÙË# Why LangChain?

The goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.
While LangChain originally st...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)
- [How to: create a custom LL...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¨query_id¤Q059ªfocus_kind¶chain_execution_result‹¢idÙ@d0b415374971f56e5fdbaf3b65eaebc95388e2fa76d9bc173836df680d7f441e¢tsËAÚóöÙŠì¨trace_id¤Q059­parent_run_idÙ$af5ade7a-6af2-4c1a-896f-a889edc03dc2¦run_idÙ$77c29f57-ff11-4247-b188-5f7bf8003a53ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙIHow can LangChain assist with evaluating generated answers automatically?§context•‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing t...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/evaluation.mdx‚´page_content_summaryÙË# Why LangChain?

The goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.
While LangChain originally st...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)
- [How to: create a custom LL...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answer ¨query_id¤Q059¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q059®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:86b41c74-fb5a-fc7e-6e98-b79bf78fe064ªfocus_kind¯chain_execution‹¢idÙ@deabffe32d4bc374d6addcef5b7afb3d4a7b42ff9def1d0f8df07d5aeda11928¢tsËAÚóöÙ¨trace_id¤Q059­parent_run_idÙ$77c29f57-ff11-4247-b188-5f7bf8003a53¦run_idÙ$ff8481c6-6135-486b-adbd-2e4a42b639b2ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙIHow can LangChain assist with evaluating generated answers automatically?§contextÚ£## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.

LangSmith documentation is hosted on a separate site.
You can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/).

### Evaluation

LangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:

- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)

# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.
This process is vital for building reliable applications.

![](/img/langsmith_evaluate.png)

[LangSmith](https://docs.smith.langchain.com/) helps with this process in a few ways:

- It makes it easier to create and curate datasets via its tracing and annotation features
- It provides an evaluation framework that helps you define metrics and run your app against your dataset
- It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code

To learn more, check out [this LangSmith guide](https://docs.smith.langchain.com/concepts/evaluation).

# Why LangChain?

The goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.
While LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.
This page will talk about the LangChain ecosystem as a whole.
Most of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!

## Features

There are several primary needs that LangChain aims to address:

### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)
- [How to: create a custom LLM class](/docs/how_to/custom_llm)
- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)
- [How to: write a custom retriever class](/docs/how_to/custom_retriever)
- [How to: write a custom document loader](/docs/how_to/document_loader_custom)
- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)
- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)
- [How to: define a custom tool](/docs/how_to/custom_tools)
- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)

### Serialization
- [How to: save and load LangChain objects](/docs/how_to/serialization)

## Use cases

These guides cover use-case specific details.

### Q&A with RAG

## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.
This diversity can make it challenging for developers to switch between providers or combine components when building applications.
LangChain exposes a standard interface for key components, making it easy to switch between providers.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q059®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:86b41c74-fb5a-fc7e-6e98-b79bf78fe064­checkpoint_nsÙ-generate:86b41c74-fb5a-fc7e-6e98-b79bf78fe064¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@ccee7c951bc82681840e445378dd7137320f3c27811dac404676edb52bf16ae7¢tsËAÚóöÙ§¨trace_idÙ$ff8481c6-6135-486b-adbd-2e4a42b639b2­parent_run_idÙ$77c29f57-ff11-4247-b188-5f7bf8003a53¦run_idÙ$ff8481c6-6135-486b-adbd-2e4a42b639b2ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ”messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: How can LangChain assist with evaluating generated answers automatically? \nContext: ## LangSmith\n\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\n\nLangSmith documentation is hosted on a separate site.\nYou can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/).\n\n### Evaluation\n\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\n\n- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)\n\n# Evaluation\n<span data-heading-keywords="evaluation,evaluate"></span>\n\nEvaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.\nIt involves testing the model\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.\nThis process is vital for building reliable applications.\n\n![](/img/langsmith_evaluate.png)\n\n[LangSmith](https://docs.smith.langchain.com/) helps with this process in a few ways:\n\n- It makes it easier to create and curate datasets via its tracing and annotation features\n- It provides an evaluation framework that helps you define metrics and run your app against your dataset\n- It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code\n\nTo learn more, check out [this LangSmith guide](https://docs.smith.langchain.com/concepts/evaluation).\n\n# Why LangChain?\n\nThe goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.\nWhile LangChain originally started as a single open source package, it has evolved into a company and a whole ecosystem.\nThis page will talk about the LangChain ecosystem as a whole.\nMost of the components within the LangChain ecosystem can be used by themselves - so if you feel particularly drawn to certain components but not others, that is totally fine! Pick and choose whichever components you like best for your own use case!\n\n## Features\n\nThere are several primary needs that LangChain aims to address:\n\n### Custom\n\nAll of LangChain components can easily be extended to support your own versions.\n\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\n- [How to: define a custom tool](/docs/how_to/custom_tools)\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\n\n### Serialization\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\n\n## Use cases\n\nThese guides cover use-case specific details.\n\n### Q&A with RAG\n\n## Features\n\nThere are several primary needs that LangChain aims to address:\n\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\nLangChain exposes a standard interface for key components, making it easy to switch between providers. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@33b75951ed134b2f8ab12b96ed5df6709e3d986904a82c83e1f623e081407b59¢tsËAÚó÷†M¨focus_msÍ
…§payload‚¤role©assistant§contentÚØLangChain, through its integration with LangSmith, assists in evaluating generated answers by providing an evaluation framework that allows developers to define metrics and test their applications against curated datasets. It also enables tracking of results over time and supports automated evaluations as part of continuous integration processes. This systematic approach ensures that the performance and effectiveness of LLM applications meet desired quality standards.ªfocus_kind¬exec_latency¨query_id¤Q059‰¢idÙ@8b5c99b231d8d2994d9d411a5b74df892a36b487d9170b0dbbfbc5e24140fa23¢tsËAÚó÷†%¨trace_idÙ$77c29f57-ff11-4247-b188-5f7bf8003a53­parent_run_idÙ$af5ade7a-6af2-4c1a-896f-a889edc03dc2¦run_idÙ$77c29f57-ff11-4247-b188-5f7bf8003a53ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚØLangChain, through its integration with LangSmith, assists in evaluating generated answers by providing an evaluation framework that allows developers to define metrics and test their applications against curated datasets. It also enables tracking of results over time and supports automated evaluations as part of continuous integration processes. This systematic approach ensures that the performance and effectiveness of LLM applications meet desired quality standards.¨query_id¤Q059ªfocus_kind¶chain_execution_result‰¢idÙ@72f898a899b603772288720caf48df5db837917e8b78f2a6202cd68fee3e3b0b¢tsËAÚó÷†.@¨trace_idÙ$af5ade7a-6af2-4c1a-896f-a889edc03dc2­parent_run_idÀ¦run_idÙ$af5ade7a-6af2-4c1a-896f-a889edc03dc2ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙIHow can LangChain assist with evaluating generated answers automatically?§context•‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing t...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/evaluation.mdx‚´page_content_summaryÙË# Why LangChain?

The goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.
While LangChain originally st...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)
- [How to: create a custom LL...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answerÚØLangChain, through its integration with LangSmith, assists in evaluating generated answers by providing an evaluation framework that allows developers to define metrics and test their applications against curated datasets. It also enables tracking of results over time and supports automated evaluations as part of continuous integration processes. This systematic approach ensures that the performance and effectiveness of LLM applications meet desired quality standards.¨query_id¤Q059ªfocus_kind¶chain_execution_result‹¢idÙ@f407cc4e55a1130896823af1d7b66cfaa2e719d95e0f6b289a691ae39dc91d8f¢tsËAÚó÷†;\¨trace_id¤Q060­parent_run_idÀ¦run_idÙ$25f9ee83-23f0-4dbe-9176-b099de4fbd8eªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙHProvide two metrics supported by LangChain's built in evaluation module.¨query_id¤Q060§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q060ªfocus_kind¯chain_execution‹¢idÙ@aee78b98f02ecb18daf3bdf37c5579cb1fa3c9a47fa0bf46d3faa2de843ac1ab¢tsËAÚó÷†JR¨trace_id¤Q060­parent_run_idÙ$25f9ee83-23f0-4dbe-9176-b099de4fbd8e¦run_idÙ$0b9a59e9-8c7f-46ff-8c07-576772ae5805ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙHProvide two metrics supported by LangChain's built in evaluation module.§context¦answer ¨query_id¤Q060¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q060®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:29a9dfb3-b9e7-3170-2dcf-d7832cbbcd32ªfocus_kind¯chain_execution‰¢idÙ@5035daf5be00a57e6026db782c8e05104ca55565f7fcfc8480ecbe382b9831fd¢tsËAÚó÷–‘¨trace_idÙ$0b9a59e9-8c7f-46ff-8c07-576772ae5805­parent_run_idÙ$25f9ee83-23f0-4dbe-9176-b099de4fbd8e¦run_idÙ$0b9a59e9-8c7f-46ff-8c07-576772ae5805ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing t...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/evaluation.mdx‚´page_content_summaryÙË### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process fr...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build compl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## ...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¨query_id¤Q060ªfocus_kind¶chain_execution_result‹¢idÙ@13de267b7563f888e094ea5b247ef98e85852658b89216c475294ddc9754f811¢tsËAÚó÷–†¨trace_id¤Q060­parent_run_idÙ$25f9ee83-23f0-4dbe-9176-b099de4fbd8e¦run_idÙ$e395c76e-0a66-4f50-be79-d627745667afªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙHProvide two metrics supported by LangChain's built in evaluation module.§context•‚´page_content_summaryÙË# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing t...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/evaluation.mdx‚´page_content_summaryÙË### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process fr...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build compl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## ...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answer ¨query_id¤Q060¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q060®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:2aa98300-025a-cd8d-552b-1fb1c84aa9d9ªfocus_kind¯chain_execution‹¢idÙ@51a6647788abf323a6174fe354c2f4f120cc968a8ec654adb02cc6108ac9f434¢tsËAÚó÷–#İ¨trace_id¤Q060­parent_run_idÙ$e395c76e-0a66-4f50-be79-d627745667af¦run_idÙ$b8361134-2558-4f4c-84d5-d9af5eab0f2aªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙHProvide two metrics supported by LangChain's built in evaluation module.§contextÚ
# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.
This process is vital for building reliable applications.

![](/img/langsmith_evaluate.png)

[LangSmith](https://docs.smith.langchain.com/) helps with this process in a few ways:

- It makes it easier to create and curate datasets via its tracing and annotation features
- It provides an evaluation framework that helps you define metrics and run your app against your dataset
- It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code

To learn more, check out [this LangSmith guide](https://docs.smith.langchain.com/concepts/evaluation).

### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.

To learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).

### Tracing
<span data-heading-keywords="trace,tracing"></span>

Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.

- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)
- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)

You can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).

## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.

LangSmith documentation is hosted on a separate site.
You can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/).

### Evaluation

LangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:

- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)

:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.

:::

## Observability and evaluation

The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. 
Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. 
High quality tracing and evaluations can help you rapidly answer these types of questions with confidence.
[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.
See our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.

:::info[Further reading]

:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## Conclusion

LangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:
- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.
- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q060®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:2aa98300-025a-cd8d-552b-1fb1c84aa9d9­checkpoint_nsÙ-generate:2aa98300-025a-cd8d-552b-1fb1c84aa9d9¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@ed296f16c61d76e3f36cd42fa533d412a74b01e8dc173a044322dd16a4e7ab73¢tsËAÚó÷–*ı¨trace_idÙ$b8361134-2558-4f4c-84d5-d9af5eab0f2a­parent_run_idÙ$e395c76e-0a66-4f50-be79-d627745667af¦run_idÙ$b8361134-2558-4f4c-84d5-d9af5eab0f2aªevent_type¬chain_output¬event_source¥chain§payload§outputsÚÿmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Provide two metrics supported by LangChain\'s built in evaluation module. \nContext: # Evaluation\n<span data-heading-keywords="evaluation,evaluate"></span>\n\nEvaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.\nIt involves testing the model\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.\nThis process is vital for building reliable applications.\n\n![](/img/langsmith_evaluate.png)\n\n[LangSmith](https://docs.smith.langchain.com/) helps with this process in a few ways:\n\n- It makes it easier to create and curate datasets via its tracing and annotation features\n- It provides an evaluation framework that helps you define metrics and run your app against your dataset\n- It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code\n\nTo learn more, check out [this LangSmith guide](https://docs.smith.langchain.com/concepts/evaluation).\n\n### Evaluation\n<span data-heading-keywords="evaluation,evaluate"></span>\n\nEvaluating performance is a vital part of building LLM-powered applications.\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\n\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\n\n### Tracing\n<span data-heading-keywords="trace,tracing"></span>\n\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\n\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\n\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).\n\n## LangSmith\n\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\n\nLangSmith documentation is hosted on a separate site.\nYou can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/).\n\n### Evaluation\n\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\n\n- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)\n\n:::info[Further reading]\n\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\n\n:::\n\n## Observability and evaluation\n\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\n\n:::info[Further reading]\n\n:::info[Further reading]\n\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\n\n:::\n\n## Conclusion\n\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/). \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@700eab870551e46ca08494805e6bcc88c0ab083590f256100fa70b795eb7a7ea¢tsËAÚó÷õq‹¨focus_msÍĞ§payload‚¤role©assistant§contentÙùLangChain's built-in evaluation module supports various metrics, including accuracy and latency. These metrics help assess the performance and effectiveness of LLM-powered applications. For more details, you can refer to the LangSmith documentation.ªfocus_kind¬exec_latency¨query_id¤Q060‰¢idÙ@aafaaa86e03a6294d585371039341ff653f5f5b68745984a68e57fac481e39ff¢tsËAÚó÷õ„0¨trace_idÙ$e395c76e-0a66-4f50-be79-d627745667af­parent_run_idÙ$25f9ee83-23f0-4dbe-9176-b099de4fbd8e¦run_idÙ$e395c76e-0a66-4f50-be79-d627745667afªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙùLangChain's built-in evaluation module supports various metrics, including accuracy and latency. These metrics help assess the performance and effectiveness of LLM-powered applications. For more details, you can refer to the LangSmith documentation.¨query_id¤Q060ªfocus_kind¶chain_execution_result‰¢idÙ@76e08f05e58cae4306056fa6f54a648257d8e9d01763c76c103d9261897b1299¢tsËAÚó÷õï¨trace_idÙ$25f9ee83-23f0-4dbe-9176-b099de4fbd8e­parent_run_idÀ¦run_idÙ$25f9ee83-23f0-4dbe-9176-b099de4fbd8eªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙHProvide two metrics supported by LangChain's built in evaluation module.§context•‚´page_content_summaryÙË# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing t...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/evaluation.mdx‚´page_content_summaryÙË### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process fr...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build compl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## ...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answerÙùLangChain's built-in evaluation module supports various metrics, including accuracy and latency. These metrics help assess the performance and effectiveness of LLM-powered applications. For more details, you can refer to the LangSmith documentation.¨query_id¤Q060ªfocus_kind¶chain_execution_result‹¢idÙ@a81a4d080a82979a458573112afd73ee28dabbbe5ab5014408a137d2fdc9e650¢tsËAÚó÷õ“İ¨trace_id¤Q061­parent_run_idÀ¦run_idÙ$a4792b72-ad82-456b-87c0-ec6aa8046d97ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙRWhich LangChain concept allows combining multiple Runnables with the `|` operator?¨query_id¤Q061§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q061ªfocus_kind¯chain_execution‹¢idÙ@21e5c595743948dba57509d092f26a4065151987641fb38810e5fa51e8ffad4f¢tsËAÚó÷õŸ$¨trace_id¤Q061­parent_run_idÙ$a4792b72-ad82-456b-87c0-ec6aa8046d97¦run_idÙ$f20bedb4-d99a-457c-99df-c92cbe5fa9c6ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙRWhich LangChain concept allows combining multiple Runnables with the `|` operator?§context¦answer ¨query_id¤Q061¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q061®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:e60d60dd-7ab1-2daa-69fa-0ed6e27875a1ªfocus_kind¯chain_execution‰¢idÙ@a14c683b2951dc148216c40f3e011659a50e4a268dda0c674ae069006c0f1bfa¢tsËAÚóøıv¨trace_idÙ$f20bedb4-d99a-457c-99df-c92cbe5fa9c6­parent_run_idÙ$a4792b72-ad82-456b-87c0-ec6aa8046d97¦run_idÙ$f20bedb4-d99a-457c-99df-c92cbe5fa9c6ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# LangChain Expression Language (LCEL)

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
:::

The **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËWe often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::n...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË```python
chain = runnable1 | runnable2
```

is Equivalent to:

```python
chain = RunnableSequence([runnable1, runnable2])
```

### The `.pipe` method

If you have moral qualms with operator overloadi...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocati...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¨query_id¤Q061ªfocus_kind¶chain_execution_result‹¢idÙ@19a518cd1729f470cbc8a3548560f3bbf075d4ecf17311624957092b51ee9024¢tsËAÚóø	¼¨trace_id¤Q061­parent_run_idÙ$a4792b72-ad82-456b-87c0-ec6aa8046d97¦run_idÙ$e9c4ddba-a520-4ba9-9439-fff2f2829b8dªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙRWhich LangChain concept allows combining multiple Runnables with the `|` operator?§context•‚´page_content_summaryÙË# LangChain Expression Language (LCEL)

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
:::

The **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËWe often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::n...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË```python
chain = runnable1 | runnable2
```

is Equivalent to:

```python
chain = RunnableSequence([runnable1, runnable2])
```

### The `.pipe` method

If you have moral qualms with operator overloadi...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocati...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answer ¨query_id¤Q061¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q061®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:85ae39d0-8f51-0bf1-366e-6d4bb0650e81ªfocus_kind¯chain_execution‹¢idÙ@8444035cf5046d7e6178e9a6e432a6d6045347fc6426bfc81b6e0ab76c8523f5¢tsËAÚóø	ú¨trace_id¤Q061­parent_run_idÙ$e9c4ddba-a520-4ba9-9439-fff2f2829b8d¦run_idÙ$4a496b29-7348-479b-bcc5-eec293446e15ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙRWhich LangChain concept allows combining multiple Runnables with the `|` operator?§contextÚÃ# LangChain Expression Language (LCEL)

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
:::

The **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.

This means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.

We often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](
https://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.

This guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

We often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::note
* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.
* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.
* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using LCEL.
:::

## Benefits of LCEL

LangChain optimizes the run-time execution of chains built with LCEL in a number of ways:

```python
chain = runnable1 | runnable2
```

is Equivalent to:

```python
chain = RunnableSequence([runnable1, runnable2])
```

### The `.pipe` method

If you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.

```python
chain = runnable1.pipe(runnable2)
```

### Coercion

LCEL applies automatic type coercion to make it easier to compose chains.

If you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.

This will make the code more verbose, but it will also make it more explicit.

#### Dictionary to RunnableParallel

Inside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.

For example, the following code:

```python
mapping = {
    "key1": runnable1,
    "key2": runnable2,
}

chain = mapping | runnable3
```

It gets automatically converted to the following:

- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocation args to runnables](/docs/how_to/binding/)
- [How to: turn any function into a runnable](/docs/how_to/functions)
- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)
- [How to: configure runnable behavior at runtime](/docs/how_to/configure)
- [How to: add message history (memory) to a chain](/docs/how_to/message_history)
- [How to: route between sub-chains](/docs/how_to/routing)
- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)
- [How to: inspect runnables](/docs/how_to/inspect)
- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)
- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)

## [LangGraph](https://langchain-ai.github.io/langgraph)¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q061®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:85ae39d0-8f51-0bf1-366e-6d4bb0650e81­checkpoint_nsÙ-generate:85ae39d0-8f51-0bf1-366e-6d4bb0650e81¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@8e8cd8164eaefeb94da4f7c58c95e20cdb4019fad251fd8ae6c4472693cfb7c4¢tsËAÚóø	×¨trace_idÙ$4a496b29-7348-479b-bcc5-eec293446e15­parent_run_idÙ$e9c4ddba-a520-4ba9-9439-fff2f2829b8d¦run_idÙ$4a496b29-7348-479b-bcc5-eec293446e15ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚĞmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Which LangChain concept allows combining multiple Runnables with the `|` operator? \nContext: # LangChain Expression Language (LCEL)\n\n:::info Prerequisites\n* [Runnable Interface](/docs/concepts/runnables)\n:::\n\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\n\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\n\nWe often refer to a `Runnable` created using LCEL as a "chain". It\'s important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\n\n# Runnable interface\n\nThe Runnable interface is the foundation for working with LangChain components, and it\'s implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](\nhttps://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.\n\nThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.\n\nWe often refer to a `Runnable` created using LCEL as a "chain". It\'s important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\n\n:::note\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using LCEL.\n:::\n\n## Benefits of LCEL\n\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\n\n```python\nchain = runnable1 | runnable2\n```\n\nis Equivalent to:\n\n```python\nchain = RunnableSequence([runnable1, runnable2])\n```\n\n### The `.pipe` method\n\nIf you have moral qualms with operator overloading, you can use the `.pipe` method instead. This is equivalent to the `|` operator.\n\n```python\nchain = runnable1.pipe(runnable2)\n```\n\n### Coercion\n\nLCEL applies automatic type coercion to make it easier to compose chains.\n\nIf you do not understand the type coercion, you can always use the `RunnableSequence` and `RunnableParallel` classes directly.\n\nThis will make the code more verbose, but it will also make it more explicit.\n\n#### Dictionary to RunnableParallel\n\nInside an LCEL expression, a dictionary is automatically converted to a `RunnableParallel`.\n\nFor example, the following code:\n\n```python\nmapping = {\n    "key1": runnable1,\n    "key2": runnable2,\n}\n\nchain = mapping | runnable3\n```\n\nIt gets automatically converted to the following:\n\n- [How to: chain runnables](/docs/how_to/sequence)\n- [How to: stream runnables](/docs/how_to/streaming)\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\n- [How to: turn any function into a runnable](/docs/how_to/functions)\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\n- [How to: route between sub-chains](/docs/how_to/routing)\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\n- [How to: inspect runnables](/docs/how_to/inspect)\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\n\n## [LangGraph](https://langchain-ai.github.io/langgraph) \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@d3dcc9f96a22b607c22f6f3f1c0130039b01490d4aabae84139471a4ce3d8e0e¢tsËAÚóøzıŞ¨focus_msÍó§payload‚¤role©assistant§contentÚ>The LangChain concept that allows combining multiple Runnables with the `|` operator is the LangChain Expression Language (LCEL). This approach enables the creation of chains by describing what should happen rather than how, optimizing runtime execution. Runnables created using LCEL are often referred to as "chains."ªfocus_kind¬exec_latency¨query_id¤Q061‰¢idÙ@a2a0e168c7ff238a3784169f6e168161945e76734615e6bf2f393f79ea7a646b¢tsËAÚóø{FV¨trace_idÙ$e9c4ddba-a520-4ba9-9439-fff2f2829b8d­parent_run_idÙ$a4792b72-ad82-456b-87c0-ec6aa8046d97¦run_idÙ$e9c4ddba-a520-4ba9-9439-fff2f2829b8dªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ>The LangChain concept that allows combining multiple Runnables with the `|` operator is the LangChain Expression Language (LCEL). This approach enables the creation of chains by describing what should happen rather than how, optimizing runtime execution. Runnables created using LCEL are often referred to as "chains."¨query_id¤Q061ªfocus_kind¶chain_execution_result‰¢idÙ@3981ef65141f7d0fc23c1233786d9db26edc4883e2fb5d47867edbf8dea3c486¢tsËAÚóø{R¨trace_idÙ$a4792b72-ad82-456b-87c0-ec6aa8046d97­parent_run_idÀ¦run_idÙ$a4792b72-ad82-456b-87c0-ec6aa8046d97ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙRWhich LangChain concept allows combining multiple Runnables with the `|` operator?§context•‚´page_content_summaryÙË# LangChain Expression Language (LCEL)

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
:::

The **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËWe often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::n...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË```python
chain = runnable1 | runnable2
```

is Equivalent to:

```python
chain = RunnableSequence([runnable1, runnable2])
```

### The `.pipe` method

If you have moral qualms with operator overloadi...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocati...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answerÚ>The LangChain concept that allows combining multiple Runnables with the `|` operator is the LangChain Expression Language (LCEL). This approach enables the creation of chains by describing what should happen rather than how, optimizing runtime execution. Runnables created using LCEL are often referred to as "chains."¨query_id¤Q061ªfocus_kind¶chain_execution_result‹¢idÙ@3b6ddedf0a7fca19aa1cbc51c7858ecb8dcf625b66193e99329e7d82adc12e74¢tsËAÚóø{[*¨trace_id¤Q062­parent_run_idÀ¦run_idÙ$91f89d4e-7c12-42e3-8395-101f23a0485cªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙeExplain how to set up tool streaming so that tool results are incrementally passed back to the model.¨query_id¤Q062§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q062ªfocus_kind¯chain_execution‹¢idÙ@7d5a85653d4007fe6ff118a6b34761b5435fa8aeaf60128dbd406a1eb163d0eb¢tsËAÚóø{k”¨trace_id¤Q062­parent_run_idÙ$91f89d4e-7c12-42e3-8395-101f23a0485c¦run_idÙ$9bc9d14a-93c7-4e39-8b93-68656b617ba7ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙeExplain how to set up tool streaming so that tool results are incrementally passed back to the model.§context¦answer ¨query_id¤Q062¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q062®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:7106cb8b-ec34-0759-418c-5136439de990ªfocus_kind¯chain_execution‰¢idÙ@f8c61465083a7b7571dee45a21121da2e09bb7188e4b949a450cb3e0dfae622f¢tsËAÚóøŒ€ú¨trace_idÙ$9bc9d14a-93c7-4e39-8b93-68656b617ba7­parent_run_idÙ$91f89d4e-7c12-42e3-8395-101f23a0485c¦run_idÙ$9bc9d14a-93c7-4e39-8b93-68656b617ba7ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applic...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËStreaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.

### 3. Streaming custom data

In some cases, you may...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËIn this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.

## What to stream in LLM...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙÍBeyond just streaming LLM output, itâ€™s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:

-...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË```python
# Tool creation
tools = [my_tool]
# Tool binding
model_with_tools = model.bind_tools(tools)
```

Similarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/)...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¨query_id¤Q062ªfocus_kind¶chain_execution_result‹¢idÙ@d5f875b3b1f113d7563827dda9bcdc88b1392cd3014c50560bcb661dc8d11b92¢tsËAÚóøŒ‡ã¨trace_id¤Q062­parent_run_idÙ$91f89d4e-7c12-42e3-8395-101f23a0485c¦run_idÙ$9fe83539-c79d-45b4-8c57-d6cbe10d4d94ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙeExplain how to set up tool streaming so that tool results are incrementally passed back to the model.§context•‚´page_content_summaryÙË# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applic...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËStreaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.

### 3. Streaming custom data

In some cases, you may...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËIn this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.

## What to stream in LLM...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙÍBeyond just streaming LLM output, itâ€™s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:

-...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË```python
# Tool creation
tools = [my_tool]
# Tool binding
model_with_tools = model.bind_tools(tools)
```

Similarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/)...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answer ¨query_id¤Q062¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q062®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:d5900837-3ebc-d14a-7220-c03ea38ad7e7ªfocus_kind¯chain_execution‹¢idÙ@eed485dd00d3992e9cdd587e1b07d35bfb32cfcfac6ae97688be6721d69ec0c2¢tsËAÚóøŒŒ;¨trace_id¤Q062­parent_run_idÙ$9fe83539-c79d-45b4-8c57-d6cbe10d4d94¦run_idÙ$bddc39ce-b095-40bc-9614-bd285a2d70eaªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙeExplain how to set up tool streaming so that tool results are incrementally passed back to the model.§contextÚİ# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applications built on [LLMs](/docs/concepts/chat_models). By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.

## Overview

Generating full responses from [LLMs](/docs/concepts/chat_models) often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.

Streaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.

### 3. Streaming custom data

In some cases, you may need to stream **custom data** that goes beyond the information provided by the pipeline or workflow structure. This custom information is injected within a specific step in the workflow, whether that step is a tool or a LangGraph node. For example, you could stream updates about what a tool is doing in real-time or the progress through a LangGraph node. This granular data, which is emitted directly from within the step, provides more detailed insights into the execution of the workflow and is especially useful in complex processes where more visibility is needed.

## Streaming APIs

In this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.

## What to stream in LLM applications

In applications involving LLMs, several types of data can be streamed to improve user experience by reducing perceived latency and increasing transparency. These include:

### 1. Streaming LLM outputs

The most common and critical data to stream is the output generated by the LLM itself. LLMs often take time to generate full responses, and by streaming the output in real-time, users can see partial results as they are produced. This provides immediate feedback and helps reduce the wait time for users.

### 2. Streaming pipeline or workflow progress

Beyond just streaming LLM output, itâ€™s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:

Beyond just streaming LLM output, itâ€™s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:

- **In LangGraph Workflows:**
With [LangGraph](/docs/concepts/architecture#langgraph), workflows are composed of nodes and edges that represent various steps. Streaming here involves tracking changes to the **graph state** as individual **nodes** request updates. This allows for more granular monitoring of which node in the workflow is currently active, giving real-time updates about the status of the workflow as it progresses through different stages.

```python
# Tool creation
tools = [my_tool]
# Tool binding
model_with_tools = model.bind_tools(tools)
```

Similarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. 
Providers support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.
LangChain's [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:

```python
# Define schema
schema = ...
# Bind schema to model
model_with_structure = model.with_structured_output(schema)
```

### Example: retrievers¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q062®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:d5900837-3ebc-d14a-7220-c03ea38ad7e7­checkpoint_nsÙ-generate:d5900837-3ebc-d14a-7220-c03ea38ad7e7¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@c0853e16bd5590f644654f6e03fbb77f46e8cc35a7903d666a654fa46a880a5f¢tsËAÚóøŒ’¨¨trace_idÙ$bddc39ce-b095-40bc-9614-bd285a2d70ea­parent_run_idÙ$9fe83539-c79d-45b4-8c57-d6cbe10d4d94¦run_idÙ$bddc39ce-b095-40bc-9614-bd285a2d70eaªevent_type¬chain_output¬event_source¥chain§payload§outputsÚÚmessages=[HumanMessage(content="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Explain how to set up tool streaming so that tool results are incrementally passed back to the model. \nContext: # Streaming\n\n:::info Prerequisites\n* [Runnable Interface](/docs/concepts/runnables)\n* [Chat Models](/docs/concepts/chat_models)\n:::\n\n**Streaming** is crucial for enhancing the responsiveness of applications built on [LLMs](/docs/concepts/chat_models). By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\n## Overview\n\nGenerating full responses from [LLMs](/docs/concepts/chat_models) often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.\n\nStreaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.\n\n### 3. Streaming custom data\n\nIn some cases, you may need to stream **custom data** that goes beyond the information provided by the pipeline or workflow structure. This custom information is injected within a specific step in the workflow, whether that step is a tool or a LangGraph node. For example, you could stream updates about what a tool is doing in real-time or the progress through a LangGraph node. This granular data, which is emitted directly from within the step, provides more detailed insights into the execution of the workflow and is especially useful in complex processes where more visibility is needed.\n\n## Streaming APIs\n\nIn this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.\n\n## What to stream in LLM applications\n\nIn applications involving LLMs, several types of data can be streamed to improve user experience by reducing perceived latency and increasing transparency. These include:\n\n### 1. Streaming LLM outputs\n\nThe most common and critical data to stream is the output generated by the LLM itself. LLMs often take time to generate full responses, and by streaming the output in real-time, users can see partial results as they are produced. This provides immediate feedback and helps reduce the wait time for users.\n\n### 2. Streaming pipeline or workflow progress\n\nBeyond just streaming LLM output, itâ€™s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:\n\nBeyond just streaming LLM output, itâ€™s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:\n\n- **In LangGraph Workflows:**\nWith [LangGraph](/docs/concepts/architecture#langgraph), workflows are composed of nodes and edges that represent various steps. Streaming here involves tracking changes to the **graph state** as individual **nodes** request updates. This allows for more granular monitoring of which node in the workflow is currently active, giving real-time updates about the status of the workflow as it progresses through different stages.\n\n```python\n# Tool creation\ntools = [my_tool]\n# Tool binding\nmodel_with_tools = model.bind_tools(tools)\n```\n\nSimilarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/) is an extremely common use case. \nProviders support different approaches for this, including [JSON mode or tool calling](https://platform.openai.com/docs/guides/structured-outputs), with different APIs.\nLangChain's [chat model](/docs/concepts/chat_models/) interface provides a common way to produce structured outputs using the `with_structured_output()` method:\n\n```python\n# Define schema\nschema = ...\n# Bind schema to model\nmodel_with_structure = model.with_structured_output(schema)\n```\n\n### Example: retrievers \nAnswer:", additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@8dae14fdc793caa1f8347a598479317b2732061bc40dc42f89bd72cd93e4ce1d¢tsËAÚóøÿ`ú¨focus_msÍ§payload‚¤role©assistant§contentÚ±To set up tool streaming for incrementally passing results back to the model, first create your tools and bind them to the model using the `model.bind_tools(tools)` method. This allows the model to utilize the tools during execution, enabling it to stream outputs as they are generated. Additionally, ensure that your application is designed to handle and display these intermediate results in real-time for improved user experience.ªfocus_kind¬exec_latency¨query_id¤Q062‰¢idÙ@56b2db1452dcc35f76f48aeb8da7b277d8dd289d439b657ed55cab58b26d0025¢tsËAÚóøÿnÛ¨trace_idÙ$9fe83539-c79d-45b4-8c57-d6cbe10d4d94­parent_run_idÙ$91f89d4e-7c12-42e3-8395-101f23a0485c¦run_idÙ$9fe83539-c79d-45b4-8c57-d6cbe10d4d94ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ±To set up tool streaming for incrementally passing results back to the model, first create your tools and bind them to the model using the `model.bind_tools(tools)` method. This allows the model to utilize the tools during execution, enabling it to stream outputs as they are generated. Additionally, ensure that your application is designed to handle and display these intermediate results in real-time for improved user experience.¨query_id¤Q062ªfocus_kind¶chain_execution_result‰¢idÙ@722b8671f27e213bd6f30636ca7f11f5d539ca1f7ba0f14d7e4cc58c91d58469¢tsËAÚóøÿx6¨trace_idÙ$91f89d4e-7c12-42e3-8395-101f23a0485c­parent_run_idÀ¦run_idÙ$91f89d4e-7c12-42e3-8395-101f23a0485cªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙeExplain how to set up tool streaming so that tool results are incrementally passed back to the model.§context•‚´page_content_summaryÙË# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applic...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËStreaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.

### 3. Streaming custom data

In some cases, you may...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËIn this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.

## What to stream in LLM...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙÍBeyond just streaming LLM output, itâ€™s useful to stream progress through more complex workflows or pipelines, giving users a sense of how the application is progressing overall. This could include:

-...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË```python
# Tool creation
tools = [my_tool]
# Tool binding
model_with_tools = model.bind_tools(tools)
```

Similarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/)...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answerÚ±To set up tool streaming for incrementally passing results back to the model, first create your tools and bind them to the model using the `model.bind_tools(tools)` method. This allows the model to utilize the tools during execution, enabling it to stream outputs as they are generated. Additionally, ensure that your application is designed to handle and display these intermediate results in real-time for improved user experience.¨query_id¤Q062ªfocus_kind¶chain_execution_result‹¢idÙ@e5cb34e9445f7ca49fe0e428b2bd56bb3a0ca8241e00ef61ee46e927a6b91540¢tsËAÚóøÿ€|¨trace_id¤Q063­parent_run_idÀ¦run_idÙ$0bda97b3-25ba-4491-85ae-46ddb83f2eadªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙJWhat is the benefit of using `RunnableLambda` over subclassing a Runnable?¨query_id¤Q063§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q063ªfocus_kind¯chain_execution‹¢idÙ@0a4e72f56732d9bf4a3e781d1e26008f7ff59cdb1d04766e306ac982100549f5¢tsËAÚóøÿ2¨trace_id¤Q063­parent_run_idÙ$0bda97b3-25ba-4491-85ae-46ddb83f2ead¦run_idÙ$b47b8c9e-e0e8-430a-b391-9a21c6e74caaªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙJWhat is the benefit of using `RunnableLambda` over subclassing a Runnable?§context¦answer ¨query_id¤Q063¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q063®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:925aaec6-8192-f176-74e2-be8c1c3fb89eªfocus_kind¯chain_execution‰¢idÙ@323b03708b90a5fa10d87834a66b975827480ec3575cc0b0a73ec2db88bca82e¢tsËAÚóùûÊ¨trace_idÙ$b47b8c9e-e0e8-430a-b391-9a21c6e74caa­parent_run_idÙ$0bda97b3-25ba-4491-85ae-46ddb83f2ead¦run_idÙ$b47b8c9e-e0e8-430a-b391-9a21c6e74caaªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËWe often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::n...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q063ªfocus_kind¶chain_execution_result‹¢idÙ@7ce2474ee88621ccb6cfda192714c6c54f8d5b703f2ffdf659dbba7c3e09bf24¢tsËAÚóùJ¨trace_id¤Q063­parent_run_idÙ$0bda97b3-25ba-4491-85ae-46ddb83f2ead¦run_idÙ$755ec2f6-eaff-4bd9-812a-4fe5d7888f84ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙJWhat is the benefit of using `RunnableLambda` over subclassing a Runnable?§context•‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËWe often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::n...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answer ¨query_id¤Q063¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q063®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:e58c0522-324c-9295-c612-4a8468712d44ªfocus_kind¯chain_execution‹¢idÙ@605bb4e7d7d57f583b1a584bf46002d179a8aedf450be234ffa219fdb792a665¢tsËAÚóù¨trace_id¤Q063­parent_run_idÙ$755ec2f6-eaff-4bd9-812a-4fe5d7888f84¦run_idÙ$0b10e77a-9f1a-489f-9df8-6eee8d13b739ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙJWhat is the benefit of using `RunnableLambda` over subclassing a Runnable?§contextÚÈ## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)](/docs/concepts/lcel) to compose
multiple Runnables and you need to add custom processing logic in one of the steps.

There are two ways to create a custom Runnable from a function:

* `RunnableLambda`: Use this for simple transformations where streaming is not required.
* `RunnableGenerator`: use this for more complex transformations when streaming is needed.

See the [How to run custom functions](/docs/how_to/functions) guide for more information on how to use `RunnableLambda` and `RunnableGenerator`.

:::important
Users should not try to subclass Runnables to create a new custom Runnable. It is
much more complex and error-prone than simply using `RunnableLambda` or `RunnableGenerator`.
:::

## Configurable runnables

# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](
https://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.

This guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
and is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.
:::

Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.

To simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

We often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::note
* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.
* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.
* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using LCEL.
:::

## Benefits of LCEL

LangChain optimizes the run-time execution of chains built with LCEL in a number of ways:

## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.
* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.
* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.
* Inspected: Schematic information about Runnable's input, output, and configuration can be accessed.
* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.

Please review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.

<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q063®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:e58c0522-324c-9295-c612-4a8468712d44­checkpoint_nsÙ-generate:e58c0522-324c-9295-c612-4a8468712d44¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@9ff97a1a4383fd517cade6f70b5e5f89199fb77c82d5b5f7c6609bd2b65279d6¢tsËAÚóùË¨trace_idÙ$0b10e77a-9f1a-489f-9df8-6eee8d13b739­parent_run_idÙ$755ec2f6-eaff-4bd9-812a-4fe5d7888f84¦run_idÙ$0b10e77a-9f1a-489f-9df8-6eee8d13b739ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ·messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What is the benefit of using `RunnableLambda` over subclassing a Runnable? \nContext: ## Creating a runnable from a function {#custom-runnables}\n\nYou may need to create a custom Runnable that runs arbitrary logic. This is especially\nuseful if using [LangChain Expression Language (LCEL)](/docs/concepts/lcel) to compose\nmultiple Runnables and you need to add custom processing logic in one of the steps.\n\nThere are two ways to create a custom Runnable from a function:\n\n* `RunnableLambda`: Use this for simple transformations where streaming is not required.\n* `RunnableGenerator`: use this for more complex transformations when streaming is needed.\n\nSee the [How to run custom functions](/docs/how_to/functions) guide for more information on how to use `RunnableLambda` and `RunnableGenerator`.\n\n:::important\nUsers should not try to subclass Runnables to create a new custom Runnable. It is\nmuch more complex and error-prone than simply using `RunnableLambda` or `RunnableGenerator`.\n:::\n\n## Configurable runnables\n\n# Runnable interface\n\nThe Runnable interface is the foundation for working with LangChain components, and it\'s implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](\nhttps://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.\n\nThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.\n\n## Configurable runnables\n\n:::note\nThis is an advanced feature that is unnecessary for most users.\n\nIt helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)\nand is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.\n:::\n\nSometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.\n\nTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:\n\nWe often refer to a `Runnable` created using LCEL as a "chain". It\'s important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\n\n:::note\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using LCEL.\n:::\n\n## Benefits of LCEL\n\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\n\n## Overview of runnable interface\n\nThe Runnable way defines a standard interface that allows a Runnable component to be:\n\n* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.\n* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.\n* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.\n* Inspected: Schematic information about Runnable\'s input, output, and configuration can be accessed.\n* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.\n\nPlease review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.\n\n<a id="batch"></a>\n### Optimized parallel execution (batch)\n<span data-heading-keywords="batch"></span> \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@bf6f22e3481ad6f07eed2cc5802b8e5bafe502ec9349bc3c2b7afc473f44a3f1¢tsËAÚóù®¨focus_msÍ°§payload‚¤role©assistant§contentÚThe benefit of using `RunnableLambda` over subclassing a Runnable is that it simplifies the creation of custom Runnables for straightforward transformations without the complexity and potential errors associated with subclassing. `RunnableLambda` is specifically designed for simple use cases, making it easier to implement and maintain. In contrast, subclassing can be more complex and error-prone.ªfocus_kind¬exec_latency¨query_id¤Q063‰¢idÙ@f9ce52a763fbec8a884449f7b9afad093cf0554a613229c6cf9b683053b90911¢tsËAÚóù †¨trace_idÙ$755ec2f6-eaff-4bd9-812a-4fe5d7888f84­parent_run_idÙ$0bda97b3-25ba-4491-85ae-46ddb83f2ead¦run_idÙ$755ec2f6-eaff-4bd9-812a-4fe5d7888f84ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚThe benefit of using `RunnableLambda` over subclassing a Runnable is that it simplifies the creation of custom Runnables for straightforward transformations without the complexity and potential errors associated with subclassing. `RunnableLambda` is specifically designed for simple use cases, making it easier to implement and maintain. In contrast, subclassing can be more complex and error-prone.¨query_id¤Q063ªfocus_kind¶chain_execution_result‰¢idÙ@52bc244df65d353425101ec5efe64054ff960ce3c6dea259e4efe280b378b370¢tsËAÚóù'5¨trace_idÙ$0bda97b3-25ba-4491-85ae-46ddb83f2ead­parent_run_idÀ¦run_idÙ$0bda97b3-25ba-4491-85ae-46ddb83f2eadªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙJWhat is the benefit of using `RunnableLambda` over subclassing a Runnable?§context•‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËWe often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::n...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚThe benefit of using `RunnableLambda` over subclassing a Runnable is that it simplifies the creation of custom Runnables for straightforward transformations without the complexity and potential errors associated with subclassing. `RunnableLambda` is specifically designed for simple use cases, making it easier to implement and maintain. In contrast, subclassing can be more complex and error-prone.¨query_id¤Q063ªfocus_kind¶chain_execution_result‹¢idÙ@f25a7aeeaf5660748ebfe8987ec992e293cecb5877f2f0e189fb7ab15c869751¢tsËAÚóù.g¨trace_id¤Q064­parent_run_idÀ¦run_idÙ$0b04bdaa-7bf1-4f03-b0a8-63e1e45e3c97ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙfWhich two documents together explain how to build a streaming question answering chain with citations?¨query_id¤Q064§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q064ªfocus_kind¯chain_execution‹¢idÙ@d21cbc91f2a46417e94460b3711e12a12d55dbf384d24815d8d7504d90cca6a1¢tsËAÚóù8Î¨trace_id¤Q064­parent_run_idÙ$0b04bdaa-7bf1-4f03-b0a8-63e1e45e3c97¦run_idÙ$734abacf-3480-4a6f-b0a2-6a191f426771ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙfWhich two documents together explain how to build a streaming question answering chain with citations?§context¦answer ¨query_id¤Q064¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q064®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:d23d23bc-a0d3-43ad-62a7-334f15f45d05ªfocus_kind¯chain_execution‰¢idÙ@eccdca85aa8cb695b2c71c56c8f03b411429002044d63dead6b83ba6190852e6¢tsËAÚóù™|²¨trace_idÙ$734abacf-3480-4a6f-b0a2-6a191f426771­parent_run_idÙ$0b04bdaa-7bf1-4f03-b0a8-63e1e45e3c97¦run_idÙ$734abacf-3480-4a6f-b0a2-6a191f426771ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)
- [How to: create a custom LL...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË### Serialization
- [How to: save and load LangChain objects](/docs/how_to/serialization)

## Use cases

These guides cover use-case specific details.

### Q&A with RAG

Retrieval Augmented Generation...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË| [Multi-query](/docs/how_to/MultiQueryRetriever/)                                                          | When you want to ensure high recall in retrieval by providing multiple phrasings of a ques...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË- [Chat models and prompts](/docs/tutorials/llm_chain): Build a simple LLM application with [prompt templates](/docs/concepts/prompt_templates) and [chat models](/docs/concepts/chat_models).
- [Semant...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË### Q&A over SQL + CSV

You can use LLMs to do question answering over tabular data.
For a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).

- [How to: use prompting to improve re...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¨query_id¤Q064ªfocus_kind¶chain_execution_result‹¢idÙ@ea7717c2be834f84fa41a8c7f2c00dab7eb6c6dc0078739157c344b290386af2¢tsËAÚóù™†¨trace_id¤Q064­parent_run_idÙ$0b04bdaa-7bf1-4f03-b0a8-63e1e45e3c97¦run_idÙ$4877e0cb-fee5-41a2-96ff-fb7932289e47ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙfWhich two documents together explain how to build a streaming question answering chain with citations?§context•‚´page_content_summaryÙË### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)
- [How to: create a custom LL...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË### Serialization
- [How to: save and load LangChain objects](/docs/how_to/serialization)

## Use cases

These guides cover use-case specific details.

### Q&A with RAG

Retrieval Augmented Generation...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË| [Multi-query](/docs/how_to/MultiQueryRetriever/)                                                          | When you want to ensure high recall in retrieval by providing multiple phrasings of a ques...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË- [Chat models and prompts](/docs/tutorials/llm_chain): Build a simple LLM application with [prompt templates](/docs/concepts/prompt_templates) and [chat models](/docs/concepts/chat_models).
- [Semant...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË### Q&A over SQL + CSV

You can use LLMs to do question answering over tabular data.
For a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).

- [How to: use prompting to improve re...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answer ¨query_id¤Q064¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q064®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:839fd142-58e3-3d04-9e21-ff7bbdf0ef4eªfocus_kind¯chain_execution‹¢idÙ@7da4fff9a4a734a81ca9c4f303137937b081948ea73154278375911a6345ee8f¢tsËAÚóù™Œ¨trace_id¤Q064­parent_run_idÙ$4877e0cb-fee5-41a2-96ff-fb7932289e47¦run_idÙ$8058167e-b8ac-407a-bea6-8bb14ff8eb10ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙfWhich two documents together explain how to build a streaming question answering chain with citations?§contextÚ@### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)
- [How to: create a custom LLM class](/docs/how_to/custom_llm)
- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)
- [How to: write a custom retriever class](/docs/how_to/custom_retriever)
- [How to: write a custom document loader](/docs/how_to/document_loader_custom)
- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)
- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)
- [How to: define a custom tool](/docs/how_to/custom_tools)
- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)

### Serialization
- [How to: save and load LangChain objects](/docs/how_to/serialization)

## Use cases

These guides cover use-case specific details.

### Q&A with RAG

### Serialization
- [How to: save and load LangChain objects](/docs/how_to/serialization)

## Use cases

These guides cover use-case specific details.

### Q&A with RAG

Retrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.
For a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).

- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)
- [How to: stream](/docs/how_to/qa_streaming/)
- [How to: return sources](/docs/how_to/qa_sources/)
- [How to: return citations](/docs/how_to/qa_citations/)
- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)


### Extraction

Extraction is when you use LLMs to extract structured information from unstructured text.
For a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).

| [Multi-query](/docs/how_to/MultiQueryRetriever/)                                                          | When you want to ensure high recall in retrieval by providing multiple phrasings of a question. | Rewrite the user question with multiple phrasings, retrieve documents for each rewritten question, return the unique documents for all queries.                                                                                                                                        |
| [Decomposition](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb) | When a question can be broken down into smaller subproblems.                                    | Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).                                                           |

- [Chat models and prompts](/docs/tutorials/llm_chain): Build a simple LLM application with [prompt templates](/docs/concepts/prompt_templates) and [chat models](/docs/concepts/chat_models).
- [Semantic search](/docs/tutorials/retrievers): Build a semantic search engine over a PDF with [document loaders](/docs/concepts/document_loaders), [embedding models](/docs/concepts/embedding_models/), and [vector stores](/docs/concepts/vectorstores/).
- [Classification](/docs/tutorials/classification): Classify text into categories or labels using [chat models](/docs/concepts/chat_models) with [structured outputs](/docs/concepts/structured_outputs/).
- [Extraction](/docs/tutorials/extraction): Extract structured data from text and other unstructured media using [chat models](/docs/concepts/chat_models) and [few-shot examples](/docs/concepts/few_shot_prompting/).

Refer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.

## Orchestration

### Q&A over SQL + CSV

You can use LLMs to do question answering over tabular data.
For a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).

- [How to: use prompting to improve results](/docs/how_to/sql_prompting)
- [How to: do query validation](/docs/how_to/sql_query_checking)
- [How to: deal with large databases](/docs/how_to/sql_large_db)
- [How to: deal with CSV files](/docs/how_to/sql_csv)

### Q&A over graph databases

You can use an LLM to do question answering over graph databases.
For a high-level tutorial, check out [this guide](/docs/tutorials/graph/).

- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)
- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)

### Summarization

LLMs can summarize and otherwise distill desired information from text, including
large volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization).¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q064®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:839fd142-58e3-3d04-9e21-ff7bbdf0ef4e­checkpoint_nsÙ-generate:839fd142-58e3-3d04-9e21-ff7bbdf0ef4e¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@20456a3d1bc5dd69b47ad8972ce1731d68aedc2243a5ed6af16b1334a2f9cf18¢tsËAÚóù™“’¨trace_idÙ$8058167e-b8ac-407a-bea6-8bb14ff8eb10­parent_run_idÙ$4877e0cb-fee5-41a2-96ff-fb7932289e47¦run_idÙ$8058167e-b8ac-407a-bea6-8bb14ff8eb10ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚTmessages=[HumanMessage(content="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Which two documents together explain how to build a streaming question answering chain with citations? \nContext: ### Custom\n\nAll of LangChain components can easily be extended to support your own versions.\n\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\n- [How to: create a custom embeddings class](/docs/how_to/custom_embeddings)\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\n- [How to: write a custom document loader](/docs/how_to/document_loader_custom)\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\n- [How to: create custom callback handlers](/docs/how_to/custom_callbacks)\n- [How to: define a custom tool](/docs/how_to/custom_tools)\n- [How to: dispatch custom callback events](/docs/how_to/callbacks_custom_events)\n\n### Serialization\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\n\n## Use cases\n\nThese guides cover use-case specific details.\n\n### Q&A with RAG\n\n### Serialization\n- [How to: save and load LangChain objects](/docs/how_to/serialization)\n\n## Use cases\n\nThese guides cover use-case specific details.\n\n### Q&A with RAG\n\nRetrieval Augmented Generation (RAG) is a way to connect LLMs to external sources of data.\nFor a high-level tutorial on RAG, check out [this guide](/docs/tutorials/rag/).\n\n- [How to: add chat history](/docs/how_to/qa_chat_history_how_to/)\n- [How to: stream](/docs/how_to/qa_streaming/)\n- [How to: return sources](/docs/how_to/qa_sources/)\n- [How to: return citations](/docs/how_to/qa_citations/)\n- [How to: do per-user retrieval](/docs/how_to/qa_per_user/)\n\n\n### Extraction\n\nExtraction is when you use LLMs to extract structured information from unstructured text.\nFor a high level tutorial on extraction, check out [this guide](/docs/tutorials/extraction/).\n\n| [Multi-query](/docs/how_to/MultiQueryRetriever/)                                                          | When you want to ensure high recall in retrieval by providing multiple phrasings of a question. | Rewrite the user question with multiple phrasings, retrieve documents for each rewritten question, return the unique documents for all queries.                                                                                                                                        |\n| [Decomposition](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb) | When a question can be broken down into smaller subproblems.                                    | Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).                                                           |\n\n- [Chat models and prompts](/docs/tutorials/llm_chain): Build a simple LLM application with [prompt templates](/docs/concepts/prompt_templates) and [chat models](/docs/concepts/chat_models).\n- [Semantic search](/docs/tutorials/retrievers): Build a semantic search engine over a PDF with [document loaders](/docs/concepts/document_loaders), [embedding models](/docs/concepts/embedding_models/), and [vector stores](/docs/concepts/vectorstores/).\n- [Classification](/docs/tutorials/classification): Classify text into categories or labels using [chat models](/docs/concepts/chat_models) with [structured outputs](/docs/concepts/structured_outputs/).\n- [Extraction](/docs/tutorials/extraction): Extract structured data from text and other unstructured media using [chat models](/docs/concepts/chat_models) and [few-shot examples](/docs/concepts/few_shot_prompting/).\n\nRefer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.\n\n## Orchestration\n\n### Q&A over SQL + CSV\n\nYou can use LLMs to do question answering over tabular data.\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).\n\n- [How to: use prompting to improve results](/docs/how_to/sql_prompting)\n- [How to: do query validation](/docs/how_to/sql_query_checking)\n- [How to: deal with large databases](/docs/how_to/sql_large_db)\n- [How to: deal with CSV files](/docs/how_to/sql_csv)\n\n### Q&A over graph databases\n\nYou can use an LLM to do question answering over graph databases.\nFor a high-level tutorial, check out [this guide](/docs/tutorials/graph/).\n\n- [How to: add a semantic layer over the database](/docs/how_to/graph_semantic)\n- [How to: construct knowledge graphs](/docs/how_to/graph_constructing)\n\n### Summarization\n\nLLMs can summarize and otherwise distill desired information from text, including\nlarge volumes of text. For a high-level tutorial, check out [this guide](/docs/tutorials/summarization). \nAnswer:", additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@4899bfde8e3f3ae67ab57f8e6b08a1a299348df0b0d189e30c9618bf8a5f8fdc¢tsËAÚóùâe¦¨focus_msÍq§payload‚¤role©assistant§contentÚ%The two documents that explain how to build a streaming question answering chain with citations are "How to: stream" and "How to: return citations." These guides provide specific instructions for implementing streaming capabilities and managing citation outputs in a question answering system.ªfocus_kind¬exec_latency¨query_id¤Q064‰¢idÙ@21013111261a2c069879a0b6b2479785853a291f41ec7ac9a4a0954dd7c31af5¢tsËAÚóùâwŸ¨trace_idÙ$4877e0cb-fee5-41a2-96ff-fb7932289e47­parent_run_idÙ$0b04bdaa-7bf1-4f03-b0a8-63e1e45e3c97¦run_idÙ$4877e0cb-fee5-41a2-96ff-fb7932289e47ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ%The two documents that explain how to build a streaming question answering chain with citations are "How to: stream" and "How to: return citations." These guides provide specific instructions for implementing streaming capabilities and managing citation outputs in a question answering system.¨query_id¤Q064ªfocus_kind¶chain_execution_result‰¢idÙ@6490a816ce0245b61c6c179f073255a587a2e79f72fa915f9679d4601660d705¢tsËAÚóùâ}¨trace_idÙ$0b04bdaa-7bf1-4f03-b0a8-63e1e45e3c97­parent_run_idÀ¦run_idÙ$0b04bdaa-7bf1-4f03-b0a8-63e1e45e3c97ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙfWhich two documents together explain how to build a streaming question answering chain with citations?§context•‚´page_content_summaryÙË### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)
- [How to: create a custom LL...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË### Serialization
- [How to: save and load LangChain objects](/docs/how_to/serialization)

## Use cases

These guides cover use-case specific details.

### Q&A with RAG

Retrieval Augmented Generation...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË| [Multi-query](/docs/how_to/MultiQueryRetriever/)                                                          | When you want to ensure high recall in retrieval by providing multiple phrasings of a ques...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË- [Chat models and prompts](/docs/tutorials/llm_chain): Build a simple LLM application with [prompt templates](/docs/concepts/prompt_templates) and [chat models](/docs/concepts/chat_models).
- [Semant...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË### Q&A over SQL + CSV

You can use LLMs to do question answering over tabular data.
For a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).

- [How to: use prompting to improve re...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answerÚ%The two documents that explain how to build a streaming question answering chain with citations are "How to: stream" and "How to: return citations." These guides provide specific instructions for implementing streaming capabilities and managing citation outputs in a question answering system.¨query_id¤Q064ªfocus_kind¶chain_execution_result‹¢idÙ@7c82b8d2e0ee0cd89ff24fb53eeaa581856c3e494af44867f5e4628ebe86d2d6¢tsËAÚóùâ‚¨trace_id¤Q065­parent_run_idÀ¦run_idÙ$279bf0e6-d6f1-4819-9b95-d12bb7471e2dªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ7How do you measure BLEU score in LangChain evaluations?¨query_id¤Q065§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q065ªfocus_kind¯chain_execution‹¢idÙ@189dd065252b7deb252b27b8379c70add2d2b1ccf158a86c3e6114a68911b9d6¢tsËAÚóùâŠ?¨trace_id¤Q065­parent_run_idÙ$279bf0e6-d6f1-4819-9b95-d12bb7471e2d¦run_idÙ$6b5001b5-e939-452e-89e4-3f1f2b42825bªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ7How do you measure BLEU score in LangChain evaluations?§context¦answer ¨query_id¤Q065¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q065®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:8ba51304-61db-72d8-0fab-2dce46b96e37ªfocus_kind¯chain_execution‰¢idÙ@6bf0972e79b1007efd279b6db9015e042426b7a7f2ad2c132444c6877b3eda3c¢tsËAÚóùõƒõ¨trace_idÙ$6b5001b5-e939-452e-89e4-3f1f2b42825b­parent_run_idÙ$279bf0e6-d6f1-4819-9b95-d12bb7471e2d¦run_idÙ$6b5001b5-e939-452e-89e4-3f1f2b42825bªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË:::

### Integrations

LangChain offers many embedding model integrations which you can find [on the embedding models](/docs/integrations/text_embedding/) integrations page.

## Measure similarity

Ea...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing t...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/evaluation.mdx‚´page_content_summaryÙË### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process fr...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build compl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¨query_id¤Q065ªfocus_kind¶chain_execution_result‹¢idÙ@84492bc60bf23ecd43fc72a62a7463109e414e35b1420c06c23b7e3a115a917e¢tsËAÚóùõŒ*¨trace_id¤Q065­parent_run_idÙ$279bf0e6-d6f1-4819-9b95-d12bb7471e2d¦run_idÙ$9cd46deb-831a-4d0c-a381-82e085ed8ed2ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ7How do you measure BLEU score in LangChain evaluations?§context•‚´page_content_summaryÙË:::

### Integrations

LangChain offers many embedding model integrations which you can find [on the embedding models](/docs/integrations/text_embedding/) integrations page.

## Measure similarity

Ea...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing t...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/evaluation.mdx‚´page_content_summaryÙË### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process fr...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build compl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answer ¨query_id¤Q065¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q065®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:5580fff7-5c0d-385c-5bff-c9407bee2e6eªfocus_kind¯chain_execution‹¢idÙ@3fcf40daa815185d15010a29ebe3bb7ace6d4bb13410425e232e3eb8b8fd20ea¢tsËAÚóùõ‘°¨trace_id¤Q065­parent_run_idÙ$9cd46deb-831a-4d0c-a381-82e085ed8ed2¦run_idÙ$780657b7-3ee2-4d46-a6e2-3e4ff7c19ab8ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ7How do you measure BLEU score in LangChain evaluations?§contextÚ»:::

### Integrations

LangChain offers many embedding model integrations which you can find [on the embedding models](/docs/integrations/text_embedding/) integrations page.

## Measure similarity

Each embedding is essentially a set of coordinates, often in a high-dimensional space. 
In this space, the position of each point (embedding) reflects the meaning of its corresponding text.
Just as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space. 
This allows for intuitive comparisons between different pieces of text.
By reducing text to these numerical representations, we can use simple mathematical operations to quickly measure how alike two pieces of text are, regardless of their original length or structure.
Some common similarity metrics include:

## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.

LangSmith documentation is hosted on a separate site.
You can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/).

### Evaluation

LangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:

- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)

# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.
This process is vital for building reliable applications.

![](/img/langsmith_evaluate.png)

[LangSmith](https://docs.smith.langchain.com/) helps with this process in a few ways:

- It makes it easier to create and curate datasets via its tracing and annotation features
- It provides an evaluation framework that helps you define metrics and run your app against your dataset
- It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code

To learn more, check out [this LangSmith guide](https://docs.smith.langchain.com/concepts/evaluation).

### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.

To learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).

### Tracing
<span data-heading-keywords="trace,tracing"></span>

Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.

- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)
- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)

You can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).

:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.

:::

## Observability and evaluation

The pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. 
Developers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. 
High quality tracing and evaluations can help you rapidly answer these types of questions with confidence.
[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.
See our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.

:::info[Further reading]¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q065®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:5580fff7-5c0d-385c-5bff-c9407bee2e6e­checkpoint_nsÙ-generate:5580fff7-5c0d-385c-5bff-c9407bee2e6e¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@9c04044f38311a7465235267b82c9c852ae6b1b2c9fc77afef89c11cf288471a¢tsËAÚóùõ—Ø¨trace_idÙ$780657b7-3ee2-4d46-a6e2-3e4ff7c19ab8­parent_run_idÙ$9cd46deb-831a-4d0c-a381-82e085ed8ed2¦run_idÙ$780657b7-3ee2-4d46-a6e2-3e4ff7c19ab8ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ¡messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: How do you measure BLEU score in LangChain evaluations? \nContext: :::\n\n### Integrations\n\nLangChain offers many embedding model integrations which you can find [on the embedding models](/docs/integrations/text_embedding/) integrations page.\n\n## Measure similarity\n\nEach embedding is essentially a set of coordinates, often in a high-dimensional space. \nIn this space, the position of each point (embedding) reflects the meaning of its corresponding text.\nJust as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space. \nThis allows for intuitive comparisons between different pieces of text.\nBy reducing text to these numerical representations, we can use simple mathematical operations to quickly measure how alike two pieces of text are, regardless of their original length or structure.\nSome common similarity metrics include:\n\n## LangSmith\n\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\n\nLangSmith documentation is hosted on a separate site.\nYou can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/).\n\n### Evaluation\n\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\n\n- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)\n\n# Evaluation\n<span data-heading-keywords="evaluation,evaluate"></span>\n\nEvaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.\nIt involves testing the model\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.\nThis process is vital for building reliable applications.\n\n![](/img/langsmith_evaluate.png)\n\n[LangSmith](https://docs.smith.langchain.com/) helps with this process in a few ways:\n\n- It makes it easier to create and curate datasets via its tracing and annotation features\n- It provides an evaluation framework that helps you define metrics and run your app against your dataset\n- It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code\n\nTo learn more, check out [this LangSmith guide](https://docs.smith.langchain.com/concepts/evaluation).\n\n### Evaluation\n<span data-heading-keywords="evaluation,evaluate"></span>\n\nEvaluating performance is a vital part of building LLM-powered applications.\nLangSmith helps with every step of the process from creating a dataset to defining metrics to running evaluators.\n\nTo learn more, check out the [LangSmith evaluation how-to guides](https://docs.smith.langchain.com/how_to_guides#evaluation).\n\n### Tracing\n<span data-heading-keywords="trace,tracing"></span>\n\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\n\n- [How to: trace with LangChain](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain)\n- [How to: add metadata and tags to traces](https://docs.smith.langchain.com/how_to_guides/tracing/trace_with_langchain#add-metadata-and-tags-to-traces)\n\nYou can see general tracing-related how-tos [in this section of the LangSmith docs](https://docs.smith.langchain.com/how_to_guides/tracing).\n\n:::info[Further reading]\n\nHave a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build complex applications.\n\n:::\n\n## Observability and evaluation\n\nThe pace of AI application development is often rate-limited by high-quality evaluations because there is a paradox of choice. \nDevelopers often wonder how to engineer their prompt or which LLM best balances accuracy, latency, and cost. \nHigh quality tracing and evaluations can help you rapidly answer these types of questions with confidence.\n[LangSmith](https://docs.smith.langchain.com/) is our platform that supports observability and evaluation for AI applications.\nSee our conceptual guides on [evaluations](https://docs.smith.langchain.com/concepts/evaluation) and [tracing](https://docs.smith.langchain.com/concepts/tracing) for more details.\n\n:::info[Further reading] \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@4e714f82e2cf6769056880626a0cb8332dc32566a7ff93a491d77d7db8962d3e¢tsËAÚóú;é¨focus_msÍD§payload‚¤role©assistant§contentÚThe provided context does not specify how to measure the BLEU score in LangChain evaluations. It discusses evaluation processes and tools like LangSmith but does not mention BLEU specifically. Therefore, I don't know how to measure BLEU score in LangChain evaluations.ªfocus_kind¬exec_latency¨query_id¤Q065‰¢idÙ@124ba39740d642728befc8593577bda4d1d278092342c0e06022acca425021ed¢tsËAÚóú;‘’¨trace_idÙ$9cd46deb-831a-4d0c-a381-82e085ed8ed2­parent_run_idÙ$279bf0e6-d6f1-4819-9b95-d12bb7471e2d¦run_idÙ$9cd46deb-831a-4d0c-a381-82e085ed8ed2ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚThe provided context does not specify how to measure the BLEU score in LangChain evaluations. It discusses evaluation processes and tools like LangSmith but does not mention BLEU specifically. Therefore, I don't know how to measure BLEU score in LangChain evaluations.¨query_id¤Q065ªfocus_kind¶chain_execution_result‰¢idÙ@0312de15ffe0363d6711314744eb8b14056a25a4087f7754b660b64e8cb1f453¢tsËAÚóú;Ø¨trace_idÙ$279bf0e6-d6f1-4819-9b95-d12bb7471e2d­parent_run_idÀ¦run_idÙ$279bf0e6-d6f1-4819-9b95-d12bb7471e2dªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ7How do you measure BLEU score in LangChain evaluations?§context•‚´page_content_summaryÙË:::

### Integrations

LangChain offers many embedding model integrations which you can find [on the embedding models](/docs/integrations/text_embedding/) integrations page.

## Measure similarity

Ea...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing t...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/evaluation.mdx‚´page_content_summaryÙË### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process fr...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build compl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answerÚThe provided context does not specify how to measure the BLEU score in LangChain evaluations. It discusses evaluation processes and tools like LangSmith but does not mention BLEU specifically. Therefore, I don't know how to measure BLEU score in LangChain evaluations.¨query_id¤Q065ªfocus_kind¶chain_execution_result‹¢idÙ@b5deec195fe39e7d9ded1da233cab792cd6d2efac1f1d74d707dc762e39f21a9¢tsËAÚóú;«G¨trace_id¤Q066­parent_run_idÀ¦run_idÙ$c94b0928-9239-46c8-913a-b843ecf9ade7ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ>What does the `ConversationBufferWindowMemory` keep in memory?¨query_id¤Q066§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q066ªfocus_kind¯chain_execution‹¢idÙ@975ea806b512884f6bf7166d89191823ca11fcb52c3ed873adb29339ce2af5ae¢tsËAÚóú;Í«¨trace_id¤Q066­parent_run_idÙ$c94b0928-9239-46c8-913a-b843ecf9ade7¦run_idÙ$1ca4aa0f-c392-4f58-b753-8b1dfe3200deªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ>What does the `ConversationBufferWindowMemory` keep in memory?§context¦answer ¨query_id¤Q066¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q066®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:eaad3052-c434-30ea-8521-c9ac6a53fd89ªfocus_kind¯chain_execution‰¢idÙ@b6e5762285371b6f7c82b0abd742bea018505a81fd26f4e807da42976d306776¢tsËAÚóúLø4¨trace_idÙ$1ca4aa0f-c392-4f58-b753-8b1dfe3200de­parent_run_idÙ$c94b0928-9239-46c8-913a-b843ecf9ade7¦run_idÙ$1ca4aa0f-c392-4f58-b753-8b1dfe3200deªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

A full conversation often invo...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙËWhile processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
    -...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q066ªfocus_kind¶chain_execution_result‹¢idÙ@a221d50f88882156fa0dd6b30694680a691513b92858b378e6ac04769c5d4aec¢tsËAÚóúM ‚¨trace_id¤Q066­parent_run_idÙ$c94b0928-9239-46c8-913a-b843ecf9ade7¦run_idÙ$35c77e26-3f69-4909-9be1-4bc4dc9e87c7ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ>What does the `ConversationBufferWindowMemory` keep in memory?§context•‚´page_content_summaryÙËThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

A full conversation often invo...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙËWhile processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
    -...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answer ¨query_id¤Q066¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q066®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:c4e9c1ed-85e8-8f31-392f-60c71c0b5fbdªfocus_kind¯chain_execution‹¢idÙ@b044cdab58c35f5dfbea8ded6e740842414da9cf7d429da1ae80fc453f7caa0f¢tsËAÚóúMl¨trace_id¤Q066­parent_run_idÙ$35c77e26-3f69-4909-9be1-4bc4dc9e87c7¦run_idÙ$b6887452-1411-4e71-979d-22eaede64d59ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ>What does the `ConversationBufferWindowMemory` keep in memory?§contextÚïThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

A full conversation often involves a combination of two patterns of alternating messages:

1. The **user** and the **assistant** representing a back-and-forth conversation.
2. The **assistant** and **tool messages** representing an ["agentic" workflow](/docs/concepts/agents) where the assistant is invoking tools to perform specific tasks.

## Managing chat history

Since chat models have a maximum limit on input size, it's important to manage chat history and trim it as needed to avoid exceeding the [context window](/docs/concepts/chat_models/#context-window).

While processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

While processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
    - The first message is either a "user" message or a "system" message, followed by a "user" and then an "assistant" message.
    - The last message should be either a "user" message or a "tool" message containing the result of a tool call.
- When using [tool calling](/docs/concepts/tool_calling), a "tool" message should only follow an "assistant" message that requested the tool invocation.

:::tip
Understanding correct conversation structure is essential for being able to properly implement
[memory](https://langchain-ai.github.io/langgraph/concepts/memory/) in chat models.
:::

## Related resources

## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and long-term memory in chat models using [LangGraph](https://langchain-ai.github.io/langgraph/).

### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.

The reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?

An alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.

## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. Please read more about
the technique in the [structured outputs](/docs/concepts/structured_outputs) guide.

## Multimodality

Large Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as [multimodality](/docs/concepts/multimodality).

Currently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.

## Context window

A chat model's context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q066®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:c4e9c1ed-85e8-8f31-392f-60c71c0b5fbd­checkpoint_nsÙ-generate:c4e9c1ed-85e8-8f31-392f-60c71c0b5fbd¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@e9f01cd6900a1ca58656812087b5772d11ec2ff4fbf2478279bc048bf47e8b37¢tsËAÚóúM¨trace_idÙ$b6887452-1411-4e71-979d-22eaede64d59­parent_run_idÙ$35c77e26-3f69-4909-9be1-4bc4dc9e87c7¦run_idÙ$b6887452-1411-4e71-979d-22eaede64d59ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚËmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What does the `ConversationBufferWindowMemory` keep in memory? \nContext: The **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.\n\nA full conversation often involves a combination of two patterns of alternating messages:\n\n1. The **user** and the **assistant** representing a back-and-forth conversation.\n2. The **assistant** and **tool messages** representing an ["agentic" workflow](/docs/concepts/agents) where the assistant is invoking tools to perform specific tasks.\n\n## Managing chat history\n\nSince chat models have a maximum limit on input size, it\'s important to manage chat history and trim it as needed to avoid exceeding the [context window](/docs/concepts/chat_models/#context-window).\n\nWhile processing chat history, it\'s essential to preserve a correct conversation structure. \n\nKey guidelines for managing chat history:\n\nWhile processing chat history, it\'s essential to preserve a correct conversation structure. \n\nKey guidelines for managing chat history:\n\n- The conversation should follow one of these structures:\n    - The first message is either a "user" message or a "system" message, followed by a "user" and then an "assistant" message.\n    - The last message should be either a "user" message or a "tool" message containing the result of a tool call.\n- When using [tool calling](/docs/concepts/tool_calling), a "tool" message should only follow an "assistant" message that requested the tool invocation.\n\n:::tip\nUnderstanding correct conversation structure is essential for being able to properly implement\n[memory](https://langchain-ai.github.io/langgraph/concepts/memory/) in chat models.\n:::\n\n## Related resources\n\n## Related resources\n\n- [How to trim messages](/docs/how_to/trim_messages/)\n- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and long-term memory in chat models using [LangGraph](https://langchain-ai.github.io/langgraph/).\n\n### Caching\n\nChat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.\n\nThe reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?\n\nAn alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.\n\n## Structured outputs\n\nChat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely\nuseful for information extraction tasks. Please read more about\nthe technique in the [structured outputs](/docs/concepts/structured_outputs) guide.\n\n## Multimodality\n\nLarge Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as [multimodality](/docs/concepts/multimodality).\n\nCurrently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.\n\n## Context window\n\nA chat model\'s context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@70fb55e8edd3b4190be46eaef51b2faa138d2c213c6fcd3f89f7a58c5596693d¢tsËAÚóúàƒ‡¨focus_msÍÿ§payload‚¤role©assistant§contentÚ[The `ConversationBufferWindowMemory` keeps a record of the conversation history, including messages from both the user and the assistant. It manages this history to ensure the conversation structure is preserved while adhering to the model's context window limitations. This allows the assistant to maintain context and continuity in the dialogue.ªfocus_kind¬exec_latency¨query_id¤Q066‰¢idÙ@28ce2ba1a5714909b081a97680c5ad3aef27ab7f542dc10deab02bc4bb5a23f3¢tsËAÚóúà˜¨trace_idÙ$35c77e26-3f69-4909-9be1-4bc4dc9e87c7­parent_run_idÙ$c94b0928-9239-46c8-913a-b843ecf9ade7¦run_idÙ$35c77e26-3f69-4909-9be1-4bc4dc9e87c7ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ[The `ConversationBufferWindowMemory` keeps a record of the conversation history, including messages from both the user and the assistant. It manages this history to ensure the conversation structure is preserved while adhering to the model's context window limitations. This allows the assistant to maintain context and continuity in the dialogue.¨query_id¤Q066ªfocus_kind¶chain_execution_result‰¢idÙ@18f625c886de21fdccbac8de146f21de8555fbcb38c9dd7ef1e3916a73045be1¢tsËAÚóúà¥/¨trace_idÙ$c94b0928-9239-46c8-913a-b843ecf9ade7­parent_run_idÀ¦run_idÙ$c94b0928-9239-46c8-913a-b843ecf9ade7ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ>What does the `ConversationBufferWindowMemory` keep in memory?§context•‚´page_content_summaryÙËThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

A full conversation often invo...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙËWhile processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
    -...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚ[The `ConversationBufferWindowMemory` keeps a record of the conversation history, including messages from both the user and the assistant. It manages this history to ensure the conversation structure is preserved while adhering to the model's context window limitations. This allows the assistant to maintain context and continuity in the dialogue.¨query_id¤Q066ªfocus_kind¶chain_execution_result‹¢idÙ@3bd552064274af26a4e6507678b64097a8d7c750afc9ed657884204af988c913¢tsËAÚóúà®¨trace_id¤Q067­parent_run_idÀ¦run_idÙ$d531e329-afd6-4350-9daa-41809ce5e1f6ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ?Name a loader designed specifically for PDF files in LangChain.¨query_id¤Q067§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q067ªfocus_kind¯chain_execution‹¢idÙ@308d87da2fb3f55a326c08ab5a00a62a5defed336d591007eae5a4e066d75f5f¢tsËAÚóúà¸ö¨trace_id¤Q067­parent_run_idÙ$d531e329-afd6-4350-9daa-41809ce5e1f6¦run_idÙ$c56903b9-a193-4373-9d8a-a284be52d0cfªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ?Name a loader designed specifically for PDF files in LangChain.§context¦answer ¨query_id¤Q067¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q067®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:d63a5135-e11e-3753-0f7e-06cfef0accf4ªfocus_kind¯chain_execution‰¢idÙ@5f056ea43c66b08867c82e1906abb0f7d89858e4d2c6156a85c1d110cab60512¢tsËAÚóúîP#¨trace_idÙ$c56903b9-a193-4373-9d8a-a284be52d0cf­parent_run_idÙ$d531e329-afd6-4350-9daa-41809ce5e1f6¦run_idÙ$c56903b9-a193-4373-9d8a-a284be52d0cfªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Document loaders
<span data-heading-keywords="document loader,document loaders"></span>

:::info[Prerequisites]

* [Document loaders API reference](/docs/how_to/#document-loaders)
:::

Document load...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/document_loaders.mdx‚´page_content_summaryÙË## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader

[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning 
b...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË# How to load JSON

[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and trans...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙË### Prerequisite

An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-s...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙËIt uses the `jq` python package. Check out this [manual](https://stedolan.github.io/jq/manual/#Basicfilters) for a detailed documentation of the `jq` syntax.

Here we will demonstrate: 

- How to load...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx¨query_id¤Q067ªfocus_kind¶chain_execution_result‹¢idÙ@dc44f6f140a0cdc524d930fe284ae6e538ad26ac665994c12c3d38b0b9ffba03¢tsËAÚóúîXK¨trace_id¤Q067­parent_run_idÙ$d531e329-afd6-4350-9daa-41809ce5e1f6¦run_idÙ$8254e54d-0422-4e71-97cb-085c2dc41c67ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ?Name a loader designed specifically for PDF files in LangChain.§context•‚´page_content_summaryÙË# Document loaders
<span data-heading-keywords="document loader,document loaders"></span>

:::info[Prerequisites]

* [Document loaders API reference](/docs/how_to/#document-loaders)
:::

Document load...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/document_loaders.mdx‚´page_content_summaryÙË## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader

[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning 
b...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË# How to load JSON

[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and trans...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙË### Prerequisite

An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-s...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙËIt uses the `jq` python package. Check out this [manual](https://stedolan.github.io/jq/manual/#Basicfilters) for a detailed documentation of the `jq` syntax.

Here we will demonstrate: 

- How to load...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx¦answer ¨query_id¤Q067¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q067®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:5fd86019-92be-7814-0dd7-fcdb4bba9bc7ªfocus_kind¯chain_execution‹¢idÙ@b16541cb99bb8eb832cc31a43dd4169bf59802a15af7b5619967a3fa99285bb2¢tsËAÚóúî]İ¨trace_id¤Q067­parent_run_idÙ$8254e54d-0422-4e71-97cb-085c2dc41c67¦run_idÙ$6e1f1a14-da2c-4cc9-8534-2359e784db45ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ?Name a loader designed specifically for PDF files in LangChain.§contextÚì# Document loaders
<span data-heading-keywords="document loader,document loaders"></span>

:::info[Prerequisites]

* [Document loaders API reference](/docs/how_to/#document-loaders)
:::

Document loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.

## Integrations

You can find available integrations on the [Document loaders integrations page](/docs/integrations/document_loaders/).

## Interface

Documents loaders implement the [BaseLoader interface](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html).

Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the `.load` method or `.lazy_load`.

Here's a simple example:

```python
from langchain_community.document_loaders.csv_loader import CSVLoader

## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader

[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning 
based service that extracts texts (including handwriting), tables, document structures (e.g., titles, section headings, etc.) and key-value-pairs from
digital or scanned PDFs, images, Office and HTML files. Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`.

This [current implementation](https://aka.ms/di-langchain) of a loader using `Document Intelligence` can incorporate content page-wise and turn it into LangChain documents. The default output format is markdown, which can be easily chained with `MarkdownHeaderTextSplitter` for semantic document chunking. You can also use `mode="single"` or `mode="page"` to return pure texts in a single page or document split by page.

### Prerequisite

# How to load JSON

[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attributeâ€“value pairs and arrays (or other serializable values).

[JSON Lines](https://jsonlines.org/) is a file format where each line is a valid JSON value.

LangChain implements a [JSONLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.json_loader.JSONLoader.html) 
to convert JSON and JSONL data into LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) 
objects. It uses a specified [jq schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files, allowing for the extraction of specific fields into the content 
and metadata of the LangChain Document.

### Prerequisite

An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0) to create one if you don't have. You will be passing `<endpoint>` and `<key>` as parameters to the loader.

```python
%pip install --upgrade --quiet  langchain langchain-community azure-ai-documentintelligence

from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader

file_path = "<filepath>"
endpoint = "<endpoint>"
key = "<key>"
loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint, api_key=key, file_path=file_path, api_model="prebuilt-layout"
)

documents = loader.load()
```

It uses the `jq` python package. Check out this [manual](https://stedolan.github.io/jq/manual/#Basicfilters) for a detailed documentation of the `jq` syntax.

Here we will demonstrate: 

- How to load JSON and JSONL data into the content of a LangChain `Document`;
- How to load JSON and JSONL data into metadata associated with a `Document`.


```python
#!pip install jq
```


```python
from langchain_community.document_loaders import JSONLoader
```


```python
import json
from pathlib import Path
from pprint import pprint


file_path='./example_data/facebook_chat.json'
data = json.loads(Path(file_path).read_text())
```¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q067®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:5fd86019-92be-7814-0dd7-fcdb4bba9bc7­checkpoint_nsÙ-generate:5fd86019-92be-7814-0dd7-fcdb4bba9bc7¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@58f56e05cc2d76314acf3529db0800f29db473b35447f09582e4b790f4bf76aa¢tsËAÚóúîdö¨trace_idÙ$6e1f1a14-da2c-4cc9-8534-2359e784db45­parent_run_idÙ$8254e54d-0422-4e71-97cb-085c2dc41c67¦run_idÙ$6e1f1a14-da2c-4cc9-8534-2359e784db45ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚêmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Name a loader designed specifically for PDF files in LangChain. \nContext: # Document loaders\n<span data-heading-keywords="document loader,document loaders"></span>\n\n:::info[Prerequisites]\n\n* [Document loaders API reference](/docs/how_to/#document-loaders)\n:::\n\nDocument loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.\n\n## Integrations\n\nYou can find available integrations on the [Document loaders integrations page](/docs/integrations/document_loaders/).\n\n## Interface\n\nDocuments loaders implement the [BaseLoader interface](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html).\n\nEach DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the `.load` method or `.lazy_load`.\n\nHere\'s a simple example:\n\n```python\nfrom langchain_community.document_loaders.csv_loader import CSVLoader\n\n## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader\n\n[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning \nbased service that extracts texts (including handwriting), tables, document structures (e.g., titles, section headings, etc.) and key-value-pairs from\ndigital or scanned PDFs, images, Office and HTML files. Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`.\n\nThis [current implementation](https://aka.ms/di-langchain) of a loader using `Document Intelligence` can incorporate content page-wise and turn it into LangChain documents. The default output format is markdown, which can be easily chained with `MarkdownHeaderTextSplitter` for semantic document chunking. You can also use `mode="single"` or `mode="page"` to return pure texts in a single page or document split by page.\n\n### Prerequisite\n\n# How to load JSON\n\n[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and transmit data objects consisting of attributeâ€“value pairs and arrays (or other serializable values).\n\n[JSON Lines](https://jsonlines.org/) is a file format where each line is a valid JSON value.\n\nLangChain implements a [JSONLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.json_loader.JSONLoader.html) \nto convert JSON and JSONL data into LangChain [Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document) \nobjects. It uses a specified [jq schema](https://en.wikipedia.org/wiki/Jq_(programming_language)) to parse the JSON files, allowing for the extraction of specific fields into the content \nand metadata of the LangChain Document.\n\n### Prerequisite\n\nAn Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0) to create one if you don\'t have. You will be passing `<endpoint>` and `<key>` as parameters to the loader.\n\n```python\n%pip install --upgrade --quiet  langchain langchain-community azure-ai-documentintelligence\n\nfrom langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n\nfile_path = "<filepath>"\nendpoint = "<endpoint>"\nkey = "<key>"\nloader = AzureAIDocumentIntelligenceLoader(\n    api_endpoint=endpoint, api_key=key, file_path=file_path, api_model="prebuilt-layout"\n)\n\ndocuments = loader.load()\n```\n\nIt uses the `jq` python package. Check out this [manual](https://stedolan.github.io/jq/manual/#Basicfilters) for a detailed documentation of the `jq` syntax.\n\nHere we will demonstrate: \n\n- How to load JSON and JSONL data into the content of a LangChain `Document`;\n- How to load JSON and JSONL data into metadata associated with a `Document`.\n\n\n```python\n#!pip install jq\n```\n\n\n```python\nfrom langchain_community.document_loaders import JSONLoader\n```\n\n\n```python\nimport json\nfrom pathlib import Path\nfrom pprint import pprint\n\n\nfile_path=\'./example_data/facebook_chat.json\'\ndata = json.loads(Path(file_path).read_text())\n``` \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@08b5e6597331e7cec07a3e3cdb6a4472d9caf0563ac973774c508f19622bca28¢tsËAÚóû5Â¹¨focus_msÍZ§payload‚¤role©assistant§contentÚThe loader designed specifically for PDF files in LangChain is the `AzureAIDocumentIntelligenceLoader`. This loader utilizes Azure's Document Intelligence service to extract text and structure from PDF documents. It can return content in various formats, including markdown.ªfocus_kind¬exec_latency¨query_id¤Q067‰¢idÙ@f4877496f10e3f2c766ef5e67bb517c3a2e8c9298f9a6a5bdc80939a04262a1a¢tsËAÚóû5Òæ¨trace_idÙ$8254e54d-0422-4e71-97cb-085c2dc41c67­parent_run_idÙ$d531e329-afd6-4350-9daa-41809ce5e1f6¦run_idÙ$8254e54d-0422-4e71-97cb-085c2dc41c67ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚThe loader designed specifically for PDF files in LangChain is the `AzureAIDocumentIntelligenceLoader`. This loader utilizes Azure's Document Intelligence service to extract text and structure from PDF documents. It can return content in various formats, including markdown.¨query_id¤Q067ªfocus_kind¶chain_execution_result‰¢idÙ@922ef1a244d24f4684153b412b715fb3eada883dab26ba35f78bffd06638a2a3¢tsËAÚóû5Ûb¨trace_idÙ$d531e329-afd6-4350-9daa-41809ce5e1f6­parent_run_idÀ¦run_idÙ$d531e329-afd6-4350-9daa-41809ce5e1f6ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ?Name a loader designed specifically for PDF files in LangChain.§context•‚´page_content_summaryÙË# Document loaders
<span data-heading-keywords="document loader,document loaders"></span>

:::info[Prerequisites]

* [Document loaders API reference](/docs/how_to/#document-loaders)
:::

Document load...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/document_loaders.mdx‚´page_content_summaryÙË## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader

[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning 
b...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË# How to load JSON

[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and trans...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙË### Prerequisite

An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-s...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙËIt uses the `jq` python package. Check out this [manual](https://stedolan.github.io/jq/manual/#Basicfilters) for a detailed documentation of the `jq` syntax.

Here we will demonstrate: 

- How to load...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx¦answerÚThe loader designed specifically for PDF files in LangChain is the `AzureAIDocumentIntelligenceLoader`. This loader utilizes Azure's Document Intelligence service to extract text and structure from PDF documents. It can return content in various formats, including markdown.¨query_id¤Q067ªfocus_kind¶chain_execution_result‹¢idÙ@ee6ff2a491bf6c0c4017eae7c226dc71dc54160d4ed3abcd4cb0888a2e94000f¢tsËAÚóû5âğ¨trace_id¤Q068­parent_run_idÀ¦run_idÙ$2c621588-e865-4e10-9a14-b185cdf1e10bªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙLWhich concept guide covers the standard streaming APIs exposed by Runnables?¨query_id¤Q068§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q068ªfocus_kind¯chain_execution‹¢idÙ@cd744a4986317e09fcdbd5adae4cce4e768a2d13834ececff1f867ab900f344a¢tsËAÚóû5í¨trace_id¤Q068­parent_run_idÙ$2c621588-e865-4e10-9a14-b185cdf1e10b¦run_idÙ$0098ac05-de1b-4b53-a79c-5a0237ae1e5cªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙLWhich concept guide covers the standard streaming APIs exposed by Runnables?§context¦answer ¨query_id¤Q068¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q068®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:7892def8-3dd8-376f-4956-3d2f9598528aªfocus_kind¯chain_execution‰¢idÙ@e9a771b92f7bd36bad9bb330d232b777080b2fee07fac33a03f7ad9780567a37¢tsËAÚóûBoµ¨trace_idÙ$0098ac05-de1b-4b53-a79c-5a0237ae1e5c­parent_run_idÙ$2c621588-e865-4e10-9a14-b185cdf1e10b¦run_idÙ$0098ac05-de1b-4b53-a79c-5a0237ae1e5cªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËPlease refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.

## Streaming APIs
<span data-heading-keywords="streaming-api"></span>

Streaming is critical in ma...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Relate...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËPlease refer to the [Streaming Conceptual Guide](/docs/concepts/streaming) for more details on how to stream in LangChain.

## Input and output types

Every `Runnable` is characterized by an input and...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËPlease review the [async programming in LangChain guide](/docs/concepts/async) for more information on writing async code with LangChain.

## Related Resources

Please see the following how-to guides ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¨query_id¤Q068ªfocus_kind¶chain_execution_result‹¢idÙ@08db255ee7a8f410476ec72a7977fe40ffe5614886ac0d18be03026f2a4701b3¢tsËAÚóûB{Z¨trace_id¤Q068­parent_run_idÙ$2c621588-e865-4e10-9a14-b185cdf1e10b¦run_idÙ$0fc40719-8059-4f4c-98fa-4ae2dc30d59dªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙLWhich concept guide covers the standard streaming APIs exposed by Runnables?§context•‚´page_content_summaryÙËPlease refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.

## Streaming APIs
<span data-heading-keywords="streaming-api"></span>

Streaming is critical in ma...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Relate...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËPlease refer to the [Streaming Conceptual Guide](/docs/concepts/streaming) for more details on how to stream in LangChain.

## Input and output types

Every `Runnable` is characterized by an input and...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËPlease review the [async programming in LangChain guide](/docs/concepts/async) for more information on writing async code with LangChain.

## Related Resources

Please see the following how-to guides ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¦answer ¨query_id¤Q068¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q068®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:afd2b4ec-de60-49e5-42b8-b7321c3aa48aªfocus_kind¯chain_execution‹¢idÙ@0784e8c88633043998ee51baf17a6af5f2de5456bc0816d19673fa25a027a8a9¢tsËAÚóûBƒE¨trace_id¤Q068­parent_run_idÙ$0fc40719-8059-4f4c-98fa-4ae2dc30d59d¦run_idÙ$f1bfd3a8-227d-4189-8efe-f92e8d7be195ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙLWhich concept guide covers the standard streaming APIs exposed by Runnables?§contextÚPlease refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.

## Streaming APIs
<span data-heading-keywords="streaming-api"></span>

Streaming is critical in making applications based on LLMs feel responsive to end-users.

Runnables expose the following three streaming APIs:

1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream): yields the output a Runnable as it is generated.
2. The async `astream_events`: a more advanced streaming API that allows streaming intermediate steps and final output
3. The **legacy** async `astream_log`: a legacy streaming API that streams intermediate steps and final output

This guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Related Resources
* The ["Runnable" Interface API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) provides a detailed overview of the Runnable interface and its methods.
* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel).
:::

## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](
https://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.

This guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

Please refer to the [Streaming Conceptual Guide](/docs/concepts/streaming) for more details on how to stream in LangChain.

## Input and output types

Every `Runnable` is characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself.

Runnable methods that result in the execution of the Runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) work with these input and output types.

* invoke: Accepts an input and returns an output.
* batch: Accepts a list of inputs and returns a list of outputs.
* stream: Accepts an input and returns a generator that yields outputs.

The **input type** and **output type** vary by component:

Please review the [async programming in LangChain guide](/docs/concepts/async) for more information on writing async code with LangChain.

## Related Resources

Please see the following how-to guides for specific examples of streaming in LangChain:
* [LangGraph conceptual guide on streaming](https://langchain-ai.github.io/langgraph/concepts/streaming/)
* [LangGraph streaming how-to guides](https://langchain-ai.github.io/langgraph/how-tos/#streaming)
* [How to stream runnables](/docs/how_to/streaming/): This how-to guide goes over common streaming patterns with LangChain components (e.g., chat models) and with [LCEL](/docs/concepts/lcel).
* [How to stream chat models](/docs/how_to/chat_streaming/)
* [How to stream tool calls](/docs/how_to/tool_streaming/)

For writing custom data to the stream, please see the following resources:¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q068®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:afd2b4ec-de60-49e5-42b8-b7321c3aa48a­checkpoint_nsÙ-generate:afd2b4ec-de60-49e5-42b8-b7321c3aa48a¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@fb1d9486860ef4d2b26200560a1e690e41831edaba2d4a34a73a92a4d5c81c82¢tsËAÚóûB‘.¨trace_idÙ$f1bfd3a8-227d-4189-8efe-f92e8d7be195­parent_run_idÙ$0fc40719-8059-4f4c-98fa-4ae2dc30d59d¦run_idÙ$f1bfd3a8-227d-4189-8efe-f92e8d7be195ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚrmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Which concept guide covers the standard streaming APIs exposed by Runnables? \nContext: Please refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.\n\n## Streaming APIs\n<span data-heading-keywords="streaming-api"></span>\n\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\n\nRunnables expose the following three streaming APIs:\n\n1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream): yields the output a Runnable as it is generated.\n2. The async `astream_events`: a more advanced streaming API that allows streaming intermediate steps and final output\n3. The **legacy** async `astream_log`: a legacy streaming API that streams intermediate steps and final output\n\nThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.\n\n:::info Related Resources\n* The ["Runnable" Interface API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) provides a detailed overview of the Runnable interface and its methods.\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel).\n:::\n\n## Overview of runnable interface\n\nThe Runnable way defines a standard interface that allows a Runnable component to be:\n\n# Runnable interface\n\nThe Runnable interface is the foundation for working with LangChain components, and it\'s implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](\nhttps://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.\n\nThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.\n\nPlease refer to the [Streaming Conceptual Guide](/docs/concepts/streaming) for more details on how to stream in LangChain.\n\n## Input and output types\n\nEvery `Runnable` is characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself.\n\nRunnable methods that result in the execution of the Runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) work with these input and output types.\n\n* invoke: Accepts an input and returns an output.\n* batch: Accepts a list of inputs and returns a list of outputs.\n* stream: Accepts an input and returns a generator that yields outputs.\n\nThe **input type** and **output type** vary by component:\n\nPlease review the [async programming in LangChain guide](/docs/concepts/async) for more information on writing async code with LangChain.\n\n## Related Resources\n\nPlease see the following how-to guides for specific examples of streaming in LangChain:\n* [LangGraph conceptual guide on streaming](https://langchain-ai.github.io/langgraph/concepts/streaming/)\n* [LangGraph streaming how-to guides](https://langchain-ai.github.io/langgraph/how-tos/#streaming)\n* [How to stream runnables](/docs/how_to/streaming/): This how-to guide goes over common streaming patterns with LangChain components (e.g., chat models) and with [LCEL](/docs/concepts/lcel).\n* [How to stream chat models](/docs/how_to/chat_streaming/)\n* [How to stream tool calls](/docs/how_to/tool_streaming/)\n\nFor writing custom data to the stream, please see the following resources: \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@b6c3a4185410bfcdbedc721d6bf3756ee7c99a8c3d202fdb9608ce764cbd3867¢tsËAÚóû«q¨focus_msÍh§payload‚¤role©assistant§contentÚ}The concept guide that covers the standard streaming APIs exposed by Runnables is the [Streaming Conceptual Guide](/docs/concepts/streaming). This guide provides details on how to stream in LangChain, including the various streaming APIs available. For more information on async programming, you can also refer to the [Async Programming with LangChain](/docs/concepts/async) guide.ªfocus_kind¬exec_latency¨query_id¤Q068‰¢idÙ@fbf5c644463dc6fbf683a17cd7e480f8a66fbbe3fcea870d4d2b979c9d4385be¢tsËAÚóû«š ¨trace_idÙ$0fc40719-8059-4f4c-98fa-4ae2dc30d59d­parent_run_idÙ$2c621588-e865-4e10-9a14-b185cdf1e10b¦run_idÙ$0fc40719-8059-4f4c-98fa-4ae2dc30d59dªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ}The concept guide that covers the standard streaming APIs exposed by Runnables is the [Streaming Conceptual Guide](/docs/concepts/streaming). This guide provides details on how to stream in LangChain, including the various streaming APIs available. For more information on async programming, you can also refer to the [Async Programming with LangChain](/docs/concepts/async) guide.¨query_id¤Q068ªfocus_kind¶chain_execution_result‰¢idÙ@97892d96e4f2ad16108603b1d3038d5216b6e8979add701d2db74aac31c03c5d¢tsËAÚóû«¤¨trace_idÙ$2c621588-e865-4e10-9a14-b185cdf1e10b­parent_run_idÀ¦run_idÙ$2c621588-e865-4e10-9a14-b185cdf1e10bªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙLWhich concept guide covers the standard streaming APIs exposed by Runnables?§context•‚´page_content_summaryÙËPlease refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.

## Streaming APIs
<span data-heading-keywords="streaming-api"></span>

Streaming is critical in ma...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Relate...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËPlease refer to the [Streaming Conceptual Guide](/docs/concepts/streaming) for more details on how to stream in LangChain.

## Input and output types

Every `Runnable` is characterized by an input and...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËPlease review the [async programming in LangChain guide](/docs/concepts/async) for more information on writing async code with LangChain.

## Related Resources

Please see the following how-to guides ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¦answerÚ}The concept guide that covers the standard streaming APIs exposed by Runnables is the [Streaming Conceptual Guide](/docs/concepts/streaming). This guide provides details on how to stream in LangChain, including the various streaming APIs available. For more information on async programming, you can also refer to the [Async Programming with LangChain](/docs/concepts/async) guide.¨query_id¤Q068ªfocus_kind¶chain_execution_result‹¢idÙ@fa56bc206e2cb49f4ded602d053ff2ea0d21ccf23928e91699f38eb824634b08¢tsËAÚóû«¯q¨trace_id¤Q069­parent_run_idÀ¦run_idÙ$1befe0ba-9a4b-47fb-914f-6f77e2cec7b2ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ:What is the advantage of using `LangServe` with Runnables?¨query_id¤Q069§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q069ªfocus_kind¯chain_execution‹¢idÙ@9c4229e4530cfc11f610116a3c8c9ef41337688f80ece628c2ec1cde5d9a8005¢tsËAÚóû«»®¨trace_id¤Q069­parent_run_idÙ$1befe0ba-9a4b-47fb-914f-6f77e2cec7b2¦run_idÙ$de008e66-f9c6-4d91-9fd9-06d314991d8bªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ:What is the advantage of using `LangServe` with Runnables?§context¦answer ¨query_id¤Q069¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q069®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:c009aff2-79a7-8584-f02a-eec65569bcecªfocus_kind¯chain_execution‰¢idÙ@08302f23948d78751e598f19984eae3e40be717ce320e11af5f36e0fbfc7843c¢tsËAÚóûºå.¨trace_idÙ$de008e66-f9c6-4d91-9fd9-06d314991d8b­parent_run_idÙ$1befe0ba-9a4b-47fb-914f-6f77e2cec7b2¦run_idÙ$de008e66-f9c6-4d91-9fd9-06d314991d8bªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËWe often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::n...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Relate...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q069ªfocus_kind¶chain_execution_result‹¢idÙ@3224d134414d714f599e91a5ca75b877d09ada92313d4c0319f9a906cdf599e8¢tsËAÚóûºğØ¨trace_id¤Q069­parent_run_idÙ$1befe0ba-9a4b-47fb-914f-6f77e2cec7b2¦run_idÙ$79d60741-62ce-48a1-bdd5-093b9ba0b31aªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ:What is the advantage of using `LangServe` with Runnables?§context•‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËWe often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::n...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Relate...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answer ¨query_id¤Q069¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q069®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:e0f99ca8-6413-492b-6fda-42e4a93c7a37ªfocus_kind¯chain_execution‹¢idÙ@3683c29d2ac292da8079792b45a0f5af82f431f792ad6e772b3daa727232ec78¢tsËAÚóûºú¹¨trace_id¤Q069­parent_run_idÙ$79d60741-62ce-48a1-bdd5-093b9ba0b31a¦run_idÙ$7a1ec382-5179-4039-9948-a7983e555a2fªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ:What is the advantage of using `LangServe` with Runnables?§contextÚw# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](
https://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.

This guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

We often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::note
* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.
* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.
* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using LCEL.
:::

## Benefits of LCEL

LangChain optimizes the run-time execution of chains built with LCEL in a number of ways:

This guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Related Resources
* The ["Runnable" Interface API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) provides a detailed overview of the Runnable interface and its methods.
* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel).
:::

## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
and is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.
:::

Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.

To simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)](/docs/concepts/lcel) to compose
multiple Runnables and you need to add custom processing logic in one of the steps.

There are two ways to create a custom Runnable from a function:

* `RunnableLambda`: Use this for simple transformations where streaming is not required.
* `RunnableGenerator`: use this for more complex transformations when streaming is needed.

See the [How to run custom functions](/docs/how_to/functions) guide for more information on how to use `RunnableLambda` and `RunnableGenerator`.

:::important
Users should not try to subclass Runnables to create a new custom Runnable. It is
much more complex and error-prone than simply using `RunnableLambda` or `RunnableGenerator`.
:::

## Configurable runnables¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q069®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:e0f99ca8-6413-492b-6fda-42e4a93c7a37­checkpoint_nsÙ-generate:e0f99ca8-6413-492b-6fda-42e4a93c7a37¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@15a0957699aab78f4ba67a6747f215df98c84f385de58ed74fd7c656cff1266b¢tsËAÚóû»¨trace_idÙ$7a1ec382-5179-4039-9948-a7983e555a2f­parent_run_idÙ$79d60741-62ce-48a1-bdd5-093b9ba0b31a¦run_idÙ$7a1ec382-5179-4039-9948-a7983e555a2fªevent_type¬chain_output¬event_source¥chain§payload§outputsÚPmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What is the advantage of using `LangServe` with Runnables? \nContext: # Runnable interface\n\nThe Runnable interface is the foundation for working with LangChain components, and it\'s implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](\nhttps://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.\n\nThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.\n\nWe often refer to a `Runnable` created using LCEL as a "chain". It\'s important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\n\n:::note\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using LCEL.\n:::\n\n## Benefits of LCEL\n\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\n\nThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.\n\n:::info Related Resources\n* The ["Runnable" Interface API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable) provides a detailed overview of the Runnable interface and its methods.\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel).\n:::\n\n## Overview of runnable interface\n\nThe Runnable way defines a standard interface that allows a Runnable component to be:\n\n## Configurable runnables\n\n:::note\nThis is an advanced feature that is unnecessary for most users.\n\nIt helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)\nand is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.\n:::\n\nSometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.\n\nTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:\n\n## Creating a runnable from a function {#custom-runnables}\n\nYou may need to create a custom Runnable that runs arbitrary logic. This is especially\nuseful if using [LangChain Expression Language (LCEL)](/docs/concepts/lcel) to compose\nmultiple Runnables and you need to add custom processing logic in one of the steps.\n\nThere are two ways to create a custom Runnable from a function:\n\n* `RunnableLambda`: Use this for simple transformations where streaming is not required.\n* `RunnableGenerator`: use this for more complex transformations when streaming is needed.\n\nSee the [How to run custom functions](/docs/how_to/functions) guide for more information on how to use `RunnableLambda` and `RunnableGenerator`.\n\n:::important\nUsers should not try to subclass Runnables to create a new custom Runnable. It is\nmuch more complex and error-prone than simply using `RunnableLambda` or `RunnableGenerator`.\n:::\n\n## Configurable runnables \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@123ca23133f591925714dd1fbca573ea653d44699f236505f39a0bf7e6ce0213¢tsËAÚóü(,Ô¨focus_msÍ©§payload‚¤role©assistant§contentÚÁThe advantage of using `LangServe` with Runnables is that it allows for the deployment of configurable Runnables, enabling users to experiment with different parameters and models at runtime. This flexibility simplifies the process of adjusting settings, such as temperature in chat models, without needing to modify the underlying code. Additionally, it enhances the ability to compose complex chains using the LangChain Expression Language (LCEL).ªfocus_kind¬exec_latency¨query_id¤Q069‰¢idÙ@de53e47e52d8ebdab562b618753ae846328d3125323850c95f7337d4df88b39b¢tsËAÚóü(D¨trace_idÙ$79d60741-62ce-48a1-bdd5-093b9ba0b31a­parent_run_idÙ$1befe0ba-9a4b-47fb-914f-6f77e2cec7b2¦run_idÙ$79d60741-62ce-48a1-bdd5-093b9ba0b31aªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚÁThe advantage of using `LangServe` with Runnables is that it allows for the deployment of configurable Runnables, enabling users to experiment with different parameters and models at runtime. This flexibility simplifies the process of adjusting settings, such as temperature in chat models, without needing to modify the underlying code. Additionally, it enhances the ability to compose complex chains using the LangChain Expression Language (LCEL).¨query_id¤Q069ªfocus_kind¶chain_execution_result‰¢idÙ@55cfc8008b3bd7b973fc9732a7d93db9c11971e1d015652fdc0c2a80f1f8e897¢tsËAÚóü(N­¨trace_idÙ$1befe0ba-9a4b-47fb-914f-6f77e2cec7b2­parent_run_idÀ¦run_idÙ$1befe0ba-9a4b-47fb-914f-6f77e2cec7b2ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ:What is the advantage of using `LangServe` with Runnables?§context•‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËWe often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::n...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Relate...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚÁThe advantage of using `LangServe` with Runnables is that it allows for the deployment of configurable Runnables, enabling users to experiment with different parameters and models at runtime. This flexibility simplifies the process of adjusting settings, such as temperature in chat models, without needing to modify the underlying code. Additionally, it enhances the ability to compose complex chains using the LangChain Expression Language (LCEL).¨query_id¤Q069ªfocus_kind¶chain_execution_result‹¢idÙ@f4f33cd447909c5586651fcd7c963ba45ebbba21c770d03a624893dd389ba8b5¢tsËAÚóü(c,¨trace_id¤Q070­parent_run_idÀ¦run_idÙ$e1e28aa9-4d76-4cc0-a6b2-e32c68353d6dªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ1Explain how ChatModels support multimodal inputs.¨query_id¤Q070§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q070ªfocus_kind¯chain_execution‹¢idÙ@3140828cbae486f5260962d38fc94c96320091cf2811045b2a14b2c8682820ae¢tsËAÚóü(Í$¨trace_id¤Q070­parent_run_idÙ$e1e28aa9-4d76-4cc0-a6b2-e32c68353d6d¦run_idÙ$ef21be67-2a53-4fe5-b10f-76312a5404c4ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ1Explain how ChatModels support multimodal inputs.§context¦answer ¨query_id¤Q070¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q070®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:baf3c06c-006e-aa29-c413-dbce5864ed39ªfocus_kind¯chain_execution‰¢idÙ@f07d11ef6eaefc159067752454c0de2634da534f912fd4e16148c6e7ef6b2f8c¢tsËAÚóü<¹í¨trace_idÙ$ef21be67-2a53-4fe5-b10f-76312a5404c4­parent_run_idÙ$e1e28aa9-4d76-4cc0-a6b2-e32c68353d6d¦run_idÙ$ef21be67-2a53-4fe5-b10f-76312a5404c4ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË:::info Pre-requisites
* [Chat models](/docs/concepts/chat_models)
* [Messages](/docs/concepts/messages)
:::
 
LangChain supports multimodal data as input to chat models:

1. Following provider-specif...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË# Multimodality

## Overview

**Multimodality** refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various compo...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË### What kind of multimodality is supported?

#### Inputs

Some models can accept multimodal inputs, such as images, audio, video, or files.
The types of multimodal inputs supported depend on the mode...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË# Chat models

## Overview

Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË#### Tools

Currently, no chat model is designed to work **directly** with multimodal data in a [tool call request](/docs/concepts/tool_calling) or [ToolMessage](/docs/concepts/tool_calling) result.

...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx¨query_id¤Q070ªfocus_kind¶chain_execution_result‹¢idÙ@beee561e7684af57f63882b5b0aceed24481b077cf14258ac3c9880fd6d3a15b¢tsËAÚóü<Ï-¨trace_id¤Q070­parent_run_idÙ$e1e28aa9-4d76-4cc0-a6b2-e32c68353d6d¦run_idÙ$795173ba-d235-4478-9d64-62567de5291cªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ1Explain how ChatModels support multimodal inputs.§context•‚´page_content_summaryÙË:::info Pre-requisites
* [Chat models](/docs/concepts/chat_models)
* [Messages](/docs/concepts/messages)
:::
 
LangChain supports multimodal data as input to chat models:

1. Following provider-specif...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË# Multimodality

## Overview

**Multimodality** refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various compo...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË### What kind of multimodality is supported?

#### Inputs

Some models can accept multimodal inputs, such as images, audio, video, or files.
The types of multimodal inputs supported depend on the mode...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË# Chat models

## Overview

Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË#### Tools

Currently, no chat model is designed to work **directly** with multimodal data in a [tool call request](/docs/concepts/tool_calling) or [ToolMessage](/docs/concepts/tool_calling) result.

...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx¦answer ¨query_id¤Q070¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q070®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:f24956d6-0702-45ff-9dd1-2a5c88d5a83eªfocus_kind¯chain_execution‹¢idÙ@2502d69bb18ef99c79c1530d66075fd10b4c3125ae28031db6eb09233ee7ea4c¢tsËAÚóü<Ö=¨trace_id¤Q070­parent_run_idÙ$795173ba-d235-4478-9d64-62567de5291c¦run_idÙ$2236df69-6a27-4831-a51f-150348f42a63ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ1Explain how ChatModels support multimodal inputs.§contextÚC:::info Pre-requisites
* [Chat models](/docs/concepts/chat_models)
* [Messages](/docs/concepts/messages)
:::
 
LangChain supports multimodal data as input to chat models:

1. Following provider-specific formats
2. Adhering to a cross-provider standard (see [how-to guides](/docs/how_to/#multimodal) for detail)

### How to use multimodal models

* Use the [chat model integration table](/docs/integrations/chat/) to identify which models support multimodality.
* Reference the [relevant how-to guides](/docs/how_to/#multimodal) for specific examples of how to use multimodal models.

### What kind of multimodality is supported?

#### Inputs

# Multimodality

## Overview

**Multimodality** refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various components, allowing models and systems to handle and process a mix of these data types seamlessly.

- **Chat Models**: These could, in theory, accept and generate multimodal inputs and outputs, handling a variety of data types like text, images, audio, and video.
- **Embedding Models**: Embedding Models can represent multimodal content, embedding various forms of dataâ€”such as text, images, and audioâ€”into vector spaces.
- **Vector Stores**: Vector stores could search over embeddings that represent multimodal data, enabling retrieval across different types of information.

## Multimodality in chat models

:::info Pre-requisites
* [Chat models](/docs/concepts/chat_models)
* [Messages](/docs/concepts/messages)
:::
 
LangChain supports multimodal data as input to chat models:

### What kind of multimodality is supported?

#### Inputs

Some models can accept multimodal inputs, such as images, audio, video, or files.
The types of multimodal inputs supported depend on the model provider. For instance,
[OpenAI](/docs/integrations/chat/openai/),
[Anthropic](/docs/integrations/chat/anthropic/), and
[Google Gemini](/docs/integrations/chat/google_generative_ai/)
support documents like PDFs as inputs.

The gist of passing multimodal inputs to a chat model is to use content blocks that
specify a type and corresponding data. For example, to pass an image to a chat model
as URL:

```python
from langchain_core.messages import HumanMessage

message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the weather in this image:"},
        {
            "type": "image",
            "source_type": "url",
            "url": "https://...",
        },
    ],
)
response = model.invoke([message])
```

We can also pass the image as in-line data:

# Chat models

## Overview

Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.

Modern LLMs are typically accessed through a chat model interface that takes a list of [messages](/docs/concepts/messages) as input and returns a [message](/docs/concepts/messages) as output.

The newest generation of chat models offer additional capabilities:

#### Tools

Currently, no chat model is designed to work **directly** with multimodal data in a [tool call request](/docs/concepts/tool_calling) or [ToolMessage](/docs/concepts/tool_calling) result.

However, a chat model can easily interact with multimodal data by invoking tools with references (e.g., a URL) to the multimodal data, rather than the data itself. For example, any model capable of [tool calling](/docs/concepts/tool_calling) can be equipped with tools to download and process images, audio, or video.

## Multimodality in embedding models

:::info Prerequisites
* [Embedding Models](/docs/concepts/embedding_models)
:::

**Embeddings** are vector representations of data used for tasks like similarity search and retrieval.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q070®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:f24956d6-0702-45ff-9dd1-2a5c88d5a83e­checkpoint_nsÙ-generate:f24956d6-0702-45ff-9dd1-2a5c88d5a83e¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@165a2ae4ce54a72cc85736f9c91c23ae64e34fc64d99a87b3fafa3eb5b26e272¢tsËAÚóü<ã½¨trace_idÙ$2236df69-6a27-4831-a51f-150348f42a63­parent_run_idÙ$795173ba-d235-4478-9d64-62567de5291c¦run_idÙ$2236df69-6a27-4831-a51f-150348f42a63ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ1messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Explain how ChatModels support multimodal inputs. \nContext: :::info Pre-requisites\n* [Chat models](/docs/concepts/chat_models)\n* [Messages](/docs/concepts/messages)\n:::\n \nLangChain supports multimodal data as input to chat models:\n\n1. Following provider-specific formats\n2. Adhering to a cross-provider standard (see [how-to guides](/docs/how_to/#multimodal) for detail)\n\n### How to use multimodal models\n\n* Use the [chat model integration table](/docs/integrations/chat/) to identify which models support multimodality.\n* Reference the [relevant how-to guides](/docs/how_to/#multimodal) for specific examples of how to use multimodal models.\n\n### What kind of multimodality is supported?\n\n#### Inputs\n\n# Multimodality\n\n## Overview\n\n**Multimodality** refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various components, allowing models and systems to handle and process a mix of these data types seamlessly.\n\n- **Chat Models**: These could, in theory, accept and generate multimodal inputs and outputs, handling a variety of data types like text, images, audio, and video.\n- **Embedding Models**: Embedding Models can represent multimodal content, embedding various forms of dataâ€”such as text, images, and audioâ€”into vector spaces.\n- **Vector Stores**: Vector stores could search over embeddings that represent multimodal data, enabling retrieval across different types of information.\n\n## Multimodality in chat models\n\n:::info Pre-requisites\n* [Chat models](/docs/concepts/chat_models)\n* [Messages](/docs/concepts/messages)\n:::\n \nLangChain supports multimodal data as input to chat models:\n\n### What kind of multimodality is supported?\n\n#### Inputs\n\nSome models can accept multimodal inputs, such as images, audio, video, or files.\nThe types of multimodal inputs supported depend on the model provider. For instance,\n[OpenAI](/docs/integrations/chat/openai/),\n[Anthropic](/docs/integrations/chat/anthropic/), and\n[Google Gemini](/docs/integrations/chat/google_generative_ai/)\nsupport documents like PDFs as inputs.\n\nThe gist of passing multimodal inputs to a chat model is to use content blocks that\nspecify a type and corresponding data. For example, to pass an image to a chat model\nas URL:\n\n```python\nfrom langchain_core.messages import HumanMessage\n\nmessage = HumanMessage(\n    content=[\n        {"type": "text", "text": "Describe the weather in this image:"},\n        {\n            "type": "image",\n            "source_type": "url",\n            "url": "https://...",\n        },\n    ],\n)\nresponse = model.invoke([message])\n```\n\nWe can also pass the image as in-line data:\n\n# Chat models\n\n## Overview\n\nLarge Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\n\nModern LLMs are typically accessed through a chat model interface that takes a list of [messages](/docs/concepts/messages) as input and returns a [message](/docs/concepts/messages) as output.\n\nThe newest generation of chat models offer additional capabilities:\n\n#### Tools\n\nCurrently, no chat model is designed to work **directly** with multimodal data in a [tool call request](/docs/concepts/tool_calling) or [ToolMessage](/docs/concepts/tool_calling) result.\n\nHowever, a chat model can easily interact with multimodal data by invoking tools with references (e.g., a URL) to the multimodal data, rather than the data itself. For example, any model capable of [tool calling](/docs/concepts/tool_calling) can be equipped with tools to download and process images, audio, or video.\n\n## Multimodality in embedding models\n\n:::info Prerequisites\n* [Embedding Models](/docs/concepts/embedding_models)\n:::\n\n**Embeddings** are vector representations of data used for tasks like similarity search and retrieval. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@7429d2380bf35ccefa54d63341272585217e14ca97165f70e737dd79cf8f086d¢tsËAÚóüœ*w¨focus_msÍĞ§payload‚¤role©assistant§contentÚ{Chat models support multimodal inputs by accepting various data types such as text, images, audio, and video, depending on the model provider. Users can pass these inputs using content blocks that specify the type and corresponding data, like URLs for images. For specific implementation details, users can refer to the integration tables and how-to guides provided by LangChain.ªfocus_kind¬exec_latency¨query_id¤Q070‰¢idÙ@72e6d08de7b55f69aac80ff01ae56f920a48ff35ea49fca60c93ecd204ead9e4¢tsËAÚóüœ@İ¨trace_idÙ$795173ba-d235-4478-9d64-62567de5291c­parent_run_idÙ$e1e28aa9-4d76-4cc0-a6b2-e32c68353d6d¦run_idÙ$795173ba-d235-4478-9d64-62567de5291cªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ{Chat models support multimodal inputs by accepting various data types such as text, images, audio, and video, depending on the model provider. Users can pass these inputs using content blocks that specify the type and corresponding data, like URLs for images. For specific implementation details, users can refer to the integration tables and how-to guides provided by LangChain.¨query_id¤Q070ªfocus_kind¶chain_execution_result‰¢idÙ@9e0bbd65533ccbf03bba37b9d53916bc73b2934601f69e960563403843c99f61¢tsËAÚóüœK¨trace_idÙ$e1e28aa9-4d76-4cc0-a6b2-e32c68353d6d­parent_run_idÀ¦run_idÙ$e1e28aa9-4d76-4cc0-a6b2-e32c68353d6dªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ1Explain how ChatModels support multimodal inputs.§context•‚´page_content_summaryÙË:::info Pre-requisites
* [Chat models](/docs/concepts/chat_models)
* [Messages](/docs/concepts/messages)
:::
 
LangChain supports multimodal data as input to chat models:

1. Following provider-specif...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË# Multimodality

## Overview

**Multimodality** refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various compo...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË### What kind of multimodality is supported?

#### Inputs

Some models can accept multimodal inputs, such as images, audio, video, or files.
The types of multimodal inputs supported depend on the mode...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË# Chat models

## Overview

Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË#### Tools

Currently, no chat model is designed to work **directly** with multimodal data in a [tool call request](/docs/concepts/tool_calling) or [ToolMessage](/docs/concepts/tool_calling) result.

...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx¦answerÚ{Chat models support multimodal inputs by accepting various data types such as text, images, audio, and video, depending on the model provider. Users can pass these inputs using content blocks that specify the type and corresponding data, like URLs for images. For specific implementation details, users can refer to the integration tables and how-to guides provided by LangChain.¨query_id¤Q070ªfocus_kind¶chain_execution_result‹¢idÙ@ddf5dee09d7abf58c67e84e597ae6c4cc2be812881c62373c50aeb6202f510b9¢tsËAÚóüœTÖ¨trace_id¤Q071­parent_run_idÀ¦run_idÙ$00b20a84-c28f-4a47-b78f-56f8a7543344ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙCWhat two steps are recommended to recover from ratelimiting errors?¨query_id¤Q071§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q071ªfocus_kind¯chain_execution‹¢idÙ@c88bc61ede2013637ffb06c7da1bb2cf9e938c0932a52b7400b84c18e44b61b8¢tsËAÚóüœ`¨trace_id¤Q071­parent_run_idÙ$00b20a84-c28f-4a47-b78f-56f8a7543344¦run_idÙ$ce2b83b7-8117-4c8d-9052-8596664ad25eªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙCWhat two steps are recommended to recover from ratelimiting errors?§context¦answer ¨query_id¤Q071¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q071®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:c4ec1a58-1193-856f-fd76-ba1d51c5cf0dªfocus_kind¯chain_execution‰¢idÙ@42e7a658f2c3e0a7d17b2c82f26ad8e4d933fe5da7d5d34265191e98dcc8b2d2¢tsËAÚóü¬§¨trace_idÙ$ce2b83b7-8117-4c8d-9052-8596664ad25e­parent_run_idÙ$00b20a84-c28f-4a47-b78f-56f8a7543344¦run_idÙ$ce2b83b7-8117-4c8d-9052-8596664ad25eªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙÊIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

You have a few options to deal with rate limits:...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË| [Step-back](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)     | When a higher-level conceptual understanding is required.                                 ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx¨query_id¤Q071ªfocus_kind¶chain_execution_result‹¢idÙ@37c6032b8b2d59b3542463aa0191f8bd2991cece16de6a1a7da7a752d5f90380¢tsËAÚóü¬£Ï¨trace_id¤Q071­parent_run_idÙ$00b20a84-c28f-4a47-b78f-56f8a7543344¦run_idÙ$fea756dc-df0a-4621-8f10-af7b896bd858ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙCWhat two steps are recommended to recover from ratelimiting errors?§context•‚´page_content_summaryÙÊIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

You have a few options to deal with rate limits:...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË| [Step-back](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)     | When a higher-level conceptual understanding is required.                                 ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx¦answer ¨query_id¤Q071¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q071®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:1923ba68-e756-44d2-7925-d57b8ff68eb1ªfocus_kind¯chain_execution‹¢idÙ@8572072d3dced92b8c2fff8080ca001481660ee4f2106fc1f13f8d379f72218f¢tsËAÚóü¬¬¹¨trace_id¤Q071­parent_run_idÙ$fea756dc-df0a-4621-8f10-af7b896bd858¦run_idÙ$db5b887f-f385-4d36-9e0a-07d6632c9783ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙCWhat two steps are recommended to recover from ratelimiting errors?§contextÚ‘If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

You have a few options to deal with rate limits:

1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.
2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information.

3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.

If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).

The size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.

## Advanced topics
 
### Rate-limiting

Many chat model providers impose a limit on the number of requests that can be made in a given time period.

If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

| [Step-back](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)     | When a higher-level conceptual understanding is required.                                       | First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question. [Paper](https://arxiv.org/pdf/2310.06117).                                            |
| [HyDE](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)          | If you have challenges retrieving relevant documents using the raw user inputs.                 | Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches. [Paper](https://arxiv.org/abs/2212.10496). |¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q071®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:1923ba68-e756-44d2-7925-d57b8ff68eb1­checkpoint_nsÙ-generate:1923ba68-e756-44d2-7925-d57b8ff68eb1¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@4021be3cdf9d7993ad2578a1f300876ba94eb801d3bc974eb2ee32d44ec63a5c¢tsËAÚóü¬³´¨trace_idÙ$db5b887f-f385-4d36-9e0a-07d6632c9783­parent_run_idÙ$fea756dc-df0a-4621-8f10-af7b896bd858¦run_idÙ$db5b887f-f385-4d36-9e0a-07d6632c9783ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚJmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What two steps are recommended to recover from ratelimiting errors? \nContext: If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n\nYou have a few options to deal with rate limits:\n\n1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.\n2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information.\n\n3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\n\nIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\n\nThe size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.\n\n## Advanced topics\n \n### Rate-limiting\n\nMany chat model providers impose a limit on the number of requests that can be made in a given time period.\n\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n\n| [Step-back](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)     | When a higher-level conceptual understanding is required.                                       | First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question. [Paper](https://arxiv.org/pdf/2310.06117).                                            |\n| [HyDE](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)          | If you have challenges retrieving relevant documents using the raw user inputs.                 | Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches. [Paper](https://arxiv.org/abs/2212.10496). | \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@121a649b6cfe76504dcbe3f6131e4e84c38702fc06da07f35cc72b23cbeadf21¢tsËAÚóı-DĞ¨focus_msÍØ§payload‚¤role©assistant§contentÙÿTo recover from rate limiting errors, you should wait a certain amount of time before retrying the request, increasing the wait time with each subsequent error. Additionally, you can use the `max_retries` parameter to control the number of retry attempts.ªfocus_kind¬exec_latency¨query_id¤Q071‰¢idÙ@b10ab222ad2e5db855cc7c1ae3f7dde68a0d8d1faef17ad23005299ae6a1e74b¢tsËAÚóı-XT¨trace_idÙ$fea756dc-df0a-4621-8f10-af7b896bd858­parent_run_idÙ$00b20a84-c28f-4a47-b78f-56f8a7543344¦run_idÙ$fea756dc-df0a-4621-8f10-af7b896bd858ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙÿTo recover from rate limiting errors, you should wait a certain amount of time before retrying the request, increasing the wait time with each subsequent error. Additionally, you can use the `max_retries` parameter to control the number of retry attempts.¨query_id¤Q071ªfocus_kind¶chain_execution_result‰¢idÙ@e1903984c4687cd373eca819c6e20cc97641550f8f88fc7c8a524fa579dfc8e0¢tsËAÚóı-j¨trace_idÙ$00b20a84-c28f-4a47-b78f-56f8a7543344­parent_run_idÀ¦run_idÙ$00b20a84-c28f-4a47-b78f-56f8a7543344ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙCWhat two steps are recommended to recover from ratelimiting errors?§context•‚´page_content_summaryÙÊIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

You have a few options to deal with rate limits:...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË| [Step-back](https://github.com/langchain-ai/rag-from-scratch/blob/main/rag_from_scratch_5_to_9.ipynb)     | When a higher-level conceptual understanding is required.                                 ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx¦answerÙÿTo recover from rate limiting errors, you should wait a certain amount of time before retrying the request, increasing the wait time with each subsequent error. Additionally, you can use the `max_retries` parameter to control the number of retry attempts.¨query_id¤Q071ªfocus_kind¶chain_execution_result‹¢idÙ@e1a746fcf28083cab1b41583765eed9e908f246d22d140ce31243fada51ee5f4¢tsËAÚóı-ti¨trace_id¤Q072­parent_run_idÀ¦run_idÙ$813e218b-9551-4689-bfcf-2055b3c0004cªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙCWhich parameter allows a Runnable to stream events as JSON for UIs?¨query_id¤Q072§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q072ªfocus_kind¯chain_execution‹¢idÙ@96bc43bb54156ae623c23f2b5b0abdb3a9897fa1d0e4de6a1eb0aaddc1f51492¢tsËAÚóı-}ê¨trace_id¤Q072­parent_run_idÙ$813e218b-9551-4689-bfcf-2055b3c0004c¦run_idÙ$e4588fe8-a9ac-482a-bdb6-0d2fb82bf357ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙCWhich parameter allows a Runnable to stream events as JSON for UIs?§context¦answer ¨query_id¤Q072¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q072®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:ab996a56-c1b7-536c-1ca5-c57cffd4cbeeªfocus_kind¯chain_execution‰¢idÙ@6acddc94b1f2f76128b07f59cde52c725c42a19ccd086e5aeeeacbfd405010dc¢tsËAÚóı8FR¨trace_idÙ$e4588fe8-a9ac-482a-bdb6-0d2fb82bf357­parent_run_idÙ$813e218b-9551-4689-bfcf-2055b3c0004c¦run_idÙ$e4588fe8-a9ac-482a-bdb6-0d2fb82bf357ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Streaming APIs

LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), includin...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË- **In LCEL Pipelines:**
Streaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙÍ#### Usage with LCEL

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the outp...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q072ªfocus_kind¶chain_execution_result‹¢idÙ@09161f976ef6535bd5c543d32fc49e46c7ad270151b538ef09d893ac1ab7568f¢tsËAÚóı9DE¨trace_id¤Q072­parent_run_idÙ$813e218b-9551-4689-bfcf-2055b3c0004c¦run_idÙ$aa081041-43ec-4e4a-8e88-ddcb8f1e980cªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙCWhich parameter allows a Runnable to stream events as JSON for UIs?§context•‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Streaming APIs

LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), includin...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË- **In LCEL Pipelines:**
Streaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙÍ#### Usage with LCEL

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the outp...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answer ¨query_id¤Q072¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q072®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:186fcea0-ac52-405d-03ac-381eac658d40ªfocus_kind¯chain_execution‹¢idÙ@5f74ca5429059fbf7c185b41a819b0c005c9b1d4ead284cfa67213b47d897d86¢tsËAÚóı9TX¨trace_id¤Q072­parent_run_idÙ$aa081041-43ec-4e4a-8e88-ddcb8f1e980c¦run_idÙ$91682be0-9dfc-4573-90ce-8924dd119aedªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙCWhich parameter allows a Runnable to stream events as JSON for UIs?§contextÚò## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.
* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.
* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.
* Inspected: Schematic information about Runnable's input, output, and configuration can be accessed.
* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.

Please review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.

<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

## Streaming APIs

LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), including [LLMs](/docs/concepts/chat_models), [compiled LangGraph graphs](https://langchain-ai.github.io/langgraph/concepts/low_level/), and any Runnable generated with [LCEL](/docs/concepts/lcel).

- **In LCEL Pipelines:**
Streaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of the pipeline execute, you can stream which sub-runnable is currently running, providing real-time insight into the overall pipeline's progress.

Streaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.

### 3. Streaming custom data

#### Usage with LCEL

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the output of the last step in the chain. This allows the final processed result to be streamed incrementally. **LCEL** tries to optimize streaming latency in pipelines so that the streaming results from the last step are available as soon as possible.



### `astream_events`
<span data-heading-keywords="astream_events,stream_events,stream events"></span>

:::tip
Use the `astream_events` API to access custom data and intermediate outputs from LLM applications built entirely with [LCEL](/docs/concepts/lcel). 

While this API is available for use with [LangGraph](/docs/concepts/architecture#langgraph) as well, it is usually not necessary when working with LangGraph, as the `stream` and `astream` methods provide comprehensive streaming capabilities for LangGraph graphs.
:::

## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
and is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.
:::

Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.

To simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q072®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:186fcea0-ac52-405d-03ac-381eac658d40­checkpoint_nsÙ-generate:186fcea0-ac52-405d-03ac-381eac658d40¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@beeaa607bcea43c3f561d4657b7c42ce0442a47c98a245f77d120f749a652e88¢tsËAÚóı9€^¨trace_idÙ$91682be0-9dfc-4573-90ce-8924dd119aed­parent_run_idÙ$aa081041-43ec-4e4a-8e88-ddcb8f1e980c¦run_idÙ$91682be0-9dfc-4573-90ce-8924dd119aedªevent_type¬chain_output¬event_source¥chain§payload§outputsÚÌmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Which parameter allows a Runnable to stream events as JSON for UIs? \nContext: ## Overview of runnable interface\n\nThe Runnable way defines a standard interface that allows a Runnable component to be:\n\n* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.\n* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.\n* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.\n* Inspected: Schematic information about Runnable\'s input, output, and configuration can be accessed.\n* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.\n\nPlease review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.\n\n<a id="batch"></a>\n### Optimized parallel execution (batch)\n<span data-heading-keywords="batch"></span>\n\n## Streaming APIs\n\nLangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), including [LLMs](/docs/concepts/chat_models), [compiled LangGraph graphs](https://langchain-ai.github.io/langgraph/concepts/low_level/), and any Runnable generated with [LCEL](/docs/concepts/lcel).\n\n- **In LCEL Pipelines:**\nStreaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of the pipeline execute, you can stream which sub-runnable is currently running, providing real-time insight into the overall pipeline\'s progress.\n\nStreaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.\n\n### 3. Streaming custom data\n\n#### Usage with LCEL\n\nIf you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the output of the last step in the chain. This allows the final processed result to be streamed incrementally. **LCEL** tries to optimize streaming latency in pipelines so that the streaming results from the last step are available as soon as possible.\n\n\n\n### `astream_events`\n<span data-heading-keywords="astream_events,stream_events,stream events"></span>\n\n:::tip\nUse the `astream_events` API to access custom data and intermediate outputs from LLM applications built entirely with [LCEL](/docs/concepts/lcel). \n\nWhile this API is available for use with [LangGraph](/docs/concepts/architecture#langgraph) as well, it is usually not necessary when working with LangGraph, as the `stream` and `astream` methods provide comprehensive streaming capabilities for LangGraph graphs.\n:::\n\n## Configurable runnables\n\n:::note\nThis is an advanced feature that is unnecessary for most users.\n\nIt helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)\nand is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.\n:::\n\nSometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.\n\nTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime: \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@119a213f629ef386b4ddaf5a7ad8b004501a5ce932337fab601faea415b613e9¢tsËAÚóı´£ß¨focus_msÍc§payload‚¤role©assistant§contentÚQThe parameter that allows a Runnable to stream events as JSON for UIs is the `astream_events` API. This API provides access to custom data and intermediate outputs from applications built with the LangChain Expression Language (LCEL). It enables real-time streaming of results, enhancing user experience by providing incremental updates.ªfocus_kind¬exec_latency¨query_id¤Q072‰¢idÙ@42a1e3982ae36c6e9f7e8668c09c31b5f8431f98ae754c8c735eb431f08e27aa¢tsËAÚóı´·–¨trace_idÙ$aa081041-43ec-4e4a-8e88-ddcb8f1e980c­parent_run_idÙ$813e218b-9551-4689-bfcf-2055b3c0004c¦run_idÙ$aa081041-43ec-4e4a-8e88-ddcb8f1e980cªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚQThe parameter that allows a Runnable to stream events as JSON for UIs is the `astream_events` API. This API provides access to custom data and intermediate outputs from applications built with the LangChain Expression Language (LCEL). It enables real-time streaming of results, enhancing user experience by providing incremental updates.¨query_id¤Q072ªfocus_kind¶chain_execution_result‰¢idÙ@decbf6ae39895c91c51491acaff0a0676d4e070eebdf78ddd186400bc6faebab¢tsËAÚóı´Âü¨trace_idÙ$813e218b-9551-4689-bfcf-2055b3c0004c­parent_run_idÀ¦run_idÙ$813e218b-9551-4689-bfcf-2055b3c0004cªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙCWhich parameter allows a Runnable to stream events as JSON for UIs?§context•‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Streaming APIs

LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), includin...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË- **In LCEL Pipelines:**
Streaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙÍ#### Usage with LCEL

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the outp...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚQThe parameter that allows a Runnable to stream events as JSON for UIs is the `astream_events` API. This API provides access to custom data and intermediate outputs from applications built with the LangChain Expression Language (LCEL). It enables real-time streaming of results, enhancing user experience by providing incremental updates.¨query_id¤Q072ªfocus_kind¶chain_execution_result‹¢idÙ@9eb23508d9dae43a1c018180f7e3cfd2ba6f484dccf2024a7bff0544206cfeb1¢tsËAÚóı´Ñ‰¨trace_id¤Q073­parent_run_idÀ¦run_idÙ$aacf3478-9438-46df-81ab-57eaf336bac8ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙDProvide a code snippet to embed text with a local HuggingFace model.¨query_id¤Q073§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q073ªfocus_kind¯chain_execution‹¢idÙ@85fb8428946a0a03cad70e9a238763c44cc70ff0c3ce62a41bd786e80a050998¢tsËAÚóı´İÊ¨trace_id¤Q073­parent_run_idÙ$aacf3478-9438-46df-81ab-57eaf336bac8¦run_idÙ$4e749d33-60ae-4d2d-a416-220b479fb6b2ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙDProvide a code snippet to embed text with a local HuggingFace model.§context¦answer ¨query_id¤Q073¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q073®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:ae13cb37-c3cc-90b3-9251-1b29c629e953ªfocus_kind¯chain_execution‰¢idÙ@4cb1e8980088ff5c2afc854fb7a4b619d6d57393b6e65942d5e64a183b4b5672¢tsËAÚóıÈU‚¨trace_idÙ$4e749d33-60ae-4d2d-a416-220b479fb6b2­parent_run_idÙ$aacf3478-9438-46df-81ab-57eaf336bac8¦run_idÙ$4e749d33-60ae-4d2d-a416-220b479fb6b2ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Embedding models
<span data-heading-keywords="embedding,embeddings"></span>

:::info[Prerequisites]

* [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents....¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË# Text embedding models

:::info
Head to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers.
:::

The Embeddings class is...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/embed_text.mdx‚´page_content_summaryÙË<EmbeddingTabs customVarName="embeddings_model" />

### `embed_documents`
#### Embed list of texts

Use `.embed_documents` to embed a list of strings, recovering a list of embeddings:

```python
embed...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/embed_text.mdx‚´page_content_summaryÙË- `embed_documents`: For embedding multiple texts (documents)
- `embed_query`: For embedding a single text (query)

This distinction is important, as some providers employ different embedding strategi...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË```python
from langchain_core.messages import HumanMessage

message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the weather in this image:"},
        {"type": "image_url",...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx¨query_id¤Q073ªfocus_kind¶chain_execution_result‹¢idÙ@945770327a65c61e39d01d1bafb61fa0733efddc590b0696d03756be6eee61ca¢tsËAÚóıÈc‘¨trace_id¤Q073­parent_run_idÙ$aacf3478-9438-46df-81ab-57eaf336bac8¦run_idÙ$be6428ae-773e-4044-ac7c-aeb3f44737c6ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙDProvide a code snippet to embed text with a local HuggingFace model.§context•‚´page_content_summaryÙË# Embedding models
<span data-heading-keywords="embedding,embeddings"></span>

:::info[Prerequisites]

* [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents....¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË# Text embedding models

:::info
Head to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers.
:::

The Embeddings class is...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/embed_text.mdx‚´page_content_summaryÙË<EmbeddingTabs customVarName="embeddings_model" />

### `embed_documents`
#### Embed list of texts

Use `.embed_documents` to embed a list of strings, recovering a list of embeddings:

```python
embed...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/embed_text.mdx‚´page_content_summaryÙË- `embed_documents`: For embedding multiple texts (documents)
- `embed_query`: For embedding a single text (query)

This distinction is important, as some providers employ different embedding strategi...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË```python
from langchain_core.messages import HumanMessage

message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the weather in this image:"},
        {"type": "image_url",...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx¦answer ¨query_id¤Q073¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q073®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:d8b97d26-0e8f-1005-8c46-b8b74e0b10ffªfocus_kind¯chain_execution‹¢idÙ@2cdff8580c71c77ca4d672ff27ad98e02912402356edb61815940e404b171218¢tsËAÚóıÈiØ¨trace_id¤Q073­parent_run_idÙ$be6428ae-773e-4044-ac7c-aeb3f44737c6¦run_idÙ$ae8fe492-f173-4b87-9c42-059f08634980ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙDProvide a code snippet to embed text with a local HuggingFace model.§contextÚ# Embedding models
<span data-heading-keywords="embedding,embeddings"></span>

:::info[Prerequisites]

* [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)

:::

:::info[Note]
This conceptual overview focuses on text-based embedding models.

Embedding models can also be [multimodal](/docs/concepts/multimodality) though such models are not currently supported by LangChain.
:::

# Text embedding models

:::info
Head to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers.
:::

The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.

Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.

<EmbeddingTabs customVarName="embeddings_model" />

### `embed_documents`
#### Embed list of texts

Use `.embed_documents` to embed a list of strings, recovering a list of embeddings:

```python
embeddings = embeddings_model.embed_documents(
    [
        "Hi there!",
        "Oh, hello!",
        "What's your name?",
        "My friends call me World",
        "Hello World!"
    ]
)
len(embeddings), len(embeddings[0])
```

```output
(5, 1536)
```


### `embed_query`
#### Embed single query
Use `.embed_query` to embed a single piece of text (e.g., for the purpose of comparing to other embedded pieces of texts).

```python
embedded_query = embeddings_model.embed_query("What was the name mentioned in the conversation?")
embedded_query[:5]
```

```output
[0.0053587136790156364,
 -0.0004999046213924885,
 0.038883671164512634,
 -0.003001077566295862,
 -0.00900818221271038]
```

- `embed_documents`: For embedding multiple texts (documents)
- `embed_query`: For embedding a single text (query)

This distinction is important, as some providers employ different embedding strategies for documents (which are to be searched) versus queries (the search input itself).
To illustrate, here's a practical example using LangChain's `.embed_documents` method to embed a list of strings:

```python
from langchain_openai import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings()
embeddings = embeddings_model.embed_documents(
    [
        "Hi there!",
        "Oh, hello!",
        "What's your name?",
        "My friends call me World",
        "Hello World!"
    ]
)
len(embeddings), len(embeddings[0])
(5, 1536)
```

For convenience, you can also use the `embed_query` method to embed a single text:

```python
query_embedding = embeddings_model.embed_query("What is the meaning of life?")
```

:::info[Further reading]

```python
from langchain_core.messages import HumanMessage

message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the weather in this image:"},
        {"type": "image_url", "image_url": {"url": image_url}},
    ],
)
response = model.invoke([message])
```

Otherwise, chat models will typically accept the native, provider-specific content
block format. See [chat model integrations](/docs/integrations/chat/) for detail
on specific providers.


#### Outputs

Some chat models support multimodal outputs, such as images and audio. Multimodal
outputs will appear as part of the [AIMessage](/docs/concepts/messages/#aimessage)
response object. See for example:

- Generating [audio outputs](/docs/integrations/chat/openai/#audio-generation-preview) with OpenAI;
- Generating [image outputs](/docs/integrations/chat/google_generative_ai/#multimodal-usage) with Google Gemini.

#### Tools¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q073®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:d8b97d26-0e8f-1005-8c46-b8b74e0b10ff­checkpoint_nsÙ-generate:d8b97d26-0e8f-1005-8c46-b8b74e0b10ff¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@fa14c0e6c2c4fe4a48c60e997018e6847385da8be7ce73584f400e744cafee85¢tsËAÚóıÈp'¨trace_idÙ$ae8fe492-f173-4b87-9c42-059f08634980­parent_run_idÙ$be6428ae-773e-4044-ac7c-aeb3f44737c6¦run_idÙ$ae8fe492-f173-4b87-9c42-059f08634980ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ.messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Provide a code snippet to embed text with a local HuggingFace model. \nContext: # Embedding models\n<span data-heading-keywords="embedding,embeddings"></span>\n\n:::info[Prerequisites]\n\n* [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html)\n\n:::\n\n:::info[Note]\nThis conceptual overview focuses on text-based embedding models.\n\nEmbedding models can also be [multimodal](/docs/concepts/multimodality) though such models are not currently supported by LangChain.\n:::\n\n# Text embedding models\n\n:::info\nHead to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers.\n:::\n\nThe Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.\n\nEmbeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.\n\n<EmbeddingTabs customVarName="embeddings_model" />\n\n### `embed_documents`\n#### Embed list of texts\n\nUse `.embed_documents` to embed a list of strings, recovering a list of embeddings:\n\n```python\nembeddings = embeddings_model.embed_documents(\n    [\n        "Hi there!",\n        "Oh, hello!",\n        "What\'s your name?",\n        "My friends call me World",\n        "Hello World!"\n    ]\n)\nlen(embeddings), len(embeddings[0])\n```\n\n```output\n(5, 1536)\n```\n\n\n### `embed_query`\n#### Embed single query\nUse `.embed_query` to embed a single piece of text (e.g., for the purpose of comparing to other embedded pieces of texts).\n\n```python\nembedded_query = embeddings_model.embed_query("What was the name mentioned in the conversation?")\nembedded_query[:5]\n```\n\n```output\n[0.0053587136790156364,\n -0.0004999046213924885,\n 0.038883671164512634,\n -0.003001077566295862,\n -0.00900818221271038]\n```\n\n- `embed_documents`: For embedding multiple texts (documents)\n- `embed_query`: For embedding a single text (query)\n\nThis distinction is important, as some providers employ different embedding strategies for documents (which are to be searched) versus queries (the search input itself).\nTo illustrate, here\'s a practical example using LangChain\'s `.embed_documents` method to embed a list of strings:\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nembeddings_model = OpenAIEmbeddings()\nembeddings = embeddings_model.embed_documents(\n    [\n        "Hi there!",\n        "Oh, hello!",\n        "What\'s your name?",\n        "My friends call me World",\n        "Hello World!"\n    ]\n)\nlen(embeddings), len(embeddings[0])\n(5, 1536)\n```\n\nFor convenience, you can also use the `embed_query` method to embed a single text:\n\n```python\nquery_embedding = embeddings_model.embed_query("What is the meaning of life?")\n```\n\n:::info[Further reading]\n\n```python\nfrom langchain_core.messages import HumanMessage\n\nmessage = HumanMessage(\n    content=[\n        {"type": "text", "text": "Describe the weather in this image:"},\n        {"type": "image_url", "image_url": {"url": image_url}},\n    ],\n)\nresponse = model.invoke([message])\n```\n\nOtherwise, chat models will typically accept the native, provider-specific content\nblock format. See [chat model integrations](/docs/integrations/chat/) for detail\non specific providers.\n\n\n#### Outputs\n\nSome chat models support multimodal outputs, such as images and audio. Multimodal\noutputs will appear as part of the [AIMessage](/docs/concepts/messages/#aimessage)\nresponse object. See for example:\n\n- Generating [audio outputs](/docs/integrations/chat/openai/#audio-generation-preview) with OpenAI;\n- Generating [image outputs](/docs/integrations/chat/google_generative_ai/#multimodal-usage) with Google Gemini.\n\n#### Tools \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@686b59614cd48f1ef91b69140340f9ac37679844d6d1b4e365e8ef341b87c483¢tsËAÚóş8Bl¨focus_msÍÓ§payload‚¤role©assistant§contentÚQTo embed text using a local Hugging Face model, you can use the following code snippet:

```python
from langchain_huggingface import HuggingFaceEmbeddings
embeddings_model = HuggingFaceEmbeddings()
embeddings = embeddings_model.embed_documents(["Hi there!", "Oh, hello!"])
```

This will create embeddings for the provided list of texts.ªfocus_kind¬exec_latency¨query_id¤Q073‰¢idÙ@3ade10a5d719a5c6c1368dfe113745898dd68962a0735780624c591d4633c074¢tsËAÚóş8UX¨trace_idÙ$be6428ae-773e-4044-ac7c-aeb3f44737c6­parent_run_idÙ$aacf3478-9438-46df-81ab-57eaf336bac8¦run_idÙ$be6428ae-773e-4044-ac7c-aeb3f44737c6ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚQTo embed text using a local Hugging Face model, you can use the following code snippet:

```python
from langchain_huggingface import HuggingFaceEmbeddings
embeddings_model = HuggingFaceEmbeddings()
embeddings = embeddings_model.embed_documents(["Hi there!", "Oh, hello!"])
```

This will create embeddings for the provided list of texts.¨query_id¤Q073ªfocus_kind¶chain_execution_result‰¢idÙ@dcb5fd36edaeac955cbdb58ca84c86ff1a85537e601eeedb520109234b346f81¢tsËAÚóş8a¨trace_idÙ$aacf3478-9438-46df-81ab-57eaf336bac8­parent_run_idÀ¦run_idÙ$aacf3478-9438-46df-81ab-57eaf336bac8ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙDProvide a code snippet to embed text with a local HuggingFace model.§context•‚´page_content_summaryÙË# Embedding models
<span data-heading-keywords="embedding,embeddings"></span>

:::info[Prerequisites]

* [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents....¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË# Text embedding models

:::info
Head to [Integrations](/docs/integrations/text_embedding/) for documentation on built-in integrations with text embedding model providers.
:::

The Embeddings class is...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/embed_text.mdx‚´page_content_summaryÙË<EmbeddingTabs customVarName="embeddings_model" />

### `embed_documents`
#### Embed list of texts

Use `.embed_documents` to embed a list of strings, recovering a list of embeddings:

```python
embed...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/embed_text.mdx‚´page_content_summaryÙË- `embed_documents`: For embedding multiple texts (documents)
- `embed_query`: For embedding a single text (query)

This distinction is important, as some providers employ different embedding strategi...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË```python
from langchain_core.messages import HumanMessage

message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the weather in this image:"},
        {"type": "image_url",...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx¦answerÚQTo embed text using a local Hugging Face model, you can use the following code snippet:

```python
from langchain_huggingface import HuggingFaceEmbeddings
embeddings_model = HuggingFaceEmbeddings()
embeddings = embeddings_model.embed_documents(["Hi there!", "Oh, hello!"])
```

This will create embeddings for the provided list of texts.¨query_id¤Q073ªfocus_kind¶chain_execution_result‹¢idÙ@d0cfcaf050a344ecac353378f47560a10513edb625c0758442c51f26c6a87381¢tsËAÚóş8o ¨trace_id¤Q074­parent_run_idÀ¦run_idÙ$159ee442-048f-442f-b65a-72f4a202d8b0ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙEHow does the `ContextualCompressionRetriever` reduce context windows?¨query_id¤Q074§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q074ªfocus_kind¯chain_execution‹¢idÙ@e20497032b541046de6388802aab0d91c065ac5babf91a0b38c8afc9ef9a1f44¢tsËAÚóş8ƒÓ¨trace_id¤Q074­parent_run_idÙ$159ee442-048f-442f-b65a-72f4a202d8b0¦run_idÙ$09b39e52-9acc-47fc-b71b-5436b387a8faªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙEHow does the `ContextualCompressionRetriever` reduce context windows?§context¦answer ¨query_id¤Q074¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q074®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:c808a1c9-1a13-898d-d209-c443affb9581ªfocus_kind¯chain_execution‰¢idÙ@31b1a50a28fed9ac0c84d69ebc970bfe34faf06618e018b5039ad470a2bfb87e¢tsËAÚóşIÃ	¨trace_idÙ$09b39e52-9acc-47fc-b71b-5436b387a8fa­parent_run_idÙ$159ee442-048f-442f-b65a-72f4a202d8b0¦run_idÙ$09b39e52-9acc-47fc-b71b-5436b387a8faªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small ...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextu...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we comb...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¨query_id¤Q074ªfocus_kind¶chain_execution_result‹¢idÙ@5c915dfedc0ebe00a70d465e646bb4243097163fd89c855332432ea0069df7e7¢tsËAÚóşIÑ¨trace_id¤Q074­parent_run_idÙ$159ee442-048f-442f-b65a-72f4a202d8b0¦run_idÙ$46cfbe89-f22f-4a3a-a10e-7d67b7ac3d8bªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙEHow does the `ContextualCompressionRetriever` reduce context windows?§context•‚´page_content_summaryÙË![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small ...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextu...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we comb...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¦answer ¨query_id¤Q074¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q074®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:5627dbb7-e4f4-1b7d-f542-330f7c1ddec3ªfocus_kind¯chain_execution‹¢idÙ@f78a2f41fe75c3cd00360d1369017d5dbc1a23f29a55c7ac6eec090387616a4d¢tsËAÚóşIÙs¨trace_id¤Q074­parent_run_idÙ$46cfbe89-f22f-4a3a-a10e-7d67b7ac3d8b¦run_idÙ$182e3a91-afd4-4297-9598-e1c300f1c121ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙEHow does the `ContextualCompressionRetriever` reduce context windows?§contextÚº![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small chunk size for indexing documents in a vectorstore. 
If you return *only* the chunks as the retrieval result, then the model will have lost the original document context for the chunks. 

LangChain has two different retrievers that can be used to address this challenge. 
The [Multi-Vector](/docs/how_to/multi_vector/) retriever allows the user to use any document transformation (e.g., use an LLM to write a summary of the document) for indexing while retaining linkage to the source document. 
The [ParentDocument](/docs/how_to/parent_document_retriever/) retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.

## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. Please read more about
the technique in the [structured outputs](/docs/concepts/structured_outputs) guide.

## Multimodality

Large Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as [multimodality](/docs/concepts/multimodality).

Currently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.

## Context window

A chat model's context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.

- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)
- [How to: write a custom retriever class](/docs/how_to/custom_retriever)
- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)
- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)
- [How to: reorder retrieved results to mitigate the "lost in the middle" effect](/docs/how_to/long_context_reorder)
- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)
- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)
- [How to: generate metadata filters](/docs/how_to/self_query)
- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)

If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).

The size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.

## Advanced topics
 
### Rate-limiting

Many chat model providers impose a limit on the number of requests that can be made in a given time period.

If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we combine search results from many retrievers? 
This motivates the concept of re-ranking, which takes the output of multiple retrievers and combines them using a more sophisticated algorithm such as [Reciprocal Rank Fusion (RRF)](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf).

### Source document retention 

Many retrievers utilize some kind of index to make documents easily searchable.
The process of indexing can include a transformation step (e.g., vectorstores often use document splitting). 
Whatever transformation is used, can be very useful to retain a link between the *transformed document* and the original, giving the retriever the ability to return the *original* document.

![Retrieval with full docs](/img/retriever_full_docs.png)¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q074®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:5627dbb7-e4f4-1b7d-f542-330f7c1ddec3­checkpoint_nsÙ-generate:5627dbb7-e4f4-1b7d-f542-330f7c1ddec3¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@a3e3bc2f52042201f8679839a78dc9b270821301e488a704a5f66bc21801a0b7¢tsËAÚóşIäû¨trace_idÙ$182e3a91-afd4-4297-9598-e1c300f1c121­parent_run_idÙ$46cfbe89-f22f-4a3a-a10e-7d67b7ac3d8b¦run_idÙ$182e3a91-afd4-4297-9598-e1c300f1c121ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ¢messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: How does the `ContextualCompressionRetriever` reduce context windows? \nContext: ![Retrieval with full docs](/img/retriever_full_docs.png)\n\nThis is particularly useful in AI applications, because it ensures no loss in document context for the model.\nFor example, you may use small chunk size for indexing documents in a vectorstore. \nIf you return *only* the chunks as the retrieval result, then the model will have lost the original document context for the chunks. \n\nLangChain has two different retrievers that can be used to address this challenge. \nThe [Multi-Vector](/docs/how_to/multi_vector/) retriever allows the user to use any document transformation (e.g., use an LLM to write a summary of the document) for indexing while retaining linkage to the source document. \nThe [ParentDocument](/docs/how_to/parent_document_retriever/) retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.\n\n## Structured outputs\n\nChat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely\nuseful for information extraction tasks. Please read more about\nthe technique in the [structured outputs](/docs/concepts/structured_outputs) guide.\n\n## Multimodality\n\nLarge Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as [multimodality](/docs/concepts/multimodality).\n\nCurrently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.\n\n## Context window\n\nA chat model\'s context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.\n\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\n- [How to: reorder retrieved results to mitigate the "lost in the middle" effect](/docs/how_to/long_context_reorder)\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\n- [How to: generate metadata filters](/docs/how_to/self_query)\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\n\nIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\n\nThe size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.\n\n## Advanced topics\n \n### Rate-limiting\n\nMany chat model providers impose a limit on the number of requests that can be made in a given time period.\n\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n\n```python\n# Initialize the ensemble retriever\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]\n)\n```\n\nWhen ensembling, how do we combine search results from many retrievers? \nThis motivates the concept of re-ranking, which takes the output of multiple retrievers and combines them using a more sophisticated algorithm such as [Reciprocal Rank Fusion (RRF)](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf).\n\n### Source document retention \n\nMany retrievers utilize some kind of index to make documents easily searchable.\nThe process of indexing can include a transformation step (e.g., vectorstores often use document splitting). \nWhatever transformation is used, can be very useful to retain a link between the *transformed document* and the original, giving the retriever the ability to return the *original* document.\n\n![Retrieval with full docs](/img/retriever_full_docs.png) \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@71d2717df1367dfd83a568828ba529d04d391112f361e7e9f5bd8c11e3d0af59¢tsËAÚóş›ä¨focus_msÍú§payload‚¤role©assistant§contentÚ–The `ContextualCompressionRetriever` reduces context windows by allowing the use of smaller chunk sizes for indexing documents while retaining the linkage to the original document. This ensures that even when only chunks are retrieved, the model does not lose the overall context of the document. By maintaining this connection, it effectively compresses the data without sacrificing important information.ªfocus_kind¬exec_latency¨query_id¤Q074‰¢idÙ@da36c7db26a9f37da859b702ba74fbe58a68a52dc08406b3b33f097092e7f083¢tsËAÚóş›‘>¨trace_idÙ$46cfbe89-f22f-4a3a-a10e-7d67b7ac3d8b­parent_run_idÙ$159ee442-048f-442f-b65a-72f4a202d8b0¦run_idÙ$46cfbe89-f22f-4a3a-a10e-7d67b7ac3d8bªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ–The `ContextualCompressionRetriever` reduces context windows by allowing the use of smaller chunk sizes for indexing documents while retaining the linkage to the original document. This ensures that even when only chunks are retrieved, the model does not lose the overall context of the document. By maintaining this connection, it effectively compresses the data without sacrificing important information.¨query_id¤Q074ªfocus_kind¶chain_execution_result‰¢idÙ@245034e5898f606008e31ce79621cd96e712f0391918b8aaa0581c74f225f98c¢tsËAÚóş›™o¨trace_idÙ$159ee442-048f-442f-b65a-72f4a202d8b0­parent_run_idÀ¦run_idÙ$159ee442-048f-442f-b65a-72f4a202d8b0ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙEHow does the `ContextualCompressionRetriever` reduce context windows?§context•‚´page_content_summaryÙË![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small ...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextu...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we comb...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¦answerÚ–The `ContextualCompressionRetriever` reduces context windows by allowing the use of smaller chunk sizes for indexing documents while retaining the linkage to the original document. This ensures that even when only chunks are retrieved, the model does not lose the overall context of the document. By maintaining this connection, it effectively compresses the data without sacrificing important information.¨query_id¤Q074ªfocus_kind¶chain_execution_result‹¢idÙ@bc1cbc47103e3692242f43c995818d6cad41fcb78b014b5ecb1a5b104a770ab8¢tsËAÚóş›ŸË¨trace_id¤Q075­parent_run_idÀ¦run_idÙ$c4e37596-b531-4329-8917-c6960523ffaaªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙWWhat guide should you consult for configuring runtime chain internals like temperature?¨query_id¤Q075§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q075ªfocus_kind¯chain_execution‹¢idÙ@9dd4647d835936b6c668f652d5de9e4836cd83ee0f2e827e1d8f74b94af06f50¢tsËAÚóş›¨~¨trace_id¤Q075­parent_run_idÙ$c4e37596-b531-4329-8917-c6960523ffaa¦run_idÙ$7dfb0f08-46f3-4eaa-a62f-9e74fc813d84ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙWWhat guide should you consult for configuring runtime chain internals like temperature?§context¦answer ¨query_id¤Q075¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q075®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:6c27a5ff-0726-ba37-f0f0-309924f22ed1ªfocus_kind¯chain_execution‰¢idÙ@df683d5e8ef2994ac644a1315fcbcbcc034abc73610bb72ae2b9ba84fbd0c226¢tsËAÚóşÓRó¨trace_idÙ$7dfb0f08-46f3-4eaa-a62f-9e74fc813d84­parent_run_idÙ$c4e37596-b531-4329-8917-c6960523ffaa¦run_idÙ$7dfb0f08-46f3-4eaa-a62f-9e74fc813d84ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).

For guides on how to do specific tasks with LCEL, check...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

* `configurable_fields`: This method allows you to configure specific **attribute...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocati...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËRefer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.

## Orchestration

Get started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble La...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q075ªfocus_kind¶chain_execution_result‹¢idÙ@2536c7cf3eee7f4737cc0b4a5d139a3cc18bbc48448552e5f9af2ea966edfa41¢tsËAÚóşÓZ¨trace_id¤Q075­parent_run_idÙ$c4e37596-b531-4329-8917-c6960523ffaa¦run_idÙ$f6f766a1-681c-4ae3-b312-037bc3626afbªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙWWhat guide should you consult for configuring runtime chain internals like temperature?§context•‚´page_content_summaryÙËIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).

For guides on how to do specific tasks with LCEL, check...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

* `configurable_fields`: This method allows you to configure specific **attribute...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocati...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËRefer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.

## Orchestration

Get started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble La...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answer ¨query_id¤Q075¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q075®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:4a0fd956-357a-0732-178c-486550b68e58ªfocus_kind¯chain_execution‹¢idÙ@6e7fa70673088748abcdf9b5e38a1a03590f229ef91bf4a1ebfe74f81acac503¢tsËAÚóşÓ_¨trace_id¤Q075­parent_run_idÙ$f6f766a1-681c-4ae3-b312-037bc3626afb¦run_idÙ$d0f15d25-3f73-4a9b-8bcc-b268c16a9103ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙWWhat guide should you consult for configuring runtime chain internals like temperature?§contextÚIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).

For guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).

To simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

* `configurable_fields`: This method allows you to configure specific **attributes** in a Runnable. For example, the `temperature` attribute of a chat model.
* `configurable_alternatives`: This method enables you to specify **alternative** Runnables that can be run during runtime. For example, you could specify a list of different chat models that can be used.

See the [How to configure runtime chain internals](/docs/how_to/configure) guide for more information on how to configure runtime chain internals.

- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocation args to runnables](/docs/how_to/binding/)
- [How to: turn any function into a runnable](/docs/how_to/functions)
- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)
- [How to: configure runnable behavior at runtime](/docs/how_to/configure)
- [How to: add message history (memory) to a chain](/docs/how_to/message_history)
- [How to: route between sub-chains](/docs/how_to/routing)
- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)
- [How to: inspect runnables](/docs/how_to/inspect)
- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)
- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)

## [LangGraph](https://langchain-ai.github.io/langgraph)

Refer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.

## Orchestration

Get started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble LangChain components into full-featured applications.

## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
and is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.
:::

Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.

To simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q075®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:4a0fd956-357a-0732-178c-486550b68e58­checkpoint_nsÙ-generate:4a0fd956-357a-0732-178c-486550b68e58¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@a69c9fb49e1c15a15b1ab0494f302cbd8dd7fc93f946bce6c005874da7c539fc¢tsËAÚóşÓe0¨trace_idÙ$d0f15d25-3f73-4a9b-8bcc-b268c16a9103­parent_run_idÙ$f6f766a1-681c-4ae3-b312-037bc3626afb¦run_idÙ$d0f15d25-3f73-4a9b-8bcc-b268c16a9103ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚúmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What guide should you consult for configuring runtime chain internals like temperature? \nContext: If you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).\n\nFor guides on how to do specific tasks with LCEL, check out [the relevant how-to guides](/docs/how_to/#langchain-expression-language-lcel).\n\nTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:\n\n* `configurable_fields`: This method allows you to configure specific **attributes** in a Runnable. For example, the `temperature` attribute of a chat model.\n* `configurable_alternatives`: This method enables you to specify **alternative** Runnables that can be run during runtime. For example, you could specify a list of different chat models that can be used.\n\nSee the [How to configure runtime chain internals](/docs/how_to/configure) guide for more information on how to configure runtime chain internals.\n\n- [How to: chain runnables](/docs/how_to/sequence)\n- [How to: stream runnables](/docs/how_to/streaming)\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\n- [How to: turn any function into a runnable](/docs/how_to/functions)\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\n- [How to: route between sub-chains](/docs/how_to/routing)\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\n- [How to: inspect runnables](/docs/how_to/inspect)\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\n\n## [LangGraph](https://langchain-ai.github.io/langgraph)\n\nRefer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.\n\n## Orchestration\n\nGet started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble LangChain components into full-featured applications.\n\n## Configurable runnables\n\n:::note\nThis is an advanced feature that is unnecessary for most users.\n\nIt helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)\nand is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.\n:::\n\nSometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.\n\nTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime: \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@936952fb1037d103d4bfd5750d27671b42d02cfb47b64b31736304949cd6cf75¢tsËAÚóÿä%¨focus_msÍß§payload‚¤role©assistant§contentÙéYou should consult the guide titled "How to configure runtime chain internals" for information on configuring attributes like temperature. This guide provides detailed instructions on how to manage runtime configurations effectively.ªfocus_kind¬exec_latency¨query_id¤Q075‰¢idÙ@66a93e7cf6125b74785dce61f6a6aae8a08d4b95f6ba4c2fe9e8c383297e6060¢tsËAÚóÿğt¨trace_idÙ$f6f766a1-681c-4ae3-b312-037bc3626afb­parent_run_idÙ$c4e37596-b531-4329-8917-c6960523ffaa¦run_idÙ$f6f766a1-681c-4ae3-b312-037bc3626afbªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙéYou should consult the guide titled "How to configure runtime chain internals" for information on configuring attributes like temperature. This guide provides detailed instructions on how to manage runtime configurations effectively.¨query_id¤Q075ªfocus_kind¶chain_execution_result‰¢idÙ@85e1e2a1f1d43558d8e3cffb09faf962145380c1a7f9a56f549eae694cb763cd¢tsËAÚóÿûv¨trace_idÙ$c4e37596-b531-4329-8917-c6960523ffaa­parent_run_idÀ¦run_idÙ$c4e37596-b531-4329-8917-c6960523ffaaªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙWWhat guide should you consult for configuring runtime chain internals like temperature?§context•‚´page_content_summaryÙËIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).

For guides on how to do specific tasks with LCEL, check...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

* `configurable_fields`: This method allows you to configure specific **attribute...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocati...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËRefer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.

## Orchestration

Get started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble La...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÙéYou should consult the guide titled "How to configure runtime chain internals" for information on configuring attributes like temperature. This guide provides detailed instructions on how to manage runtime configurations effectively.¨query_id¤Q075ªfocus_kind¶chain_execution_result‹¢idÙ@ea857a8e8ac4d1a3710bdeb532ffbbfc047ea5c9ac7f79853734a13043c612bf¢tsËAÚóÿ&¨trace_id¤Q076­parent_run_idÀ¦run_idÙ$c97194ae-e4c9-4d4c-a664-b84c7a820751ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙKWhich selector uses Maximal Marginal Relevance (MMR) to diversify examples?¨query_id¤Q076§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q076ªfocus_kind¯chain_execution‹¢idÙ@7e778424942f32aba804d512622d63983624cea9894010da47421fe4dd2d2b86¢tsËAÚóÿÙ¨trace_id¤Q076­parent_run_idÙ$c97194ae-e4c9-4d4c-a664-b84c7a820751¦run_idÙ$94d40bb0-a523-4cfb-97b3-18c0b045b29eªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙKWhich selector uses Maximal Marginal Relevance (MMR) to diversify examples?§context¦answer ¨query_id¤Q076¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q076®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:4563e7aa-5d07-1211-2ce2-f4488bb3608eªfocus_kind¯chain_execution‰¢idÙ@44e4d3d4400ca04e8d6bc13fe6371e38dc9966652ac4acdf33745017688d65f1¢tsËAÚóÿ#Õ$¨trace_idÙ$94d40bb0-a523-4cfb-97b3-18c0b045b29e­parent_run_idÙ$c97194ae-e4c9-4d4c-a664-b84c7a820751¦run_idÙ$94d40bb0-a523-4cfb-97b3-18c0b045b29eªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË| [Maximal Marginal Relevance (MMR)](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeV...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙËWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal ma...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)
:::

## Overview

One common prompting technique for...¨metadata¦sourceÙP/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/example_selectors.mdx‚´page_content_summaryÙËAt a high-level, the basic ways to generate examples are:
- Manual: a person/people generates examples they think are useful.
- Better model: a better (presumably more expensive/slower) model's respon...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/few_shot_prompting.mdx¨query_id¤Q076ªfocus_kind¶chain_execution_result‹¢idÙ@2d7fb3b477cbf831f134148c657d1f3bab40a26e50129d218e0c43afd03f8317¢tsËAÚóÿ#İ×¨trace_id¤Q076­parent_run_idÙ$c97194ae-e4c9-4d4c-a664-b84c7a820751¦run_idÙ$0bc4935d-0670-4a64-9a6c-7afc078fe63cªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙKWhich selector uses Maximal Marginal Relevance (MMR) to diversify examples?§context•‚´page_content_summaryÙË| [Maximal Marginal Relevance (MMR)](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeV...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙËWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal ma...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)
:::

## Overview

One common prompting technique for...¨metadata¦sourceÙP/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/example_selectors.mdx‚´page_content_summaryÙËAt a high-level, the basic ways to generate examples are:
- Manual: a person/people generates examples they think are useful.
- Better model: a better (presumably more expensive/slower) model's respon...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/few_shot_prompting.mdx¦answer ¨query_id¤Q076¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q076®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:b1060b07-9c47-fe8e-9071-c14f23fa2004ªfocus_kind¯chain_execution‹¢idÙ@d268fa06eb1cbe8105371cec126373e0299c679849c6ed5a8957da5f740e5b4b¢tsËAÚóÿ#â~¨trace_id¤Q076­parent_run_idÙ$0bc4935d-0670-4a64-9a6c-7afc078fe63c¦run_idÙ$673648e6-2690-431b-a6b5-d20cbbd05f5dªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙKWhich selector uses Maximal Marginal Relevance (MMR) to diversify examples?§contextÚ| [Maximal Marginal Relevance (MMR)](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.max_marginal_relevance_search) | When needing to diversify search results.             | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.                                        |

While algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.
As a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches. 
At the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.

### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs/how_to/example_selectors)
- [How to: select examples by length](/docs/how_to/example_selectors_length_based)
- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)
- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)
- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)
- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)

### LLMs

What LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.

# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)
:::

## Overview

One common prompting technique for achieving better performance is to include examples as part of the prompt. This is known as [few-shot prompting](/docs/concepts/few_shot_prompting).

This gives the [language model](/docs/concepts/chat_models/) concrete examples of how it should behave.
Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.

**Example Selectors** are classes responsible for selecting and then formatting examples into prompts.

## Related resources

* [Example selector how-to guides](/docs/how_to/#example-selectors)

At a high-level, the basic ways to generate examples are:
- Manual: a person/people generates examples they think are useful.
- Better model: a better (presumably more expensive/slower) model's responses are used as examples for a worse (presumably cheaper/faster) model.
- User feedback: users (or labelers) leave feedback on interactions with the application and examples are generated based on that feedback (for example, all interactions with positive feedback could be turned into examples).
- LLM feedback: same as user feedback but the process is automated by having models evaluate themselves.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q076®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:b1060b07-9c47-fe8e-9071-c14f23fa2004­checkpoint_nsÙ-generate:b1060b07-9c47-fe8e-9071-c14f23fa2004¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@04427d3f5d9c996a674944f5d0db3248d9ccf8727cb66af533598248b15e19ce¢tsËAÚóÿ#ê¨trace_idÙ$673648e6-2690-431b-a6b5-d20cbbd05f5d­parent_run_idÙ$0bc4935d-0670-4a64-9a6c-7afc078fe63c¦run_idÙ$673648e6-2690-431b-a6b5-d20cbbd05f5dªevent_type¬chain_output¬event_source¥chain§payload§outputsÚğmessages=[HumanMessage(content="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Which selector uses Maximal Marginal Relevance (MMR) to diversify examples? \nContext: | [Maximal Marginal Relevance (MMR)](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeVectorStore.max_marginal_relevance_search) | When needing to diversify search results.             | MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.                                        |\n\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\nFor example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\nAs a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches. \nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.\n\n### Example selectors\n\n[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\n\n- [How to: use example selectors](/docs/how_to/example_selectors)\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\n\n### LLMs\n\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\n\n# Example selectors\n\n:::note Prerequisites\n\n- [Chat models](/docs/concepts/chat_models/)\n- [Few-shot prompting](/docs/concepts/few_shot_prompting/)\n:::\n\n## Overview\n\nOne common prompting technique for achieving better performance is to include examples as part of the prompt. This is known as [few-shot prompting](/docs/concepts/few_shot_prompting).\n\nThis gives the [language model](/docs/concepts/chat_models/) concrete examples of how it should behave.\nSometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.\n\n**Example Selectors** are classes responsible for selecting and then formatting examples into prompts.\n\n## Related resources\n\n* [Example selector how-to guides](/docs/how_to/#example-selectors)\n\nAt a high-level, the basic ways to generate examples are:\n- Manual: a person/people generates examples they think are useful.\n- Better model: a better (presumably more expensive/slower) model's responses are used as examples for a worse (presumably cheaper/faster) model.\n- User feedback: users (or labelers) leave feedback on interactions with the application and examples are generated based on that feedback (for example, all interactions with positive feedback could be turned into examples).\n- LLM feedback: same as user feedback but the process is automated by having models evaluate themselves. \nAnswer:", additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@ccafd2ad56d8b253707d07a9bc6efe53bf584065649bc68dca34b2fe47ffe626¢tsËAÚóÿ‚#t¨focus_msÍÀ§payload‚¤role©assistant§contentÚ5The selector that uses Maximal Marginal Relevance (MMR) to diversify examples is the "Example Selector by Maximal Marginal Relevance." This selector is designed to ensure a more diverse set of results by re-ranking after the initial similarity search. It helps avoid returning similar and redundant documents.ªfocus_kind¬exec_latency¨query_id¤Q076‰¢idÙ@940445f0a6ec165dc81323ac5ad83c3313e2e49b51ca8e1f86472b394c22d096¢tsËAÚóÿ‚/ß¨trace_idÙ$0bc4935d-0670-4a64-9a6c-7afc078fe63c­parent_run_idÙ$c97194ae-e4c9-4d4c-a664-b84c7a820751¦run_idÙ$0bc4935d-0670-4a64-9a6c-7afc078fe63cªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ5The selector that uses Maximal Marginal Relevance (MMR) to diversify examples is the "Example Selector by Maximal Marginal Relevance." This selector is designed to ensure a more diverse set of results by re-ranking after the initial similarity search. It helps avoid returning similar and redundant documents.¨query_id¤Q076ªfocus_kind¶chain_execution_result‰¢idÙ@b86f1aa3dcf15db2305eb0a1cbba948ada9c01de22f0a30362fe87b5e3eda630¢tsËAÚóÿ‚>l¨trace_idÙ$c97194ae-e4c9-4d4c-a664-b84c7a820751­parent_run_idÀ¦run_idÙ$c97194ae-e4c9-4d4c-a664-b84c7a820751ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙKWhich selector uses Maximal Marginal Relevance (MMR) to diversify examples?§context•‚´page_content_summaryÙË| [Maximal Marginal Relevance (MMR)](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#langchain_pinecone.vectorstores.PineconeV...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙËWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal ma...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)
:::

## Overview

One common prompting technique for...¨metadata¦sourceÙP/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/example_selectors.mdx‚´page_content_summaryÙËAt a high-level, the basic ways to generate examples are:
- Manual: a person/people generates examples they think are useful.
- Better model: a better (presumably more expensive/slower) model's respon...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/few_shot_prompting.mdx¦answerÚ5The selector that uses Maximal Marginal Relevance (MMR) to diversify examples is the "Example Selector by Maximal Marginal Relevance." This selector is designed to ensure a more diverse set of results by re-ranking after the initial similarity search. It helps avoid returning similar and redundant documents.¨query_id¤Q076ªfocus_kind¶chain_execution_result‹¢idÙ@04aa2001a02b5e4997681ca0f7be4071f3894e826cc8741072be693ece79a57c¢tsËAÚóÿ‚F¨trace_id¤Q077­parent_run_idÀ¦run_idÙ$3940529a-6dfe-4349-9360-f7dd35138618ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙcWhat is the recommended way to expose tool schemas to chat models that use OpenAI function calling?¨query_id¤Q077§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q077ªfocus_kind¯chain_execution‹¢idÙ@f24f056b0f2fe8d3ae5415e48412bea5c7878ca02e773a88c7bdfd7972833916¢tsËAÚóÿ‚N
¨trace_id¤Q077­parent_run_idÙ$3940529a-6dfe-4349-9360-f7dd35138618¦run_idÙ$6342afc0-51c9-4190-b28b-6d335995baa7ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙcWhat is the recommended way to expose tool schemas to chat models that use OpenAI function calling?§context¦answer ¨query_id¤Q077¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q077®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:54aa60e1-478c-17f2-4177-d2fa55955ed8ªfocus_kind¯chain_execution‰¢idÙ@22a56e2749abfe3ecf2717cae5a2e734e3c5082f59447ae7581bbfa66363894b¢tsËAÚóÿ–ğ»¨trace_idÙ$6342afc0-51c9-4190-b28b-6d335995baa7­parent_run_idÙ$3940529a-6dfe-4349-9360-f7dd35138618¦run_idÙ$6342afc0-51c9-4190-b28b-6d335995baa7ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË### Using tool calling

Many [model providers support](/docs/integrations/chat/) tool calling, a concept discussed in more detail in our [tool calling guide](/docs/concepts/tool_calling/).
In short, t...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙË# Tool calling

:::info[Prerequisites]
* [Tools](/docs/concepts/tools)
* [Chat Models](/docs/concepts/chat_models)
:::


## Overview 

Many AI applications interact directly with humans. In these case...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙË### Example: chat models 

Many [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://la...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË### JSON mode

In addition to tool calling, some model providers support a feature called `JSON mode`. 
This supports JSON schema definition as input and enforces the model to produce a conforming JSO...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙËSome important things to note:

- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration fo...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q077ªfocus_kind¶chain_execution_result‹¢idÙ@1213cb2b7b1fc26ba571b14f936a35ea153d2d16cd0ee1301ed11120efe268b2¢tsËAÚóÿ— ¨trace_id¤Q077­parent_run_idÙ$3940529a-6dfe-4349-9360-f7dd35138618¦run_idÙ$89aa97dc-f8c1-4fdd-ad8a-3f6cee4fc4bdªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙcWhat is the recommended way to expose tool schemas to chat models that use OpenAI function calling?§context•‚´page_content_summaryÙË### Using tool calling

Many [model providers support](/docs/integrations/chat/) tool calling, a concept discussed in more detail in our [tool calling guide](/docs/concepts/tool_calling/).
In short, t...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙË# Tool calling

:::info[Prerequisites]
* [Tools](/docs/concepts/tools)
* [Chat Models](/docs/concepts/chat_models)
:::


## Overview 

Many AI applications interact directly with humans. In these case...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙË### Example: chat models 

Many [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://la...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË### JSON mode

In addition to tool calling, some model providers support a feature called `JSON mode`. 
This supports JSON schema definition as input and enforces the model to produce a conforming JSO...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙËSome important things to note:

- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration fo...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answer ¨query_id¤Q077¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q077®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:cfd250e1-efa2-f3a5-9f55-3fa49dc4f9b1ªfocus_kind¯chain_execution‹¢idÙ@62c5767b4ebfe09357b0a24c08b328862033b1bff24285f774ba05e269cf756a¢tsËAÚóÿ—¡¨trace_id¤Q077­parent_run_idÙ$89aa97dc-f8c1-4fdd-ad8a-3f6cee4fc4bd¦run_idÙ$cdf8bbdb-3520-4f6d-8a71-d6f2424dbc40ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙcWhat is the recommended way to expose tool schemas to chat models that use OpenAI function calling?§contextÚû### Using tool calling

Many [model providers support](/docs/integrations/chat/) tool calling, a concept discussed in more detail in our [tool calling guide](/docs/concepts/tool_calling/).
In short, tool calling involves binding a tool to a model and, when appropriate, the model can *decide* to call this tool and ensure its response conforms to the tool's schema.
With this in mind, the central concept is straightforward: *simply bind our schema to a model as a tool!*
Here is an example using the `ResponseFormatter` schema defined above:

```python
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o", temperature=0)
# Bind responseformatter schema as a tool to the model
model_with_tools = model.bind_tools([ResponseFormatter])
# Invoke the model
ai_msg = model_with_tools.invoke("What is the powerhouse of the cell?")
```

# Tool calling

:::info[Prerequisites]
* [Tools](/docs/concepts/tools)
* [Chat Models](/docs/concepts/chat_models)
:::


## Overview 

Many AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language.
But what about cases where we want a model to also interact *directly* with systems, such as databases or an API?
These systems often have a particular input schema; for example, APIs frequently have a required payload structure.
This need motivates the concept of *tool calling*. You can use [tool calling](https://platform.openai.com/docs/guides/function-calling/example-use-cases) to request model responses that match a particular schema.

:::info
You will sometimes hear the term `function calling`. We use this term interchangeably with `tool calling`. 
:::

![Conceptual overview of tool calling](/img/tool_calling_concept.png)

## Key concepts

### Example: chat models 

Many [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.
The APIs for each provider differ. 
LangChain's [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):

```python
# Tool creation
tools = [my_tool]
# Tool binding
model_with_tools = model.bind_tools(tools)
```

### JSON mode

In addition to tool calling, some model providers support a feature called `JSON mode`. 
This supports JSON schema definition as input and enforces the model to produce a conforming JSON output.
You can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).
Here is an example of how to use JSON mode with OpenAI:

```python
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o").with_structured_output(method="json_mode")
ai_msg = model.invoke("Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]")
ai_msg
{'random_ints': [45, 67, 12, 34, 89, 23, 78, 56, 90, 11]}
```

## Structured output method 

There are a few challenges when producing structured output with the above methods: 

(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.

Some important things to note:

- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can't be supported on these.
- Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. `langchain-openai`, `langchain-anthropic`, etc.), they're not enforced on models in `langchain-community`.

Chat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model head to the their respective [API reference](https://python.langchain.com/api_reference/) for that model.

## Tool calling

Chat models can call [tools](/docs/concepts/tools) to perform tasks such as fetching data from a database, making API requests, or running custom code. Please
see the [tool calling](/docs/concepts/tool_calling) guide for more information.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q077®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:cfd250e1-efa2-f3a5-9f55-3fa49dc4f9b1­checkpoint_nsÙ-generate:cfd250e1-efa2-f3a5-9f55-3fa49dc4f9b1¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@3c6cf923edcb000eecaa874fc1efb28cf0c1cd68c57e2eba20c17d71604f2e67¢tsËAÚóÿ—ø¨trace_idÙ$cdf8bbdb-3520-4f6d-8a71-d6f2424dbc40­parent_run_idÙ$89aa97dc-f8c1-4fdd-ad8a-3f6cee4fc4bd¦run_idÙ$cdf8bbdb-3520-4f6d-8a71-d6f2424dbc40ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What is the recommended way to expose tool schemas to chat models that use OpenAI function calling? \nContext: ### Using tool calling\n\nMany [model providers support](/docs/integrations/chat/) tool calling, a concept discussed in more detail in our [tool calling guide](/docs/concepts/tool_calling/).\nIn short, tool calling involves binding a tool to a model and, when appropriate, the model can *decide* to call this tool and ensure its response conforms to the tool\'s schema.\nWith this in mind, the central concept is straightforward: *simply bind our schema to a model as a tool!*\nHere is an example using the `ResponseFormatter` schema defined above:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model="gpt-4o", temperature=0)\n# Bind responseformatter schema as a tool to the model\nmodel_with_tools = model.bind_tools([ResponseFormatter])\n# Invoke the model\nai_msg = model_with_tools.invoke("What is the powerhouse of the cell?")\n```\n\n# Tool calling\n\n:::info[Prerequisites]\n* [Tools](/docs/concepts/tools)\n* [Chat Models](/docs/concepts/chat_models)\n:::\n\n\n## Overview \n\nMany AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language.\nBut what about cases where we want a model to also interact *directly* with systems, such as databases or an API?\nThese systems often have a particular input schema; for example, APIs frequently have a required payload structure.\nThis need motivates the concept of *tool calling*. You can use [tool calling](https://platform.openai.com/docs/guides/function-calling/example-use-cases) to request model responses that match a particular schema.\n\n:::info\nYou will sometimes hear the term `function calling`. We use this term interchangeably with `tool calling`. \n:::\n\n![Conceptual overview of tool calling](/img/tool_calling_concept.png)\n\n## Key concepts\n\n### Example: chat models \n\nMany [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/)), that allows a developer to request model responses that match a particular schema.\nThe APIs for each provider differ. \nLangChain\'s [chat model](/docs/concepts/chat_models/) interface provides a common way to bind [tools](/docs/concepts/tools) to a model in order to support [tool calling](/docs/concepts/tool_calling/):\n\n```python\n# Tool creation\ntools = [my_tool]\n# Tool binding\nmodel_with_tools = model.bind_tools(tools)\n```\n\n### JSON mode\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`. \nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model="gpt-4o").with_structured_output(method="json_mode")\nai_msg = model.invoke("Return a JSON object with key \'random_ints\' and a value of 10 random ints in [0-99]")\nai_msg\n{\'random_ints\': [45, 67, 12, 34, 89, 23, 78, 56, 90, 11]}\n```\n\n## Structured output method \n\nThere are a few challenges when producing structured output with the above methods: \n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\nSome important things to note:\n\n- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can\'t be supported on these.\n- Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. `langchain-openai`, `langchain-anthropic`, etc.), they\'re not enforced on models in `langchain-community`.\n\nChat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model head to the their respective [API reference](https://python.langchain.com/api_reference/) for that model.\n\n## Tool calling\n\nChat models can call [tools](/docs/concepts/tools) to perform tasks such as fetching data from a database, making API requests, or running custom code. Please\nsee the [tool calling](/docs/concepts/tool_calling) guide for more information. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@8efc3b0795eed37bd533e90e7f3d8074c247d11f1e288c0a6a8d6513ae47ce75¢tsËAÚô ‰Æ¨focus_msÍõ§payload‚¤role©assistant§contentÚrThe recommended way to expose tool schemas to chat models using OpenAI function calling is to bind the schema as a tool to the model. This can be done using the `bind_tools` method, where you pass the schema as a list of tools. For example, you would create a model instance and then bind the schema like this: `model_with_tools = model.bind_tools([ResponseFormatter])`.ªfocus_kind¬exec_latency¨query_id¤Q077‰¢idÙ@a1f1d3bfffec36a6fbd4b80c73dce80de675aeac3d4eb484dd1cd1ee6cf173e9¢tsËAÚô ¨¨trace_idÙ$89aa97dc-f8c1-4fdd-ad8a-3f6cee4fc4bd­parent_run_idÙ$3940529a-6dfe-4349-9360-f7dd35138618¦run_idÙ$89aa97dc-f8c1-4fdd-ad8a-3f6cee4fc4bdªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚrThe recommended way to expose tool schemas to chat models using OpenAI function calling is to bind the schema as a tool to the model. This can be done using the `bind_tools` method, where you pass the schema as a list of tools. For example, you would create a model instance and then bind the schema like this: `model_with_tools = model.bind_tools([ResponseFormatter])`.¨query_id¤Q077ªfocus_kind¶chain_execution_result‰¢idÙ@ec6cc48d1909d86c15143fdeac3511881c29b721927cfb8b2c6f7fb2e8b00658¢tsËAÚô ¯ê¨trace_idÙ$3940529a-6dfe-4349-9360-f7dd35138618­parent_run_idÀ¦run_idÙ$3940529a-6dfe-4349-9360-f7dd35138618ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙcWhat is the recommended way to expose tool schemas to chat models that use OpenAI function calling?§context•‚´page_content_summaryÙË### Using tool calling

Many [model providers support](/docs/integrations/chat/) tool calling, a concept discussed in more detail in our [tool calling guide](/docs/concepts/tool_calling/).
In short, t...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙË# Tool calling

:::info[Prerequisites]
* [Tools](/docs/concepts/tools)
* [Chat Models](/docs/concepts/chat_models)
:::


## Overview 

Many AI applications interact directly with humans. In these case...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙË### Example: chat models 

Many [model providers](/docs/concepts/chat_models/) support [tool calling](/docs/concepts/tool_calling/), a critical feature for many applications (e.g., [agents](https://la...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË### JSON mode

In addition to tool calling, some model providers support a feature called `JSON mode`. 
This supports JSON schema definition as input and enforces the model to produce a conforming JSO...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙËSome important things to note:

- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration fo...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚrThe recommended way to expose tool schemas to chat models using OpenAI function calling is to bind the schema as a tool to the model. This can be done using the `bind_tools` method, where you pass the schema as a list of tools. For example, you would create a model instance and then bind the schema like this: `model_with_tools = model.bind_tools([ResponseFormatter])`.¨query_id¤Q077ªfocus_kind¶chain_execution_result‹¢idÙ@f86a39be1fcf44b60c582203f1c81daa33deaa92eb04610e072b5b041a971d14¢tsËAÚô µ¨trace_id¤Q078­parent_run_idÀ¦run_idÙ$cebd8e0a-d0d6-41de-8fac-925e0638e8a5ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙKExplain the advantage of using `batch_as_completed` for long running tasks.¨query_id¤Q078§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q078ªfocus_kind¯chain_execution‹¢idÙ@da62416ab1207712c0b98b01c89a25734412407a12114f6e91fef7f300d2dbf7¢tsËAÚô ¼Ë¨trace_id¤Q078­parent_run_idÙ$cebd8e0a-d0d6-41de-8fac-925e0638e8a5¦run_idÙ$2c82b16f-e6b0-4383-b5b7-0afb04ec2ecdªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙKExplain the advantage of using `batch_as_completed` for long running tasks.§context¦answer ¨query_id¤Q078¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q078®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:b48f924e-ccda-a68b-1009-888066725579ªfocus_kind¯chain_execution‰¢idÙ@e00a1bb3efc9ad1d6279b386f2ec3654118bf5a8e294c03de03be5e374f194ad¢tsËAÚô -5e¨trace_idÙ$2c82b16f-e6b0-4383-b5b7-0afb04ec2ecd­parent_run_idÙ$cebd8e0a-d0d6-41de-8fac-925e0638e8a5¦run_idÙ$2c82b16f-e6b0-4383-b5b7-0afb04ec2ecdªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for user...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¨query_id¤Q078ªfocus_kind¶chain_execution_result‹¢idÙ@2fcfc975b2722f674e16f5c6ee3430aa567763a78c6f98d44cb02fa8bd9bc8d5¢tsËAÚô -?†¨trace_id¤Q078­parent_run_idÙ$cebd8e0a-d0d6-41de-8fac-925e0638e8a5¦run_idÙ$786ec550-5c95-47a5-b5ea-47e4b3725619ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙKExplain the advantage of using `batch_as_completed` for long running tasks.§context•‚´page_content_summaryÙËThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for user...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¦answer ¨query_id¤Q078¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q078®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:f2fb79a7-a0dd-91ed-9761-3d24d473cbddªfocus_kind¯chain_execution‹¢idÙ@218762ca3c782c1ca4b50aec3611f4104e28555a9e6e5952249bcb4f6315955c¢tsËAÚô -EJ¨trace_id¤Q078­parent_run_idÙ$786ec550-5c95-47a5-b5ea-47e4b3725619¦run_idÙ$7b377063-82c5-4947-b0d3-89a8d999703bªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙKExplain the advantage of using `batch_as_completed` for long running tasks.§contextÚÁThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution.

Some Runnables may provide their own implementations of `batch` and `batch_as_completed` that are optimized for their specific use case (e.g.,
rely on a `batch` API provided by a model provider).

:::note
The async versions of `abatch` and `abatch_as_completed` relies on asyncio's [gather](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather) and [as_completed](https://docs.python.org/3/library/asyncio-task.html#asyncio.as_completed) functions to run the `ainvoke` method in parallel.
:::

<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to process multiple inputs in parallel.

Using these methods can significantly improve performance when needing to process multiple independent inputs, as the
processing can be done in parallel instead of sequentially.

The two batching options are:

* `batch`: Process multiple inputs in parallel, returning results in the same order as the inputs.
* `batch_as_completed`: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.

:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency` attribute in the `RunnableConfig` dictionary. See the [RunnableConfig](/docs/concepts/runnables/#runnableconfig) for more information.

Chat Models also have a built-in [rate limiter](/docs/concepts/chat_models#rate-limiting) that can be used to control the rate at which requests are made.
:::

### Asynchronous support
<span data-heading-keywords="async-api"></span>

Runnables expose an asynchronous API, allowing them to be called using the `await` syntax in Python. Asynchronous methods can be identified by the "a" prefix (e.g., `ainvoke`, `abatch`, `astream`, `abatch_as_completed`).

Please refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.

## Streaming APIs
<span data-heading-keywords="streaming-api"></span>

## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.
* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.
* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.
* Inspected: Schematic information about Runnable's input, output, and configuration can be accessed.
* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.

Please review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.

<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.
- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q078®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:f2fb79a7-a0dd-91ed-9761-3d24d473cbdd­checkpoint_nsÙ-generate:f2fb79a7-a0dd-91ed-9761-3d24d473cbdd¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@43d93b2d403f16a42a68c62f27565e14e33d078da31e4e451273788953c0133e¢tsËAÚô -K=¨trace_idÙ$7b377063-82c5-4947-b0d3-89a8d999703b­parent_run_idÙ$786ec550-5c95-47a5-b5ea-47e4b3725619¦run_idÙ$7b377063-82c5-4947-b0d3-89a8d999703bªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ¦messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Explain the advantage of using `batch_as_completed` for long running tasks. \nContext: The default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution.\n\nSome Runnables may provide their own implementations of `batch` and `batch_as_completed` that are optimized for their specific use case (e.g.,\nrely on a `batch` API provided by a model provider).\n\n:::note\nThe async versions of `abatch` and `abatch_as_completed` relies on asyncio\'s [gather](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather) and [as_completed](https://docs.python.org/3/library/asyncio-task.html#asyncio.as_completed) functions to run the `ainvoke` method in parallel.\n:::\n\n<a id="batch"></a>\n### Optimized parallel execution (batch)\n<span data-heading-keywords="batch"></span>\n\nLangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to process multiple inputs in parallel.\n\nUsing these methods can significantly improve performance when needing to process multiple independent inputs, as the\nprocessing can be done in parallel instead of sequentially.\n\nThe two batching options are:\n\n* `batch`: Process multiple inputs in parallel, returning results in the same order as the inputs.\n* `batch_as_completed`: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.\n\n:::tip\nWhen processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency` attribute in the `RunnableConfig` dictionary. See the [RunnableConfig](/docs/concepts/runnables/#runnableconfig) for more information.\n\nChat Models also have a built-in [rate limiter](/docs/concepts/chat_models#rate-limiting) that can be used to control the rate at which requests are made.\n:::\n\n### Asynchronous support\n<span data-heading-keywords="async-api"></span>\n\nRunnables expose an asynchronous API, allowing them to be called using the `await` syntax in Python. Asynchronous methods can be identified by the "a" prefix (e.g., `ainvoke`, `abatch`, `astream`, `abatch_as_completed`).\n\nPlease refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.\n\n## Streaming APIs\n<span data-heading-keywords="streaming-api"></span>\n\n## Overview of runnable interface\n\nThe Runnable way defines a standard interface that allows a Runnable component to be:\n\n* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.\n* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.\n* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.\n* Inspected: Schematic information about Runnable\'s input, output, and configuration can be accessed.\n* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.\n\nPlease review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.\n\n<a id="batch"></a>\n### Optimized parallel execution (batch)\n<span data-heading-keywords="batch"></span>\n\n- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/concepts/runnables/#optimized-parallel-execution-batch). Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\n- **Guaranteed Async support**: Any chain built with LCEL can be run asynchronously using the [Runnable Async API](/docs/concepts/runnables/#asynchronous-support). This can be useful when running chains in a server environment where you want to handle large number of requests concurrently. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@ac13b61fdd406adcdc6133d10c669ee7cb001b38c5f5cfea02235819fc2ec88c¢tsËAÚô Ÿœ¨focus_msÍù§payload‚¤role©assistant§contentÚThe advantage of using `batch_as_completed` for long-running tasks is that it allows for parallel processing of multiple inputs, returning results as they complete rather than in a fixed order. This can significantly improve performance, especially for I/O-bound tasks, by reducing overall latency. Additionally, it simplifies thread management for users, as the execution is handled efficiently in the background.ªfocus_kind¬exec_latency¨query_id¤Q078‰¢idÙ@297bcfefe008c94c12ada6d069b2ac4f94c216988756445c145257d0001bc6d1¢tsËAÚô Ÿ Ç¨trace_idÙ$786ec550-5c95-47a5-b5ea-47e4b3725619­parent_run_idÙ$cebd8e0a-d0d6-41de-8fac-925e0638e8a5¦run_idÙ$786ec550-5c95-47a5-b5ea-47e4b3725619ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚThe advantage of using `batch_as_completed` for long-running tasks is that it allows for parallel processing of multiple inputs, returning results as they complete rather than in a fixed order. This can significantly improve performance, especially for I/O-bound tasks, by reducing overall latency. Additionally, it simplifies thread management for users, as the execution is handled efficiently in the background.¨query_id¤Q078ªfocus_kind¶chain_execution_result‰¢idÙ@615111d8dafdbb443b97eaa70013a63553920a1e82328fa4b5cf690491d51c7e¢tsËAÚô Ÿ©÷¨trace_idÙ$cebd8e0a-d0d6-41de-8fac-925e0638e8a5­parent_run_idÀ¦run_idÙ$cebd8e0a-d0d6-41de-8fac-925e0638e8a5ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙKExplain the advantage of using `batch_as_completed` for long running tasks.§context•‚´page_content_summaryÙËThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for user...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¦answerÚThe advantage of using `batch_as_completed` for long-running tasks is that it allows for parallel processing of multiple inputs, returning results as they complete rather than in a fixed order. This can significantly improve performance, especially for I/O-bound tasks, by reducing overall latency. Additionally, it simplifies thread management for users, as the execution is handled efficiently in the background.¨query_id¤Q078ªfocus_kind¶chain_execution_result‹¢idÙ@06b38b0b1325672b0209fa39ea961c8319bd0d495cec097f96283da2a2468513¢tsËAÚô Ÿ´|¨trace_id¤Q079­parent_run_idÀ¦run_idÙ$6888c39f-23d0-4810-b8c7-1f1f244aee65ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙNWhich two files outline strategies to trim messages to fit the context window?¨query_id¤Q079§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q079ªfocus_kind¯chain_execution‹¢idÙ@c277e749567abf8b5c1d18c04f8bf932483fc89b7745fafd56f0cb83459301ff¢tsËAÚô Ÿ¾M¨trace_id¤Q079­parent_run_idÙ$6888c39f-23d0-4810-b8c7-1f1f244aee65¦run_idÙ$8ca3a10d-1caf-4df8-860e-efcdbaa633d5ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙNWhich two files outline strategies to trim messages to fit the context window?§context¦answer ¨query_id¤Q079¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q079®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:82a7291c-94b4-b210-95a8-4c1d1c0647cbªfocus_kind¯chain_execution‰¢idÙ@04ca2075cf1672e200c96d88afc0453de51af553d9b8631eccbf397192b572dc¢tsËAÚô ®ã¨trace_idÙ$8ca3a10d-1caf-4df8-860e-efcdbaa633d5­parent_run_idÙ$6888c39f-23d0-4810-b8c7-1f1f244aee65¦run_idÙ$8ca3a10d-1caf-4df8-860e-efcdbaa633d5ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙËThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

A full conversation often invo...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËWhile processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
    -...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Messages

[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.

- [How to: trim messages](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¨query_id¤Q079ªfocus_kind¶chain_execution_result‹¢idÙ@db13f2d39e29e1c9f7d755f4cfd605c7c70c143f037428b6a0ee0e2be6b3429d¢tsËAÚô ®ël¨trace_id¤Q079­parent_run_idÙ$6888c39f-23d0-4810-b8c7-1f1f244aee65¦run_idÙ$2d20df8f-bd14-410c-a470-f15e190bde67ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙNWhich two files outline strategies to trim messages to fit the context window?§context•‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙËThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

A full conversation often invo...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËWhile processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
    -...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Messages

[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.

- [How to: trim messages](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answer ¨query_id¤Q079¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q079®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:fb82da19-a60d-6384-9654-5f36817c5e90ªfocus_kind¯chain_execution‹¢idÙ@e701214b330b3e9126f20c052bf0ef7f0111397bd3d0800d1a83a52befd448c9¢tsËAÚô ®ïĞ¨trace_id¤Q079­parent_run_idÙ$2d20df8f-bd14-410c-a470-f15e190bde67¦run_idÙ$77c33581-dbbb-4e83-8c4b-2fba84418cd9ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙNWhich two files outline strategies to trim messages to fit the context window?§contextÚB## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and long-term memory in chat models using [LangGraph](https://langchain-ai.github.io/langgraph/).

The **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

A full conversation often involves a combination of two patterns of alternating messages:

1. The **user** and the **assistant** representing a back-and-forth conversation.
2. The **assistant** and **tool messages** representing an ["agentic" workflow](/docs/concepts/agents) where the assistant is invoking tools to perform specific tasks.

## Managing chat history

Since chat models have a maximum limit on input size, it's important to manage chat history and trim it as needed to avoid exceeding the [context window](/docs/concepts/chat_models/#context-window).

While processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. Please read more about
the technique in the [structured outputs](/docs/concepts/structured_outputs) guide.

## Multimodality

Large Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as [multimodality](/docs/concepts/multimodality).

Currently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.

## Context window

A chat model's context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.

While processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
    - The first message is either a "user" message or a "system" message, followed by a "user" and then an "assistant" message.
    - The last message should be either a "user" message or a "tool" message containing the result of a tool call.
- When using [tool calling](/docs/concepts/tool_calling), a "tool" message should only follow an "assistant" message that requested the tool invocation.

:::tip
Understanding correct conversation structure is essential for being able to properly implement
[memory](https://langchain-ai.github.io/langgraph/concepts/memory/) in chat models.
:::

## Related resources

### Messages

[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.

- [How to: trim messages](/docs/how_to/trim_messages/)
- [How to: filter messages](/docs/how_to/filter_messages/)
- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)

### Prompt templates

[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.

- [How to: use few shot examples](/docs/how_to/few_shot_examples)
- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)
- [How to: partially format prompt templates](/docs/how_to/prompts_partial)
- [How to: compose prompts together](/docs/how_to/prompts_composition)
- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)

### Example selectors¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q079®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:fb82da19-a60d-6384-9654-5f36817c5e90­checkpoint_nsÙ-generate:fb82da19-a60d-6384-9654-5f36817c5e90¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@bc4635eea7aa7e3723c2756a99349dcea9ca4171223393cfc04372d17363bddd¢tsËAÚô ®ö¨trace_idÙ$77c33581-dbbb-4e83-8c4b-2fba84418cd9­parent_run_idÙ$2d20df8f-bd14-410c-a470-f15e190bde67¦run_idÙ$77c33581-dbbb-4e83-8c4b-2fba84418cd9ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ:messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Which two files outline strategies to trim messages to fit the context window? \nContext: ## Related resources\n\n- [How to trim messages](/docs/how_to/trim_messages/)\n- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and long-term memory in chat models using [LangGraph](https://langchain-ai.github.io/langgraph/).\n\nThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.\n\nA full conversation often involves a combination of two patterns of alternating messages:\n\n1. The **user** and the **assistant** representing a back-and-forth conversation.\n2. The **assistant** and **tool messages** representing an ["agentic" workflow](/docs/concepts/agents) where the assistant is invoking tools to perform specific tasks.\n\n## Managing chat history\n\nSince chat models have a maximum limit on input size, it\'s important to manage chat history and trim it as needed to avoid exceeding the [context window](/docs/concepts/chat_models/#context-window).\n\nWhile processing chat history, it\'s essential to preserve a correct conversation structure. \n\nKey guidelines for managing chat history:\n\n## Structured outputs\n\nChat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely\nuseful for information extraction tasks. Please read more about\nthe technique in the [structured outputs](/docs/concepts/structured_outputs) guide.\n\n## Multimodality\n\nLarge Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as [multimodality](/docs/concepts/multimodality).\n\nCurrently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.\n\n## Context window\n\nA chat model\'s context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.\n\nWhile processing chat history, it\'s essential to preserve a correct conversation structure. \n\nKey guidelines for managing chat history:\n\n- The conversation should follow one of these structures:\n    - The first message is either a "user" message or a "system" message, followed by a "user" and then an "assistant" message.\n    - The last message should be either a "user" message or a "tool" message containing the result of a tool call.\n- When using [tool calling](/docs/concepts/tool_calling), a "tool" message should only follow an "assistant" message that requested the tool invocation.\n\n:::tip\nUnderstanding correct conversation structure is essential for being able to properly implement\n[memory](https://langchain-ai.github.io/langgraph/concepts/memory/) in chat models.\n:::\n\n## Related resources\n\n### Messages\n\n[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.\n\n- [How to: trim messages](/docs/how_to/trim_messages/)\n- [How to: filter messages](/docs/how_to/filter_messages/)\n- [How to: merge consecutive messages of the same type](/docs/how_to/merge_message_runs/)\n\n### Prompt templates\n\n[Prompt Templates](/docs/concepts/prompt_templates) are responsible for formatting user input into a format that can be passed to a language model.\n\n- [How to: use few shot examples](/docs/how_to/few_shot_examples)\n- [How to: use few shot examples in chat models](/docs/how_to/few_shot_examples_chat/)\n- [How to: partially format prompt templates](/docs/how_to/prompts_partial)\n- [How to: compose prompts together](/docs/how_to/prompts_composition)\n- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)\n\n### Example selectors \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@14f5060988121f58afb935568be06367d8f2b50998783c3aed895ec6c7160b53¢tsËAÚôJÇ¨focus_msÍ»§payload‚¤role©assistant§contentÙ‰The two files that outline strategies to trim messages to fit the context window are "How to trim messages" and "How to filter messages."ªfocus_kind¬exec_latency¨query_id¤Q079‰¢idÙ@9e605850d60a7cfda0605f9fcb37386adba0b862fa35605aed0ec26b68b552fe¢tsËAÚôkß¨trace_idÙ$2d20df8f-bd14-410c-a470-f15e190bde67­parent_run_idÙ$6888c39f-23d0-4810-b8c7-1f1f244aee65¦run_idÙ$2d20df8f-bd14-410c-a470-f15e190bde67ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙ‰The two files that outline strategies to trim messages to fit the context window are "How to trim messages" and "How to filter messages."¨query_id¤Q079ªfocus_kind¶chain_execution_result‰¢idÙ@384619cb333f73cf8d014c421ed2640b75076db6fc59392ce8b2a08391f1ba77¢tsËAÚô„Q¨trace_idÙ$6888c39f-23d0-4810-b8c7-1f1f244aee65­parent_run_idÀ¦run_idÙ$6888c39f-23d0-4810-b8c7-1f1f244aee65ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙNWhich two files outline strategies to trim messages to fit the context window?§context•‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙËThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

A full conversation often invo...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËWhile processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
    -...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Messages

[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.

- [How to: trim messages](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answerÙ‰The two files that outline strategies to trim messages to fit the context window are "How to trim messages" and "How to filter messages."¨query_id¤Q079ªfocus_kind¶chain_execution_result‹¢idÙ@9c6153df4652ecf624310b9d2ccf5a7082c19952a62065e7c0bc408f9b001cc7¢tsËAÚô¿Ü¨trace_id¤Q080­parent_run_idÀ¦run_idÙ$eb8ff32d-5314-44ba-86bc-232d9cdd46e1ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙIWhat does the `max_concurrency` field not control when using a ChatModel?¨query_id¤Q080§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q080ªfocus_kind¯chain_execution‹¢idÙ@db8b9824ee71c7bc6ec4a4ff5ba39e8baafc257668ae0a1baf51b3ce289cb2f1¢tsËAÚô’[¨trace_id¤Q080­parent_run_idÙ$eb8ff32d-5314-44ba-86bc-232d9cdd46e1¦run_idÙ$abd8b249-8df2-4df0-96c3-8ffc22285f6bªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙIWhat does the `max_concurrency` field not control when using a ChatModel?§context¦answer ¨query_id¤Q080¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q080®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:70140e21-f7b3-2032-acaf-324594cf4b5bªfocus_kind¯chain_execution‰¢idÙ@87d0e4cc80dba69cc86cd6ebba6ce956b943248cd3772c303c2300b27f807f9d¢tsËAÚô4¨trace_idÙ$abd8b249-8df2-4df0-96c3-8ffc22285f6b­parent_run_idÙ$eb8ff32d-5314-44ba-86bc-232d9cdd46e1¦run_idÙ$abd8b249-8df2-4df0-96c3-8ffc22285f6bªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q080ªfocus_kind¶chain_execution_result‹¢idÙ@c8550708f86c4de17cce9afcd155f5e467be5cadcc68bc5eab7bb668ea2f0448¢tsËAÚô4×¨trace_id¤Q080­parent_run_idÙ$eb8ff32d-5314-44ba-86bc-232d9cdd46e1¦run_idÙ$81a1e41e-0331-4997-beef-3bb30aadece6ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙIWhat does the `max_concurrency` field not control when using a ChatModel?§context•‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answer ¨query_id¤Q080¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q080®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:9db7c1df-0b28-725a-a773-93de80b79ca8ªfocus_kind¯chain_execution‹¢idÙ@8a0f9f89a2bb5f627db5c794df20c2ea9b4c0e1db2e5da51165ec8fe233d05f0¢tsËAÚô4j¨trace_id¤Q080­parent_run_idÙ$81a1e41e-0331-4997-beef-3bb30aadece6¦run_idÙ$50240607-0517-4c90-81f2-9a6381f6d30aªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙIWhat does the `max_concurrency` field not control when using a ChatModel?§contextÚ—1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.
2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information.

3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.

# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead to infinite recursion if not handled properly. To prevent this, you can set a `recursion_limit` in the `RunnableConfig` dictionary. This will limit the number of times a Runnable can recurse.

### Setting max concurrency

If using the `batch` or `batch_as_completed` methods, you can set the `max_concurrency` attribute in the `RunnableConfig` dictionary to control the maximum number of parallel calls to make. This can be useful when you want to limit the number of parallel calls to prevent overloading a server or API.


:::tip
If you're trying to rate limit the number of requests made by a **Chat Model**, you can use the built-in [rate limiter](/docs/concepts/chat_models#rate-limiting) instead of setting `max_concurrency`, which will be more effective.

## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. Please read more about
the technique in the [structured outputs](/docs/concepts/structured_outputs) guide.

## Multimodality

Large Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as [multimodality](/docs/concepts/multimodality).

Currently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.

## Context window

A chat model's context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.

If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).

The size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.

## Advanced topics
 
### Rate-limiting

Many chat model providers impose a limit on the number of requests that can be made in a given time period.

If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q080®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:9db7c1df-0b28-725a-a773-93de80b79ca8­checkpoint_nsÙ-generate:9db7c1df-0b28-725a-a773-93de80b79ca8¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@e339d906ea25053da3197d22daea8eb076401ecbce555cea69d04bda3136203d¢tsËAÚô4!¨trace_idÙ$50240607-0517-4c90-81f2-9a6381f6d30a­parent_run_idÙ$81a1e41e-0331-4997-beef-3bb30aadece6¦run_idÙ$50240607-0517-4c90-81f2-9a6381f6d30aªevent_type¬chain_output¬event_source¥chain§payload§outputsÚtmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What does the `max_concurrency` field not control when using a ChatModel? \nContext: 1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.\n2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information.\n\n3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\n\n# Do something with the run_id\n```\n\n### Setting recursion limit\n\n:::note\nThis is an advanced feature that is unnecessary for most users.\n:::\n\nSome Runnables may return other Runnables, which can lead to infinite recursion if not handled properly. To prevent this, you can set a `recursion_limit` in the `RunnableConfig` dictionary. This will limit the number of times a Runnable can recurse.\n\n### Setting max concurrency\n\nIf using the `batch` or `batch_as_completed` methods, you can set the `max_concurrency` attribute in the `RunnableConfig` dictionary to control the maximum number of parallel calls to make. This can be useful when you want to limit the number of parallel calls to prevent overloading a server or API.\n\n\n:::tip\nIf you\'re trying to rate limit the number of requests made by a **Chat Model**, you can use the built-in [rate limiter](/docs/concepts/chat_models#rate-limiting) instead of setting `max_concurrency`, which will be more effective.\n\n## Structured outputs\n\nChat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely\nuseful for information extraction tasks. Please read more about\nthe technique in the [structured outputs](/docs/concepts/structured_outputs) guide.\n\n## Multimodality\n\nLarge Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as [multimodality](/docs/concepts/multimodality).\n\nCurrently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.\n\n## Context window\n\nA chat model\'s context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.\n\nIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\n\nThe size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.\n\n## Advanced topics\n \n### Rate-limiting\n\nMany chat model providers impose a limit on the number of requests that can be made in a given time period.\n\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@00e914f9112bd0b61dd0ce94409bbd593ccbf4d04f813105adfaea19257fe646¢tsËAÚôÁäï¨focus_msÍ¦§payload‚¤role©assistant§contentÚœThe `max_concurrency` field does not control the rate at which requests are made to the model provider; that is managed by the `rate_limiter` parameter. Additionally, it does not affect the number of retries allowed after hitting a rate limit, which is controlled by the `max_retries` parameter. Essentially, `max_concurrency` is focused on parallel calls, while rate limiting and retries are handled separately.ªfocus_kind¬exec_latency¨query_id¤Q080‰¢idÙ@f8f5b6264d0aaabb788fbdac6c8206cb3b685f1a0714a1b23537166461130c1b¢tsËAÚôÁø¾¨trace_idÙ$81a1e41e-0331-4997-beef-3bb30aadece6­parent_run_idÙ$eb8ff32d-5314-44ba-86bc-232d9cdd46e1¦run_idÙ$81a1e41e-0331-4997-beef-3bb30aadece6ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚœThe `max_concurrency` field does not control the rate at which requests are made to the model provider; that is managed by the `rate_limiter` parameter. Additionally, it does not affect the number of retries allowed after hitting a rate limit, which is controlled by the `max_retries` parameter. Essentially, `max_concurrency` is focused on parallel calls, while rate limiting and retries are handled separately.¨query_id¤Q080ªfocus_kind¶chain_execution_result‰¢idÙ@010c855b9aa038115def3232d4dd3ab47b27c9cfb3e71fe97cd6596a74d33027¢tsËAÚôÂ!¨trace_idÙ$eb8ff32d-5314-44ba-86bc-232d9cdd46e1­parent_run_idÀ¦run_idÙ$eb8ff32d-5314-44ba-86bc-232d9cdd46e1ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙIWhat does the `max_concurrency` field not control when using a ChatModel?§context•‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚœThe `max_concurrency` field does not control the rate at which requests are made to the model provider; that is managed by the `rate_limiter` parameter. Additionally, it does not affect the number of retries allowed after hitting a rate limit, which is controlled by the `max_retries` parameter. Essentially, `max_concurrency` is focused on parallel calls, while rate limiting and retries are handled separately.¨query_id¤Q080ªfocus_kind¶chain_execution_result‹¢idÙ@24ac36b2cc393fc420fb530802a44b65baa11cace91c062b3630b22fe9eb6097¢tsËAÚôÂˆ¨trace_id¤Q081­parent_run_idÀ¦run_idÙ$2e72488b-2c7c-4ba2-8697-c4f26d67d190ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙBWhich document explains how to call tools in parallel with OpenAI?¨query_id¤Q081§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q081ªfocus_kind¯chain_execution‹¢idÙ@23512c086d9a576b46b372fb810ec5f2bcc1d53bc38574531e7e09c6aaac986b¢tsËAÚôÂ#¨trace_id¤Q081­parent_run_idÙ$2e72488b-2c7c-4ba2-8697-c4f26d67d190¦run_idÙ$d3903bce-56db-497d-b59a-d36891303925ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙBWhich document explains how to call tools in parallel with OpenAI?§context¦answer ¨query_id¤Q081¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q081®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:ad456464-d848-de56-3ea5-1bb3f427149fªfocus_kind¯chain_execution‰¢idÙ@06ace755c9b141077e6a3c4476f8a909b3f4a3c061414c39ab4cf42ac186d225¢tsËAÚôÎ¨trace_idÙ$d3903bce-56db-497d-b59a-d36891303925­parent_run_idÙ$2e72488b-2c7c-4ba2-8697-c4f26d67d190¦run_idÙ$d3903bce-56db-497d-b59a-d36891303925ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË- [How to: create tools](/docs/how_to/custom_tools)
- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)
- [How to: use chat models to call tools](/docs/how_to/tool_calling)
- [How ...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË- [How to: do function/tool calling](/docs/how_to/tool_calling)
- [How to: get models to return structured output](/docs/how_to/structured_output)
- [How to: cache model responses](/docs/how_to/chat_m...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## Tool execution

[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.

[LangGraph]...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙË- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to mo...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Tool calling

:::info[Prerequisites]
* [Tools](/docs/concepts/tools)
* [Chat Models](/docs/concepts/chat_models)
:::


## Overview 

Many AI applications interact directly with humans. In these case...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx¨query_id¤Q081ªfocus_kind¶chain_execution_result‹¢idÙ@c379b021580389e1f511cee4b9332c6195879a792d34458d283f3fdbac36c87c¢tsËAÚôÎ©¨trace_id¤Q081­parent_run_idÙ$2e72488b-2c7c-4ba2-8697-c4f26d67d190¦run_idÙ$71fd0bd0-937c-445e-ade5-563ef2f53fb0ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙBWhich document explains how to call tools in parallel with OpenAI?§context•‚´page_content_summaryÙË- [How to: create tools](/docs/how_to/custom_tools)
- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)
- [How to: use chat models to call tools](/docs/how_to/tool_calling)
- [How ...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË- [How to: do function/tool calling](/docs/how_to/tool_calling)
- [How to: get models to return structured output](/docs/how_to/structured_output)
- [How to: cache model responses](/docs/how_to/chat_m...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## Tool execution

[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.

[LangGraph]...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙË- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to mo...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Tool calling

:::info[Prerequisites]
* [Tools](/docs/concepts/tools)
* [Chat Models](/docs/concepts/chat_models)
:::


## Overview 

Many AI applications interact directly with humans. In these case...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx¦answer ¨query_id¤Q081¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q081®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:3ab57b7a-01d4-d8b5-d730-1b8bdf761c77ªfocus_kind¯chain_execution‹¢idÙ@f3ed4b934c230060604f4974df52831518abeebaa3ae31726b13b09158ecd9a2¢tsËAÚôÎ‚¨trace_id¤Q081­parent_run_idÙ$71fd0bd0-937c-445e-ade5-563ef2f53fb0¦run_idÙ$4a0d4e63-4a81-4a7c-93d1-9e799713c68eªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙBWhich document explains how to call tools in parallel with OpenAI?§contextÚ[- [How to: create tools](/docs/how_to/custom_tools)
- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)
- [How to: use chat models to call tools](/docs/how_to/tool_calling)
- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)
- [How to: pass run time values to tools](/docs/how_to/tool_runtime)
- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)
- [How to: handle tool errors](/docs/how_to/tools_error)
- [How to: force models to call a tool](/docs/how_to/tool_choice)
- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)
- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)
- [How to: stream events from a tool](/docs/how_to/tool_stream_events)
- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)

- [How to: do function/tool calling](/docs/how_to/tool_calling)
- [How to: get models to return structured output](/docs/how_to/structured_output)
- [How to: cache model responses](/docs/how_to/chat_model_caching)
- [How to: get log probabilities](/docs/how_to/logprobs)
- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)
- [How to: stream a response back](/docs/how_to/chat_streaming)
- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)
- [How to: track response metadata across providers](/docs/how_to/response_metadata)
- [How to: use chat model to call tools](/docs/how_to/tool_calling)
- [How to: stream tool calls](/docs/how_to/tool_streaming)
- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)
- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)
- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)
- [How to: force a specific tool call](/docs/how_to/tool_choice)

## Tool execution

[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.

[LangGraph](https://langchain-ai.github.io/langgraph/) offers pre-built components (e.g., [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)) that will often invoke the tool in behalf of the user.

:::info[Further reading]

* See our [how-to guide](/docs/how_to/tool_calling/) on tool calling.
* See the [LangGraph documentation on using ToolNode](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/).

:::

## Best practices

When designing [tools](/docs/concepts/tools/) to be used by a model, it is important to keep in mind that:

- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)
- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)

# Tool calling

:::info[Prerequisites]
* [Tools](/docs/concepts/tools)
* [Chat Models](/docs/concepts/chat_models)
:::


## Overview 

Many AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language.
But what about cases where we want a model to also interact *directly* with systems, such as databases or an API?
These systems often have a particular input schema; for example, APIs frequently have a required payload structure.
This need motivates the concept of *tool calling*. You can use [tool calling](https://platform.openai.com/docs/guides/function-calling/example-use-cases) to request model responses that match a particular schema.

:::info
You will sometimes hear the term `function calling`. We use this term interchangeably with `tool calling`. 
:::

![Conceptual overview of tool calling](/img/tool_calling_concept.png)

## Key concepts¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q081®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:3ab57b7a-01d4-d8b5-d730-1b8bdf761c77­checkpoint_nsÙ-generate:3ab57b7a-01d4-d8b5-d730-1b8bdf761c77¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@d3b3935e047fdd06049d72f337383bcfd07c34baea9a9e9c95ea7bb1bd2679e5¢tsËAÚôÎ%‰¨trace_idÙ$4a0d4e63-4a81-4a7c-93d1-9e799713c68e­parent_run_idÙ$71fd0bd0-937c-445e-ade5-563ef2f53fb0¦run_idÙ$4a0d4e63-4a81-4a7c-93d1-9e799713c68eªevent_type¬chain_output¬event_source¥chain§payload§outputsÚCmessages=[HumanMessage(content="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Which document explains how to call tools in parallel with OpenAI? \nContext: - [How to: create tools](/docs/how_to/custom_tools)\n- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)\n- [How to: use chat models to call tools](/docs/how_to/tool_calling)\n- [How to: pass tool outputs to chat models](/docs/how_to/tool_results_pass_to_model)\n- [How to: pass run time values to tools](/docs/how_to/tool_runtime)\n- [How to: add a human-in-the-loop for tools](/docs/how_to/tools_human)\n- [How to: handle tool errors](/docs/how_to/tools_error)\n- [How to: force models to call a tool](/docs/how_to/tool_choice)\n- [How to: disable parallel tool calling](/docs/how_to/tool_calling_parallel)\n- [How to: access the `RunnableConfig` from a tool](/docs/how_to/tool_configure)\n- [How to: stream events from a tool](/docs/how_to/tool_stream_events)\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\n\n- [How to: do function/tool calling](/docs/how_to/tool_calling)\n- [How to: get models to return structured output](/docs/how_to/structured_output)\n- [How to: cache model responses](/docs/how_to/chat_model_caching)\n- [How to: get log probabilities](/docs/how_to/logprobs)\n- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)\n- [How to: stream a response back](/docs/how_to/chat_streaming)\n- [How to: track token usage](/docs/how_to/chat_token_usage_tracking)\n- [How to: track response metadata across providers](/docs/how_to/response_metadata)\n- [How to: use chat model to call tools](/docs/how_to/tool_calling)\n- [How to: stream tool calls](/docs/how_to/tool_streaming)\n- [How to: handle rate limits](/docs/how_to/chat_model_rate_limiting)\n- [How to: few shot prompt tool behavior](/docs/how_to/tools_few_shot)\n- [How to: bind model-specific formatted tools](/docs/how_to/tools_model_specific)\n- [How to: force a specific tool call](/docs/how_to/tool_choice)\n\n## Tool execution\n\n[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.\n\n[LangGraph](https://langchain-ai.github.io/langgraph/) offers pre-built components (e.g., [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)) that will often invoke the tool in behalf of the user.\n\n:::info[Further reading]\n\n* See our [how-to guide](/docs/how_to/tool_calling/) on tool calling.\n* See the [LangGraph documentation on using ToolNode](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/).\n\n:::\n\n## Best practices\n\nWhen designing [tools](/docs/concepts/tools/) to be used by a model, it is important to keep in mind that:\n\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)\n\n# Tool calling\n\n:::info[Prerequisites]\n* [Tools](/docs/concepts/tools)\n* [Chat Models](/docs/concepts/chat_models)\n:::\n\n\n## Overview \n\nMany AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language.\nBut what about cases where we want a model to also interact *directly* with systems, such as databases or an API?\nThese systems often have a particular input schema; for example, APIs frequently have a required payload structure.\nThis need motivates the concept of *tool calling*. You can use [tool calling](https://platform.openai.com/docs/guides/function-calling/example-use-cases) to request model responses that match a particular schema.\n\n:::info\nYou will sometimes hear the term `function calling`. We use this term interchangeably with `tool calling`. \n:::\n\n![Conceptual overview of tool calling](/img/tool_calling_concept.png)\n\n## Key concepts \nAnswer:", additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@24838a1982cb6de72654be349f16ba57676a13f0af5df211bc39c51320806cc9¢tsËAÚôSè*¨focus_msÍ)§payload‚¤role©assistant§contentÙßThe document that explains how to call tools in parallel with OpenAI is titled "How to: disable parallel tool calling." You can find it in the context provided. For more details, refer to the specific guide on tool calling.ªfocus_kind¬exec_latency¨query_id¤Q081‰¢idÙ@01901f1c68222b733f6f23a1cbf7d296bd09906beeb24944be719742a69fcb96¢tsËAÚôSú'¨trace_idÙ$71fd0bd0-937c-445e-ade5-563ef2f53fb0­parent_run_idÙ$2e72488b-2c7c-4ba2-8697-c4f26d67d190¦run_idÙ$71fd0bd0-937c-445e-ade5-563ef2f53fb0ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙßThe document that explains how to call tools in parallel with OpenAI is titled "How to: disable parallel tool calling." You can find it in the context provided. For more details, refer to the specific guide on tool calling.¨query_id¤Q081ªfocus_kind¶chain_execution_result‰¢idÙ@81af68f16fd950f3dfd198a21a0309bfff2869775af32b71e21d0602e73bfda9¢tsËAÚôT†¨trace_idÙ$2e72488b-2c7c-4ba2-8697-c4f26d67d190­parent_run_idÀ¦run_idÙ$2e72488b-2c7c-4ba2-8697-c4f26d67d190ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙBWhich document explains how to call tools in parallel with OpenAI?§context•‚´page_content_summaryÙË- [How to: create tools](/docs/how_to/custom_tools)
- [How to: use built-in tools and toolkits](/docs/how_to/tools_builtin)
- [How to: use chat models to call tools](/docs/how_to/tool_calling)
- [How ...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË- [How to: do function/tool calling](/docs/how_to/tool_calling)
- [How to: get models to return structured output](/docs/how_to/structured_output)
- [How to: cache model responses](/docs/how_to/chat_m...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## Tool execution

[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.

[LangGraph]...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙË- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to mo...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Tool calling

:::info[Prerequisites]
* [Tools](/docs/concepts/tools)
* [Chat Models](/docs/concepts/chat_models)
:::


## Overview 

Many AI applications interact directly with humans. In these case...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx¦answerÙßThe document that explains how to call tools in parallel with OpenAI is titled "How to: disable parallel tool calling." You can find it in the context provided. For more details, refer to the specific guide on tool calling.¨query_id¤Q081ªfocus_kind¶chain_execution_result‹¢idÙ@63ebeb5ab72b6dd77718abcec71533e754e8c275298aa5297def84cc9a98159a¢tsËAÚôTE¨trace_id¤Q082­parent_run_idÀ¦run_idÙ$8da699a3-76c8-49e9-b314-f4d03b071033ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ?Describe how `RunnableGenerator` differs from `RunnableLambda`.¨query_id¤Q082§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q082ªfocus_kind¯chain_execution‹¢idÙ@1e55c87936618cfcd2fb1896e8f7eb9b73355590836b2b32767f7d6570b5d900¢tsËAÚôT+‘¨trace_id¤Q082­parent_run_idÙ$8da699a3-76c8-49e9-b314-f4d03b071033¦run_idÙ$bb727858-91b4-4812-a91e-88f9cdea59abªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ?Describe how `RunnableGenerator` differs from `RunnableLambda`.§context¦answer ¨query_id¤Q082¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q082®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:7c2a92b4-2557-ba10-e828-f6447bf4030eªfocus_kind¯chain_execution‰¢idÙ@bf57a633ef69f37b71cd18428a0764282d32947138412a3393775f2c1dc12769¢tsËAÚôm\¨trace_idÙ$bb727858-91b4-4812-a91e-88f9cdea59ab­parent_run_idÙ$8da699a3-76c8-49e9-b314-f4d03b071033¦run_idÙ$bb727858-91b4-4812-a91e-88f9cdea59abªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËFor example, the following code:

```python
mapping = {
    "key1": runnable1,
    "key2": runnable2,
}

chain = mapping | runnable3
```

It gets automatically converted to the following:

```python
c...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q082ªfocus_kind¶chain_execution_result‹¢idÙ@d1f6c3da91009572e6b85b3ed5e4aae8d1b901df43e610433ebba9ec25674161¢tsËAÚômÀ¨trace_id¤Q082­parent_run_idÙ$8da699a3-76c8-49e9-b314-f4d03b071033¦run_idÙ$137efd4d-85e4-4e44-8b9a-21eac26b6b15ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ?Describe how `RunnableGenerator` differs from `RunnableLambda`.§context•‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËFor example, the following code:

```python
mapping = {
    "key1": runnable1,
    "key2": runnable2,
}

chain = mapping | runnable3
```

It gets automatically converted to the following:

```python
c...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answer ¨query_id¤Q082¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q082®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:b5c6124b-8099-a85e-514a-56ca641e0989ªfocus_kind¯chain_execution‹¢idÙ@ea786000353b985add0b223cc1b4bf9b460746e85d8d2dd67f77c776dfb18253¢tsËAÚôm¨trace_id¤Q082­parent_run_idÙ$137efd4d-85e4-4e44-8b9a-21eac26b6b15¦run_idÙ$a44ef36e-d718-4bf5-9ebb-9af98ab7a6d6ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ?Describe how `RunnableGenerator` differs from `RunnableLambda`.§contextÚË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)](/docs/concepts/lcel) to compose
multiple Runnables and you need to add custom processing logic in one of the steps.

There are two ways to create a custom Runnable from a function:

* `RunnableLambda`: Use this for simple transformations where streaming is not required.
* `RunnableGenerator`: use this for more complex transformations when streaming is needed.

See the [How to run custom functions](/docs/how_to/functions) guide for more information on how to use `RunnableLambda` and `RunnableGenerator`.

:::important
Users should not try to subclass Runnables to create a new custom Runnable. It is
much more complex and error-prone than simply using `RunnableLambda` or `RunnableGenerator`.
:::

## Configurable runnables

# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](
https://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.

This guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

For example, the following code:

```python
mapping = {
    "key1": runnable1,
    "key2": runnable2,
}

chain = mapping | runnable3
```

It gets automatically converted to the following:

```python
chain = RunnableSequence([RunnableParallel(mapping), runnable3])
```

:::caution
You have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:

```python
mapping.invoke(some_input)
```
:::

#### Function to RunnableLambda

Inside an LCEL expression, a function is automatically converted to a `RunnableLambda`.

```
def some_func(x):
    return x

chain = some_func | runnable1
```

It gets automatically converted to the following:

```python
chain = RunnableSequence([RunnableLambda(some_func), runnable1])
```

## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.
* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.
* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.
* Inspected: Schematic information about Runnable's input, output, and configuration can be accessed.
* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.

Please review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.

<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)
and is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.
:::

Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.

To simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q082®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:b5c6124b-8099-a85e-514a-56ca641e0989­checkpoint_nsÙ-generate:b5c6124b-8099-a85e-514a-56ca641e0989¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@d267bff652e37b08e6334411fd61b2f174e97ab96d118ea0cd6fc19aa654a4c6¢tsËAÚômÆ¨trace_idÙ$a44ef36e-d718-4bf5-9ebb-9af98ab7a6d6­parent_run_idÙ$137efd4d-85e4-4e44-8b9a-21eac26b6b15¦run_idÙ$a44ef36e-d718-4bf5-9ebb-9af98ab7a6d6ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚÌmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Describe how `RunnableGenerator` differs from `RunnableLambda`. \nContext: ## Creating a runnable from a function {#custom-runnables}\n\nYou may need to create a custom Runnable that runs arbitrary logic. This is especially\nuseful if using [LangChain Expression Language (LCEL)](/docs/concepts/lcel) to compose\nmultiple Runnables and you need to add custom processing logic in one of the steps.\n\nThere are two ways to create a custom Runnable from a function:\n\n* `RunnableLambda`: Use this for simple transformations where streaming is not required.\n* `RunnableGenerator`: use this for more complex transformations when streaming is needed.\n\nSee the [How to run custom functions](/docs/how_to/functions) guide for more information on how to use `RunnableLambda` and `RunnableGenerator`.\n\n:::important\nUsers should not try to subclass Runnables to create a new custom Runnable. It is\nmuch more complex and error-prone than simply using `RunnableLambda` or `RunnableGenerator`.\n:::\n\n## Configurable runnables\n\n# Runnable interface\n\nThe Runnable interface is the foundation for working with LangChain components, and it\'s implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](\nhttps://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.\n\nThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.\n\nFor example, the following code:\n\n```python\nmapping = {\n    "key1": runnable1,\n    "key2": runnable2,\n}\n\nchain = mapping | runnable3\n```\n\nIt gets automatically converted to the following:\n\n```python\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\n```\n\n:::caution\nYou have to be careful because the `mapping` dictionary is not a `RunnableParallel` object, it is just a dictionary. This means that the following code will raise an `AttributeError`:\n\n```python\nmapping.invoke(some_input)\n```\n:::\n\n#### Function to RunnableLambda\n\nInside an LCEL expression, a function is automatically converted to a `RunnableLambda`.\n\n```\ndef some_func(x):\n    return x\n\nchain = some_func | runnable1\n```\n\nIt gets automatically converted to the following:\n\n```python\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\n```\n\n## Overview of runnable interface\n\nThe Runnable way defines a standard interface that allows a Runnable component to be:\n\n* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.\n* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.\n* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.\n* Inspected: Schematic information about Runnable\'s input, output, and configuration can be accessed.\n* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.\n\nPlease review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.\n\n<a id="batch"></a>\n### Optimized parallel execution (batch)\n<span data-heading-keywords="batch"></span>\n\n## Configurable runnables\n\n:::note\nThis is an advanced feature that is unnecessary for most users.\n\nIt helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel)\nand is leveraged by [LangServe](/docs/concepts/architecture#langserve) for deployed Runnables.\n:::\n\nSometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.\n\nTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime: \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@4e1ca8c8a442c0d0832eb182e4b3d8e4bfb66ef927da5f02a6dba494a55ce581¢tsËAÚôí˜Ì¨focus_msÍ×§payload‚¤role©assistant§contentÚ¿`RunnableLambda` is designed for simple transformations where streaming is not required, while `RunnableGenerator` is intended for more complex transformations that involve streaming. This distinction allows developers to choose the appropriate runnable based on the complexity and requirements of their processing logic. In summary, use `RunnableLambda` for straightforward tasks and `RunnableGenerator` when streaming capabilities are necessary.ªfocus_kind¬exec_latency¨query_id¤Q082‰¢idÙ@dd64b06f781374b0cf6f0f3c0a0dbd638ca38fff50076480d4cca23141fbc2d4¢tsËAÚôí¤ø¨trace_idÙ$137efd4d-85e4-4e44-8b9a-21eac26b6b15­parent_run_idÙ$8da699a3-76c8-49e9-b314-f4d03b071033¦run_idÙ$137efd4d-85e4-4e44-8b9a-21eac26b6b15ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ¿`RunnableLambda` is designed for simple transformations where streaming is not required, while `RunnableGenerator` is intended for more complex transformations that involve streaming. This distinction allows developers to choose the appropriate runnable based on the complexity and requirements of their processing logic. In summary, use `RunnableLambda` for straightforward tasks and `RunnableGenerator` when streaming capabilities are necessary.¨query_id¤Q082ªfocus_kind¶chain_execution_result‰¢idÙ@e20cd9afc8105d70a7ab64afb177f6c6c001c7ba8ce6aa7eadde0bd6ad2ba0b7¢tsËAÚôí¬Ş¨trace_idÙ$8da699a3-76c8-49e9-b314-f4d03b071033­parent_run_idÀ¦run_idÙ$8da699a3-76c8-49e9-b314-f4d03b071033ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ?Describe how `RunnableGenerator` differs from `RunnableLambda`.§context•‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËFor example, the following code:

```python
mapping = {
    "key1": runnable1,
    "key2": runnable2,
}

chain = mapping | runnable3
```

It gets automatically converted to the following:

```python
c...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚ¿`RunnableLambda` is designed for simple transformations where streaming is not required, while `RunnableGenerator` is intended for more complex transformations that involve streaming. This distinction allows developers to choose the appropriate runnable based on the complexity and requirements of their processing logic. In summary, use `RunnableLambda` for straightforward tasks and `RunnableGenerator` when streaming capabilities are necessary.¨query_id¤Q082ªfocus_kind¶chain_execution_result‹¢idÙ@14491d75c2e41aaa854ceec33b8e36f4251e8febae577a08b79b66e0984a12ec¢tsËAÚôí²î¨trace_id¤Q083­parent_run_idÀ¦run_idÙ$a77dad32-e363-448e-9910-f09d4b09086cªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ@Which vector store integration supports time weighted relevance?¨query_id¤Q083§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q083ªfocus_kind¯chain_execution‹¢idÙ@01d1c252466fff00087c9f0e917c2d0325df980f53ee6f23009943d0e7cb629f¢tsËAÚôí»é¨trace_id¤Q083­parent_run_idÙ$a77dad32-e363-448e-9910-f09d4b09086c¦run_idÙ$ade7bb92-9399-484a-aa37-6516235d2a3aªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ@Which vector store integration supports time weighted relevance?§context¦answer ¨query_id¤Q083¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q083®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:2a6265d4-02f8-90c7-d25c-8934e404f2fbªfocus_kind¯chain_execution‰¢idÙ@4b356ae36281170eb24e58fbc9862d5e826795c10d77370ff5bd1e656e3eb77a¢tsËAÚô¨trace_idÙ$ade7bb92-9399-484a-aa37-6516235d2a3a­parent_run_idÙ$a77dad32-e363-448e-9910-f09d4b09086c¦run_idÙ$ade7bb92-9399-484a-aa37-6516235d2a3aªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËThese vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.

Vector stores are frequently used to search over unstructured data, ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË::: 

#### Vector indexes

Vector indexes are an alternative way to index and store unstructured data.
See our conceptual guide on [vectorstores](/docs/concepts/vectorstores/) for a detailed overview....¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.
* See the [Elast...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË# Vector stores
<span data-heading-keywords="vector,vectorstore,vectorstores,vector store,vector stores"></span>

:::info[Prerequisites]

* [Embeddings](/docs/concepts/embedding_models/)
* [Text split...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙÁAs use cases involving multimodal search and retrieval tasks become more common, we expect to expand the vector store interface to accommodate other data types like images, audio, and video....¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx¨query_id¤Q083ªfocus_kind¶chain_execution_result‹¢idÙ@e060124fcc8a15d8933f4405deed8adb9f7d03c725f8ae2d2e696812ae782078¢tsËAÚô!1¨trace_id¤Q083­parent_run_idÙ$a77dad32-e363-448e-9910-f09d4b09086c¦run_idÙ$f9a3e4f6-ccb1-4845-8d0f-ed1a996f1390ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ@Which vector store integration supports time weighted relevance?§context•‚´page_content_summaryÙËThese vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.

Vector stores are frequently used to search over unstructured data, ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË::: 

#### Vector indexes

Vector indexes are an alternative way to index and store unstructured data.
See our conceptual guide on [vectorstores](/docs/concepts/vectorstores/) for a detailed overview....¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.
* See the [Elast...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË# Vector stores
<span data-heading-keywords="vector,vectorstore,vectorstores,vector store,vector stores"></span>

:::info[Prerequisites]

* [Embeddings](/docs/concepts/embedding_models/)
* [Text split...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙÁAs use cases involving multimodal search and retrieval tasks become more common, we expect to expand the vector store interface to accommodate other data types like images, audio, and video....¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx¦answer ¨query_id¤Q083¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q083®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:daf6f401-0836-b391-8bbb-f22e6bf154bfªfocus_kind¯chain_execution‹¢idÙ@660d39e44842c6ca0e9c72fd5f3edf2610f7c945905b59eaa609eb07444c7e01¢tsËAÚô%·¨trace_id¤Q083­parent_run_idÙ$f9a3e4f6-ccb1-4845-8d0f-ed1a996f1390¦run_idÙ$3d766b79-80db-4d77-9a4b-38b6efb1cfa5ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ@Which vector store integration supports time weighted relevance?§contextÚèThese vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.

Vector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.

![Vector stores](/img/vectorstores.png)

## Integrations

LangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.

Please see the [full list of LangChain vectorstore integrations](/docs/integrations/vectorstores/).

## Interface

LangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations.

The interface consists of basic methods for writing, deleting and searching for documents in the vector store.

The key methods are:

::: 

#### Vector indexes

Vector indexes are an alternative way to index and store unstructured data.
See our conceptual guide on [vectorstores](/docs/concepts/vectorstores/) for a detailed overview.  
In short, rather than using word frequencies, vectorstores use an [embedding model](/docs/concepts/embedding_models/) to compress documents into high-dimensional vector representation. 
This allows for efficient similarity search over embedding vectors using simple mathematical operations like cosine similarity.

:::info[Further reading]

* See our [how-to guide](/docs/how_to/vectorstore_retriever/) for more details on working with vectorstores.
* See our [list of vectorstore integrations](/docs/integrations/vectorstores/).
* See Cameron Wolfe's [blog post](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2) on the basics of vector search.

:::

#### Relational databases

:::info[Further reading]

* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.
* See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.

::: 

### Vector store 

[Vector stores](/docs/concepts/vectorstores/) are a powerful and efficient way to index and retrieve unstructured data. 
A vectorstore can be used as a retriever by calling the `as_retriever()` method.

```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```

## Advanced retrieval patterns

### Ensemble

# Vector stores
<span data-heading-keywords="vector,vectorstore,vectorstores,vector store,vector stores"></span>

:::info[Prerequisites]

* [Embeddings](/docs/concepts/embedding_models/)
* [Text splitters](/docs/concepts/text_splitters/)

:::
:::info[Note]

This conceptual overview focuses on text-based indexing and retrieval for simplicity. 
However, embedding models can be [multi-modal](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)
and vector stores can be used to store and retrieve a variety of data types beyond text.
:::

## Overview

Vector stores are specialized data stores that enable indexing and retrieving information based on vector representations.

These vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.

As use cases involving multimodal search and retrieval tasks become more common, we expect to expand the vector store interface to accommodate other data types like images, audio, and video.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q083®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:daf6f401-0836-b391-8bbb-f22e6bf154bf­checkpoint_nsÙ-generate:daf6f401-0836-b391-8bbb-f22e6bf154bf¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@383e90c0a08cc0d9070c3fa3ce91a7129f85a6c9db955f008f4cd2dc379fafb9¢tsËAÚô,ú¨trace_idÙ$3d766b79-80db-4d77-9a4b-38b6efb1cfa5­parent_run_idÙ$f9a3e4f6-ccb1-4845-8d0f-ed1a996f1390¦run_idÙ$3d766b79-80db-4d77-9a4b-38b6efb1cfa5ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚÜmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Which vector store integration supports time weighted relevance? \nContext: These vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.\n\nVector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.\n\n![Vector stores](/img/vectorstores.png)\n\n## Integrations\n\nLangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.\n\nPlease see the [full list of LangChain vectorstore integrations](/docs/integrations/vectorstores/).\n\n## Interface\n\nLangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations.\n\nThe interface consists of basic methods for writing, deleting and searching for documents in the vector store.\n\nThe key methods are:\n\n::: \n\n#### Vector indexes\n\nVector indexes are an alternative way to index and store unstructured data.\nSee our conceptual guide on [vectorstores](/docs/concepts/vectorstores/) for a detailed overview.  \nIn short, rather than using word frequencies, vectorstores use an [embedding model](/docs/concepts/embedding_models/) to compress documents into high-dimensional vector representation. \nThis allows for efficient similarity search over embedding vectors using simple mathematical operations like cosine similarity.\n\n:::info[Further reading]\n\n* See our [how-to guide](/docs/how_to/vectorstore_retriever/) for more details on working with vectorstores.\n* See our [list of vectorstore integrations](/docs/integrations/vectorstores/).\n* See Cameron Wolfe\'s [blog post](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2) on the basics of vector search.\n\n:::\n\n#### Relational databases\n\n:::info[Further reading]\n\n* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.\n* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.\n* See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.\n\n::: \n\n### Vector store \n\n[Vector stores](/docs/concepts/vectorstores/) are a powerful and efficient way to index and retrieve unstructured data. \nA vectorstore can be used as a retriever by calling the `as_retriever()` method.\n\n```python\nvectorstore = MyVectorStore()\nretriever = vectorstore.as_retriever()\n```\n\n## Advanced retrieval patterns\n\n### Ensemble\n\n# Vector stores\n<span data-heading-keywords="vector,vectorstore,vectorstores,vector store,vector stores"></span>\n\n:::info[Prerequisites]\n\n* [Embeddings](/docs/concepts/embedding_models/)\n* [Text splitters](/docs/concepts/text_splitters/)\n\n:::\n:::info[Note]\n\nThis conceptual overview focuses on text-based indexing and retrieval for simplicity. \nHowever, embedding models can be [multi-modal](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-multimodal-embeddings)\nand vector stores can be used to store and retrieve a variety of data types beyond text.\n:::\n\n## Overview\n\nVector stores are specialized data stores that enable indexing and retrieving information based on vector representations.\n\nThese vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.\n\nAs use cases involving multimodal search and retrieval tasks become more common, we expect to expand the vector store interface to accommodate other data types like images, audio, and video. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@ac2cd32fc51ef6c6b4d31127eabbeb8a7fe187208aa688900f925f02297eea29¢tsËAÚô=,„¨focus_msÍ©§payload‚¤role©assistant§contentÙ‹The retrieved context does not specify which vector store integration supports time weighted relevance. Therefore, I don't know the answer.ªfocus_kind¬exec_latency¨query_id¤Q083‰¢idÙ@6d33a8471154bc38e549781dea07142bcba806b90e72eb039d5fdbd4c04f1177¢tsËAÚô=8¨¨trace_idÙ$f9a3e4f6-ccb1-4845-8d0f-ed1a996f1390­parent_run_idÙ$a77dad32-e363-448e-9910-f09d4b09086c¦run_idÙ$f9a3e4f6-ccb1-4845-8d0f-ed1a996f1390ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙ‹The retrieved context does not specify which vector store integration supports time weighted relevance. Therefore, I don't know the answer.¨query_id¤Q083ªfocus_kind¶chain_execution_result‰¢idÙ@06d56e933889b835bc8712a17d071468ab76d087595e6f53886e360788c81bbb¢tsËAÚô=A¨trace_idÙ$a77dad32-e363-448e-9910-f09d4b09086c­parent_run_idÀ¦run_idÙ$a77dad32-e363-448e-9910-f09d4b09086cªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ@Which vector store integration supports time weighted relevance?§context•‚´page_content_summaryÙËThese vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.

Vector stores are frequently used to search over unstructured data, ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË::: 

#### Vector indexes

Vector indexes are an alternative way to index and store unstructured data.
See our conceptual guide on [vectorstores](/docs/concepts/vectorstores/) for a detailed overview....¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.
* See the [Elast...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË# Vector stores
<span data-heading-keywords="vector,vectorstore,vectorstores,vector store,vector stores"></span>

:::info[Prerequisites]

* [Embeddings](/docs/concepts/embedding_models/)
* [Text split...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙÁAs use cases involving multimodal search and retrieval tasks become more common, we expect to expand the vector store interface to accommodate other data types like images, audio, and video....¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx¦answerÙ‹The retrieved context does not specify which vector store integration supports time weighted relevance. Therefore, I don't know the answer.¨query_id¤Q083ªfocus_kind¶chain_execution_result‹¢idÙ@5bcecb3010921141eaae2e7831acd38286c33a808620d45ab923a71d7eb9a2d6¢tsËAÚô=Gú¨trace_id¤Q084­parent_run_idÀ¦run_idÙ$36a81ab6-9373-4876-8484-364f57e7f05dªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ@Explain how to use `run_name` for easier debugging in LangSmith.¨query_id¤Q084§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q084ªfocus_kind¯chain_execution‹¢idÙ@69b09b75110b2eb4f77a9114b088089fa9fe7a355a6e0364c7b4f9ab6f116fc5¢tsËAÚô=Q¨trace_id¤Q084­parent_run_idÙ$36a81ab6-9373-4876-8484-364f57e7f05d¦run_idÙ$9fd17a31-d3a0-47bd-869e-861ce3f1ff55ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ@Explain how to use `run_name` for easier debugging in LangSmith.§context¦answer ¨query_id¤Q084¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q084®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:385462f8-6b0f-6faf-9ec6-b7a981bddc98ªfocus_kind¯chain_execution‰¢idÙ@7c9c0bcc5966141d4c441fe6835ff796ba8d9936116d02b964d226bab01a51c7¢tsËAÚôJ\¦¨trace_idÙ$9fd17a31-d3a0-47bd-869e-861ce3f1ff55­parent_run_idÙ$36a81ab6-9373-4876-8484-364f57e7f05d¦run_idÙ$9fd17a31-d3a0-47bd-869e-861ce3f1ff55ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË### Setting custom run name, tags, and metadata

The `run_name`, `tags`, and `metadata` attributes of the `RunnableConfig` dictionary can be used to set custom values for the run name, tags, and metad...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Tracing

<span data-heading-keywords="trace,tracing"></span>

A trace is essentially a series of steps that your application takes to go from input to output.
Traces contain individual steps called ...¨metadata¦sourceÙF/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tracing.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocati...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¨query_id¤Q084ªfocus_kind¶chain_execution_result‹¢idÙ@f3ce5473e8d80359fc43e9c4b98d30c079d5aa87aa16673bb6d6352287657d20¢tsËAÚôJir¨trace_id¤Q084­parent_run_idÙ$36a81ab6-9373-4876-8484-364f57e7f05d¦run_idÙ$69e59807-6486-490c-ae2e-0b119372275cªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ@Explain how to use `run_name` for easier debugging in LangSmith.§context•‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË### Setting custom run name, tags, and metadata

The `run_name`, `tags`, and `metadata` attributes of the `RunnableConfig` dictionary can be used to set custom values for the run name, tags, and metad...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Tracing

<span data-heading-keywords="trace,tracing"></span>

A trace is essentially a series of steps that your application takes to go from input to output.
Traces contain individual steps called ...¨metadata¦sourceÙF/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tracing.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocati...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answer ¨query_id¤Q084¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q084®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:3ce282f4-f7c3-6113-fd0f-7375f5775fedªfocus_kind¯chain_execution‹¢idÙ@0f1e5ae311e8d07beadb59397073d369ba2b6e5afbe4b9fb7a5b2e1ff3ef21d2¢tsËAÚôJqã¨trace_id¤Q084­parent_run_idÙ$69e59807-6486-490c-ae2e-0b119372275c¦run_idÙ$64714817-2382-4c30-9428-e057523fb8bbªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ@Explain how to use `run_name` for easier debugging in LangSmith.§contextÚ„## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.

LangSmith documentation is hosted on a separate site.
You can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/).

### Evaluation

LangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:

- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)

### Setting custom run name, tags, and metadata

The `run_name`, `tags`, and `metadata` attributes of the `RunnableConfig` dictionary can be used to set custom values for the run name, tags, and metadata for a given Runnable.

The `run_name` is a string that can be used to set a custom name for the run. This name will be used in logs and other places to identify the run. It is not inherited by sub-calls.

The `tags` and `metadata` attributes are lists and dictionaries, respectively, that can be used to set custom tags and metadata for the run. These values are inherited by sub-calls.

Using these attributes can be useful for tracking and debugging runs, as they will be surfaced in [LangSmith](https://docs.smith.langchain.com/) as trace attributes that you can
filter and search on.

The attributes will also be propagated to [callbacks](/docs/concepts/callbacks), and will appear in streaming APIs like [astream_events](/docs/concepts/streaming) as part of each event in the stream.

# Tracing

<span data-heading-keywords="trace,tracing"></span>

A trace is essentially a series of steps that your application takes to go from input to output.
Traces contain individual steps called `runs`. These can be individual calls from a model, retriever,
tool, or sub-chains.
Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.

For a deeper dive, check out [this LangSmith conceptual guide](https://docs.smith.langchain.com/concepts/tracing).

[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).

[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.

- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocation args to runnables](/docs/how_to/binding/)
- [How to: turn any function into a runnable](/docs/how_to/functions)
- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)
- [How to: configure runnable behavior at runtime](/docs/how_to/configure)
- [How to: add message history (memory) to a chain](/docs/how_to/message_history)
- [How to: route between sub-chains](/docs/how_to/routing)
- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)
- [How to: inspect runnables](/docs/how_to/inspect)
- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)
- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)

## [LangGraph](https://langchain-ai.github.io/langgraph)¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q084®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:3ce282f4-f7c3-6113-fd0f-7375f5775fed­checkpoint_nsÙ-generate:3ce282f4-f7c3-6113-fd0f-7375f5775fed¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@fd29fa5c17641b0d0596735f8edae1f552e8fba26f1f8bfadb7f9789b264c79a¢tsËAÚôJ{Ğ¨trace_idÙ$64714817-2382-4c30-9428-e057523fb8bb­parent_run_idÙ$69e59807-6486-490c-ae2e-0b119372275c¦run_idÙ$64714817-2382-4c30-9428-e057523fb8bbªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ\messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Explain how to use `run_name` for easier debugging in LangSmith. \nContext: ## LangSmith\n\nLangSmith allows you to closely trace, monitor and evaluate your LLM application.\nIt seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of your chains as you build.\n\nLangSmith documentation is hosted on a separate site.\nYou can peruse [LangSmith tutorials here](https://docs.smith.langchain.com/).\n\n### Evaluation\n\nLangSmith helps you evaluate the performance of your LLM applications. The tutorial below is a great way to get started:\n\n- [Evaluate your LLM application](https://docs.smith.langchain.com/tutorials/Developers/evaluation)\n\n### Setting custom run name, tags, and metadata\n\nThe `run_name`, `tags`, and `metadata` attributes of the `RunnableConfig` dictionary can be used to set custom values for the run name, tags, and metadata for a given Runnable.\n\nThe `run_name` is a string that can be used to set a custom name for the run. This name will be used in logs and other places to identify the run. It is not inherited by sub-calls.\n\nThe `tags` and `metadata` attributes are lists and dictionaries, respectively, that can be used to set custom tags and metadata for the run. These values are inherited by sub-calls.\n\nUsing these attributes can be useful for tracking and debugging runs, as they will be surfaced in [LangSmith](https://docs.smith.langchain.com/) as trace attributes that you can\nfilter and search on.\n\nThe attributes will also be propagated to [callbacks](/docs/concepts/callbacks), and will appear in streaming APIs like [astream_events](/docs/concepts/streaming) as part of each event in the stream.\n\n# Tracing\n\n<span data-heading-keywords="trace,tracing"></span>\n\nA trace is essentially a series of steps that your application takes to go from input to output.\nTraces contain individual steps called `runs`. These can be individual calls from a model, retriever,\ntool, or sub-chains.\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\n\nFor a deeper dive, check out [this LangSmith conceptual guide](https://docs.smith.langchain.com/concepts/tracing).\n\n[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like including [persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/), [human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/), or [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\n\n[LangSmith](https://docs.smith.langchain.com/) makes it possible to iterate with confidence on your applications, by providing LLM-specific observability and framework for testing and evaluating your application.\n\n- [How to: chain runnables](/docs/how_to/sequence)\n- [How to: stream runnables](/docs/how_to/streaming)\n- [How to: invoke runnables in parallel](/docs/how_to/parallel/)\n- [How to: add default invocation args to runnables](/docs/how_to/binding/)\n- [How to: turn any function into a runnable](/docs/how_to/functions)\n- [How to: pass through inputs from one chain step to the next](/docs/how_to/passthrough)\n- [How to: configure runnable behavior at runtime](/docs/how_to/configure)\n- [How to: add message history (memory) to a chain](/docs/how_to/message_history)\n- [How to: route between sub-chains](/docs/how_to/routing)\n- [How to: create a dynamic (self-constructing) chain](/docs/how_to/dynamic_chain/)\n- [How to: inspect runnables](/docs/how_to/inspect)\n- [How to: add fallbacks to a runnable](/docs/how_to/fallbacks)\n- [How to: pass runtime secrets to a runnable](/docs/how_to/runnable_runtime_secrets)\n\n## [LangGraph](https://langchain-ai.github.io/langgraph) \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@4eb7d82cc0d5d3c1f41466cb62023808d26d91a1053b171c702b889579e2ac50¢tsËAÚôşF(¨focus_msÍ
ø§payload‚¤role©assistant§contentÚhTo use `run_name` for easier debugging in LangSmith, set it as a custom string in the `RunnableConfig` dictionary for your run. This name will help identify the run in logs and other outputs, making it easier to trace and monitor your LLM application. Remember that the `run_name` is not inherited by sub-calls, so you may need to set it for each relevant run.ªfocus_kind¬exec_latency¨query_id¤Q084‰¢idÙ@b7fbbb1f07af22a653b87c8356845d69d88063bd9af1b348c0190244441a9285¢tsËAÚôşVï¨trace_idÙ$69e59807-6486-490c-ae2e-0b119372275c­parent_run_idÙ$36a81ab6-9373-4876-8484-364f57e7f05d¦run_idÙ$69e59807-6486-490c-ae2e-0b119372275cªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚhTo use `run_name` for easier debugging in LangSmith, set it as a custom string in the `RunnableConfig` dictionary for your run. This name will help identify the run in logs and other outputs, making it easier to trace and monitor your LLM application. Remember that the `run_name` is not inherited by sub-calls, so you may need to set it for each relevant run.¨query_id¤Q084ªfocus_kind¶chain_execution_result‰¢idÙ@f0008ecce25869faec8e3ee88e62ab1ae8ff4592018f03b35c19c65eed6e440f¢tsËAÚôş_³¨trace_idÙ$36a81ab6-9373-4876-8484-364f57e7f05d­parent_run_idÀ¦run_idÙ$36a81ab6-9373-4876-8484-364f57e7f05dªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ@Explain how to use `run_name` for easier debugging in LangSmith.§context•‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË### Setting custom run name, tags, and metadata

The `run_name`, `tags`, and `metadata` attributes of the `RunnableConfig` dictionary can be used to set custom values for the run name, tags, and metad...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Tracing

<span data-heading-keywords="trace,tracing"></span>

A trace is essentially a series of steps that your application takes to go from input to output.
Traces contain individual steps called ...¨metadata¦sourceÙF/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tracing.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocati...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answerÚhTo use `run_name` for easier debugging in LangSmith, set it as a custom string in the `RunnableConfig` dictionary for your run. This name will help identify the run in logs and other outputs, making it easier to trace and monitor your LLM application. Remember that the `run_name` is not inherited by sub-calls, so you may need to set it for each relevant run.¨query_id¤Q084ªfocus_kind¶chain_execution_result‹¢idÙ@acf9fd9269da6bd6d1aab71854aebbcfa4daac378b8db356d066e47857dd15d6¢tsËAÚôşhw¨trace_id¤Q085­parent_run_idÀ¦run_idÙ$ba86fd76-6d6a-47d4-979c-0452667195e4ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙXWhich two parameters must be provided to initialize `AzureAIDocumentIntelligenceLoader`?¨query_id¤Q085§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q085ªfocus_kind¯chain_execution‹¢idÙ@b37335bc079e11432c74a6dd2ed1949761e51c8a2cc4638182d33ea3a261a9d0¢tsËAÚôşb¨trace_id¤Q085­parent_run_idÙ$ba86fd76-6d6a-47d4-979c-0452667195e4¦run_idÙ$5eb3828b-1d8c-4c40-9415-1fd515e7d265ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙXWhich two parameters must be provided to initialize `AzureAIDocumentIntelligenceLoader`?§context¦answer ¨query_id¤Q085¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q085®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:a8d11bc5-4270-e71d-47a0-c3ee32e0a0caªfocus_kind¯chain_execution‰¢idÙ@ddbc810b7afd43f2bff332d3b9c647df81deea7df274a36cfc45ef0a46d7bcca¢tsËAÚôº¡¨trace_idÙ$5eb3828b-1d8c-4c40-9415-1fd515e7d265­parent_run_idÙ$ba86fd76-6d6a-47d4-979c-0452667195e4¦run_idÙ$5eb3828b-1d8c-4c40-9415-1fd515e7d265ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË### Prerequisite

An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-s...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader

[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning 
b...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË# How to load Microsoft Office files

The [Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, ...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË| Parameter      | Description                                                                                                                                                                          ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Document loaders
<span data-heading-keywords="document loader,document loaders"></span>

:::info[Prerequisites]

* [Document loaders API reference](/docs/how_to/#document-loaders)
:::

Document load...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/document_loaders.mdx¨query_id¤Q085ªfocus_kind¶chain_execution_result‹¢idÙ@52391455d9728664d95db8c221cf2fea3c6811692711b7c6fc6f27094dbca1ec¢tsËAÚôÅ¨trace_id¤Q085­parent_run_idÙ$ba86fd76-6d6a-47d4-979c-0452667195e4¦run_idÙ$b4e513d1-7e2a-462f-8796-eb1e58b25a0eªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙXWhich two parameters must be provided to initialize `AzureAIDocumentIntelligenceLoader`?§context•‚´page_content_summaryÙË### Prerequisite

An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-s...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader

[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning 
b...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË# How to load Microsoft Office files

The [Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, ...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË| Parameter      | Description                                                                                                                                                                          ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Document loaders
<span data-heading-keywords="document loader,document loaders"></span>

:::info[Prerequisites]

* [Document loaders API reference](/docs/how_to/#document-loaders)
:::

Document load...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/document_loaders.mdx¦answer ¨query_id¤Q085¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q085®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:982a864d-66cd-9bf8-54cf-d6428a2e9fa5ªfocus_kind¯chain_execution‹¢idÙ@4b2ae8d0cc3eeaad5ff631edb0f96eddc45ddfcb922d480d544f2eac3f983531¢tsËAÚôÍ¨trace_id¤Q085­parent_run_idÙ$b4e513d1-7e2a-462f-8796-eb1e58b25a0e¦run_idÙ$cc7c7b55-1952-4859-ad5a-8d00846f6e76ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙXWhich two parameters must be provided to initialize `AzureAIDocumentIntelligenceLoader`?§contextÚ$### Prerequisite

An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0) to create one if you don't have. You will be passing `<endpoint>` and `<key>` as parameters to the loader.

```python
%pip install --upgrade --quiet  langchain langchain-community azure-ai-documentintelligence

from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader

file_path = "<filepath>"
endpoint = "<endpoint>"
key = "<key>"
loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint, api_key=key, file_path=file_path, api_model="prebuilt-layout"
)

documents = loader.load()
```

## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader

[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning 
based service that extracts texts (including handwriting), tables, document structures (e.g., titles, section headings, etc.) and key-value-pairs from
digital or scanned PDFs, images, Office and HTML files. Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`.

This [current implementation](https://aka.ms/di-langchain) of a loader using `Document Intelligence` can incorporate content page-wise and turn it into LangChain documents. The default output format is markdown, which can be easily chained with `MarkdownHeaderTextSplitter` for semantic document chunking. You can also use `mode="single"` or `mode="page"` to return pure texts in a single page or document split by page.

### Prerequisite

# How to load Microsoft Office files

The [Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.

This covers how to load commonly used file formats including `DOCX`, `XLSX` and `PPTX` documents into a LangChain 
[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document)
object that we can use downstream.


## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader

| Parameter      | Description                                                                                                                                                                                                                                                                                                    |
|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `model`        | The name or identifier of the specific AI model you want to use (e.g., `"gpt-3.5-turbo"` or `"gpt-4"`).                                                                                                                                                                                                        |

# Document loaders
<span data-heading-keywords="document loader,document loaders"></span>

:::info[Prerequisites]

* [Document loaders API reference](/docs/how_to/#document-loaders)
:::

Document loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.

## Integrations

You can find available integrations on the [Document loaders integrations page](/docs/integrations/document_loaders/).

## Interface

Documents loaders implement the [BaseLoader interface](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html).

Each DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the `.load` method or `.lazy_load`.

Here's a simple example:

```python
from langchain_community.document_loaders.csv_loader import CSVLoader¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q085®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:982a864d-66cd-9bf8-54cf-d6428a2e9fa5­checkpoint_nsÙ-generate:982a864d-66cd-9bf8-54cf-d6428a2e9fa5¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@e2d8c65466b62c94a6a332c48b796af8c2e04174f9d0d8b8db6c23b973ec54fa¢tsËAÚôÔw¨trace_idÙ$cc7c7b55-1952-4859-ad5a-8d00846f6e76­parent_run_idÙ$b4e513d1-7e2a-462f-8796-eb1e58b25a0e¦run_idÙ$cc7c7b55-1952-4859-ad5a-8d00846f6e76ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ!messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Which two parameters must be provided to initialize `AzureAIDocumentIntelligenceLoader`? \nContext: ### Prerequisite\n\nAn Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-services/document-intelligence/create-document-intelligence-resource?view=doc-intel-4.0.0) to create one if you don\'t have. You will be passing `<endpoint>` and `<key>` as parameters to the loader.\n\n```python\n%pip install --upgrade --quiet  langchain langchain-community azure-ai-documentintelligence\n\nfrom langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n\nfile_path = "<filepath>"\nendpoint = "<endpoint>"\nkey = "<key>"\nloader = AzureAIDocumentIntelligenceLoader(\n    api_endpoint=endpoint, api_key=key, file_path=file_path, api_model="prebuilt-layout"\n)\n\ndocuments = loader.load()\n```\n\n## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader\n\n[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning \nbased service that extracts texts (including handwriting), tables, document structures (e.g., titles, section headings, etc.) and key-value-pairs from\ndigital or scanned PDFs, images, Office and HTML files. Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`.\n\nThis [current implementation](https://aka.ms/di-langchain) of a loader using `Document Intelligence` can incorporate content page-wise and turn it into LangChain documents. The default output format is markdown, which can be easily chained with `MarkdownHeaderTextSplitter` for semantic document chunking. You can also use `mode="single"` or `mode="page"` to return pure texts in a single page or document split by page.\n\n### Prerequisite\n\n# How to load Microsoft Office files\n\nThe [Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, and Microsoft OneNote. It is available for Microsoft Windows and macOS operating systems. It is also available on Android and iOS.\n\nThis covers how to load commonly used file formats including `DOCX`, `XLSX` and `PPTX` documents into a LangChain \n[Document](https://python.langchain.com/api_reference/core/documents/langchain_core.documents.base.Document.html#langchain_core.documents.base.Document)\nobject that we can use downstream.\n\n\n## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader\n\n| Parameter      | Description                                                                                                                                                                                                                                                                                                    |\n|----------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `model`        | The name or identifier of the specific AI model you want to use (e.g., `"gpt-3.5-turbo"` or `"gpt-4"`).                                                                                                                                                                                                        |\n\n# Document loaders\n<span data-heading-keywords="document loader,document loaders"></span>\n\n:::info[Prerequisites]\n\n* [Document loaders API reference](/docs/how_to/#document-loaders)\n:::\n\nDocument loaders are designed to load document objects. LangChain has hundreds of integrations with various data sources to load data from: Slack, Notion, Google Drive, etc.\n\n## Integrations\n\nYou can find available integrations on the [Document loaders integrations page](/docs/integrations/document_loaders/).\n\n## Interface\n\nDocuments loaders implement the [BaseLoader interface](https://python.langchain.com/api_reference/core/document_loaders/langchain_core.document_loaders.base.BaseLoader.html).\n\nEach DocumentLoader has its own specific parameters, but they can all be invoked in the same way with the `.load` method or `.lazy_load`.\n\nHere\'s a simple example:\n\n```python\nfrom langchain_community.document_loaders.csv_loader import CSVLoader \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@05b6fe6348d210e4d57a41b62de70b3176c995d02f79e5414055bdea40dcea45¢tsËAÚôm‚J¨focus_msÍÖ§payload‚¤role©assistant§contentÚTo initialize `AzureAIDocumentIntelligenceLoader`, you must provide the `<endpoint>` and `<key>` parameters. These are essential for connecting to the Azure AI Document Intelligence resource. Additionally, you will also need to specify the `file_path` and optionally the `api_model`.ªfocus_kind¬exec_latency¨query_id¤Q085‰¢idÙ@9eb4b30fa560236769a5a34e1be25bbb3968bf0a158ae099cf791be1be1b7669¢tsËAÚômŠ¨trace_idÙ$b4e513d1-7e2a-462f-8796-eb1e58b25a0e­parent_run_idÙ$ba86fd76-6d6a-47d4-979c-0452667195e4¦run_idÙ$b4e513d1-7e2a-462f-8796-eb1e58b25a0eªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚTo initialize `AzureAIDocumentIntelligenceLoader`, you must provide the `<endpoint>` and `<key>` parameters. These are essential for connecting to the Azure AI Document Intelligence resource. Additionally, you will also need to specify the `file_path` and optionally the `api_model`.¨query_id¤Q085ªfocus_kind¶chain_execution_result‰¢idÙ@ea8d7c1f2e11faf8d0a4bae9e0dd0c2eac875bf5a5ee865840fb08cda9198307¢tsËAÚômœN¨trace_idÙ$ba86fd76-6d6a-47d4-979c-0452667195e4­parent_run_idÀ¦run_idÙ$ba86fd76-6d6a-47d4-979c-0452667195e4ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙXWhich two parameters must be provided to initialize `AzureAIDocumentIntelligenceLoader`?§context•‚´page_content_summaryÙË### Prerequisite

An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-s...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader

[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning 
b...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË# How to load Microsoft Office files

The [Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, ...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË| Parameter      | Description                                                                                                                                                                          ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Document loaders
<span data-heading-keywords="document loader,document loaders"></span>

:::info[Prerequisites]

* [Document loaders API reference](/docs/how_to/#document-loaders)
:::

Document load...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/document_loaders.mdx¦answerÚTo initialize `AzureAIDocumentIntelligenceLoader`, you must provide the `<endpoint>` and `<key>` parameters. These are essential for connecting to the Azure AI Document Intelligence resource. Additionally, you will also need to specify the `file_path` and optionally the `api_model`.¨query_id¤Q085ªfocus_kind¶chain_execution_result‹¢idÙ@166796353d0712d079003eeb148d679ed5695722deca6c85eaf6dd4fa6c32c14¢tsËAÚôm¥b¨trace_id¤Q086­parent_run_idÀ¦run_idÙ$fda11c1d-5f25-4a76-937a-f94662bdae3dªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ@What is the main benefit of `asam_stream` events over `astream`?¨query_id¤Q086§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q086ªfocus_kind¯chain_execution‹¢idÙ@e727634c566bebde367fd9d428c018b251ec5a7c785eaac854a2edcb94c0d6f9¢tsËAÚômµ¿¨trace_id¤Q086­parent_run_idÙ$fda11c1d-5f25-4a76-937a-f94662bdae3d¦run_idÙ$1ffa6ec7-e896-4ea4-bc12-622693228fc1ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ@What is the main benefit of `asam_stream` events over `astream`?§context¦answer ¨query_id¤Q086¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q086®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:b79a0b7b-2771-3381-26c4-935c605935b5ªfocus_kind¯chain_execution‰¢idÙ@bbf4fba39b3dc0e135740b55bfbed031051fb693e59103770c5278f1ab473b09¢tsËAÚô~\¨trace_idÙ$1ffa6ec7-e896-4ea4-bc12-622693228fc1­parent_run_idÙ$fda11c1d-5f25-4a76-937a-f94662bdae3d¦run_idÙ$1ffa6ec7-e896-4ea4-bc12-622693228fc1ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙÍ#### Usage with LCEL

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the outp...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËThe [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming b...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË:::note
In addition, there is a **legacy** async [astream_log](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runna...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËmodel = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¨query_id¤Q086ªfocus_kind¶chain_execution_result‹¢idÙ@4194dc297c189d55c4a06e1c4088c23bb088ece033cb903633e3775b1e92bcc9¢tsËAÚô~´¨trace_id¤Q086­parent_run_idÙ$fda11c1d-5f25-4a76-937a-f94662bdae3d¦run_idÙ$4417bc55-f7b7-4a17-b219-cca449930cc1ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ@What is the main benefit of `asam_stream` events over `astream`?§context•‚´page_content_summaryÙÍ#### Usage with LCEL

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the outp...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËThe [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming b...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË:::note
In addition, there is a **legacy** async [astream_log](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runna...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËmodel = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¦answer ¨query_id¤Q086¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q086®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:6339069c-ac55-9e54-dcf5-c6a05a887d09ªfocus_kind¯chain_execution‹¢idÙ@9e390d84e5fc81a8f52a915a6fa5636996b680604c99d7c068f6ca2978c046e1¢tsËAÚô~g¨trace_id¤Q086­parent_run_idÙ$4417bc55-f7b7-4a17-b219-cca449930cc1¦run_idÙ$f1a2470f-1138-41a4-8771-69c46fc9562fªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ@What is the main benefit of `asam_stream` events over `astream`?§contextÚ>#### Usage with LCEL

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the output of the last step in the chain. This allows the final processed result to be streamed incrementally. **LCEL** tries to optimize streaming latency in pipelines so that the streaming results from the last step are available as soon as possible.



### `astream_events`
<span data-heading-keywords="astream_events,stream_events,stream events"></span>

:::tip
Use the `astream_events` API to access custom data and intermediate outputs from LLM applications built entirely with [LCEL](/docs/concepts/lcel). 

While this API is available for use with [LangGraph](/docs/concepts/architecture#langgraph) as well, it is usually not necessary when working with LangGraph, as the `stream` and `astream` methods provide comprehensive streaming capabilities for LangGraph graphs.
:::

The [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming behavior.

#### Usage with chat models

When using `stream()` or `astream()` with chat models, the output is streamed as [AIMessageChunks](/docs/concepts/messages#aimessagechunk) as it is generated by the LLM. This allows you to present or process the LLM's output incrementally as it's being produced, which is particularly useful in interactive applications or interfaces.

#### Usage with LangGraph

[LangGraph](/docs/concepts/architecture#langgraph) compiled graphs are [Runnables](/docs/concepts/runnables) and support the standard streaming APIs.

1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream): Use to stream outputs from individual Runnables (e.g., a chat model) as they are generated or stream any workflow created with LangGraph.
2. The async only [astream_events](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream_events): Use this API to get access to custom events and intermediate outputs from LLM  applications built entirely with [LCEL](/docs/concepts/lcel). Note that this API is available, but not needed when working with LangGraph.

:::note
In addition, there is a **legacy** async [astream_log](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream_log) API. This API is not recommended for new projects it is more complex and less feature-rich than the other streaming APIs.
:::

### `stream()` and `astream()`

The `stream()` method returns an iterator that yields chunks of output synchronously as they are produced. You can use a `for` loop to process each chunk in real-time. For example, when using an LLM, this allows the output to be streamed incrementally as it is generated, reducing the wait time for users.

model = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for event in chain.astream_events({"topic": "parrot"}):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(event, end="|", flush=True)
```

You can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!

See [this guide](/docs/how_to/streaming/#using-stream-events) for more detailed information on how to use `.astream_events()`, including a table listing available events.

## Writing custom data to the stream

To write custom data to the stream, you will need to choose one of the following methods based on the component you are working with:¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q086®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:6339069c-ac55-9e54-dcf5-c6a05a887d09­checkpoint_nsÙ-generate:6339069c-ac55-9e54-dcf5-c6a05a887d09¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@1c53ee3c7d13fe7270cec83bc109f837efcdc2da1f7a90a0bc9f06fb63d38843¢tsËAÚô~8¨trace_idÙ$f1a2470f-1138-41a4-8771-69c46fc9562f­parent_run_idÙ$4417bc55-f7b7-4a17-b219-cca449930cc1¦run_idÙ$f1a2470f-1138-41a4-8771-69c46fc9562fªevent_type¬chain_output¬event_source¥chain§payload§outputsÚmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What is the main benefit of `asam_stream` events over `astream`? \nContext: #### Usage with LCEL\n\nIf you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the output of the last step in the chain. This allows the final processed result to be streamed incrementally. **LCEL** tries to optimize streaming latency in pipelines so that the streaming results from the last step are available as soon as possible.\n\n\n\n### `astream_events`\n<span data-heading-keywords="astream_events,stream_events,stream events"></span>\n\n:::tip\nUse the `astream_events` API to access custom data and intermediate outputs from LLM applications built entirely with [LCEL](/docs/concepts/lcel). \n\nWhile this API is available for use with [LangGraph](/docs/concepts/architecture#langgraph) as well, it is usually not necessary when working with LangGraph, as the `stream` and `astream` methods provide comprehensive streaming capabilities for LangGraph graphs.\n:::\n\nThe [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming behavior.\n\n#### Usage with chat models\n\nWhen using `stream()` or `astream()` with chat models, the output is streamed as [AIMessageChunks](/docs/concepts/messages#aimessagechunk) as it is generated by the LLM. This allows you to present or process the LLM\'s output incrementally as it\'s being produced, which is particularly useful in interactive applications or interfaces.\n\n#### Usage with LangGraph\n\n[LangGraph](/docs/concepts/architecture#langgraph) compiled graphs are [Runnables](/docs/concepts/runnables) and support the standard streaming APIs.\n\n1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream): Use to stream outputs from individual Runnables (e.g., a chat model) as they are generated or stream any workflow created with LangGraph.\n2. The async only [astream_events](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream_events): Use this API to get access to custom events and intermediate outputs from LLM  applications built entirely with [LCEL](/docs/concepts/lcel). Note that this API is available, but not needed when working with LangGraph.\n\n:::note\nIn addition, there is a **legacy** async [astream_log](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.astream_log) API. This API is not recommended for new projects it is more complex and less feature-rich than the other streaming APIs.\n:::\n\n### `stream()` and `astream()`\n\nThe `stream()` method returns an iterator that yields chunks of output synchronously as they are produced. You can use a `for` loop to process each chunk in real-time. For example, when using an LLM, this allows the output to be streamed incrementally as it is generated, reducing the wait time for users.\n\nmodel = ChatAnthropic(model="claude-3-sonnet-20240229")\n\nprompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")\nparser = StrOutputParser()\nchain = prompt | model | parser\n\nasync for event in chain.astream_events({"topic": "parrot"}):\n    kind = event["event"]\n    if kind == "on_chat_model_stream":\n        print(event, end="|", flush=True)\n```\n\nYou can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!\n\nSee [this guide](/docs/how_to/streaming/#using-stream-events) for more detailed information on how to use `.astream_events()`, including a table listing available events.\n\n## Writing custom data to the stream\n\nTo write custom data to the stream, you will need to choose one of the following methods based on the component you are working with: \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@b83f15e0ef8b351487de0208dd8635782e9972ebbf2f2be6acd094a3c381fe0b¢tsËAÚôòÊ½¨focus_msÍ§payload‚¤role©assistant§contentÚÁThe main benefit of `asam_stream` events over `astream` is that it allows access to custom data and intermediate outputs from LLM applications built with LangChain's Expression Language (LCEL). This provides more granular control and insight into the streaming process, which can be particularly useful for debugging or enhancing the application's functionality. In contrast, `astream` focuses on streaming outputs without the additional event data.ªfocus_kind¬exec_latency¨query_id¤Q086‰¢idÙ@18c82a495726720c0b218d9b73bc6d49fb32c5c112f5791adb6a62535d00d68e¢tsËAÚôòŞn¨trace_idÙ$4417bc55-f7b7-4a17-b219-cca449930cc1­parent_run_idÙ$fda11c1d-5f25-4a76-937a-f94662bdae3d¦run_idÙ$4417bc55-f7b7-4a17-b219-cca449930cc1ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚÁThe main benefit of `asam_stream` events over `astream` is that it allows access to custom data and intermediate outputs from LLM applications built with LangChain's Expression Language (LCEL). This provides more granular control and insight into the streaming process, which can be particularly useful for debugging or enhancing the application's functionality. In contrast, `astream` focuses on streaming outputs without the additional event data.¨query_id¤Q086ªfocus_kind¶chain_execution_result‰¢idÙ@212bcaf0f24569d67211bc4bf5f496724eb32db0f830074b7aefe756c93fa849¢tsËAÚôòè-¨trace_idÙ$fda11c1d-5f25-4a76-937a-f94662bdae3d­parent_run_idÀ¦run_idÙ$fda11c1d-5f25-4a76-937a-f94662bdae3dªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ@What is the main benefit of `asam_stream` events over `astream`?§context•‚´page_content_summaryÙÍ#### Usage with LCEL

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the outp...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËThe [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming b...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË:::note
In addition, there is a **legacy** async [astream_log](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runna...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËmodel = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¦answerÚÁThe main benefit of `asam_stream` events over `astream` is that it allows access to custom data and intermediate outputs from LLM applications built with LangChain's Expression Language (LCEL). This provides more granular control and insight into the streaming process, which can be particularly useful for debugging or enhancing the application's functionality. In contrast, `astream` focuses on streaming outputs without the additional event data.¨query_id¤Q086ªfocus_kind¶chain_execution_result‹¢idÙ@920786ceccd5e1e36cec336d40fd3f6ac1e8aa013f7efef149a441c327a34bee¢tsËAÚôòñR¨trace_id¤Q087­parent_run_idÀ¦run_idÙ$f8a97472-e70b-4daa-ba83-a94bb7bccbe4ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙKHow do you specify hidden arguments in a Tool so the model cannot set them?¨query_id¤Q087§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q087ªfocus_kind¯chain_execution‹¢idÙ@04ef7c81509bc1f3d1b3d7a85f23ff91df22c040890ee719977deadad4343a35¢tsËAÚôòúÆ¨trace_id¤Q087­parent_run_idÙ$f8a97472-e70b-4daa-ba83-a94bb7bccbe4¦run_idÙ$fcb92bdc-4db9-4337-bd32-a84ecf7304e0ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙKHow do you specify hidden arguments in a Tool so the model cannot set them?§context¦answer ¨query_id¤Q087¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q087®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:89ac0c2b-9966-d119-8671-445cf78d71c0ªfocus_kind¯chain_execution‰¢idÙ@62bca29c0742fad75ac9f7b7042feff4d93cdc387b6b16b0fb68c7ea39ac6ca3¢tsËAÚôş¹¨trace_idÙ$fcb92bdc-4db9-4337-bd32-a84ecf7304e0­parent_run_idÙ$f8a97472-e70b-4daa-ba83-a94bb7bccbe4¦run_idÙ$fcb92bdc-4db9-4337-bd32-a84ecf7304e0ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string liter...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Tool execution

[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.

[LangGraph]...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙËThese requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples onl...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/few_shot_prompting.mdx‚´page_content_summaryÙË- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to mo...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¨query_id¤Q087ªfocus_kind¶chain_execution_result‹¢idÙ@a987eaca20535009eb16538e143a601bfdcbbb49a248d8ceb899d17a20e58d38¢tsËAÚôşÃ¨¨trace_id¤Q087­parent_run_idÙ$f8a97472-e70b-4daa-ba83-a94bb7bccbe4¦run_idÙ$c9e007c7-1d2a-4350-923d-a0236999df40ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙKHow do you specify hidden arguments in a Tool so the model cannot set them?§context•‚´page_content_summaryÙËYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string liter...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Tool execution

[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.

[LangGraph]...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙËThese requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples onl...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/few_shot_prompting.mdx‚´page_content_summaryÙË- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to mo...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answer ¨query_id¤Q087¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q087®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:75f15cba-c3ed-c846-a0bc-f0af1dcc1bb2ªfocus_kind¯chain_execution‹¢idÙ@7dcf9c7c3dc12be2bdf6bd0f1c0ddaf18c9cd798840037362a91839cd26ae0ff¢tsËAÚôşË!¨trace_id¤Q087­parent_run_idÙ$c9e007c7-1d2a-4350-923d-a0236999df40¦run_idÙ$0614006d-2484-4999-bc44-b7dfe6e80723ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙKHow do you specify hidden arguments in a Tool so the model cannot set them?§contextÚÜYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string literal"]** -- Adds a description to the argument that will be exposed in the tool's schema.

### InjectedToolArg

There are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the `InjectedToolArg` annotation, which allows certain parameters to be hidden from the tool's schema.

For example, if a tool requires a `user_id` to be injected dynamically at runtime, it can be structured in this way:

```python
from langchain_core.tools import tool, InjectedToolArg

@tool
def user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:
    """Tool that processes input data."""
    return f"User {user_id} processed {input_data}"
```

See [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's function signature to configure the run time behavior of the tool.

The following type annotations will end up **removing** the argument from the tool's schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.

- **InjectedToolArg**: Value should be injected manually at runtime using `.invoke` or `.ainvoke`.
- **RunnableConfig**: Pass in the RunnableConfig object to the tool.
- **InjectedState**: Pass in the overall state of the LangGraph graph to the tool.
- **InjectedStore**: Pass in the LangGraph store object to the tool.

You can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

## Tool execution

[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.

[LangGraph](https://langchain-ai.github.io/langgraph/) offers pre-built components (e.g., [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)) that will often invoke the tool in behalf of the user.

:::info[Further reading]

* See our [how-to guide](/docs/how_to/tool_calling/) on tool calling.
* See the [LangGraph documentation on using ToolNode](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/).

:::

## Best practices

When designing [tools](/docs/concepts/tools/) to be used by a model, it is important to keep in mind that:

These requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples only include expected tool calls and not the actual tool outputs, you can try adding dummy ToolMessages / AIMessages to the end of each example with generic contents to satisfy the API constraints.
In these cases it's especially worth experimenting with inserting your examples as strings versus messages, as having dummy messages can adversely affect certain models.

You can see a case study of how Anthropic and OpenAI respond to different few-shot prompting techniques on two different tool calling benchmarks [here](https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/).

- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)
- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets)¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q087®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:75f15cba-c3ed-c846-a0bc-f0af1dcc1bb2­checkpoint_nsÙ-generate:75f15cba-c3ed-c846-a0bc-f0af1dcc1bb2¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@952ce73b76002a29dd62b94374b7d562396383db78589776f5d9474a0b27f794¢tsËAÚôşÒ[¨trace_idÙ$0614006d-2484-4999-bc44-b7dfe6e80723­parent_run_idÙ$c9e007c7-1d2a-4350-923d-a0236999df40¦run_idÙ$0614006d-2484-4999-bc44-b7dfe6e80723ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚÉmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: How do you specify hidden arguments in a Tool so the model cannot set them? \nContext: You can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool\'s schema.\n\n- **Annotated[..., "string literal"]** -- Adds a description to the argument that will be exposed in the tool\'s schema.\n\n### InjectedToolArg\n\nThere are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the `InjectedToolArg` annotation, which allows certain parameters to be hidden from the tool\'s schema.\n\nFor example, if a tool requires a `user_id` to be injected dynamically at runtime, it can be structured in this way:\n\n```python\nfrom langchain_core.tools import tool, InjectedToolArg\n\n@tool\ndef user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:\n    """Tool that processes input data."""\n    return f"User {user_id} processed {input_data}"\n```\n\nSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.\n\n## Special type annotations\n\nThere are a number of special type annotations that can be used in the tool\'s function signature to configure the run time behavior of the tool.\n\nThe following type annotations will end up **removing** the argument from the tool\'s schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.\n\n- **InjectedToolArg**: Value should be injected manually at runtime using `.invoke` or `.ainvoke`.\n- **RunnableConfig**: Pass in the RunnableConfig object to the tool.\n- **InjectedState**: Pass in the overall state of the LangGraph graph to the tool.\n- **InjectedStore**: Pass in the LangGraph store object to the tool.\n\nYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool\'s schema.\n\n## Tool execution\n\n[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.\n\n[LangGraph](https://langchain-ai.github.io/langgraph/) offers pre-built components (e.g., [`ToolNode`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.tool_node.ToolNode)) that will often invoke the tool in behalf of the user.\n\n:::info[Further reading]\n\n* See our [how-to guide](/docs/how_to/tool_calling/) on tool calling.\n* See the [LangGraph documentation on using ToolNode](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/).\n\n:::\n\n## Best practices\n\nWhen designing [tools](/docs/concepts/tools/) to be used by a model, it is important to keep in mind that:\n\nThese requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples only include expected tool calls and not the actual tool outputs, you can try adding dummy ToolMessages / AIMessages to the end of each example with generic contents to satisfy the API constraints.\nIn these cases it\'s especially worth experimenting with inserting your examples as strings versus messages, as having dummy messages can adversely affect certain models.\n\nYou can see a case study of how Anthropic and OpenAI respond to different few-shot prompting techniques on two different tool calling benchmarks [here](https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/).\n\n- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)\n- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)\n- [How to: add ad-hoc tool calling capability to models](/docs/how_to/tools_prompting)\n- [How to: pass in runtime secrets](/docs/how_to/runnable_runtime_secrets) \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@c08026897fd3060a1e53c61f37745e5af8eb3638dd492b6e3b7b61ff2aa60c4f¢tsËAÚô”ïS¨focus_msÍ	)§payload‚¤role©assistant§contentÚkTo specify hidden arguments in a Tool so the model cannot set them, you can use the `InjectedToolArg` annotation. This allows certain parameters to be injected manually at runtime without being exposed in the tool's schema. Other special annotations like `RunnableConfig`, `InjectedState`, and `InjectedStore` can also be used to remove arguments from the schema.ªfocus_kind¬exec_latency¨query_id¤Q087‰¢idÙ@e81e95bef625f35956db30ddef3d340787a56ef640a57d348ec08fa88198023b¢tsËAÚô•1¨trace_idÙ$c9e007c7-1d2a-4350-923d-a0236999df40­parent_run_idÙ$f8a97472-e70b-4daa-ba83-a94bb7bccbe4¦run_idÙ$c9e007c7-1d2a-4350-923d-a0236999df40ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚkTo specify hidden arguments in a Tool so the model cannot set them, you can use the `InjectedToolArg` annotation. This allows certain parameters to be injected manually at runtime without being exposed in the tool's schema. Other special annotations like `RunnableConfig`, `InjectedState`, and `InjectedStore` can also be used to remove arguments from the schema.¨query_id¤Q087ªfocus_kind¶chain_execution_result‰¢idÙ@7a24cf4da7d6903db3c11e0652b64a7b18acda3d1eae91fc74723a35a7f85e67¢tsËAÚô•®¨trace_idÙ$f8a97472-e70b-4daa-ba83-a94bb7bccbe4­parent_run_idÀ¦run_idÙ$f8a97472-e70b-4daa-ba83-a94bb7bccbe4ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙKHow do you specify hidden arguments in a Tool so the model cannot set them?§context•‚´page_content_summaryÙËYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string liter...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Tool execution

[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.

[LangGraph]...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙËThese requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples onl...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/few_shot_prompting.mdx‚´page_content_summaryÙË- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to mo...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answerÚkTo specify hidden arguments in a Tool so the model cannot set them, you can use the `InjectedToolArg` annotation. This allows certain parameters to be injected manually at runtime without being exposed in the tool's schema. Other special annotations like `RunnableConfig`, `InjectedState`, and `InjectedStore` can also be used to remove arguments from the schema.¨query_id¤Q087ªfocus_kind¶chain_execution_result‹¢idÙ@d19c8610b84bfea3909b6d67cd4009d5d08d5eebe648fd7137f1602070bdfae1¢tsËAÚô•ğ¨trace_id¤Q088­parent_run_idÀ¦run_idÙ$d614c44e-f8d1-49bf-b4d2-3ae4c68e20dfªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙHWhich guide demonstrates building a SQL retriever over a large database?¨query_id¤Q088§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q088ªfocus_kind¯chain_execution‹¢idÙ@6149460759f94f50f24f6b9a28e143f7963f6f1646dc106ab881261a5557cee2¢tsËAÚô•!°¨trace_id¤Q088­parent_run_idÙ$d614c44e-f8d1-49bf-b4d2-3ae4c68e20df¦run_idÙ$fc373735-c18d-47bf-8c15-d26b36386953ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙHWhich guide demonstrates building a SQL retriever over a large database?§context¦answer ¨query_id¤Q088¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q088®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:7637bfce-8656-cd0b-ae20-f170326275f4ªfocus_kind¯chain_execution‰¢idÙ@006f0eb358cdbdfb7cfa03aa30c7fe499499dea6fd8148fefafb4c50d798a3e8¢tsËAÚô¡¨trace_idÙ$fc373735-c18d-47bf-8c15-d26b36386953­parent_run_idÙ$d614c44e-f8d1-49bf-b4d2-3ae4c68e20df¦run_idÙ$fc373735-c18d-47bf-8c15-d26b36386953ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË:::
 
## Common types

Despite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.

### Search apis

It's important to note that retrievers don't n...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙËYou can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextu...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Retrieval

:::info[Prerequisites]

* [Retrievers](/docs/concepts/retrievers/)
* [Vector stores](/docs/concepts/vectorstores/)
* [Embeddings](/docs/concepts/embedding_models/)
* [Text splitters](/doc...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË### Query analysis

Query Analysis is the task of using an LLM to generate a query to send to a retriever.
For a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#quer...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¨query_id¤Q088ªfocus_kind¶chain_execution_result‹¢idÙ@cc18a0031db01d8cd7101e04f913cb18ac34d933db6730f2cae08708580ac8b8¢tsËAÚô¡­¨trace_id¤Q088­parent_run_idÙ$d614c44e-f8d1-49bf-b4d2-3ae4c68e20df¦run_idÙ$dc4ed089-8f94-45b9-b5c0-61573de00fa2ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙHWhich guide demonstrates building a SQL retriever over a large database?§context•‚´page_content_summaryÙË:::
 
## Common types

Despite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.

### Search apis

It's important to note that retrievers don't n...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙËYou can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextu...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Retrieval

:::info[Prerequisites]

* [Retrievers](/docs/concepts/retrievers/)
* [Vector stores](/docs/concepts/vectorstores/)
* [Embeddings](/docs/concepts/embedding_models/)
* [Text splitters](/doc...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË### Query analysis

Query Analysis is the task of using an LLM to generate a query to send to a retriever.
For a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#quer...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answer ¨query_id¤Q088¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q088®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:7ee50922-edbd-aa19-5975-d0bcd5fcd9ecªfocus_kind¯chain_execution‹¢idÙ@f6f737360494098dcfcb90a0920f401b9fa8f2a4fcba1cd673439b3d21da184e¢tsËAÚô¡&¨trace_id¤Q088­parent_run_idÙ$dc4ed089-8f94-45b9-b5c0-61573de00fa2¦run_idÙ$ee3fd434-8727-4e27-be81-c2abdc7f1615ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙHWhich guide demonstrates building a SQL retriever over a large database?§contextÚÉ:::
 
## Common types

Despite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.

### Search apis

It's important to note that retrievers don't need to actually *store* documents. 
For example, we can be built retrievers on top of search APIs that simply return search results! 
See our retriever integrations with [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/) or [Wikipedia Search](/docs/integrations/retrievers/wikipedia/). 

### Relational or graph database

Retrievers can be built on top of relational or graph databases.
In these cases, [query analysis](/docs/concepts/retrieval/) techniques to construct a structured query from natural language is critical.
For example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) retriever to be transformed into a SQL query behind the scenes.

:::info[Further reading]

You can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for databases that typically require structured query languages.
For example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) to be transformed into a SQL query behind the scenes.
Regardless of the underlying retrieval system, all retrievers in LangChain share a common interface. You can use them with the simple `invoke` method:


```python
docs = retriever.invoke(query)
```

:::info[Further reading]

* See our [conceptual guide on retrievers](/docs/concepts/retrievers/).
* See our [how-to guide](/docs/how_to/#retrievers) on working with retrievers.

:::

- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)
- [How to: write a custom retriever class](/docs/how_to/custom_retriever)
- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)
- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)
- [How to: reorder retrieved results to mitigate the "lost in the middle" effect](/docs/how_to/long_context_reorder)
- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)
- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)
- [How to: generate metadata filters](/docs/how_to/self_query)
- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)

# Retrieval

:::info[Prerequisites]

* [Retrievers](/docs/concepts/retrievers/)
* [Vector stores](/docs/concepts/vectorstores/)
* [Embeddings](/docs/concepts/embedding_models/)
* [Text splitters](/docs/concepts/text_splitters/)

:::

:::danger[Security]
 
Some of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).
There are inherent risks in doing this. 
Make sure that your database connection permissions are scoped as narrowly as possible for your application's needs. 
This will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases. 
For more on general security best practices, see our [security guide](/docs/security/).

:::

## Overview 

Retrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets. 
These systems accommodate various data formats:

### Query analysis

Query Analysis is the task of using an LLM to generate a query to send to a retriever.
For a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).

- [How to: add examples to the prompt](/docs/how_to/query_few_shot)
- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)
- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)
- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)
- [How to: construct filters](/docs/how_to/query_constructing_filters)
- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)

### Q&A over SQL + CSV

You can use LLMs to do question answering over tabular data.
For a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/).¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q088®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:7ee50922-edbd-aa19-5975-d0bcd5fcd9ec­checkpoint_nsÙ-generate:7ee50922-edbd-aa19-5975-d0bcd5fcd9ec¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@7b61bf0a12ca49411a073714b94a55e36641c218365e8a9ae2dadda663257688¢tsËAÚô¡"y¨trace_idÙ$ee3fd434-8727-4e27-be81-c2abdc7f1615­parent_run_idÙ$dc4ed089-8f94-45b9-b5c0-61573de00fa2¦run_idÙ$ee3fd434-8727-4e27-be81-c2abdc7f1615ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚÍmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Which guide demonstrates building a SQL retriever over a large database? \nContext: :::\n \n## Common types\n\nDespite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.\n\n### Search apis\n\nIt\'s important to note that retrievers don\'t need to actually *store* documents. \nFor example, we can be built retrievers on top of search APIs that simply return search results! \nSee our retriever integrations with [Amazon Kendra](/docs/integrations/retrievers/amazon_kendra_retriever/) or [Wikipedia Search](/docs/integrations/retrievers/wikipedia/). \n\n### Relational or graph database\n\nRetrievers can be built on top of relational or graph databases.\nIn these cases, [query analysis](/docs/concepts/retrieval/) techniques to construct a structured query from natural language is critical.\nFor example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) retriever to be transformed into a SQL query behind the scenes.\n\n:::info[Further reading]\n\nYou can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for databases that typically require structured query languages.\nFor example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) to be transformed into a SQL query behind the scenes.\nRegardless of the underlying retrieval system, all retrievers in LangChain share a common interface. You can use them with the simple `invoke` method:\n\n\n```python\ndocs = retriever.invoke(query)\n```\n\n:::info[Further reading]\n\n* See our [conceptual guide on retrievers](/docs/concepts/retrievers/).\n* See our [how-to guide](/docs/how_to/#retrievers) on working with retrievers.\n\n:::\n\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\n- [How to: reorder retrieved results to mitigate the "lost in the middle" effect](/docs/how_to/long_context_reorder)\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\n- [How to: generate metadata filters](/docs/how_to/self_query)\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\n\n# Retrieval\n\n:::info[Prerequisites]\n\n* [Retrievers](/docs/concepts/retrievers/)\n* [Vector stores](/docs/concepts/vectorstores/)\n* [Embeddings](/docs/concepts/embedding_models/)\n* [Text splitters](/docs/concepts/text_splitters/)\n\n:::\n\n:::danger[Security]\n \nSome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\nThere are inherent risks in doing this. \nMake sure that your database connection permissions are scoped as narrowly as possible for your application\'s needs. \nThis will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases. \nFor more on general security best practices, see our [security guide](/docs/security/).\n\n:::\n\n## Overview \n\nRetrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets. \nThese systems accommodate various data formats:\n\n### Query analysis\n\nQuery Analysis is the task of using an LLM to generate a query to send to a retriever.\nFor a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#query-analysis).\n\n- [How to: add examples to the prompt](/docs/how_to/query_few_shot)\n- [How to: handle cases where no queries are generated](/docs/how_to/query_no_queries)\n- [How to: handle multiple queries](/docs/how_to/query_multiple_queries)\n- [How to: handle multiple retrievers](/docs/how_to/query_multiple_retrievers)\n- [How to: construct filters](/docs/how_to/query_constructing_filters)\n- [How to: deal with high cardinality categorical variables](/docs/how_to/query_high_cardinality)\n\n### Q&A over SQL + CSV\n\nYou can use LLMs to do question answering over tabular data.\nFor a high-level tutorial, check out [this guide](/docs/tutorials/sql_qa/). \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@ef8a5a02be05ec6371851e015bb3dd9b3f0026eedea36d7f3d5d3fbc4b898c52¢tsËAÚôëÚw¨focus_msÍ§payload‚¤role©assistant§contentÚThe guide that demonstrates building a SQL retriever over a large database is the "Q&A over SQL + CSV" tutorial. This guide provides insights on using LLMs for question answering over tabular data. For more detailed information, you can refer to the specific tutorial mentioned.ªfocus_kind¬exec_latency¨query_id¤Q088‰¢idÙ@5750033ffffc0b34cbd6869133e852329724e4de27db293b7dadbc68d71b4d5e¢tsËAÚôëæ¼¨trace_idÙ$dc4ed089-8f94-45b9-b5c0-61573de00fa2­parent_run_idÙ$d614c44e-f8d1-49bf-b4d2-3ae4c68e20df¦run_idÙ$dc4ed089-8f94-45b9-b5c0-61573de00fa2ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚThe guide that demonstrates building a SQL retriever over a large database is the "Q&A over SQL + CSV" tutorial. This guide provides insights on using LLMs for question answering over tabular data. For more detailed information, you can refer to the specific tutorial mentioned.¨query_id¤Q088ªfocus_kind¶chain_execution_result‰¢idÙ@be532ae40fd5ec843ef65f6454280451d05b3e0b009922d510eb134ca0f04815¢tsËAÚôëî(¨trace_idÙ$d614c44e-f8d1-49bf-b4d2-3ae4c68e20df­parent_run_idÀ¦run_idÙ$d614c44e-f8d1-49bf-b4d2-3ae4c68e20dfªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙHWhich guide demonstrates building a SQL retriever over a large database?§context•‚´page_content_summaryÙË:::
 
## Common types

Despite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.

### Search apis

It's important to note that retrievers don't n...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙËYou can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextu...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Retrieval

:::info[Prerequisites]

* [Retrievers](/docs/concepts/retrievers/)
* [Vector stores](/docs/concepts/vectorstores/)
* [Embeddings](/docs/concepts/embedding_models/)
* [Text splitters](/doc...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË### Query analysis

Query Analysis is the task of using an LLM to generate a query to send to a retriever.
For a high-level tutorial on query analysis, check out [this guide](/docs/tutorials/rag/#quer...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answerÚThe guide that demonstrates building a SQL retriever over a large database is the "Q&A over SQL + CSV" tutorial. This guide provides insights on using LLMs for question answering over tabular data. For more detailed information, you can refer to the specific tutorial mentioned.¨query_id¤Q088ªfocus_kind¶chain_execution_result‹¢idÙ@b7694c56c7b9220b703841d6a3dde163fe735a0052598ed70ab5eed0896216ee¢tsËAÚôëöä¨trace_id¤Q089­parent_run_idÀ¦run_idÙ$da3c30c0-844b-4f13-a669-e1a6f4f64803ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙBWhy is semantic caching unlikely to hit after the first chat turn?¨query_id¤Q089§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q089ªfocus_kind¯chain_execution‹¢idÙ@0255f1b7e1405173add38159a27667d04e2f34427866b2b705472031d0c89d59¢tsËAÚôìÑ¨trace_id¤Q089­parent_run_idÙ$da3c30c0-844b-4f13-a669-e1a6f4f64803¦run_idÙ$929e11c6-b758-45ef-8699-997c8b39078dªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙBWhy is semantic caching unlikely to hit after the first chat turn?§context¦answer ¨query_id¤Q089¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q089®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:71e41027-ac0b-f477-7d16-2a0af38139d9ªfocus_kind¯chain_execution‰¢idÙ@158ca060ef1380318522632c2cfe63e862fcbb1ad10a0c672f90dc05a75c2cb6¢tsËAÚôü1Ç¨trace_idÙ$929e11c6-b758-45ef-8699-997c8b39078d­parent_run_idÙ$da3c30c0-844b-4f13-a669-e1a6f4f64803¦run_idÙ$929e11c6-b758-45ef-8699-997c8b39078dªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to conver...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q089ªfocus_kind¶chain_execution_result‹¢idÙ@e59e10fd1718a7e0ab29bfbb55487bde7438839bdee088e87fe843cda66f4e6e¢tsËAÚôü:›¨trace_id¤Q089­parent_run_idÙ$da3c30c0-844b-4f13-a669-e1a6f4f64803¦run_idÙ$8fd7302f-0884-4e00-b886-d48856bb4e5bªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙBWhy is semantic caching unlikely to hit after the first chat turn?§context•‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to conver...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answer ¨query_id¤Q089¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q089®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:54ad7004-9c3f-cb38-b050-6bb2025cdc78ªfocus_kind¯chain_execution‹¢idÙ@fb9fb7319de0f0b8feb1c30493482f036c9b68e80e41949a9fae37d6abe6b972¢tsËAÚôüC¨trace_id¤Q089­parent_run_idÙ$8fd7302f-0884-4e00-b886-d48856bb4e5b¦run_idÙ$7ca38854-85fa-4f43-873d-e676442196adªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙBWhy is semantic caching unlikely to hit after the first chat turn?§contextÚŞ### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.

The reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?

An alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.

A semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to convert text to a vector representation), and it's not guaranteed to capture the meaning of the input accurately.

However, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.

Please see the [how to cache chat model responses](/docs/how_to/chat_model_caching/) guide for more details.

## Related resources

* How-to guides on using chat models: [how-to guides](/docs/how_to/#chat-models).
* List of supported chat models: [chat model integrations](/docs/integrations/chat/).

### Conceptual guides

3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.

If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).

The size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.

## Advanced topics
 
### Rate-limiting

Many chat model providers impose a limit on the number of requests that can be made in a given time period.

If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.
2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q089®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:54ad7004-9c3f-cb38-b050-6bb2025cdc78­checkpoint_nsÙ-generate:54ad7004-9c3f-cb38-b050-6bb2025cdc78¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@8bd815aa023460e2a25a53bf253f58b18fd40ce655a31ca2e19593c8f3c10d43¢tsËAÚôüJ
¨trace_idÙ$7ca38854-85fa-4f43-873d-e676442196ad­parent_run_idÙ$8fd7302f-0884-4e00-b886-d48856bb4e5b¦run_idÙ$7ca38854-85fa-4f43-873d-e676442196adªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ¥messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Why is semantic caching unlikely to hit after the first chat turn? \nContext: ### Caching\n\nChat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.\n\nThe reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?\n\nAn alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.\n\nA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to convert text to a vector representation), and it\'s not guaranteed to capture the meaning of the input accurately.\n\nHowever, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.\n\nPlease see the [how to cache chat model responses](/docs/how_to/chat_model_caching/) guide for more details.\n\n## Related resources\n\n* How-to guides on using chat models: [how-to guides](/docs/how_to/#chat-models).\n* List of supported chat models: [chat model integrations](/docs/integrations/chat/).\n\n### Conceptual guides\n\n3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\n\nIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\n\nThe size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.\n\n## Advanced topics\n \n### Rate-limiting\n\nMany chat model providers impose a limit on the number of requests that can be made in a given time period.\n\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n\n1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.\n2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@a0c84b88e8e7b32fc9036b123d2425c4122273cd38a39c5fd9bc0faff32d4ce2¢tsËAÚôycÔ¨focus_msÍ¢§payload‚¤role©assistant§contentÚªSemantic caching is unlikely to hit after the first chat turn because conversations typically do not start with the exact same message, making it rare for subsequent interactions to match previous inputs. Additionally, the complexity of accurately capturing the meaning of inputs can further reduce the likelihood of cache hits. Therefore, relying on exact inputs for caching in dynamic conversations is generally ineffective.ªfocus_kind¬exec_latency¨query_id¤Q089‰¢idÙ@4f593b905e4e147f1a619a33b28e0f2d8da421916b4dd96334f0c921f58b0e52¢tsËAÚôysi¨trace_idÙ$8fd7302f-0884-4e00-b886-d48856bb4e5b­parent_run_idÙ$da3c30c0-844b-4f13-a669-e1a6f4f64803¦run_idÙ$8fd7302f-0884-4e00-b886-d48856bb4e5bªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚªSemantic caching is unlikely to hit after the first chat turn because conversations typically do not start with the exact same message, making it rare for subsequent interactions to match previous inputs. Additionally, the complexity of accurately capturing the meaning of inputs can further reduce the likelihood of cache hits. Therefore, relying on exact inputs for caching in dynamic conversations is generally ineffective.¨query_id¤Q089ªfocus_kind¶chain_execution_result‰¢idÙ@96bb0e07a28ddbc33fcf3606fbe71252faf1fb58d1f78da474915760a7bbf461¢tsËAÚôy}ş¨trace_idÙ$da3c30c0-844b-4f13-a669-e1a6f4f64803­parent_run_idÀ¦run_idÙ$da3c30c0-844b-4f13-a669-e1a6f4f64803ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙBWhy is semantic caching unlikely to hit after the first chat turn?§context•‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to conver...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚªSemantic caching is unlikely to hit after the first chat turn because conversations typically do not start with the exact same message, making it rare for subsequent interactions to match previous inputs. Additionally, the complexity of accurately capturing the meaning of inputs can further reduce the likelihood of cache hits. Therefore, relying on exact inputs for caching in dynamic conversations is generally ineffective.¨query_id¤Q089ªfocus_kind¶chain_execution_result‹¢idÙ@114b6f32377539d8e9bbcc3a1533f8bc069d29484a191f321907e8902e147f76¢tsËAÚôy‰¨trace_id¤Q090­parent_run_idÀ¦run_idÙ$ad4c7c57-0cd0-4642-a603-0202ac08c4caªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ9Describe a use case for `RecursiveCharacterTextSplitter`.¨query_id¤Q090§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q090ªfocus_kind¯chain_execution‹¢idÙ@9a1906bb06d97475f45043ca7aa768fa70d1ed181a0ab64baf80024292ddbdb3¢tsËAÚôyµ¨trace_id¤Q090­parent_run_idÙ$ad4c7c57-0cd0-4642-a603-0202ac08c4ca¦run_idÙ$937f5775-85d4-4014-b0f5-5fa0f80aa4b4ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ9Describe a use case for `RecursiveCharacterTextSplitter`.§context¦answer ¨query_id¤Q090¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q090®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:3220773c-57d2-ab00-faec-df3109760c5fªfocus_kind¯chain_execution‰¢idÙ@9b87ae98a05535f4909a4417c04ab8f45c92e924aa4101c2056100fc861645a5¢tsËAÚôŠVÁ¨trace_idÙ$937f5775-85d4-4014-b0f5-5fa0f80aa4b4­parent_run_idÙ$ad4c7c57-0cd0-4642-a603-0202ac08c4ca¦run_idÙ$937f5775-85d4-4014-b0f5-5fa0f80aa4b4ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË:::

### Text-structured based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. 
We can leverage this inherent structure to inform our splitting strategy,...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙËExample implementation using LangChain's `CharacterTextSplitter` with token-based splitting:

```python
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË# Text splitters
<span data-heading-keywords="text splitter,text splitting"></span>

:::info[Prerequisites]

* [Documents](/docs/concepts/retrievers/#interface)
* Tokenization(/docs/concepts/tokens)
:...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the how-to guide for [recursive text splitting](/docs/how_to/recursive_text_splitter/).

:::

### Document-structured based

Some documents have an inherent structure, ...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the how-to guide for [Markdown splitting](/docs/how_to/markdown_header_metadata_splitter/).
* See the how-to guide for [Recursive JSON splitting](/docs/how_to/recursive...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx¨query_id¤Q090ªfocus_kind¶chain_execution_result‹¢idÙ@8e188b8e995f6d0c703f85ee8a099c1dc9b7e7998eeba75e67490a5b43b5b855¢tsËAÚôŠdb¨trace_id¤Q090­parent_run_idÙ$ad4c7c57-0cd0-4642-a603-0202ac08c4ca¦run_idÙ$dac7684d-0af2-4dd3-9b75-a211f1bad1c6ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ9Describe a use case for `RecursiveCharacterTextSplitter`.§context•‚´page_content_summaryÙË:::

### Text-structured based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. 
We can leverage this inherent structure to inform our splitting strategy,...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙËExample implementation using LangChain's `CharacterTextSplitter` with token-based splitting:

```python
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË# Text splitters
<span data-heading-keywords="text splitter,text splitting"></span>

:::info[Prerequisites]

* [Documents](/docs/concepts/retrievers/#interface)
* Tokenization(/docs/concepts/tokens)
:...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the how-to guide for [recursive text splitting](/docs/how_to/recursive_text_splitter/).

:::

### Document-structured based

Some documents have an inherent structure, ...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the how-to guide for [Markdown splitting](/docs/how_to/markdown_header_metadata_splitter/).
* See the how-to guide for [Recursive JSON splitting](/docs/how_to/recursive...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx¦answer ¨query_id¤Q090¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q090®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:4b22239c-e682-d932-1775-da570446d465ªfocus_kind¯chain_execution‹¢idÙ@82d7420fc24761e1d17d1d08f3d72b35bece4fa3ca5fc7f2b7ba81d57f628f2f¢tsËAÚôŠl@¨trace_id¤Q090­parent_run_idÙ$dac7684d-0af2-4dd3-9b75-a211f1bad1c6¦run_idÙ$80f0bcc1-8d51-4ddc-9aaf-d9bce4d80603ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ9Describe a use case for `RecursiveCharacterTextSplitter`.§contextÚ_:::

### Text-structured based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. 
We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity.
LangChain's [`RecursiveCharacterTextSplitter`](/docs/how_to/recursive_text_splitter/) implements this concept:
- The `RecursiveCharacterTextSplitter` attempts to keep larger units (e.g., paragraphs) intact.
- If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).
- This process continues down to the word level if necessary.

Here is example usage:

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)
texts = text_splitter.split_text(document)
```

:::info[Further reading]

Example implementation using LangChain's `CharacterTextSplitter` with token-based splitting:

```python
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    encoding_name="cl100k_base", chunk_size=100, chunk_overlap=0
)
texts = text_splitter.split_text(document)
```

:::info[Further reading]

* See the how-to guide for [token-based](/docs/how_to/split_by_token/) splitting.
* See the how-to guide for [character-based](/docs/how_to/character_text_splitter/) splitting.

:::

### Text-structured based

# Text splitters
<span data-heading-keywords="text splitter,text splitting"></span>

:::info[Prerequisites]

* [Documents](/docs/concepts/retrievers/#interface)
* Tokenization(/docs/concepts/tokens)
:::

## Overview

Document splitting is often a crucial preprocessing step for many applications.
It involves breaking down large texts into smaller, manageable chunks.
This process offers several benefits, such as ensuring consistent processing of varying document lengths, overcoming input size limitations of models, and improving the quality of text representations used in retrieval systems.
There are several strategies for splitting documents, each with its own advantages.

## Key concepts

![Conceptual Overview](/img/text_splitters.png)

Text splitters split documents into smaller chunks for use in downstream applications.

## Why split documents?

There are several reasons to split documents:

:::info[Further reading]

* See the how-to guide for [recursive text splitting](/docs/how_to/recursive_text_splitter/).

:::

### Document-structured based

Some documents have an inherent structure, such as HTML, Markdown, or JSON files. 
In these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text.
Key benefits of structure-based splitting:
- Preserves the logical organization of the document
- Maintains context within each chunk
- Can be more effective for downstream tasks like retrieval or summarization

Examples of structure-based splitting:
- **Markdown**: Split based on headers (e.g., #, ##, ###)
- **HTML**: Split using tags
- **JSON**: Split by object or array elements
- **Code**: Split by functions, classes, or logical blocks

:::info[Further reading]

:::info[Further reading]

* See the how-to guide for [Markdown splitting](/docs/how_to/markdown_header_metadata_splitter/).
* See the how-to guide for [Recursive JSON splitting](/docs/how_to/recursive_json_splitter/).
* See the how-to guide for [Code splitting](/docs/how_to/code_splitter/).
* See the how-to guide for [HTML splitting](/docs/how_to/split_html/).

:::

### Semantic meaning based

Unlike the previous methods, semantic-based splitting actually considers the *content* of the text. 
While other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text's semantics.
There are several ways to implement this, but conceptually the approach is split text when there are significant changes in text *meaning*.
As an example, we can use a sliding window approach to generate embeddings, and compare the embeddings to find significant differences:¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q090®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:4b22239c-e682-d932-1775-da570446d465­checkpoint_nsÙ-generate:4b22239c-e682-d932-1775-da570446d465¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@3fee6e15e267ce4ac1ffedaaeeb955810ce231956bf8838f72863d10af1584c6¢tsËAÚôŠwû¨trace_idÙ$80f0bcc1-8d51-4ddc-9aaf-d9bce4d80603­parent_run_idÙ$dac7684d-0af2-4dd3-9b75-a211f1bad1c6¦run_idÙ$80f0bcc1-8d51-4ddc-9aaf-d9bce4d80603ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚcmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Describe a use case for `RecursiveCharacterTextSplitter`. \nContext: :::\n\n### Text-structured based\n\nText is naturally organized into hierarchical units such as paragraphs, sentences, and words. \nWe can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity.\nLangChain\'s [`RecursiveCharacterTextSplitter`](/docs/how_to/recursive_text_splitter/) implements this concept:\n- The `RecursiveCharacterTextSplitter` attempts to keep larger units (e.g., paragraphs) intact.\n- If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\n- This process continues down to the word level if necessary.\n\nHere is example usage:\n\n```python\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\ntexts = text_splitter.split_text(document)\n```\n\n:::info[Further reading]\n\nExample implementation using LangChain\'s `CharacterTextSplitter` with token-based splitting:\n\n```python\nfrom langchain_text_splitters import CharacterTextSplitter\ntext_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n    encoding_name="cl100k_base", chunk_size=100, chunk_overlap=0\n)\ntexts = text_splitter.split_text(document)\n```\n\n:::info[Further reading]\n\n* See the how-to guide for [token-based](/docs/how_to/split_by_token/) splitting.\n* See the how-to guide for [character-based](/docs/how_to/character_text_splitter/) splitting.\n\n:::\n\n### Text-structured based\n\n# Text splitters\n<span data-heading-keywords="text splitter,text splitting"></span>\n\n:::info[Prerequisites]\n\n* [Documents](/docs/concepts/retrievers/#interface)\n* Tokenization(/docs/concepts/tokens)\n:::\n\n## Overview\n\nDocument splitting is often a crucial preprocessing step for many applications.\nIt involves breaking down large texts into smaller, manageable chunks.\nThis process offers several benefits, such as ensuring consistent processing of varying document lengths, overcoming input size limitations of models, and improving the quality of text representations used in retrieval systems.\nThere are several strategies for splitting documents, each with its own advantages.\n\n## Key concepts\n\n![Conceptual Overview](/img/text_splitters.png)\n\nText splitters split documents into smaller chunks for use in downstream applications.\n\n## Why split documents?\n\nThere are several reasons to split documents:\n\n:::info[Further reading]\n\n* See the how-to guide for [recursive text splitting](/docs/how_to/recursive_text_splitter/).\n\n:::\n\n### Document-structured based\n\nSome documents have an inherent structure, such as HTML, Markdown, or JSON files. \nIn these cases, it\'s beneficial to split the document based on its structure, as it often naturally groups semantically related text.\nKey benefits of structure-based splitting:\n- Preserves the logical organization of the document\n- Maintains context within each chunk\n- Can be more effective for downstream tasks like retrieval or summarization\n\nExamples of structure-based splitting:\n- **Markdown**: Split based on headers (e.g., #, ##, ###)\n- **HTML**: Split using tags\n- **JSON**: Split by object or array elements\n- **Code**: Split by functions, classes, or logical blocks\n\n:::info[Further reading]\n\n:::info[Further reading]\n\n* See the how-to guide for [Markdown splitting](/docs/how_to/markdown_header_metadata_splitter/).\n* See the how-to guide for [Recursive JSON splitting](/docs/how_to/recursive_json_splitter/).\n* See the how-to guide for [Code splitting](/docs/how_to/code_splitter/).\n* See the how-to guide for [HTML splitting](/docs/how_to/split_html/).\n\n:::\n\n### Semantic meaning based\n\nUnlike the previous methods, semantic-based splitting actually considers the *content* of the text. \nWhile other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text\'s semantics.\nThere are several ways to implement this, but conceptually the approach is split text when there are significant changes in text *meaning*.\nAs an example, we can use a sliding window approach to generate embeddings, and compare the embeddings to find significant differences: \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@4e203d6b94f9f83f2628a2a7f5350aa5f35f8fac04a75c98fcb40f54ee965329¢tsËAÚô]y¨focus_msÍ`§payload‚¤role©assistant§contentÚûA use case for `RecursiveCharacterTextSplitter` is to preprocess large documents for natural language processing tasks by breaking them into smaller, manageable chunks while preserving the hierarchical structure of the text. This ensures that larger units like paragraphs remain intact, maintaining semantic coherence, which is beneficial for tasks such as retrieval or summarization. For example, it can be used to split a lengthy article into smaller sections without losing the context of each paragraph.ªfocus_kind¬exec_latency¨query_id¤Q090‰¢idÙ@6a5c875ee041f2343adad789889b71a292b6f4ea87938a02b2afbcf4d91b287a¢tsËAÚôuÍ¨trace_idÙ$dac7684d-0af2-4dd3-9b75-a211f1bad1c6­parent_run_idÙ$ad4c7c57-0cd0-4642-a603-0202ac08c4ca¦run_idÙ$dac7684d-0af2-4dd3-9b75-a211f1bad1c6ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚûA use case for `RecursiveCharacterTextSplitter` is to preprocess large documents for natural language processing tasks by breaking them into smaller, manageable chunks while preserving the hierarchical structure of the text. This ensures that larger units like paragraphs remain intact, maintaining semantic coherence, which is beneficial for tasks such as retrieval or summarization. For example, it can be used to split a lengthy article into smaller sections without losing the context of each paragraph.¨query_id¤Q090ªfocus_kind¶chain_execution_result‰¢idÙ@640a35ce015a2c38b2215784a9ffabe83304d4d1f20fc2092972cb16bc88562a¢tsËAÚô|ş¨trace_idÙ$ad4c7c57-0cd0-4642-a603-0202ac08c4ca­parent_run_idÀ¦run_idÙ$ad4c7c57-0cd0-4642-a603-0202ac08c4caªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ9Describe a use case for `RecursiveCharacterTextSplitter`.§context•‚´page_content_summaryÙË:::

### Text-structured based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. 
We can leverage this inherent structure to inform our splitting strategy,...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙËExample implementation using LangChain's `CharacterTextSplitter` with token-based splitting:

```python
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË# Text splitters
<span data-heading-keywords="text splitter,text splitting"></span>

:::info[Prerequisites]

* [Documents](/docs/concepts/retrievers/#interface)
* Tokenization(/docs/concepts/tokens)
:...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the how-to guide for [recursive text splitting](/docs/how_to/recursive_text_splitter/).

:::

### Document-structured based

Some documents have an inherent structure, ...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the how-to guide for [Markdown splitting](/docs/how_to/markdown_header_metadata_splitter/).
* See the how-to guide for [Recursive JSON splitting](/docs/how_to/recursive...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx¦answerÚûA use case for `RecursiveCharacterTextSplitter` is to preprocess large documents for natural language processing tasks by breaking them into smaller, manageable chunks while preserving the hierarchical structure of the text. This ensures that larger units like paragraphs remain intact, maintaining semantic coherence, which is beneficial for tasks such as retrieval or summarization. For example, it can be used to split a lengthy article into smaller sections without losing the context of each paragraph.¨query_id¤Q090ªfocus_kind¶chain_execution_result‹¢idÙ@07880efca72809cd6f291977dff4d25e671204d8cfefc4541d7fcffb28f4916a¢tsËAÚô„¨trace_id¤Q091­parent_run_idÀ¦run_idÙ$7633e6ff-9960-4d31-aea3-6d3515c978f2ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ3Give two methods to stream output from a ChatModel.¨query_id¤Q091§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q091ªfocus_kind¯chain_execution‹¢idÙ@8a07e054a0e218ddf3fc04810ca0112bc1257b6623155985c530e62dbceecb71¢tsËAÚôˆÓ¨trace_id¤Q091­parent_run_idÙ$7633e6ff-9960-4d31-aea3-6d3515c978f2¦run_idÙ$a4bd2dd5-ce01-4154-93c4-bbf9e60fb1eaªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ3Give two methods to stream output from a ChatModel.§context¦answer ¨query_id¤Q091¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q091®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:4fdcc7c6-56f9-35e8-f388-7b112f7f19c5ªfocus_kind¯chain_execution‰¢idÙ@72b7b1361573860d45949ffc3d75bccc03209809ee334c9c42a415450ea9fcfd¢tsËAÚô!)Ì¨trace_idÙ$a4bd2dd5-ce01-4154-93c4-bbf9e60fb1ea­parent_run_idÙ$7633e6ff-9960-4d31-aea3-6d3515c978f2¦run_idÙ$a4bd2dd5-ce01-4154-93c4-bbf9e60fb1eaªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËThe [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming b...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË## Streaming APIs

LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), includin...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. LangGraph's [StreamWriter](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamWriter) can be used to write custom data that will surface through **stream** and **astrea...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËmodel = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applic...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¨query_id¤Q091ªfocus_kind¶chain_execution_result‹¢idÙ@f009fd3318276256cd12c919696daf908f00185db1742637c01acfa6e431b09a¢tsËAÚô!5ü¨trace_id¤Q091­parent_run_idÙ$7633e6ff-9960-4d31-aea3-6d3515c978f2¦run_idÙ$0cc90617-bcd8-4f06-888d-a0416bee6a33ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ3Give two methods to stream output from a ChatModel.§context•‚´page_content_summaryÙËThe [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming b...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË## Streaming APIs

LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), includin...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. LangGraph's [StreamWriter](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamWriter) can be used to write custom data that will surface through **stream** and **astrea...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËmodel = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applic...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¦answer ¨query_id¤Q091¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q091®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:fe841219-0410-8c29-b80c-cc76c7cf2c9fªfocus_kind¯chain_execution‹¢idÙ@91592cd2ddf88d771315abb4a0355bca8f5b5af46b8e9de833b1cfbddbec3e16¢tsËAÚô!?W¨trace_id¤Q091­parent_run_idÙ$0cc90617-bcd8-4f06-888d-a0416bee6a33¦run_idÙ$e91aca78-3a96-4e16-999c-ac55c2ef632eªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ3Give two methods to stream output from a ChatModel.§contextÚKThe [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming behavior.

#### Usage with chat models

When using `stream()` or `astream()` with chat models, the output is streamed as [AIMessageChunks](/docs/concepts/messages#aimessagechunk) as it is generated by the LLM. This allows you to present or process the LLM's output incrementally as it's being produced, which is particularly useful in interactive applications or interfaces.

#### Usage with LangGraph

[LangGraph](/docs/concepts/architecture#langgraph) compiled graphs are [Runnables](/docs/concepts/runnables) and support the standard streaming APIs.

## Streaming APIs

LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), including [LLMs](/docs/concepts/chat_models), [compiled LangGraph graphs](https://langchain-ai.github.io/langgraph/concepts/low_level/), and any Runnable generated with [LCEL](/docs/concepts/lcel).

1. LangGraph's [StreamWriter](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamWriter) can be used to write custom data that will surface through **stream** and **astream** APIs when working with LangGraph. **Important** this is a LangGraph feature, so it is not available when working with pure LCEL. See [how to streaming custom data](https://langchain-ai.github.io/langgraph/how-tos/streaming-content/) for more information.
2. [dispatch_events](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.dispatch_custom_event.html#) / [adispatch_events](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.adispatch_custom_event.html) can be used to write custom data that will be surfaced through the **astream_events** API. See [how to dispatch custom callback events](/docs/how_to/callbacks_custom_events/#astream-events-api) for more information.

## "Auto-Streaming" Chat Models

model = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for event in chain.astream_events({"topic": "parrot"}):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(event, end="|", flush=True)
```

You can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!

See [this guide](/docs/how_to/streaming/#using-stream-events) for more detailed information on how to use `.astream_events()`, including a table listing available events.

## Writing custom data to the stream

To write custom data to the stream, you will need to choose one of the following methods based on the component you are working with:

# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applications built on [LLMs](/docs/concepts/chat_models). By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.

## Overview

Generating full responses from [LLMs](/docs/concepts/chat_models) often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q091®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:fe841219-0410-8c29-b80c-cc76c7cf2c9f­checkpoint_nsÙ-generate:fe841219-0410-8c29-b80c-cc76c7cf2c9f¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@46306b7f7408599d084429a1e3a9b89c3865bc560c2442329e2f7c5d38968cc4¢tsËAÚô!I”¨trace_idÙ$e91aca78-3a96-4e16-999c-ac55c2ef632e­parent_run_idÙ$0cc90617-bcd8-4f06-888d-a0416bee6a33¦run_idÙ$e91aca78-3a96-4e16-999c-ac55c2ef632eªevent_type¬chain_output¬event_source¥chain§payload§outputsÚmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Give two methods to stream output from a ChatModel. \nContext: The [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming behavior.\n\n#### Usage with chat models\n\nWhen using `stream()` or `astream()` with chat models, the output is streamed as [AIMessageChunks](/docs/concepts/messages#aimessagechunk) as it is generated by the LLM. This allows you to present or process the LLM\'s output incrementally as it\'s being produced, which is particularly useful in interactive applications or interfaces.\n\n#### Usage with LangGraph\n\n[LangGraph](/docs/concepts/architecture#langgraph) compiled graphs are [Runnables](/docs/concepts/runnables) and support the standard streaming APIs.\n\n## Streaming APIs\n\nLangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), including [LLMs](/docs/concepts/chat_models), [compiled LangGraph graphs](https://langchain-ai.github.io/langgraph/concepts/low_level/), and any Runnable generated with [LCEL](/docs/concepts/lcel).\n\n1. LangGraph\'s [StreamWriter](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamWriter) can be used to write custom data that will surface through **stream** and **astream** APIs when working with LangGraph. **Important** this is a LangGraph feature, so it is not available when working with pure LCEL. See [how to streaming custom data](https://langchain-ai.github.io/langgraph/how-tos/streaming-content/) for more information.\n2. [dispatch_events](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.dispatch_custom_event.html#) / [adispatch_events](https://python.langchain.com/api_reference/core/callbacks/langchain_core.callbacks.manager.adispatch_custom_event.html) can be used to write custom data that will be surfaced through the **astream_events** API. See [how to dispatch custom callback events](/docs/how_to/callbacks_custom_events/#astream-events-api) for more information.\n\n## "Auto-Streaming" Chat Models\n\nmodel = ChatAnthropic(model="claude-3-sonnet-20240229")\n\nprompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")\nparser = StrOutputParser()\nchain = prompt | model | parser\n\nasync for event in chain.astream_events({"topic": "parrot"}):\n    kind = event["event"]\n    if kind == "on_chat_model_stream":\n        print(event, end="|", flush=True)\n```\n\nYou can roughly think of it as an iterator over callback events (though the format differs) - and you can use it on almost all LangChain components!\n\nSee [this guide](/docs/how_to/streaming/#using-stream-events) for more detailed information on how to use `.astream_events()`, including a table listing available events.\n\n## Writing custom data to the stream\n\nTo write custom data to the stream, you will need to choose one of the following methods based on the component you are working with:\n\n# Streaming\n\n:::info Prerequisites\n* [Runnable Interface](/docs/concepts/runnables)\n* [Chat Models](/docs/concepts/chat_models)\n:::\n\n**Streaming** is crucial for enhancing the responsiveness of applications built on [LLMs](/docs/concepts/chat_models). By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\n## Overview\n\nGenerating full responses from [LLMs](/docs/concepts/chat_models) often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@6e39d17136707aa47de02133c65e58ec1655a174775a0e8f05f40d8df7ced9cf¢tsËAÚô­KÏ¨focus_msÍ‹§payload‚¤role©assistant§contentÚTwo methods to stream output from a ChatModel are `stream()` and `astream()`. The `stream()` method is synchronous, while `astream()` is asynchronous and designed for non-blocking workflows. Both methods allow for real-time streaming of output as it is generated by the model.ªfocus_kind¬exec_latency¨query_id¤Q091‰¢idÙ@b5256e5e4ebff263d5a8401742c89431c7b43daace7e92f2cda404627f4d2c79¢tsËAÚô­cŒ¨trace_idÙ$0cc90617-bcd8-4f06-888d-a0416bee6a33­parent_run_idÙ$7633e6ff-9960-4d31-aea3-6d3515c978f2¦run_idÙ$0cc90617-bcd8-4f06-888d-a0416bee6a33ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚTwo methods to stream output from a ChatModel are `stream()` and `astream()`. The `stream()` method is synchronous, while `astream()` is asynchronous and designed for non-blocking workflows. Both methods allow for real-time streaming of output as it is generated by the model.¨query_id¤Q091ªfocus_kind¶chain_execution_result‰¢idÙ@d53320ac67117f8ef2b36dbc33d0033be153b7002caf1b5bfe567d949e4b4cda¢tsËAÚô­o¨trace_idÙ$7633e6ff-9960-4d31-aea3-6d3515c978f2­parent_run_idÀ¦run_idÙ$7633e6ff-9960-4d31-aea3-6d3515c978f2ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ3Give two methods to stream output from a ChatModel.§context•‚´page_content_summaryÙËThe [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming b...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË## Streaming APIs

LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), includin...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. LangGraph's [StreamWriter](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamWriter) can be used to write custom data that will surface through **stream** and **astrea...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËmodel = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applic...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¦answerÚTwo methods to stream output from a ChatModel are `stream()` and `astream()`. The `stream()` method is synchronous, while `astream()` is asynchronous and designed for non-blocking workflows. Both methods allow for real-time streaming of output as it is generated by the model.¨query_id¤Q091ªfocus_kind¶chain_execution_result‹¢idÙ@5c3dbe17cd5979e1655db5499973f5e01fc36fb08083ef86908eaf4e7283b447¢tsËAÚô­wS¨trace_id¤Q092­parent_run_idÀ¦run_idÙ$91d7bfd4-812a-431e-a8f4-d484f6334527ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ[Which document teaches you to build a hybrid search retriever combining keyword and vector?¨query_id¤Q092§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q092ªfocus_kind¯chain_execution‹¢idÙ@bf6e9f960ab32dc3f39edabcfc827d0f8da9256b3b87be27bde62c43d1db5029¢tsËAÚô­ƒs¨trace_id¤Q092­parent_run_idÙ$91d7bfd4-812a-431e-a8f4-d484f6334527¦run_idÙ$56d628ac-6a44-4c36-a916-07811056a532ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ[Which document teaches you to build a hybrid search retriever combining keyword and vector?§context¦answer ¨query_id¤Q092¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q092®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:4d64bbbf-07db-f5ea-c88b-7638eadb0a32ªfocus_kind¯chain_execution‰¢idÙ@0af877ed60d4ce039e37dab82365cac8092da898a9a5d19fbf2f71064e85f2e6¢tsËAÚô¿F¨trace_idÙ$56d628ac-6a44-4c36-a916-07811056a532­parent_run_idÙ$91d7bfd4-812a-431e-a8f4-d484f6334527¦run_idÙ$56d628ac-6a44-4c36-a916-07811056a532ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË- [How to: generate metadata filters](/docs/how_to/self_query)
- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)
- [How to: use hybrid vector and keyword retrieval](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËThe particular data structure used to implement this is often an [*inverted index*](https://www.geeksforgeeks.org/inverted-index/).
This types of index contains a list of words and a mapping of each w...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË| Name                                                                                                              | When to use                                           | Description               ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË:::info[Further reading]

* See our [tutorial](/docs/tutorials/sql_qa/) for context on how to build a retreiver using a SQL database and text-to-SQL.
* See our [tutorial](/docs/tutorials/graph/) for c...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙËAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.
See this [ho...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx¨query_id¤Q092ªfocus_kind¶chain_execution_result‹¢idÙ@7d75020d907fc3416daf537c0d3b7341fb54f706f10f5e6fa6159f1de10ca089¢tsËAÚô¿¨trace_id¤Q092­parent_run_idÙ$91d7bfd4-812a-431e-a8f4-d484f6334527¦run_idÙ$7cf072bc-892e-4d80-85dc-061b38beaa50ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ[Which document teaches you to build a hybrid search retriever combining keyword and vector?§context•‚´page_content_summaryÙË- [How to: generate metadata filters](/docs/how_to/self_query)
- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)
- [How to: use hybrid vector and keyword retrieval](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËThe particular data structure used to implement this is often an [*inverted index*](https://www.geeksforgeeks.org/inverted-index/).
This types of index contains a list of words and a mapping of each w...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË| Name                                                                                                              | When to use                                           | Description               ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË:::info[Further reading]

* See our [tutorial](/docs/tutorials/sql_qa/) for context on how to build a retreiver using a SQL database and text-to-SQL.
* See our [tutorial](/docs/tutorials/graph/) for c...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙËAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.
See this [ho...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx¦answer ¨query_id¤Q092¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q092®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:bdf86098-ebba-cd45-b54b-b2273366d0f8ªfocus_kind¯chain_execution‹¢idÙ@d45c912285e4fc6cf199e3c64dd63b18bf06254069950bff8b7b3d5f0081e237¢tsËAÚô¿¨trace_id¤Q092­parent_run_idÙ$7cf072bc-892e-4d80-85dc-061b38beaa50¦run_idÙ$9c04596c-0ff9-4736-a90b-efcfe4c8b71bªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ[Which document teaches you to build a hybrid search retriever combining keyword and vector?§contextÚ,- [How to: generate metadata filters](/docs/how_to/self_query)
- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)
- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)

The particular data structure used to implement this is often an [*inverted index*](https://www.geeksforgeeks.org/inverted-index/).
This types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents. 
Using this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.
[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).

:::info[Further reading]

* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
* See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.

::: 

#### Vector indexes

| Name                                                                                                              | When to use                                           | Description                                                                                                                                  |
|-------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| [Hybrid search](/docs/integrations/retrievers/pinecone_hybrid_search/)                                            | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. [Paper](https://arxiv.org/abs/2210.11934). |

:::info[Further reading]

* See our [tutorial](/docs/tutorials/sql_qa/) for context on how to build a retreiver using a SQL database and text-to-SQL.
* See our [tutorial](/docs/tutorials/graph/) for context on how to build a retreiver using a graph database and text-to-Cypher.

:::

### Lexical search

As discussed in our conceptual review of [retrieval](/docs/concepts/retrieval/), many search engines are based upon matching words in a query to the words in each document. 
[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).
LangChain has retrievers for many popular lexical search algorithms / engines.

:::info[Further reading]

At the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.
See this [how-to guide on hybrid search](/docs/how_to/hybrid/) for more details.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q092®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:bdf86098-ebba-cd45-b54b-b2273366d0f8­checkpoint_nsÙ-generate:bdf86098-ebba-cd45-b54b-b2273366d0f8¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@a89e08808b34cacc5b628f6bb4357fe6ed018b65ab72493073a9e069882c4dd4¢tsËAÚô¿"¨trace_idÙ$9c04596c-0ff9-4736-a90b-efcfe4c8b71b­parent_run_idÙ$7cf072bc-892e-4d80-85dc-061b38beaa50¦run_idÙ$9c04596c-0ff9-4736-a90b-efcfe4c8b71bªevent_type¬chain_output¬event_source¥chain§payload§outputsÚmessages=[HumanMessage(content="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Which document teaches you to build a hybrid search retriever combining keyword and vector? \nContext: - [How to: generate metadata filters](/docs/how_to/self_query)\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\n- [How to: use hybrid vector and keyword retrieval](/docs/how_to/hybrid)\n\nThe particular data structure used to implement this is often an [*inverted index*](https://www.geeksforgeeks.org/inverted-index/).\nThis types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents. \nUsing this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\n[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\n\n:::info[Further reading]\n\n* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.\n* See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.\n\n::: \n\n#### Vector indexes\n\n| Name                                                                                                              | When to use                                           | Description                                                                                                                                  |\n|-------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|\n| [Hybrid search](/docs/integrations/retrievers/pinecone_hybrid_search/)                                            | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. [Paper](https://arxiv.org/abs/2210.11934). |\n\n:::info[Further reading]\n\n* See our [tutorial](/docs/tutorials/sql_qa/) for context on how to build a retreiver using a SQL database and text-to-SQL.\n* See our [tutorial](/docs/tutorials/graph/) for context on how to build a retreiver using a graph database and text-to-Cypher.\n\n:::\n\n### Lexical search\n\nAs discussed in our conceptual review of [retrieval](/docs/concepts/retrieval/), many search engines are based upon matching words in a query to the words in each document. \n[BM25](https://en.wikipedia.org/wiki/Okapi_BM25#:~:text=BM25%20is%20a%20bag%2Dof,slightly%20different%20components%20and%20parameters.) and [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) are [two popular lexical search algorithms](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vector-search?utm_source=profile&utm_medium=reader2).\nLangChain has retrievers for many popular lexical search algorithms / engines.\n\n:::info[Further reading]\n\nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.\nSee this [how-to guide on hybrid search](/docs/how_to/hybrid/) for more details. \nAnswer:", additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@3cc89f2fbed0d532c31d467fc4bbe54da68e366fdfc19dc7900494934c6537f2¢tsËAÚô­´¨focus_msÍH§payload‚¤role©assistant§contentÚ$The document that teaches you to build a hybrid search retriever combining keyword and vector is titled "How to: use hybrid vector and keyword retrieval." You can find it in the provided context. This guide explains how to combine keyword-based and semantic similarity approaches effectively.ªfocus_kind¬exec_latency¨query_id¤Q092‰¢idÙ@ec92cb142f15fdd2b878ab136a8c958a845267d5cc231df87799a960790aeb2a¢tsËAÚôÀQ¨trace_idÙ$7cf072bc-892e-4d80-85dc-061b38beaa50­parent_run_idÙ$91d7bfd4-812a-431e-a8f4-d484f6334527¦run_idÙ$7cf072bc-892e-4d80-85dc-061b38beaa50ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ$The document that teaches you to build a hybrid search retriever combining keyword and vector is titled "How to: use hybrid vector and keyword retrieval." You can find it in the provided context. This guide explains how to combine keyword-based and semantic similarity approaches effectively.¨query_id¤Q092ªfocus_kind¶chain_execution_result‰¢idÙ@7d92e82cb8bc168af4239923888fdac2a8447b830279e0a3f063e1da105fc497¢tsËAÚôËp¨trace_idÙ$91d7bfd4-812a-431e-a8f4-d484f6334527­parent_run_idÀ¦run_idÙ$91d7bfd4-812a-431e-a8f4-d484f6334527ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ[Which document teaches you to build a hybrid search retriever combining keyword and vector?§context•‚´page_content_summaryÙË- [How to: generate metadata filters](/docs/how_to/self_query)
- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)
- [How to: use hybrid vector and keyword retrieval](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËThe particular data structure used to implement this is often an [*inverted index*](https://www.geeksforgeeks.org/inverted-index/).
This types of index contains a list of words and a mapping of each w...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË| Name                                                                                                              | When to use                                           | Description               ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË:::info[Further reading]

* See our [tutorial](/docs/tutorials/sql_qa/) for context on how to build a retreiver using a SQL database and text-to-SQL.
* See our [tutorial](/docs/tutorials/graph/) for c...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙËAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.
See this [ho...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx¦answerÚ$The document that teaches you to build a hybrid search retriever combining keyword and vector is titled "How to: use hybrid vector and keyword retrieval." You can find it in the provided context. This guide explains how to combine keyword-based and semantic similarity approaches effectively.¨query_id¤Q092ªfocus_kind¶chain_execution_result‹¢idÙ@e26f01fcf40e96274031cc31fed67190e2fa6605275cd9c5b517b02932c10854¢tsËAÚôÓª¨trace_id¤Q093­parent_run_idÀ¦run_idÙ$950acd7c-112b-455e-86c3-8b223eea6ef5ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ>What is the purpose of `annotated` type hints in tool schemas?¨query_id¤Q093§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q093ªfocus_kind¯chain_execution‹¢idÙ@74b3ee51db6f79ac91fda9567cbfb4b696725bf8c8476b1e2819742149c0770f¢tsËAÚôåu¨trace_id¤Q093­parent_run_idÙ$950acd7c-112b-455e-86c3-8b223eea6ef5¦run_idÙ$71f08ea0-ee28-4432-969f-88d5efd9c453ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ>What is the purpose of `annotated` type hints in tool schemas?§context¦answer ¨query_id¤Q093¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q093®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:919ee04a-d70a-9296-064d-775d63053043ªfocus_kind¯chain_execution‰¢idÙ@7bad967fbeadaeec04635b81458fd05e6331d23fc660a09ae9b681b91fc2cd08¢tsËAÚô-sÿ¨trace_idÙ$71f08ea0-ee28-4432-969f-88d5efd9c453­parent_run_idÙ$950acd7c-112b-455e-86c3-8b223eea6ef5¦run_idÙ$71f08ea0-ee28-4432-969f-88d5efd9c453ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string liter...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).
...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Tool artifacts

**Tools** are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that w...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Schema definition

The central concept is that the output structure of model responses needs to be represented in some way. 
While types of objects you can use depend on the model you're working wi...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx¨query_id¤Q093ªfocus_kind¶chain_execution_result‹¢idÙ@f5fd669eb54dbbb9224d275402bbd92cce41678782ff5afe03d742694d3a3eab¢tsËAÚô-€=¨trace_id¤Q093­parent_run_idÙ$950acd7c-112b-455e-86c3-8b223eea6ef5¦run_idÙ$cd673e94-ec81-4ad1-bfd8-31e0ce4cf831ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ>What is the purpose of `annotated` type hints in tool schemas?§context•‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string liter...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).
...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Tool artifacts

**Tools** are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that w...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Schema definition

The central concept is that the output structure of model responses needs to be represented in some way. 
While types of objects you can use depend on the model you're working wi...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx¦answer ¨query_id¤Q093¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q093®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:f3c96852-49d8-3c73-c29a-ed85419ab659ªfocus_kind¯chain_execution‹¢idÙ@7bdb2b5970d4ceb5d90f96be4dced176fae7c8d3fab07d0de1f27becba4c459f¢tsËAÚô-†×¨trace_id¤Q093­parent_run_idÙ$cd673e94-ec81-4ad1-bfd8-31e0ce4cf831¦run_idÙ$12ec9580-2414-4754-91cb-afbb66eef532ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ>What is the purpose of `annotated` type hints in tool schemas?§contextÚ™See [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's function signature to configure the run time behavior of the tool.

The following type annotations will end up **removing** the argument from the tool's schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.

- **InjectedToolArg**: Value should be injected manually at runtime using `.invoke` or `.ainvoke`.
- **RunnableConfig**: Pass in the RunnableConfig object to the tool.
- **InjectedState**: Pass in the overall state of the LangGraph graph to the tool.
- **InjectedStore**: Pass in the LangGraph store object to the tool.

You can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

You can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string literal"]** -- Adds a description to the argument that will be exposed in the tool's schema.

### InjectedToolArg

There are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the `InjectedToolArg` annotation, which allows certain parameters to be hidden from the tool's schema.

For example, if a tool requires a `user_id` to be injected dynamically at runtime, it can be structured in this way:

```python
from langchain_core.tools import tool, InjectedToolArg

@tool
def user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:
    """Tool that processes input data."""
    return f"User {user_id} processed {input_data}"
```

## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).

Please see the [API reference for @tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) for more details and review the [how to create custom tools](/docs/how_to/custom_tools/) guide for examples.

## Tool artifacts

## Tool artifacts

**Tools** are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.

```python
@tool(response_format="content_and_artifact")
def some_tool(...) -> Tuple[str, Any]:
    """Tool that does something."""
    ...
    return 'Message for chat model', some_artifact 
```

See [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

## Schema definition

The central concept is that the output structure of model responses needs to be represented in some way. 
While types of objects you can use depend on the model you're working with, there are common types of objects that are typically allowed or recommended for structured output in Python.

The simplest and most common format for structured output is a JSON-like structure, which in Python can be represented as a dictionary (dict) or list (list).
JSON objects (or dicts in Python) are often used directly when the tool requires raw, flexible, and minimal-overhead structured data.

```json
{
  "answer": "The answer to the user's question",
  "followup_question": "A followup question the user could ask"
}
```

As a second example, [Pydantic](https://docs.pydantic.dev/latest/) is particularly useful for defining structured output schemas because it offers type hints and validation.
Here's an example of a Pydantic schema:¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q093®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:f3c96852-49d8-3c73-c29a-ed85419ab659­checkpoint_nsÙ-generate:f3c96852-49d8-3c73-c29a-ed85419ab659¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@0bcb9fd110a8edfde4912c0e5aa193ee32395ce139d433427ea3ec65800fccd3¢tsËAÚô-&¨trace_idÙ$12ec9580-2414-4754-91cb-afbb66eef532­parent_run_idÙ$cd673e94-ec81-4ad1-bfd8-31e0ce4cf831¦run_idÙ$12ec9580-2414-4754-91cb-afbb66eef532ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What is the purpose of `annotated` type hints in tool schemas? \nContext: See [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.\n\n## Special type annotations\n\nThere are a number of special type annotations that can be used in the tool\'s function signature to configure the run time behavior of the tool.\n\nThe following type annotations will end up **removing** the argument from the tool\'s schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.\n\n- **InjectedToolArg**: Value should be injected manually at runtime using `.invoke` or `.ainvoke`.\n- **RunnableConfig**: Pass in the RunnableConfig object to the tool.\n- **InjectedState**: Pass in the overall state of the LangGraph graph to the tool.\n- **InjectedStore**: Pass in the LangGraph store object to the tool.\n\nYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool\'s schema.\n\nYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool\'s schema.\n\n- **Annotated[..., "string literal"]** -- Adds a description to the argument that will be exposed in the tool\'s schema.\n\n### InjectedToolArg\n\nThere are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the `InjectedToolArg` annotation, which allows certain parameters to be hidden from the tool\'s schema.\n\nFor example, if a tool requires a `user_id` to be injected dynamically at runtime, it can be structured in this way:\n\n```python\nfrom langchain_core.tools import tool, InjectedToolArg\n\n@tool\ndef user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:\n    """Tool that processes input data."""\n    return f"User {user_id} processed {input_data}"\n```\n\n## Configuring the schema\n\nThe `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description\nor parse the function\'s doc-string to infer the schema).\n\nPlease see the [API reference for @tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) for more details and review the [how to create custom tools](/docs/how_to/custom_tools/) guide for examples.\n\n## Tool artifacts\n\n## Tool artifacts\n\n**Tools** are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool\'s execution that we want to make accessible to downstream components in our chain or agent, but that we don\'t want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.\n\n```python\n@tool(response_format="content_and_artifact")\ndef some_tool(...) -> Tuple[str, Any]:\n    """Tool that does something."""\n    ...\n    return \'Message for chat model\', some_artifact \n```\n\nSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.\n\n## Special type annotations\n\n## Schema definition\n\nThe central concept is that the output structure of model responses needs to be represented in some way. \nWhile types of objects you can use depend on the model you\'re working with, there are common types of objects that are typically allowed or recommended for structured output in Python.\n\nThe simplest and most common format for structured output is a JSON-like structure, which in Python can be represented as a dictionary (dict) or list (list).\nJSON objects (or dicts in Python) are often used directly when the tool requires raw, flexible, and minimal-overhead structured data.\n\n```json\n{\n  "answer": "The answer to the user\'s question",\n  "followup_question": "A followup question the user could ask"\n}\n```\n\nAs a second example, [Pydantic](https://docs.pydantic.dev/latest/) is particularly useful for defining structured output schemas because it offers type hints and validation.\nHere\'s an example of a Pydantic schema: \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@2fcea393f72f7657305546f8b65c56bb166753d2a686d2cc34c5dc2cae51f450¢tsËAÚô‘œ¨focus_msÍ§payload‚¤role©assistant§contentÚuThe purpose of `Annotated` type hints in tool schemas is to provide descriptions for arguments that will be exposed in the tool's schema. This allows for better documentation and understanding of the tool's parameters when they are presented to users or models. Additionally, it helps in configuring the runtime behavior of the tool by clarifying the role of each argument.ªfocus_kind¬exec_latency¨query_id¤Q093‰¢idÙ@02e843a953e75f02c12443914356801d1cedebbb81a6b0934cb849c1eda290ba¢tsËAÚô‘/ı¨trace_idÙ$cd673e94-ec81-4ad1-bfd8-31e0ce4cf831­parent_run_idÙ$950acd7c-112b-455e-86c3-8b223eea6ef5¦run_idÙ$cd673e94-ec81-4ad1-bfd8-31e0ce4cf831ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚuThe purpose of `Annotated` type hints in tool schemas is to provide descriptions for arguments that will be exposed in the tool's schema. This allows for better documentation and understanding of the tool's parameters when they are presented to users or models. Additionally, it helps in configuring the runtime behavior of the tool by clarifying the role of each argument.¨query_id¤Q093ªfocus_kind¶chain_execution_result‰¢idÙ@038d420f2c0d670efa21decf2dd9e4d88d99f51632e6e1bbd79bd516626ece5e¢tsËAÚô‘9G¨trace_idÙ$950acd7c-112b-455e-86c3-8b223eea6ef5­parent_run_idÀ¦run_idÙ$950acd7c-112b-455e-86c3-8b223eea6ef5ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ>What is the purpose of `annotated` type hints in tool schemas?§context•‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string liter...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).
...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Tool artifacts

**Tools** are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that w...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Schema definition

The central concept is that the output structure of model responses needs to be represented in some way. 
While types of objects you can use depend on the model you're working wi...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx¦answerÚuThe purpose of `Annotated` type hints in tool schemas is to provide descriptions for arguments that will be exposed in the tool's schema. This allows for better documentation and understanding of the tool's parameters when they are presented to users or models. Additionally, it helps in configuring the runtime behavior of the tool by clarifying the role of each argument.¨query_id¤Q093ªfocus_kind¶chain_execution_result‹¢idÙ@bd42e8ca53b5c90068ea96b43f3bc77b80c0a96f70f17a07abf953000b522744¢tsËAÚô‘B¦¨trace_id¤Q094­parent_run_idÀ¦run_idÙ$aa735b17-c982-4d44-945b-6eb5bd78e84fªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ8Explain why caching chat model responses is challenging.¨query_id¤Q094§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q094ªfocus_kind¯chain_execution‹¢idÙ@8de9a8d4de10f4df9f27c9bbcc5badd3f3bca0e2ab10f2c701818223911dc0fe¢tsËAÚô‘OY¨trace_id¤Q094­parent_run_idÙ$aa735b17-c982-4d44-945b-6eb5bd78e84f¦run_idÙ$0401ba51-63fc-4a86-b9ef-499bd172952fªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ8Explain why caching chat model responses is challenging.§context¦answer ¨query_id¤Q094¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q094®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:7cb86ad8-0c64-9fd5-6e41-35458b9be79dªfocus_kind¯chain_execution‰¢idÙ@24f3f3cb74c27c6e43a4d4a1778c15bd2594df30e5c17eec54ba2c82a74a29bc¢tsËAÚô¤	$¨trace_idÙ$0401ba51-63fc-4a86-b9ef-499bd172952f­parent_run_idÙ$aa735b17-c982-4d44-945b-6eb5bd78e84f¦run_idÙ$0401ba51-63fc-4a86-b9ef-499bd172952fªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to conver...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applic...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q094ªfocus_kind¶chain_execution_result‹¢idÙ@6630ce738f83ca3be77e14f2cb3a0cefb619d58b0c01b4c6847bab8d6793fcd1¢tsËAÚô¤8¨trace_id¤Q094­parent_run_idÙ$aa735b17-c982-4d44-945b-6eb5bd78e84f¦run_idÙ$e78a263d-0aee-4d61-a402-557e07225989ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ8Explain why caching chat model responses is challenging.§context•‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to conver...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applic...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answer ¨query_id¤Q094¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q094®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:d5430fc5-6ceb-aacc-e46b-c75c93fae78bªfocus_kind¯chain_execution‹¢idÙ@0ba5932bc8df3a764b30e2c2a409b189b301bf428d26451a1943e3521c22d407¢tsËAÚô¤a¨trace_id¤Q094­parent_run_idÙ$e78a263d-0aee-4d61-a402-557e07225989¦run_idÙ$63331986-8402-4d45-be6a-2c6e19e0814dªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ8Explain why caching chat model responses is challenging.§contextÚ¡### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.

The reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?

An alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.

A semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to convert text to a vector representation), and it's not guaranteed to capture the meaning of the input accurately.

However, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.

Please see the [how to cache chat model responses](/docs/how_to/chat_model_caching/) guide for more details.

## Related resources

* How-to guides on using chat models: [how-to guides](/docs/how_to/#chat-models).
* List of supported chat models: [chat model integrations](/docs/integrations/chat/).

### Conceptual guides

3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.

# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applications built on [LLMs](/docs/concepts/chat_models). By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.

## Overview

Generating full responses from [LLMs](/docs/concepts/chat_models) often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.

1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.
2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q094®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:d5430fc5-6ceb-aacc-e46b-c75c93fae78b­checkpoint_nsÙ-generate:d5430fc5-6ceb-aacc-e46b-c75c93fae78b¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@51581805ed19508d71f31cd44c5f4ffa36cb985ec558a64a851422828b8325ad¢tsËAÚô¤ .¨trace_idÙ$63331986-8402-4d45-be6a-2c6e19e0814d­parent_run_idÙ$e78a263d-0aee-4d61-a402-557e07225989¦run_idÙ$63331986-8402-4d45-be6a-2c6e19e0814dªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ\messages=[HumanMessage(content="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Explain why caching chat model responses is challenging. \nContext: ### Caching\n\nChat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.\n\nThe reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the **exact** inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?\n\nAn alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.\n\nA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to convert text to a vector representation), and it's not guaranteed to capture the meaning of the input accurately.\n\nHowever, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.\n\nPlease see the [how to cache chat model responses](/docs/how_to/chat_model_caching/) guide for more details.\n\n## Related resources\n\n* How-to guides on using chat models: [how-to guides](/docs/how_to/#chat-models).\n* List of supported chat models: [chat model integrations](/docs/integrations/chat/).\n\n### Conceptual guides\n\n3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\n\n# Streaming\n\n:::info Prerequisites\n* [Runnable Interface](/docs/concepts/runnables)\n* [Chat Models](/docs/concepts/chat_models)\n:::\n\n**Streaming** is crucial for enhancing the responsiveness of applications built on [LLMs](/docs/concepts/chat_models). By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs.\n\n## Overview\n\nGenerating full responses from [LLMs](/docs/concepts/chat_models) often incurs a delay of several seconds, which becomes more noticeable in complex applications with multiple model calls. Fortunately, LLMs generate responses iteratively, allowing for intermediate results to be displayed as they are produced. By streaming these intermediate outputs, LangChain enables smoother UX in LLM-powered apps and offers built-in support for streaming at the core of its design.\n\n1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.\n2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information. \nAnswer:", additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@85cd3ccb96d76049e79e9a0b4e0d20c30cf9a398bfbd7c4d5976382a157aaee7¢tsËAÚô	
¨focus_msÍô§payload‚¤role©assistant§contentÚØCaching chat model responses is challenging because exact inputs are unlikely to repeat after the initial interactions in a conversation, making cache hits rare. Additionally, semantic caching, which relies on understanding the meaning of inputs, introduces dependencies on other models and may not accurately capture the intended meaning. While caching can improve performance in specific scenarios, such as frequently asked questions, it requires careful implementation.ªfocus_kind¬exec_latency¨query_id¤Q094‰¢idÙ@88862eed2da10655cbf5b7a8d49d494accdf2651807964476d4a519c555b7b37¢tsËAÚô	!S¨trace_idÙ$e78a263d-0aee-4d61-a402-557e07225989­parent_run_idÙ$aa735b17-c982-4d44-945b-6eb5bd78e84f¦run_idÙ$e78a263d-0aee-4d61-a402-557e07225989ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚØCaching chat model responses is challenging because exact inputs are unlikely to repeat after the initial interactions in a conversation, making cache hits rare. Additionally, semantic caching, which relies on understanding the meaning of inputs, introduces dependencies on other models and may not accurately capture the intended meaning. While caching can improve performance in specific scenarios, such as frequently asked questions, it requires careful implementation.¨query_id¤Q094ªfocus_kind¶chain_execution_result‰¢idÙ@abc7a636de15dfd6fd626e0021ca564142ea1d5dfb8c69740d4be6576db2410b¢tsËAÚô	) ¨trace_idÙ$aa735b17-c982-4d44-945b-6eb5bd78e84f­parent_run_idÀ¦run_idÙ$aa735b17-c982-4d44-945b-6eb5bd78e84fªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ8Explain why caching chat model responses is challenging.§context•‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to conver...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applic...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚØCaching chat model responses is challenging because exact inputs are unlikely to repeat after the initial interactions in a conversation, making cache hits rare. Additionally, semantic caching, which relies on understanding the meaning of inputs, introduces dependencies on other models and may not accurately capture the intended meaning. While caching can improve performance in specific scenarios, such as frequently asked questions, it requires careful implementation.¨query_id¤Q094ªfocus_kind¶chain_execution_result‹¢idÙ@9d6d8c872b9d250f177137d4f5619544e3d5ac7690d74b771475e4f6648ec2ec¢tsËAÚô	.ş¨trace_id¤Q095­parent_run_idÀ¦run_idÙ$ffb8d715-65e6-4aa1-bdcc-683d51a12c2fªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙIWhich two docs together explain trimming long documents before embedding?¨query_id¤Q095§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q095ªfocus_kind¯chain_execution‹¢idÙ@1fe05883307f5e76578dcf00d2f8948e5b44950ee8b302933e1d6561df7831d1¢tsËAÚô	6¨trace_id¤Q095­parent_run_idÙ$ffb8d715-65e6-4aa1-bdcc-683d51a12c2f¦run_idÙ$719af17b-0cad-45d3-907f-87ad2811b02aªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙIWhich two docs together explain trimming long documents before embedding?§context¦answer ¨query_id¤Q095¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q095®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:657fcf81-f2bc-9a13-c516-725cdadf02e2ªfocus_kind¯chain_execution‰¢idÙ@6e9043f902ddda6eb4b043538bc9d763b9113ac30b0ec05f688019a5fa1e17c8¢tsËAÚô	0A%¨trace_idÙ$719af17b-0cad-45d3-907f-87ad2811b02a­parent_run_idÙ$ffb8d715-65e6-4aa1-bdcc-683d51a12c2f¦run_idÙ$719af17b-0cad-45d3-907f-87ad2811b02aªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Text splitters

[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.

- [How to: recursively split text](/docs/how_to/recursive_tex...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË- **Handling non-uniform document lengths**: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.
- **Overcoming model li...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙËNow, the next question is *how* to split the documents into chunks! There are several strategies, each with its own advantages.

:::info[Further reading]
* See Greg Kamradt's [chunkviz](https://chunkv...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË| [ParentDocument](/docs/how_to/parent_document_retriever/) | Vector store + Document Store | No                        | If your pages have lots of smaller pieces of distinct information that are bes...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¨query_id¤Q095ªfocus_kind¶chain_execution_result‹¢idÙ@98ff6aad1c47a5df7d221c6b3c028ee939f5860b1e43ec8beb5a47566620b9ea¢tsËAÚô	0G„¨trace_id¤Q095­parent_run_idÙ$ffb8d715-65e6-4aa1-bdcc-683d51a12c2f¦run_idÙ$68fb272b-5df7-47b9-ab7d-67fa94a2d975ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙIWhich two docs together explain trimming long documents before embedding?§context•‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Text splitters

[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.

- [How to: recursively split text](/docs/how_to/recursive_tex...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË- **Handling non-uniform document lengths**: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.
- **Overcoming model li...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙËNow, the next question is *how* to split the documents into chunks! There are several strategies, each with its own advantages.

:::info[Further reading]
* See Greg Kamradt's [chunkviz](https://chunkv...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË| [ParentDocument](/docs/how_to/parent_document_retriever/) | Vector store + Document Store | No                        | If your pages have lots of smaller pieces of distinct information that are bes...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¦answer ¨query_id¤Q095¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q095®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:57553e42-88bc-2c5f-4e55-ad2fc83cd11cªfocus_kind¯chain_execution‹¢idÙ@5bfd04472a90d29258bb85c5487f452339299e16f9a503cd4c4248598ab05b33¢tsËAÚô	0K¡¨trace_id¤Q095­parent_run_idÙ$68fb272b-5df7-47b9-ab7d-67fa94a2d975¦run_idÙ$0fb2e5ba-a97c-41a5-a283-3eedadc24f3bªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙIWhich two docs together explain trimming long documents before embedding?§contextÚ## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and long-term memory in chat models using [LangGraph](https://langchain-ai.github.io/langgraph/).

### Text splitters

[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.

- [How to: recursively split text](/docs/how_to/recursive_text_splitter)
- [How to: split HTML](/docs/how_to/split_html)
- [How to: split by character](/docs/how_to/character_text_splitter)
- [How to: split code](/docs/how_to/code_splitter)
- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)
- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)
- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)
- [How to: split by tokens](/docs/how_to/split_by_token)

### Embedding models

[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.
See [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.

- **Handling non-uniform document lengths**: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.
- **Overcoming model limitations**: Many embedding models and language models have maximum input size constraints. Splitting allows us to process documents that would otherwise exceed these limits.
- **Improving representation quality**: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.
- **Enhancing retrieval precision**: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.
- **Optimizing computational resources**: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.

Now, the next question is *how* to split the documents into chunks! There are several strategies, each with its own advantages.

:::info[Further reading]
* See Greg Kamradt's [chunkviz](https://chunkviz.up.railway.app/) to visualize different splitting strategies discussed below.
:::

## Approaches

### Length-based

The most intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit.
Key benefits of length-based splitting:
- Straightforward implementation
- Consistent chunk sizes
- Easily adaptable to different model requirements

Types of length-based splitting:
- **Token-based**: Splits text based on the number of tokens, which is useful when working with language models.
- **Character-based**: Splits text based on the number of characters, which can be more consistent across different types of text.

| [ParentDocument](/docs/how_to/parent_document_retriever/) | Vector store + Document Store | No                        | If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together. | This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks). |
| [Multi Vector](/docs/how_to/multi_vector/)                | Vector store + Document Store | Sometimes during indexing | If you are able to extract information from documents that you think is more relevant to index than the text itself.                    | This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.                                         |¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q095®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:57553e42-88bc-2c5f-4e55-ad2fc83cd11c­checkpoint_nsÙ-generate:57553e42-88bc-2c5f-4e55-ad2fc83cd11c¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@3577b14855c4081dd75c0a686ddd050656bc4067c82489e00dec153463d63a6b¢tsËAÚô	0RŞ¨trace_idÙ$0fb2e5ba-a97c-41a5-a283-3eedadc24f3b­parent_run_idÙ$68fb272b-5df7-47b9-ab7d-67fa94a2d975¦run_idÙ$0fb2e5ba-a97c-41a5-a283-3eedadc24f3bªevent_type¬chain_output¬event_source¥chain§payload§outputsÚômessages=[HumanMessage(content="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Which two docs together explain trimming long documents before embedding? \nContext: ## Related resources\n\n- [How to trim messages](/docs/how_to/trim_messages/)\n- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and long-term memory in chat models using [LangGraph](https://langchain-ai.github.io/langgraph/).\n\n### Text splitters\n\n[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.\n\n- [How to: recursively split text](/docs/how_to/recursive_text_splitter)\n- [How to: split HTML](/docs/how_to/split_html)\n- [How to: split by character](/docs/how_to/character_text_splitter)\n- [How to: split code](/docs/how_to/code_splitter)\n- [How to: split Markdown by headers](/docs/how_to/markdown_header_metadata_splitter)\n- [How to: recursively split JSON](/docs/how_to/recursive_json_splitter)\n- [How to: split text into semantic chunks](/docs/how_to/semantic-chunker)\n- [How to: split by tokens](/docs/how_to/split_by_token)\n\n### Embedding models\n\n[Embedding Models](/docs/concepts/embedding_models) take a piece of text and create a numerical representation of it.\nSee [supported integrations](/docs/integrations/text_embedding/) for details on getting started with embedding models from a specific provider.\n\n- **Handling non-uniform document lengths**: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.\n- **Overcoming model limitations**: Many embedding models and language models have maximum input size constraints. Splitting allows us to process documents that would otherwise exceed these limits.\n- **Improving representation quality**: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.\n- **Enhancing retrieval precision**: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.\n- **Optimizing computational resources**: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.\n\nNow, the next question is *how* to split the documents into chunks! There are several strategies, each with its own advantages.\n\n:::info[Further reading]\n* See Greg Kamradt's [chunkviz](https://chunkviz.up.railway.app/) to visualize different splitting strategies discussed below.\n:::\n\n## Approaches\n\n### Length-based\n\nThe most intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit.\nKey benefits of length-based splitting:\n- Straightforward implementation\n- Consistent chunk sizes\n- Easily adaptable to different model requirements\n\nTypes of length-based splitting:\n- **Token-based**: Splits text based on the number of tokens, which is useful when working with language models.\n- **Character-based**: Splits text based on the number of characters, which can be more consistent across different types of text.\n\n| [ParentDocument](/docs/how_to/parent_document_retriever/) | Vector store + Document Store | No                        | If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together. | This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks). |\n| [Multi Vector](/docs/how_to/multi_vector/)                | Vector store + Document Store | Sometimes during indexing | If you are able to extract information from documents that you think is more relevant to index than the text itself.                    | This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.                                         | \nAnswer:", additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@ad18c6612a4ee6f529bed7f25404823ebdba41de31661ea52e494aae00cc52b1¢tsËAÚô	~Ó5¨focus_msÍÊ§payload‚¤role©assistant§contentÙçThe two documents that explain trimming long documents before embedding are "How to trim messages" and "Text Splitters." These resources provide guidance on effectively managing document lengths for better processing and embedding.ªfocus_kind¬exec_latency¨query_id¤Q095‰¢idÙ@f079d4eff6820d2a75bbbf2b272dc0b4614f4b5feb0cc7fdd0ee3033820701a3¢tsËAÚô	~æû¨trace_idÙ$68fb272b-5df7-47b9-ab7d-67fa94a2d975­parent_run_idÙ$ffb8d715-65e6-4aa1-bdcc-683d51a12c2f¦run_idÙ$68fb272b-5df7-47b9-ab7d-67fa94a2d975ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙçThe two documents that explain trimming long documents before embedding are "How to trim messages" and "Text Splitters." These resources provide guidance on effectively managing document lengths for better processing and embedding.¨query_id¤Q095ªfocus_kind¶chain_execution_result‰¢idÙ@a3a2d8f6121efd09019ab8681157a74c8715cc85f73b7fe200b5eae4c4146358¢tsËAÚô	~ï–¨trace_idÙ$ffb8d715-65e6-4aa1-bdcc-683d51a12c2f­parent_run_idÀ¦run_idÙ$ffb8d715-65e6-4aa1-bdcc-683d51a12c2fªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙIWhich two docs together explain trimming long documents before embedding?§context•‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Text splitters

[Text Splitters](/docs/concepts/text_splitters) take a document and split into chunks that can be used for retrieval.

- [How to: recursively split text](/docs/how_to/recursive_tex...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË- **Handling non-uniform document lengths**: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.
- **Overcoming model li...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙËNow, the next question is *how* to split the documents into chunks! There are several strategies, each with its own advantages.

:::info[Further reading]
* See Greg Kamradt's [chunkviz](https://chunkv...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË| [ParentDocument](/docs/how_to/parent_document_retriever/) | Vector store + Document Store | No                        | If your pages have lots of smaller pieces of distinct information that are bes...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¦answerÙçThe two documents that explain trimming long documents before embedding are "How to trim messages" and "Text Splitters." These resources provide guidance on effectively managing document lengths for better processing and embedding.¨query_id¤Q095ªfocus_kind¶chain_execution_result‹¢idÙ@86ff5c074516b291e94b002597cf1b6a7d308b78d958ce5b2b65e1bce513889c¢tsËAÚô	~ø¡¨trace_id¤Q096­parent_run_idÀ¦run_idÙ$b25e8d6b-07f8-4c7e-a6a6-a940db31f199ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙYWhat class enables you to get structured JSON output directly from LLMS without a parser?¨query_id¤Q096§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q096ªfocus_kind¯chain_execution‹¢idÙ@7a9835cf465b7255a3a581f724148a111d3bddc4e97a5ff73dd871a7bf837777¢tsËAÚô	Á¨trace_id¤Q096­parent_run_idÙ$b25e8d6b-07f8-4c7e-a6a6-a940db31f199¦run_idÙ$df4cab5d-2d93-413e-b239-131c091ca872ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙYWhat class enables you to get structured JSON output directly from LLMS without a parser?§context¦answer ¨query_id¤Q096¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q096®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:866650d3-6723-609d-a104-ae72be1069c5ªfocus_kind¯chain_execution‰¢idÙ@58739d05cc626214014dbfe42d3aeeeb8c36eeb73cd7a86671e3ae92052c1b73¢tsËAÚô	‘(ª¨trace_idÙ$df4cab5d-2d93-413e-b239-131c091ca872­parent_run_idÙ$b25e8d6b-07f8-4c7e-a6a6-a940db31f199¦run_idÙ$df4cab5d-2d93-413e-b239-131c091ca872ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË### Output parsers

[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.

- [How to: parse text from message objects...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË### JSON mode

In addition to tool calling, some model providers support a feature called `JSON mode`. 
This supports JSON schema definition as input and enforces the model to produce a conforming JSO...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙË### LLMs

What LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.

- [How to: cache model responses](/docs/how_to/llm_cachin...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË| [Structured](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html#langchain.output_parsers.structured.StructuredOutputP...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/output_parsers.mdx‚´page_content_summaryÙÍ| [JSON](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html)                                                     | âœ…               ...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/output_parsers.mdx¨query_id¤Q096ªfocus_kind¶chain_execution_result‹¢idÙ@f89d706a8be9c96c5693f3ab7c5a47b16e024ffc2a2d2eb56cbdc7a11b428d06¢tsËAÚô	‘/f¨trace_id¤Q096­parent_run_idÙ$b25e8d6b-07f8-4c7e-a6a6-a940db31f199¦run_idÙ$5e938169-8957-4df1-b2bf-f7b38dd55f5fªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙYWhat class enables you to get structured JSON output directly from LLMS without a parser?§context•‚´page_content_summaryÙË### Output parsers

[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.

- [How to: parse text from message objects...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË### JSON mode

In addition to tool calling, some model providers support a feature called `JSON mode`. 
This supports JSON schema definition as input and enforces the model to produce a conforming JSO...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙË### LLMs

What LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.

- [How to: cache model responses](/docs/how_to/llm_cachin...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË| [Structured](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html#langchain.output_parsers.structured.StructuredOutputP...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/output_parsers.mdx‚´page_content_summaryÙÍ| [JSON](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html)                                                     | âœ…               ...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/output_parsers.mdx¦answer ¨query_id¤Q096¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q096®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:e4094f0a-9319-70f8-a1a7-6673a6fb1125ªfocus_kind¯chain_execution‹¢idÙ@320fe3644e09f1ad45935c40754f0127479f1e10b22372771f11a816ec9c01c1¢tsËAÚô	‘3Ê¨trace_id¤Q096­parent_run_idÙ$5e938169-8957-4df1-b2bf-f7b38dd55f5f¦run_idÙ$3e99143f-74b6-4810-ac4a-21c145221f8eªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙYWhat class enables you to get structured JSON output directly from LLMS without a parser?§contextÚ### Output parsers

[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.

- [How to: parse text from message objects](/docs/how_to/output_parser_string)
- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)
- [How to: parse JSON output](/docs/how_to/output_parser_json)
- [How to: parse XML output](/docs/how_to/output_parser_xml)
- [How to: parse YAML output](/docs/how_to/output_parser_yaml)
- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)
- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)
- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)

### Document loaders

[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.

### JSON mode

In addition to tool calling, some model providers support a feature called `JSON mode`. 
This supports JSON schema definition as input and enforces the model to produce a conforming JSON output.
You can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).
Here is an example of how to use JSON mode with OpenAI:

```python
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o").with_structured_output(method="json_mode")
ai_msg = model.invoke("Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]")
ai_msg
{'random_ints': [45, 67, 12, 34, 89, 23, 78, 56, 90, 11]}
```

## Structured output method 

There are a few challenges when producing structured output with the above methods: 

(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.

### LLMs

What LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.

- [How to: cache model responses](/docs/how_to/llm_caching)
- [How to: create a custom LLM class](/docs/how_to/custom_llm)
- [How to: stream a response back](/docs/how_to/streaming_llm)
- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)
- [How to: work with local models](/docs/how_to/local_llms)

### Output parsers

[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.

| [Structured](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html#langchain.output_parsers.structured.StructuredOutputParser)                            |                    | âœ…                       |           | `str` \| `Message` | `Dict[str, str]`     | An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.                                            |

| [JSON](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html)                                                     | âœ…                  | âœ…                       |           | `str` \| `Message` | JSON object          | Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.                                    |¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q096®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:e4094f0a-9319-70f8-a1a7-6673a6fb1125­checkpoint_nsÙ-generate:e4094f0a-9319-70f8-a1a7-6673a6fb1125¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@b5b5a80a05746e00856765ec9cac42f6b7c4eb3b20bb426bf970cfd8ad58efd2¢tsËAÚô	‘9ï¨trace_idÙ$3e99143f-74b6-4810-ac4a-21c145221f8e­parent_run_idÙ$5e938169-8957-4df1-b2bf-f7b38dd55f5f¦run_idÙ$3e99143f-74b6-4810-ac4a-21c145221f8eªevent_type¬chain_output¬event_source¥chain§payload§outputsÚúmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What class enables you to get structured JSON output directly from LLMS without a parser? \nContext: ### Output parsers\n\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.\n\n- [How to: parse text from message objects](/docs/how_to/output_parser_string)\n- [How to: use output parsers to parse an LLM response into structured format](/docs/how_to/output_parser_structured)\n- [How to: parse JSON output](/docs/how_to/output_parser_json)\n- [How to: parse XML output](/docs/how_to/output_parser_xml)\n- [How to: parse YAML output](/docs/how_to/output_parser_yaml)\n- [How to: retry when output parsing errors occur](/docs/how_to/output_parser_retry)\n- [How to: try to fix errors in output parsing](/docs/how_to/output_parser_fixing)\n- [How to: write a custom output parser class](/docs/how_to/output_parser_custom)\n\n### Document loaders\n\n[Document Loaders](/docs/concepts/document_loaders) are responsible for loading documents from a variety of sources.\n\n### JSON mode\n\nIn addition to tool calling, some model providers support a feature called `JSON mode`. \nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\nYou can find a table of model providers that support JSON mode [here](/docs/integrations/chat/).\nHere is an example of how to use JSON mode with OpenAI:\n\n```python\nfrom langchain_openai import ChatOpenAI\nmodel = ChatOpenAI(model="gpt-4o").with_structured_output(method="json_mode")\nai_msg = model.invoke("Return a JSON object with key \'random_ints\' and a value of 10 random ints in [0-99]")\nai_msg\n{\'random_ints\': [45, 67, 12, 34, 89, 23, 78, 56, 90, 11]}\n```\n\n## Structured output method \n\nThere are a few challenges when producing structured output with the above methods: \n\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n\n### LLMs\n\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\n\n- [How to: cache model responses](/docs/how_to/llm_caching)\n- [How to: create a custom LLM class](/docs/how_to/custom_llm)\n- [How to: stream a response back](/docs/how_to/streaming_llm)\n- [How to: track token usage](/docs/how_to/llm_token_usage_tracking)\n- [How to: work with local models](/docs/how_to/local_llms)\n\n### Output parsers\n\n[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.\n\n| [Structured](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html#langchain.output_parsers.structured.StructuredOutputParser)                            |                    | âœ…                       |           | `str` \\| `Message` | `Dict[str, str]`     | An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.                                            |\n\n| [JSON](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html)                                                     | âœ…                  | âœ…                       |           | `str` \\| `Message` | JSON object          | Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.                                    | \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@06456f9f572b498e7b2675b79c1cd44fa773b55e2f4ddce16fbaf00161984456¢tsËAÚô	ô£a¨focus_msÍ§payload‚¤role©assistant§contentÚRThe class that enables you to get structured JSON output directly from LLMs without a parser is the `JsonOutputParser`. It returns a JSON object as specified and is considered reliable for obtaining structured data without using function calling. Additionally, you can specify a Pydantic model to ensure the output conforms to that model.ªfocus_kind¬exec_latency¨query_id¤Q096‰¢idÙ@ea3924994ac6cefd315517c76677c81c8e29ae5e877e578fa6d7f995d40ad98b¢tsËAÚô	ô´¨trace_idÙ$5e938169-8957-4df1-b2bf-f7b38dd55f5f­parent_run_idÙ$b25e8d6b-07f8-4c7e-a6a6-a940db31f199¦run_idÙ$5e938169-8957-4df1-b2bf-f7b38dd55f5fªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚRThe class that enables you to get structured JSON output directly from LLMs without a parser is the `JsonOutputParser`. It returns a JSON object as specified and is considered reliable for obtaining structured data without using function calling. Additionally, you can specify a Pydantic model to ensure the output conforms to that model.¨query_id¤Q096ªfocus_kind¶chain_execution_result‰¢idÙ@6690a871c81b1f06a9e1cab4e7cee2d4447fd2329910130da9b2699c6884797f¢tsËAÚô	ô¼õ¨trace_idÙ$b25e8d6b-07f8-4c7e-a6a6-a940db31f199­parent_run_idÀ¦run_idÙ$b25e8d6b-07f8-4c7e-a6a6-a940db31f199ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙYWhat class enables you to get structured JSON output directly from LLMS without a parser?§context•‚´page_content_summaryÙË### Output parsers

[Output Parsers](/docs/concepts/output_parsers) are responsible for taking the output of an LLM and parsing into more structured format.

- [How to: parse text from message objects...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË### JSON mode

In addition to tool calling, some model providers support a feature called `JSON mode`. 
This supports JSON schema definition as input and enforces the model to produce a conforming JSO...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙË### LLMs

What LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.

- [How to: cache model responses](/docs/how_to/llm_cachin...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË| [Structured](https://python.langchain.com/api_reference/langchain/output_parsers/langchain.output_parsers.structured.StructuredOutputParser.html#langchain.output_parsers.structured.StructuredOutputP...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/output_parsers.mdx‚´page_content_summaryÙÍ| [JSON](https://python.langchain.com/api_reference/core/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html)                                                     | âœ…               ...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/output_parsers.mdx¦answerÚRThe class that enables you to get structured JSON output directly from LLMs without a parser is the `JsonOutputParser`. It returns a JSON object as specified and is considered reliable for obtaining structured data without using function calling. Additionally, you can specify a Pydantic model to ensure the output conforms to that model.¨query_id¤Q096ªfocus_kind¶chain_execution_result‹¢idÙ@6a6091439bf65a24dc469a475e6e4426b5013ec717dd46e25ce20480f1edc493¢tsËAÚô	ôÆ“¨trace_id¤Q097­parent_run_idÀ¦run_idÙ$cc3e44e1-d449-4f47-9789-0a2e7ef9f6eaªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ<Name three built in message field keys aside from `content`.¨query_id¤Q097§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q097ªfocus_kind¯chain_execution‹¢idÙ@65685496ee66e48308f18607e79f68725ea3aad33c0c7384efab1ea6b16f95e1¢tsËAÚô	ôÓş¨trace_id¤Q097­parent_run_idÙ$cc3e44e1-d449-4f47-9789-0a2e7ef9f6ea¦run_idÙ$4b08db9c-2a29-4d7b-baca-22b46061297bªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ<Name three built in message field keys aside from `content`.§context¦answer ¨query_id¤Q097¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q097®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:612423c4-f75b-fb5f-8c74-8f3019ad7d1cªfocus_kind¯chain_execution‰¢idÙ@00b6a0265e2e5ad67c5d7f4cd5b3298bd834714f431387ecbed12ba92a8cf0f2¢tsËAÚô
0Mà¨trace_idÙ$4b08db9c-2a29-4d7b-baca-22b46061297b­parent_run_idÙ$cc3e44e1-d449-4f47-9789-0a2e7ef9f6ea¦run_idÙ$4b08db9c-2a29-4d7b-baca-22b46061297bªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË### Content

The content of a message text or a list of dictionaries representing [multimodal data](/docs/concepts/multimodality) (e.g., images, audio, video). The exact format of the content can vary...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË| Attribute            | Standardized/Raw | Description                                                                                                                                                 ...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË## What is inside a message?

A message typically consists of the following pieces of information:

- **Role**: The role of the message (e.g., "user", "assistant").
- **Content**: The content of the m...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË# Messages

:::info Prerequisites
- [Chat Models](/docs/concepts/chat_models)
:::

## Overview

Messages are the unit of communication in [chat models](/docs/concepts/chat_models). They are used to re...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË#### content

The **content** property of an `AIMessage` represents the response generated by the chat model.

The content is either:

- **text** -- the norm for virtually all chat models.
- A **list ...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx¨query_id¤Q097ªfocus_kind¶chain_execution_result‹¢idÙ@f167f1bc93c0cbd3c96ace4728415cabd678215aebbe93fe143f15ee5cb30843¢tsËAÚô
0VT¨trace_id¤Q097­parent_run_idÙ$cc3e44e1-d449-4f47-9789-0a2e7ef9f6ea¦run_idÙ$bd82c65a-5d21-473c-af56-b6f56c324e1fªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ<Name three built in message field keys aside from `content`.§context•‚´page_content_summaryÙË### Content

The content of a message text or a list of dictionaries representing [multimodal data](/docs/concepts/multimodality) (e.g., images, audio, video). The exact format of the content can vary...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË| Attribute            | Standardized/Raw | Description                                                                                                                                                 ...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË## What is inside a message?

A message typically consists of the following pieces of information:

- **Role**: The role of the message (e.g., "user", "assistant").
- **Content**: The content of the m...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË# Messages

:::info Prerequisites
- [Chat Models](/docs/concepts/chat_models)
:::

## Overview

Messages are the unit of communication in [chat models](/docs/concepts/chat_models). They are used to re...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË#### content

The **content** property of an `AIMessage` represents the response generated by the chat model.

The content is either:

- **text** -- the norm for virtually all chat models.
- A **list ...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx¦answer ¨query_id¤Q097¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q097®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:0e41a104-7301-54dd-dc2d-66527f9e6d6dªfocus_kind¯chain_execution‹¢idÙ@502b5bc12612f5da8d29c5d7103a487c1e34b5679cb8c5b3b1e747da4e4b1981¢tsËAÚô
0[¼¨trace_id¤Q097­parent_run_idÙ$bd82c65a-5d21-473c-af56-b6f56c324e1f¦run_idÙ$3cc9fdfd-1a83-4433-a594-db3c231be975ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ<Name three built in message field keys aside from `content`.§contextÚÇ### Content

The content of a message text or a list of dictionaries representing [multimodal data](/docs/concepts/multimodality) (e.g., images, audio, video). The exact format of the content can vary between different chat model providers.

Currently, most chat models support text as the primary content type, with some models also supporting multimodal data. However, support for multimodal data is still limited across most chat model providers.

For more information see:
* [SystemMessage](#systemmessage) -- for content which should be passed to direct the conversation
* [HumanMessage](#humanmessage) -- for content in the input from the user.
* [AIMessage](#aimessage) -- for content in the response from the model.
* [Multimodality](/docs/concepts/multimodality) -- for more information on multimodal content.

### Other Message Data

Depending on the chat model provider, messages can include other data such as:

| Attribute            | Standardized/Raw | Description                                                                                                                                                                                                             |
|----------------------|:-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `content`            | Raw              | Usually a string, but can be a list of content blocks. See [content](#content) for details.                                                                                                                             |

## What is inside a message?

A message typically consists of the following pieces of information:

- **Role**: The role of the message (e.g., "user", "assistant").
- **Content**: The content of the message (e.g., text, multimodal data).
- Additional metadata: id, name, [token usage](/docs/concepts/tokens) and other model-specific metadata.

### Role

Roles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.

# Messages

:::info Prerequisites
- [Chat Models](/docs/concepts/chat_models)
:::

## Overview

Messages are the unit of communication in [chat models](/docs/concepts/chat_models). They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.

Each message has a **role** (e.g., "user", "assistant") and **content** (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.

LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.

## What is inside a message?

A message typically consists of the following pieces of information:

#### content

The **content** property of an `AIMessage` represents the response generated by the chat model.

The content is either:

- **text** -- the norm for virtually all chat models.
- A **list of dictionaries** -- Each dictionary represents a content block and is associated with a `type`.
    * Used by Anthropic for surfacing agent thought process when doing [tool calling](/docs/concepts/tool_calling).
    * Used by OpenAI for audio outputs. Please see [multi-modal content](/docs/concepts/multimodality) for more information.

:::important
The **content** property is **not** standardized across different chat model providers, mostly because there are
still few examples to generalize from.
:::

### AIMessageChunk

It is common to [stream](/docs/concepts/streaming) responses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q097®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:0e41a104-7301-54dd-dc2d-66527f9e6d6d­checkpoint_nsÙ-generate:0e41a104-7301-54dd-dc2d-66527f9e6d6d¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@8426f33ee918d0be9c20f8d9e64ac85be23658f2068efccda8f521eca4f5e541¢tsËAÚô
0c¨trace_idÙ$3cc9fdfd-1a83-4433-a594-db3c231be975­parent_run_idÙ$bd82c65a-5d21-473c-af56-b6f56c324e1f¦run_idÙ$3cc9fdfd-1a83-4433-a594-db3c231be975ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ§messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Name three built in message field keys aside from `content`. \nContext: ### Content\n\nThe content of a message text or a list of dictionaries representing [multimodal data](/docs/concepts/multimodality) (e.g., images, audio, video). The exact format of the content can vary between different chat model providers.\n\nCurrently, most chat models support text as the primary content type, with some models also supporting multimodal data. However, support for multimodal data is still limited across most chat model providers.\n\nFor more information see:\n* [SystemMessage](#systemmessage) -- for content which should be passed to direct the conversation\n* [HumanMessage](#humanmessage) -- for content in the input from the user.\n* [AIMessage](#aimessage) -- for content in the response from the model.\n* [Multimodality](/docs/concepts/multimodality) -- for more information on multimodal content.\n\n### Other Message Data\n\nDepending on the chat model provider, messages can include other data such as:\n\n| Attribute            | Standardized/Raw | Description                                                                                                                                                                                                             |\n|----------------------|:-----------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `content`            | Raw              | Usually a string, but can be a list of content blocks. See [content](#content) for details.                                                                                                                             |\n\n## What is inside a message?\n\nA message typically consists of the following pieces of information:\n\n- **Role**: The role of the message (e.g., "user", "assistant").\n- **Content**: The content of the message (e.g., text, multimodal data).\n- Additional metadata: id, name, [token usage](/docs/concepts/tokens) and other model-specific metadata.\n\n### Role\n\nRoles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.\n\n# Messages\n\n:::info Prerequisites\n- [Chat Models](/docs/concepts/chat_models)\n:::\n\n## Overview\n\nMessages are the unit of communication in [chat models](/docs/concepts/chat_models). They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\n\nEach message has a **role** (e.g., "user", "assistant") and **content** (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.\n\nLangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\n\n## What is inside a message?\n\nA message typically consists of the following pieces of information:\n\n#### content\n\nThe **content** property of an `AIMessage` represents the response generated by the chat model.\n\nThe content is either:\n\n- **text** -- the norm for virtually all chat models.\n- A **list of dictionaries** -- Each dictionary represents a content block and is associated with a `type`.\n    * Used by Anthropic for surfacing agent thought process when doing [tool calling](/docs/concepts/tool_calling).\n    * Used by OpenAI for audio outputs. Please see [multi-modal content](/docs/concepts/multimodality) for more information.\n\n:::important\nThe **content** property is **not** standardized across different chat model providers, mostly because there are\nstill few examples to generalize from.\n:::\n\n### AIMessageChunk\n\nIt is common to [stream](/docs/concepts/streaming) responses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@a7995adb07b5a8aee8eed941bc1b248bc4b3baf5c12d3e78a1cc8758a33e288d¢tsËAÚô
µ\[¨focus_msÍ§payload‚¤role©assistant§contentÚ%Three built-in message field keys aside from `content` are `role`, `id`, and `name`. The `role` distinguishes between different types of messages, while `id` and `name` serve as additional metadata. These keys help in organizing and understanding the context of the messages in a conversation.ªfocus_kind¬exec_latency¨query_id¤Q097‰¢idÙ@64b2b94178bfe4b3bbd2293b666f464f967c0112dbbf0cd31eeb3a31b4cf8ce2¢tsËAÚô
µm¨trace_idÙ$bd82c65a-5d21-473c-af56-b6f56c324e1f­parent_run_idÙ$cc3e44e1-d449-4f47-9789-0a2e7ef9f6ea¦run_idÙ$bd82c65a-5d21-473c-af56-b6f56c324e1fªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ%Three built-in message field keys aside from `content` are `role`, `id`, and `name`. The `role` distinguishes between different types of messages, while `id` and `name` serve as additional metadata. These keys help in organizing and understanding the context of the messages in a conversation.¨query_id¤Q097ªfocus_kind¶chain_execution_result‰¢idÙ@dc34f39e38e6b006a4da4eff5959949a89c87352b9208f5bb69d7dad5a15d9fa¢tsËAÚô
µw’¨trace_idÙ$cc3e44e1-d449-4f47-9789-0a2e7ef9f6ea­parent_run_idÀ¦run_idÙ$cc3e44e1-d449-4f47-9789-0a2e7ef9f6eaªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ<Name three built in message field keys aside from `content`.§context•‚´page_content_summaryÙË### Content

The content of a message text or a list of dictionaries representing [multimodal data](/docs/concepts/multimodality) (e.g., images, audio, video). The exact format of the content can vary...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË| Attribute            | Standardized/Raw | Description                                                                                                                                                 ...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË## What is inside a message?

A message typically consists of the following pieces of information:

- **Role**: The role of the message (e.g., "user", "assistant").
- **Content**: The content of the m...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË# Messages

:::info Prerequisites
- [Chat Models](/docs/concepts/chat_models)
:::

## Overview

Messages are the unit of communication in [chat models](/docs/concepts/chat_models). They are used to re...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË#### content

The **content** property of an `AIMessage` represents the response generated by the chat model.

The content is either:

- **text** -- the norm for virtually all chat models.
- A **list ...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx¦answerÚ%Three built-in message field keys aside from `content` are `role`, `id`, and `name`. The `role` distinguishes between different types of messages, while `id` and `name` serve as additional metadata. These keys help in organizing and understanding the context of the messages in a conversation.¨query_id¤Q097ªfocus_kind¶chain_execution_result‹¢idÙ@1944f496476fc9e36e4a72e65cfeeb7cbdf5cac06f285826eabb6d35853be699¢tsËAÚô
µ€ª¨trace_id¤Q098­parent_run_idÀ¦run_idÙ$44d964fe-8b69-40c1-a445-8f651f32917dªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙUWhat is the first step when creating a custom Runnable for streaming transformations?¨query_id¤Q098§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q098ªfocus_kind¯chain_execution‹¢idÙ@875fcd0a474091cc3e33b1c5aedb1b8fc53c74ecf6efa48204daf886d273d105¢tsËAÚô
µ’p¨trace_id¤Q098­parent_run_idÙ$44d964fe-8b69-40c1-a445-8f651f32917d¦run_idÙ$911d7e10-a35c-44fb-8ae0-d780d8888c86ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙUWhat is the first step when creating a custom Runnable for streaming transformations?§context¦answer ¨query_id¤Q098¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q098®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:1e97116f-34f9-c0ae-53c3-58a4dab1e30bªfocus_kind¯chain_execution‰¢idÙ@6d62a20beec58f50f55c1e26cf01285378a2940586edf2c8887747b01679cfd0¢tsËAÚô
Æ™İ¨trace_idÙ$911d7e10-a35c-44fb-8ae0-d780d8888c86­parent_run_idÙ$44d964fe-8b69-40c1-a445-8f651f32917d¦run_idÙ$911d7e10-a35c-44fb-8ae0-d780d8888c86ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËPlease refer to the [Streaming Conceptual Guide](/docs/concepts/streaming) for more details on how to stream in LangChain.

## Input and output types

Every `Runnable` is characterized by an input and...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **In LCEL Pipelines:**
Streaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q098ªfocus_kind¶chain_execution_result‹¢idÙ@f1864e408bbf8ab045de60a7a3daeafba6c3dd132ed7973907a435532bd1a211¢tsËAÚô
Æ¢™¨trace_id¤Q098­parent_run_idÙ$44d964fe-8b69-40c1-a445-8f651f32917d¦run_idÙ$f20bbb8f-379a-4cd7-b98d-86b49e7dc760ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙUWhat is the first step when creating a custom Runnable for streaming transformations?§context•‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËPlease refer to the [Streaming Conceptual Guide](/docs/concepts/streaming) for more details on how to stream in LangChain.

## Input and output types

Every `Runnable` is characterized by an input and...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **In LCEL Pipelines:**
Streaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answer ¨query_id¤Q098¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q098®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:0422787e-090f-56b8-2c7d-63334aa2a6cbªfocus_kind¯chain_execution‹¢idÙ@7eac6e1a139284b4264a0d07bae8d81f6dfed69ee3bc65ba61d35750e76e3f04¢tsËAÚô
Æ§†¨trace_id¤Q098­parent_run_idÙ$f20bbb8f-379a-4cd7-b98d-86b49e7dc760¦run_idÙ$03346889-65aa-445d-a5a8-280a815f9adcªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙUWhat is the first step when creating a custom Runnable for streaming transformations?§contextÚœ## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)](/docs/concepts/lcel) to compose
multiple Runnables and you need to add custom processing logic in one of the steps.

There are two ways to create a custom Runnable from a function:

* `RunnableLambda`: Use this for simple transformations where streaming is not required.
* `RunnableGenerator`: use this for more complex transformations when streaming is needed.

See the [How to run custom functions](/docs/how_to/functions) guide for more information on how to use `RunnableLambda` and `RunnableGenerator`.

:::important
Users should not try to subclass Runnables to create a new custom Runnable. It is
much more complex and error-prone than simply using `RunnableLambda` or `RunnableGenerator`.
:::

## Configurable runnables

## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.
* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.
* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.
* Inspected: Schematic information about Runnable's input, output, and configuration can be accessed.
* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.

Please review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.

<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

Please refer to the [Streaming Conceptual Guide](/docs/concepts/streaming) for more details on how to stream in LangChain.

## Input and output types

Every `Runnable` is characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself.

Runnable methods that result in the execution of the Runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) work with these input and output types.

* invoke: Accepts an input and returns an output.
* batch: Accepts a list of inputs and returns a list of outputs.
* stream: Accepts an input and returns a generator that yields outputs.

The **input type** and **output type** vary by component:

- **In LCEL Pipelines:**
Streaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of the pipeline execute, you can stream which sub-runnable is currently running, providing real-time insight into the overall pipeline's progress.

Streaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.

### 3. Streaming custom data

# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](
https://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.

This guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q098®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:0422787e-090f-56b8-2c7d-63334aa2a6cb­checkpoint_nsÙ-generate:0422787e-090f-56b8-2c7d-63334aa2a6cb¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@018aa49e3082d686d163ab39acd7a79929da50c97add049bda4ee92bd6ea5590¢tsËAÚô
Æ±Ğ¨trace_idÙ$03346889-65aa-445d-a5a8-280a815f9adc­parent_run_idÙ$f20bbb8f-379a-4cd7-b98d-86b49e7dc760¦run_idÙ$03346889-65aa-445d-a5a8-280a815f9adcªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ’messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What is the first step when creating a custom Runnable for streaming transformations? \nContext: ## Creating a runnable from a function {#custom-runnables}\n\nYou may need to create a custom Runnable that runs arbitrary logic. This is especially\nuseful if using [LangChain Expression Language (LCEL)](/docs/concepts/lcel) to compose\nmultiple Runnables and you need to add custom processing logic in one of the steps.\n\nThere are two ways to create a custom Runnable from a function:\n\n* `RunnableLambda`: Use this for simple transformations where streaming is not required.\n* `RunnableGenerator`: use this for more complex transformations when streaming is needed.\n\nSee the [How to run custom functions](/docs/how_to/functions) guide for more information on how to use `RunnableLambda` and `RunnableGenerator`.\n\n:::important\nUsers should not try to subclass Runnables to create a new custom Runnable. It is\nmuch more complex and error-prone than simply using `RunnableLambda` or `RunnableGenerator`.\n:::\n\n## Configurable runnables\n\n## Overview of runnable interface\n\nThe Runnable way defines a standard interface that allows a Runnable component to be:\n\n* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.\n* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.\n* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.\n* Inspected: Schematic information about Runnable\'s input, output, and configuration can be accessed.\n* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.\n\nPlease review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.\n\n<a id="batch"></a>\n### Optimized parallel execution (batch)\n<span data-heading-keywords="batch"></span>\n\nPlease refer to the [Streaming Conceptual Guide](/docs/concepts/streaming) for more details on how to stream in LangChain.\n\n## Input and output types\n\nEvery `Runnable` is characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself.\n\nRunnable methods that result in the execution of the Runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) work with these input and output types.\n\n* invoke: Accepts an input and returns an output.\n* batch: Accepts a list of inputs and returns a list of outputs.\n* stream: Accepts an input and returns a generator that yields outputs.\n\nThe **input type** and **output type** vary by component:\n\n- **In LCEL Pipelines:**\nStreaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of the pipeline execute, you can stream which sub-runnable is currently running, providing real-time insight into the overall pipeline\'s progress.\n\nStreaming pipeline or workflow progress is essential in providing users with a clear picture of where the application is in the execution process.\n\n### 3. Streaming custom data\n\n# Runnable interface\n\nThe Runnable interface is the foundation for working with LangChain components, and it\'s implemented across many of them, such as [language models](/docs/concepts/chat_models), [output parsers](/docs/concepts/output_parsers), [retrievers](/docs/concepts/retrievers), [compiled LangGraph graphs](\nhttps://langchain-ai.github.io/langgraph/concepts/low_level/#compiling-your-graph) and more.\n\nThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@40134dc069841d37cef4eaa16d8b55b24610f482b21efa884431f117a2f0f21c¢tsËAÚô/n¨focus_msÍd§payload‚¤role©assistant§contentÚxThe first step when creating a custom Runnable for streaming transformations is to choose between `RunnableLambda` for simple transformations or `RunnableGenerator` for more complex transformations that require streaming. It's important to avoid subclassing Runnables, as it can be complex and error-prone. Instead, use these predefined options to implement your custom logic.ªfocus_kind¬exec_latency¨query_id¤Q098‰¢idÙ@d159f775443ae478c5a3413bd887e178e8565d7cec3a13232605c7ebf9e44a04¢tsËAÚô/‚Ã¨trace_idÙ$f20bbb8f-379a-4cd7-b98d-86b49e7dc760­parent_run_idÙ$44d964fe-8b69-40c1-a445-8f651f32917d¦run_idÙ$f20bbb8f-379a-4cd7-b98d-86b49e7dc760ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚxThe first step when creating a custom Runnable for streaming transformations is to choose between `RunnableLambda` for simple transformations or `RunnableGenerator` for more complex transformations that require streaming. It's important to avoid subclassing Runnables, as it can be complex and error-prone. Instead, use these predefined options to implement your custom logic.¨query_id¤Q098ªfocus_kind¶chain_execution_result‰¢idÙ@f9496573aeda3fb3120cd961376800e3260126a3062a787365b79ffe10bc50e0¢tsËAÚô/‰Ó¨trace_idÙ$44d964fe-8b69-40c1-a445-8f651f32917d­parent_run_idÀ¦run_idÙ$44d964fe-8b69-40c1-a445-8f651f32917dªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙUWhat is the first step when creating a custom Runnable for streaming transformations?§context•‚´page_content_summaryÙË## Creating a runnable from a function {#custom-runnables}

You may need to create a custom Runnable that runs arbitrary logic. This is especially
useful if using [LangChain Expression Language (LCEL)...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËPlease refer to the [Streaming Conceptual Guide](/docs/concepts/streaming) for more details on how to stream in LangChain.

## Input and output types

Every `Runnable` is characterized by an input and...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **In LCEL Pipelines:**
Streaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚxThe first step when creating a custom Runnable for streaming transformations is to choose between `RunnableLambda` for simple transformations or `RunnableGenerator` for more complex transformations that require streaming. It's important to avoid subclassing Runnables, as it can be complex and error-prone. Instead, use these predefined options to implement your custom logic.¨query_id¤Q098ªfocus_kind¶chain_execution_result‹¢idÙ@90594db9a9872fc708153f195f5f45a0a69295547b63dd43e3f11bf9257c1311¢tsËAÚô/¨trace_id¤Q099­parent_run_idÀ¦run_idÙ$3a4f75de-f277-47c8-95fd-fa5711fff65fªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙOHow does LangChain differentiate official vs community chat model integrations?¨query_id¤Q099§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q099ªfocus_kind¯chain_execution‹¢idÙ@b8da565e1747e10b67805018ad50451d423143bf3b8a51b1c37ef903c6f4c337¢tsËAÚô/–t¨trace_id¤Q099­parent_run_idÙ$3a4f75de-f277-47c8-95fd-fa5711fff65f¦run_idÙ$1d7b6fc1-7edb-4eac-bf2f-d169082ece38ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙOHow does LangChain differentiate official vs community chat model integrations?§context¦answer ¨query_id¤Q099¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q099®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:5429c84f-3119-4810-26f9-1ddba6041647ªfocus_kind¯chain_execution‰¢idÙ@ff851c4f7cb6ec11b6f25a5d5c70f23f99e68aa2a228f18e7bd874ac9a55106a¢tsËAÚôDÈÅ¨trace_idÙ$1d7b6fc1-7edb-4eac-bf2f-d169082ece38­parent_run_idÙ$3a4f75de-f277-47c8-95fd-fa5711fff65f¦run_idÙ$1d7b6fc1-7edb-4eac-bf2f-d169082ece38ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Integrations

LangChain has many chat model integrations that allow you to use a wide variety of models from different providers.

These integrations are one of two types:

1. **Official models**: ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Key features

This highlights functionality that is core to using LangChain.

- [How to: return structured data from a model](/docs/how_to/structured_output/)
- [How to: use a model to call tools](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËPlease review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.

:::note
Models that do **not** include the prefix "Chat" in their name or include "LLM" as a suff...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q099ªfocus_kind¶chain_execution_result‹¢idÙ@ed2504607b34f1fc44afd51fba1ee88729c8efaa507b06ab8608fb6d8483172d¢tsËAÚôDÑ|¨trace_id¤Q099­parent_run_idÙ$3a4f75de-f277-47c8-95fd-fa5711fff65f¦run_idÙ$f8486d01-548d-4a67-9ff0-44c8fe8a610dªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙOHow does LangChain differentiate official vs community chat model integrations?§context•‚´page_content_summaryÙË## Integrations

LangChain has many chat model integrations that allow you to use a wide variety of models from different providers.

These integrations are one of two types:

1. **Official models**: ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Key features

This highlights functionality that is core to using LangChain.

- [How to: return structured data from a model](/docs/how_to/structured_output/)
- [How to: use a model to call tools](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËPlease review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.

:::note
Models that do **not** include the prefix "Chat" in their name or include "LLM" as a suff...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answer ¨query_id¤Q099¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q099®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:bb336fdb-6d7d-72af-9c2a-8afda32657c3ªfocus_kind¯chain_execution‹¢idÙ@163c6a6b4bca3b1e4bcc3ddaa271eae1fd96e92da592d70f8af1cfc67b0ace91¢tsËAÚôDÕî¨trace_id¤Q099­parent_run_idÙ$f8486d01-548d-4a67-9ff0-44c8fe8a610d¦run_idÙ$01f21ff0-ab76-4c26-a262-2a40b7680526ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙOHow does LangChain differentiate official vs community chat model integrations?§contextÚP## Integrations

LangChain has many chat model integrations that allow you to use a wide variety of models from different providers.

These integrations are one of two types:

1. **Official models**: These are models that are officially supported by LangChain and/or model provider. You can find these models in the `langchain-<provider>` packages.
2. **Community models**: There are models that are mostly contributed and supported by the community. You can find these models in the `langchain-community` package.

LangChain chat models are named with a convention that prefixes "Chat" to their class names (e.g., `ChatOllama`, `ChatAnthropic`, `ChatOpenAI`, etc.).

Please review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.

## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.
This diversity can make it challenging for developers to switch between providers or combine components when building applications.
LangChain exposes a standard interface for key components, making it easy to switch between providers.

* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/integrations/chat/) for an up-to-date list of supported models.
* Use either LangChain's [messages](/docs/concepts/messages) format or OpenAI format.
* Standard [tool calling API](/docs/concepts/tool_calling): standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.
* Standard API for [structuring outputs](/docs/concepts/structured_outputs/#structured-output-method) via the `with_structured_output` method.
* Provides support for [async programming](/docs/concepts/async), [efficient batching](/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich streaming API](/docs/concepts/streaming).

## Key features

This highlights functionality that is core to using LangChain.

- [How to: return structured data from a model](/docs/how_to/structured_output/)
- [How to: use a model to call tools](/docs/how_to/tool_calling)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: debug your LLM apps](/docs/how_to/debugging/)

## Components

These are the core building blocks you can use when building applications.

### Chat models

[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.
See [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.

Please review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.

:::note
Models that do **not** include the prefix "Chat" in their name or include "LLM" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.
:::


## Interface

LangChain chat models implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface. Because `BaseChatModel` also implements the [Runnable Interface](/docs/concepts/runnables), chat models support a [standard streaming interface](/docs/concepts/streaming), [async programming](/docs/concepts/async), optimized [batching](/docs/concepts/runnables/#optimized-parallel-execution-batch), and more. Please see the [Runnable Interface](/docs/concepts/runnables) for more details.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q099®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:bb336fdb-6d7d-72af-9c2a-8afda32657c3­checkpoint_nsÙ-generate:bb336fdb-6d7d-72af-9c2a-8afda32657c3¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@deba760b22d6b9c7853d6b056b7a8d73228518f5001f1a801268b1ba34395e52¢tsËAÚôDÜ˜¨trace_idÙ$01f21ff0-ab76-4c26-a262-2a40b7680526­parent_run_idÙ$f8486d01-548d-4a67-9ff0-44c8fe8a610d¦run_idÙ$01f21ff0-ab76-4c26-a262-2a40b7680526ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ6messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: How does LangChain differentiate official vs community chat model integrations? \nContext: ## Integrations\n\nLangChain has many chat model integrations that allow you to use a wide variety of models from different providers.\n\nThese integrations are one of two types:\n\n1. **Official models**: These are models that are officially supported by LangChain and/or model provider. You can find these models in the `langchain-<provider>` packages.\n2. **Community models**: There are models that are mostly contributed and supported by the community. You can find these models in the `langchain-community` package.\n\nLangChain chat models are named with a convention that prefixes "Chat" to their class names (e.g., `ChatOllama`, `ChatAnthropic`, `ChatOpenAI`, etc.).\n\nPlease review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.\n\n## Features\n\nThere are several primary needs that LangChain aims to address:\n\n1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components](/docs/integrations/vectorstores/) for AI applications has resulted in a wide variety of different APIs that developers need to learn and use.\nThis diversity can make it challenging for developers to switch between providers or combine components when building applications.\nLangChain exposes a standard interface for key components, making it easy to switch between providers.\n\n* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/integrations/chat/) for an up-to-date list of supported models.\n* Use either LangChain\'s [messages](/docs/concepts/messages) format or OpenAI format.\n* Standard [tool calling API](/docs/concepts/tool_calling): standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.\n* Standard API for [structuring outputs](/docs/concepts/structured_outputs/#structured-output-method) via the `with_structured_output` method.\n* Provides support for [async programming](/docs/concepts/async), [efficient batching](/docs/concepts/runnables/#optimized-parallel-execution-batch), [a rich streaming API](/docs/concepts/streaming).\n\n## Key features\n\nThis highlights functionality that is core to using LangChain.\n\n- [How to: return structured data from a model](/docs/how_to/structured_output/)\n- [How to: use a model to call tools](/docs/how_to/tool_calling)\n- [How to: stream runnables](/docs/how_to/streaming)\n- [How to: debug your LLM apps](/docs/how_to/debugging/)\n\n## Components\n\nThese are the core building blocks you can use when building applications.\n\n### Chat models\n\n[Chat Models](/docs/concepts/chat_models) are newer forms of language models that take messages in and output a message.\nSee [supported integrations](/docs/integrations/chat/) for details on getting started with chat models from a specific provider.\n\nPlease review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.\n\n:::note\nModels that do **not** include the prefix "Chat" in their name or include "LLM" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.\n:::\n\n\n## Interface\n\nLangChain chat models implement the [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html) interface. Because `BaseChatModel` also implements the [Runnable Interface](/docs/concepts/runnables), chat models support a [standard streaming interface](/docs/concepts/streaming), [async programming](/docs/concepts/async), optimized [batching](/docs/concepts/runnables/#optimized-parallel-execution-batch), and more. Please see the [Runnable Interface](/docs/concepts/runnables) for more details. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@2d36f13a7b147691e55e23c13d3711a76727a99dd4859c60facc3978c8fe9816¢tsËAÚôÖ†¨focus_msÍİ§payload‚¤role©assistant§contentÚ®LangChain differentiates official and community chat model integrations based on their support and source. Official models are supported by LangChain and/or the model provider, found in the `langchain-<provider>` packages, while community models are contributed and supported by the community, located in the `langchain-community` package. This distinction helps users identify the level of support and reliability for each model.ªfocus_kind¬exec_latency¨query_id¤Q099‰¢idÙ@1e5e092d98729875c82a0f6bb328e9d86a9223ac892c89f21b81f6292189ddac¢tsËAÚôÖ,$¨trace_idÙ$f8486d01-548d-4a67-9ff0-44c8fe8a610d­parent_run_idÙ$3a4f75de-f277-47c8-95fd-fa5711fff65f¦run_idÙ$f8486d01-548d-4a67-9ff0-44c8fe8a610dªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ®LangChain differentiates official and community chat model integrations based on their support and source. Official models are supported by LangChain and/or the model provider, found in the `langchain-<provider>` packages, while community models are contributed and supported by the community, located in the `langchain-community` package. This distinction helps users identify the level of support and reliability for each model.¨query_id¤Q099ªfocus_kind¶chain_execution_result‰¢idÙ@ae4352d28b39a227a9b4d6fa1a08a61dcada62a99b69f18ab515801513d9b5f2¢tsËAÚôÖ1Ó¨trace_idÙ$3a4f75de-f277-47c8-95fd-fa5711fff65f­parent_run_idÀ¦run_idÙ$3a4f75de-f277-47c8-95fd-fa5711fff65fªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙOHow does LangChain differentiate official vs community chat model integrations?§context•‚´page_content_summaryÙË## Integrations

LangChain has many chat model integrations that allow you to use a wide variety of models from different providers.

These integrations are one of two types:

1. **Official models**: ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Key features

This highlights functionality that is core to using LangChain.

- [How to: return structured data from a model](/docs/how_to/structured_output/)
- [How to: use a model to call tools](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËPlease review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.

:::note
Models that do **not** include the prefix "Chat" in their name or include "LLM" as a suff...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚ®LangChain differentiates official and community chat model integrations based on their support and source. Official models are supported by LangChain and/or the model provider, found in the `langchain-<provider>` packages, while community models are contributed and supported by the community, located in the `langchain-community` package. This distinction helps users identify the level of support and reliability for each model.¨query_id¤Q099ªfocus_kind¶chain_execution_result‹¢idÙ@c60e498daa58d154d28dc22429de58a4f1e9a50f5f0bacc6bcb764b0acf6f7fb¢tsËAÚôÖ6“¨trace_id¤Q100­parent_run_idÀ¦run_idÙ$86311a58-c280-4a27-8443-c07f32a51db7ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙRWhich two sources explain creating example selectors based on semantic similarity?¨query_id¤Q100§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q100ªfocus_kind¯chain_execution‹¢idÙ@8e530b3d1debce0b52e097cbc65b9d810c519f6e206bf2cb5d5df78e2e9ce7a1¢tsËAÚôÖ>£¨trace_id¤Q100­parent_run_idÙ$86311a58-c280-4a27-8443-c07f32a51db7¦run_idÙ$04233a13-1826-43e0-9df3-af5cb18d1f8eªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙRWhich two sources explain creating example selectors based on semantic similarity?§context¦answer ¨query_id¤Q100¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q100®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:3723bff7-0e0c-2aef-eea8-333ef3b3d07aªfocus_kind¯chain_execution‰¢idÙ@ab1d6974188d60f8a0ae07c90deaf921ae54f4b7080c2d61b3f74ca6912f09c5¢tsËAÚôâ>û¨trace_idÙ$04233a13-1826-43e0-9df3-af5cb18d1f8e­parent_run_idÙ$86311a58-c280-4a27-8443-c07f32a51db7¦run_idÙ$04233a13-1826-43e0-9df3-af5cb18d1f8eªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)
:::

## Overview

One common prompting technique for...¨metadata¦sourceÙP/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/example_selectors.mdx‚´page_content_summaryÙËWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal ma...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË| Name                                                                                                              | When to use                                           | Description               ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the how-to guide for [Markdown splitting](/docs/how_to/markdown_header_metadata_splitter/).
* See the how-to guide for [Recursive JSON splitting](/docs/how_to/recursive...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx¨query_id¤Q100ªfocus_kind¶chain_execution_result‹¢idÙ@dec9cf849b008b0131df0779cb3134e10c1f2cb09aeb374921746ff1829c63d1¢tsËAÚôâF¨trace_id¤Q100­parent_run_idÙ$86311a58-c280-4a27-8443-c07f32a51db7¦run_idÙ$689c9294-b8ba-45aa-9e05-27231f1e795bªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙRWhich two sources explain creating example selectors based on semantic similarity?§context•‚´page_content_summaryÙË### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)
:::

## Overview

One common prompting technique for...¨metadata¦sourceÙP/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/example_selectors.mdx‚´page_content_summaryÙËWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal ma...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË| Name                                                                                                              | When to use                                           | Description               ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the how-to guide for [Markdown splitting](/docs/how_to/markdown_header_metadata_splitter/).
* See the how-to guide for [Recursive JSON splitting](/docs/how_to/recursive...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx¦answer ¨query_id¤Q100¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q100®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:3b679772-b9f2-4f15-0072-b55c15b45de4ªfocus_kind¯chain_execution‹¢idÙ@fd626b75135565bb030fe1c71ab333382b2676d3310b4f365ceaf8bb5d8d6c6b¢tsËAÚôâK0¨trace_id¤Q100­parent_run_idÙ$689c9294-b8ba-45aa-9e05-27231f1e795b¦run_idÙ$cd92d300-7375-4003-85ad-9e0d82058dbeªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙRWhich two sources explain creating example selectors based on semantic similarity?§contextÚK### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs/how_to/example_selectors)
- [How to: select examples by length](/docs/how_to/example_selectors_length_based)
- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)
- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)
- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)
- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)

### LLMs

What LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.

# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)
:::

## Overview

One common prompting technique for achieving better performance is to include examples as part of the prompt. This is known as [few-shot prompting](/docs/concepts/few_shot_prompting).

This gives the [language model](/docs/concepts/chat_models/) concrete examples of how it should behave.
Sometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.

**Example Selectors** are classes responsible for selecting and then formatting examples into prompts.

## Related resources

* [Example selector how-to guides](/docs/how_to/#example-selectors)

While algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.
As a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches. 
At the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.

| Name                                                                                                              | When to use                                           | Description                                                                                                                                  |
|-------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| [Hybrid search](/docs/integrations/retrievers/pinecone_hybrid_search/)                                            | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. [Paper](https://arxiv.org/abs/2210.11934). |

:::info[Further reading]

* See the how-to guide for [Markdown splitting](/docs/how_to/markdown_header_metadata_splitter/).
* See the how-to guide for [Recursive JSON splitting](/docs/how_to/recursive_json_splitter/).
* See the how-to guide for [Code splitting](/docs/how_to/code_splitter/).
* See the how-to guide for [HTML splitting](/docs/how_to/split_html/).

:::

### Semantic meaning based

Unlike the previous methods, semantic-based splitting actually considers the *content* of the text. 
While other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text's semantics.
There are several ways to implement this, but conceptually the approach is split text when there are significant changes in text *meaning*.
As an example, we can use a sliding window approach to generate embeddings, and compare the embeddings to find significant differences:¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q100®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:3b679772-b9f2-4f15-0072-b55c15b45de4­checkpoint_nsÙ-generate:3b679772-b9f2-4f15-0072-b55c15b45de4¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@4772a1c3db1c0dcbf6ad6f1155e7b4cc7b6b7865530d147d8f62b9f03c4c8cb2¢tsËAÚôâQ¨trace_idÙ$cd92d300-7375-4003-85ad-9e0d82058dbe­parent_run_idÙ$689c9294-b8ba-45aa-9e05-27231f1e795b¦run_idÙ$cd92d300-7375-4003-85ad-9e0d82058dbeªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ6messages=[HumanMessage(content="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: Which two sources explain creating example selectors based on semantic similarity? \nContext: ### Example selectors\n\n[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.\n\n- [How to: use example selectors](/docs/how_to/example_selectors)\n- [How to: select examples by length](/docs/how_to/example_selectors_length_based)\n- [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)\n- [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)\n- [How to: select examples by maximal marginal relevance](/docs/how_to/example_selectors_mmr)\n- [How to: select examples from LangSmith few-shot datasets](/docs/how_to/example_selectors_langsmith/)\n\n### LLMs\n\nWhat LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.\n\n# Example selectors\n\n:::note Prerequisites\n\n- [Chat models](/docs/concepts/chat_models/)\n- [Few-shot prompting](/docs/concepts/few_shot_prompting/)\n:::\n\n## Overview\n\nOne common prompting technique for achieving better performance is to include examples as part of the prompt. This is known as [few-shot prompting](/docs/concepts/few_shot_prompting).\n\nThis gives the [language model](/docs/concepts/chat_models/) concrete examples of how it should behave.\nSometimes these examples are hardcoded into the prompt, but for more advanced situations it may be nice to dynamically select them.\n\n**Example Selectors** are classes responsible for selecting and then formatting examples into prompts.\n\n## Related resources\n\n* [Example selector how-to guides](/docs/how_to/#example-selectors)\n\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\nFor example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\nAs a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches. \nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.\n\n| Name                                                                                                              | When to use                                           | Description                                                                                                                                  |\n|-------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|\n| [Hybrid search](/docs/integrations/retrievers/pinecone_hybrid_search/)                                            | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. [Paper](https://arxiv.org/abs/2210.11934). |\n\n:::info[Further reading]\n\n* See the how-to guide for [Markdown splitting](/docs/how_to/markdown_header_metadata_splitter/).\n* See the how-to guide for [Recursive JSON splitting](/docs/how_to/recursive_json_splitter/).\n* See the how-to guide for [Code splitting](/docs/how_to/code_splitter/).\n* See the how-to guide for [HTML splitting](/docs/how_to/split_html/).\n\n:::\n\n### Semantic meaning based\n\nUnlike the previous methods, semantic-based splitting actually considers the *content* of the text. \nWhile other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text's semantics.\nThere are several ways to implement this, but conceptually the approach is split text when there are significant changes in text *meaning*.\nAs an example, we can use a sliding window approach to generate embeddings, and compare the embeddings to find significant differences: \nAnswer:", additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@40f61825f579f0d8678a60539ad49bc571124d8e9efdbd5891aba4f88c27c0d9¢tsËAÚô2åê¨focus_msÍê§payload‚¤role©assistant§contentÚThe two sources that explain creating example selectors based on semantic similarity are "[How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)" and "[How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)."ªfocus_kind¬exec_latency¨query_id¤Q100‰¢idÙ@c93fdb56a1354d7a71c654ed0ac95295744b432cc1489d80414415c2aa6c4c6d¢tsËAÚô2ø²¨trace_idÙ$689c9294-b8ba-45aa-9e05-27231f1e795b­parent_run_idÙ$86311a58-c280-4a27-8443-c07f32a51db7¦run_idÙ$689c9294-b8ba-45aa-9e05-27231f1e795bªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚThe two sources that explain creating example selectors based on semantic similarity are "[How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)" and "[How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)."¨query_id¤Q100ªfocus_kind¶chain_execution_result‰¢idÙ@76bca78e9a3cc9e17418e201d129c41f99c2afbc9a5dc08cc20f859f1a71c929¢tsËAÚô3É¨trace_idÙ$86311a58-c280-4a27-8443-c07f32a51db7­parent_run_idÀ¦run_idÙ$86311a58-c280-4a27-8443-c07f32a51db7ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙRWhich two sources explain creating example selectors based on semantic similarity?§context•‚´page_content_summaryÙË### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)
:::

## Overview

One common prompting technique for...¨metadata¦sourceÙP/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/example_selectors.mdx‚´page_content_summaryÙËWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal ma...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË| Name                                                                                                              | When to use                                           | Description               ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the how-to guide for [Markdown splitting](/docs/how_to/markdown_header_metadata_splitter/).
* See the how-to guide for [Recursive JSON splitting](/docs/how_to/recursive...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx¦answerÚThe two sources that explain creating example selectors based on semantic similarity are "[How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)" and "[How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)."¨query_id¤Q100ªfocus_kind¶chain_execution_result‹¢idÙ@01878dc2e7d4d0e711815fe6287a266ac63904de337d986c52681540071ff588¢tsËAÚô3I¨trace_id¤Q101­parent_run_idÀ¦run_idÙ$0f26841a-820c-4367-8a35-6c8c4c94100dªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙQWhat is the impact of not propagating `RunnableConfig` in Python 3.10 async code?¨query_id¤Q101§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q101ªfocus_kind¯chain_execution‹¢idÙ@10df5e5ff20a0b6b18ce64a541b8296a720485ac5b74c138a87a6ee395d5894e¢tsËAÚô3D¨trace_id¤Q101­parent_run_idÙ$0f26841a-820c-4367-8a35-6c8c4c94100d¦run_idÙ$43281248-fb59-4c67-926b-da44a7e1f556ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙQWhat is the impact of not propagating `RunnableConfig` in Python 3.10 async code?§context¦answer ¨query_id¤Q101¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q101®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:980947fd-3f78-8b25-16da-e601c9cd997cªfocus_kind¯chain_execution‰¢idÙ@2b7bd462982caf558f04e3e98c9210e91adc641a101c8b7a5f1f238fd558deb9¢tsËAÚôFÄ“¨trace_idÙ$43281248-fb59-4c67-926b-da44a7e1f556­parent_run_idÙ$0f26841a-820c-4367-8a35-6c8c4c94100d¦run_idÙ$43281248-fb59-4c67-926b-da44a7e1f556ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËIn Python 3.9 and 3.10, if you are using **async code**, you need to manually pass the `RunnableConfig` through to the `Runnable` when invoking it. 

This is due to a limitation in [asyncio's tasks](h...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËIf you're creating a custom chain or runnable, you need to remember to propagate request time
callbacks to any child objects.

:::important Async in Python&lt;=3.10

Any `RunnableLambda`, a `RunnableG...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/callbacks.mdx‚´page_content_summaryÙË## RunnableConfig

Any of the methods that are used to execute the runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) accept a second argument called
`RunnableConfig` ([API Reference](http...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË1. Declaratively using [LangChain Expression Language (LCEL)](/docs/concepts/lcel):

    ```python
    chain = prompt | chat_model | output_parser
    ```

2. Using a [custom Runnable](#custom-runnabl...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËPassing `config` to the `invoke` method is done like so:

```python
some_runnable.invoke(
   some_input, 
   config={
      'run_name': 'my_run', 
      'tags': ['tag1', 'tag2'], 
      'metadata': {'...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q101ªfocus_kind¶chain_execution_result‹¢idÙ@08bde74c2407ffb5e3290ca299c12232c5b7fc30bbae8b27bcd84ae157fa0abd¢tsËAÚôFÌî¨trace_id¤Q101­parent_run_idÙ$0f26841a-820c-4367-8a35-6c8c4c94100d¦run_idÙ$c1c7e727-ea4d-4dd3-a10d-3eebd0e94890ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙQWhat is the impact of not propagating `RunnableConfig` in Python 3.10 async code?§context•‚´page_content_summaryÙËIn Python 3.9 and 3.10, if you are using **async code**, you need to manually pass the `RunnableConfig` through to the `Runnable` when invoking it. 

This is due to a limitation in [asyncio's tasks](h...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËIf you're creating a custom chain or runnable, you need to remember to propagate request time
callbacks to any child objects.

:::important Async in Python&lt;=3.10

Any `RunnableLambda`, a `RunnableG...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/callbacks.mdx‚´page_content_summaryÙË## RunnableConfig

Any of the methods that are used to execute the runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) accept a second argument called
`RunnableConfig` ([API Reference](http...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË1. Declaratively using [LangChain Expression Language (LCEL)](/docs/concepts/lcel):

    ```python
    chain = prompt | chat_model | output_parser
    ```

2. Using a [custom Runnable](#custom-runnabl...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËPassing `config` to the `invoke` method is done like so:

```python
some_runnable.invoke(
   some_input, 
   config={
      'run_name': 'my_run', 
      'tags': ['tag1', 'tag2'], 
      'metadata': {'...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answer ¨query_id¤Q101¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q101®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:b1193e21-55f9-6e35-4d3c-2539dde17f95ªfocus_kind¯chain_execution‹¢idÙ@61ab500f8cf5d8e4fbba70afc0fed1517fec730679e977adf0f0a4d8b09d91d8¢tsËAÚôFÑò¨trace_id¤Q101­parent_run_idÙ$c1c7e727-ea4d-4dd3-a10d-3eebd0e94890¦run_idÙ$3113a40c-15cf-4b15-b27c-039024e4d939ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙQWhat is the impact of not propagating `RunnableConfig` in Python 3.10 async code?§contextÚöIn Python 3.9 and 3.10, if you are using **async code**, you need to manually pass the `RunnableConfig` through to the `Runnable` when invoking it. 

This is due to a limitation in [asyncio's tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task)  in Python 3.9 and 3.10 which did
not accept a `context` argument.

Propagating the `RunnableConfig` manually is done like so:

```python
async def foo(input, config): # <-- Note the config argument
    return await bar_runnable.ainvoke(input, config=config)
    
foo_runnable = RunnableLambda(foo)
```

:::caution
When using Python 3.10 or lower and writing async code, `RunnableConfig` cannot be propagated
automatically, and you will need to do it manually! This is a common pitfall when
attempting to stream data using `astream_events` and `astream_log` as these methods
rely on proper propagation of [callbacks](/docs/concepts/callbacks) defined inside of `RunnableConfig`.
:::

If you're creating a custom chain or runnable, you need to remember to propagate request time
callbacks to any child objects.

:::important Async in Python&lt;=3.10

Any `RunnableLambda`, a `RunnableGenerator`, or `Tool` that invokes other runnables
and is running `async` in python&lt;=3.10, will have to propagate callbacks to child
objects manually. This is because LangChain cannot automatically propagate
callbacks to child objects in this case.

This is a common reason why you may fail to see events being emitted from custom
runnables or tools.
:::

For specifics on how to use callbacks, see the [relevant how-to guides here](/docs/how_to/#callbacks).

## RunnableConfig

Any of the methods that are used to execute the runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) accept a second argument called
`RunnableConfig` ([API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html#RunnableConfig)). This argument is a dictionary that contains configuration for the Runnable that will be used
at run time during the execution of the runnable.

A `RunnableConfig` can have any of the following properties defined:

1. Declaratively using [LangChain Expression Language (LCEL)](/docs/concepts/lcel):

    ```python
    chain = prompt | chat_model | output_parser
    ```

2. Using a [custom Runnable](#custom-runnables)  (e.g., `RunnableLambda`) or using the `@tool` decorator:

    ```python
    def foo(input):
        # Note that .invoke() is used directly here
        return bar_runnable.invoke(input)
    foo_runnable = RunnableLambda(foo)
    ```

LangChain will try to propagate `RunnableConfig` automatically for both of the patterns. 

For handling the second pattern, LangChain relies on Python's [contextvars](https://docs.python.org/3/library/contextvars.html).

In Python 3.11 and above, this works out of the box, and you do not need to do anything special to propagate the `RunnableConfig` to the sub-calls.

In Python 3.9 and 3.10, if you are using **async code**, you need to manually pass the `RunnableConfig` through to the `Runnable` when invoking it.

Passing `config` to the `invoke` method is done like so:

```python
some_runnable.invoke(
   some_input, 
   config={
      'run_name': 'my_run', 
      'tags': ['tag1', 'tag2'], 
      'metadata': {'key': 'value'}
      
   }
)
```

### Propagation of RunnableConfig

Many `Runnables` are composed of other Runnables, and it is important that the `RunnableConfig` is propagated to all sub-calls made by the Runnable. This allows providing run time configuration values to the parent Runnable that are inherited by all sub-calls.

If this were not the case, it would be impossible to set and propagate [callbacks](/docs/concepts/callbacks) or other configuration values like `tags` and `metadata` which
are expected to be inherited by all sub-calls.

There are two main patterns by which new `Runnables` are created:

1. Declaratively using [LangChain Expression Language (LCEL)](/docs/concepts/lcel):

    ```python
    chain = prompt | chat_model | output_parser
    ```¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q101®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:b1193e21-55f9-6e35-4d3c-2539dde17f95­checkpoint_nsÙ-generate:b1193e21-55f9-6e35-4d3c-2539dde17f95¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@7771c3b3e25ca9daa74f07569542b4ba04120164e0d0ea1d197db4c85781457f¢tsËAÚôFØZ¨trace_idÙ$3113a40c-15cf-4b15-b27c-039024e4d939­parent_run_idÙ$c1c7e727-ea4d-4dd3-a10d-3eebd0e94890¦run_idÙ$3113a40c-15cf-4b15-b27c-039024e4d939ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚmessages=[HumanMessage(content="You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\nQuestion: What is the impact of not propagating `RunnableConfig` in Python 3.10 async code? \nContext: In Python 3.9 and 3.10, if you are using **async code**, you need to manually pass the `RunnableConfig` through to the `Runnable` when invoking it. \n\nThis is due to a limitation in [asyncio's tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create_task)  in Python 3.9 and 3.10 which did\nnot accept a `context` argument.\n\nPropagating the `RunnableConfig` manually is done like so:\n\n```python\nasync def foo(input, config): # <-- Note the config argument\n    return await bar_runnable.ainvoke(input, config=config)\n    \nfoo_runnable = RunnableLambda(foo)\n```\n\n:::caution\nWhen using Python 3.10 or lower and writing async code, `RunnableConfig` cannot be propagated\nautomatically, and you will need to do it manually! This is a common pitfall when\nattempting to stream data using `astream_events` and `astream_log` as these methods\nrely on proper propagation of [callbacks](/docs/concepts/callbacks) defined inside of `RunnableConfig`.\n:::\n\nIf you're creating a custom chain or runnable, you need to remember to propagate request time\ncallbacks to any child objects.\n\n:::important Async in Python&lt;=3.10\n\nAny `RunnableLambda`, a `RunnableGenerator`, or `Tool` that invokes other runnables\nand is running `async` in python&lt;=3.10, will have to propagate callbacks to child\nobjects manually. This is because LangChain cannot automatically propagate\ncallbacks to child objects in this case.\n\nThis is a common reason why you may fail to see events being emitted from custom\nrunnables or tools.\n:::\n\nFor specifics on how to use callbacks, see the [relevant how-to guides here](/docs/how_to/#callbacks).\n\n## RunnableConfig\n\nAny of the methods that are used to execute the runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) accept a second argument called\n`RunnableConfig` ([API Reference](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.config.RunnableConfig.html#RunnableConfig)). This argument is a dictionary that contains configuration for the Runnable that will be used\nat run time during the execution of the runnable.\n\nA `RunnableConfig` can have any of the following properties defined:\n\n1. Declaratively using [LangChain Expression Language (LCEL)](/docs/concepts/lcel):\n\n    ```python\n    chain = prompt | chat_model | output_parser\n    ```\n\n2. Using a [custom Runnable](#custom-runnables)  (e.g., `RunnableLambda`) or using the `@tool` decorator:\n\n    ```python\n    def foo(input):\n        # Note that .invoke() is used directly here\n        return bar_runnable.invoke(input)\n    foo_runnable = RunnableLambda(foo)\n    ```\n\nLangChain will try to propagate `RunnableConfig` automatically for both of the patterns. \n\nFor handling the second pattern, LangChain relies on Python's [contextvars](https://docs.python.org/3/library/contextvars.html).\n\nIn Python 3.11 and above, this works out of the box, and you do not need to do anything special to propagate the `RunnableConfig` to the sub-calls.\n\nIn Python 3.9 and 3.10, if you are using **async code**, you need to manually pass the `RunnableConfig` through to the `Runnable` when invoking it.\n\nPassing `config` to the `invoke` method is done like so:\n\n```python\nsome_runnable.invoke(\n   some_input, \n   config={\n      'run_name': 'my_run', \n      'tags': ['tag1', 'tag2'], \n      'metadata': {'key': 'value'}\n      \n   }\n)\n```\n\n### Propagation of RunnableConfig\n\nMany `Runnables` are composed of other Runnables, and it is important that the `RunnableConfig` is propagated to all sub-calls made by the Runnable. This allows providing run time configuration values to the parent Runnable that are inherited by all sub-calls.\n\nIf this were not the case, it would be impossible to set and propagate [callbacks](/docs/concepts/callbacks) or other configuration values like `tags` and `metadata` which\nare expected to be inherited by all sub-calls.\n\nThere are two main patterns by which new `Runnables` are created:\n\n1. Declaratively using [LangChain Expression Language (LCEL)](/docs/concepts/lcel):\n\n    ```python\n    chain = prompt | chat_model | output_parser\n    ``` \nAnswer:", additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@f498c3b7b4457891b076066b4db4c4f5d882d5b272ce298d7833cdd12b765c09¢tsËAÚôíG_¨focus_msÍ
(§payload‚¤role©assistant§contentÚÃNot propagating `RunnableConfig` in Python 3.10 async code can lead to issues where callbacks and configuration values, such as tags and metadata, are not inherited by sub-calls. This manual propagation is necessary because `asyncio` tasks in Python 3.9 and 3.10 do not accept a `context` argument, making automatic propagation impossible. As a result, you may fail to see events emitted from custom runnables or tools, which can hinder functionality.ªfocus_kind¬exec_latency¨query_id¤Q101‰¢idÙ@ce5811db77dfef127a0bf3491aaa5e66db546c69af58a3adf557f31397badf32¢tsËAÚôí[’¨trace_idÙ$c1c7e727-ea4d-4dd3-a10d-3eebd0e94890­parent_run_idÙ$0f26841a-820c-4367-8a35-6c8c4c94100d¦run_idÙ$c1c7e727-ea4d-4dd3-a10d-3eebd0e94890ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚÃNot propagating `RunnableConfig` in Python 3.10 async code can lead to issues where callbacks and configuration values, such as tags and metadata, are not inherited by sub-calls. This manual propagation is necessary because `asyncio` tasks in Python 3.9 and 3.10 do not accept a `context` argument, making automatic propagation impossible. As a result, you may fail to see events emitted from custom runnables or tools, which can hinder functionality.¨query_id¤Q101ªfocus_kind¶chain_execution_result‰¢idÙ@e0302a003381b9a4843b1abfd7ec00324e289795d89ca063849b752a6999279a¢tsËAÚôíf<¨trace_idÙ$0f26841a-820c-4367-8a35-6c8c4c94100d­parent_run_idÀ¦run_idÙ$0f26841a-820c-4367-8a35-6c8c4c94100dªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙQWhat is the impact of not propagating `RunnableConfig` in Python 3.10 async code?§context•‚´page_content_summaryÙËIn Python 3.9 and 3.10, if you are using **async code**, you need to manually pass the `RunnableConfig` through to the `Runnable` when invoking it. 

This is due to a limitation in [asyncio's tasks](h...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËIf you're creating a custom chain or runnable, you need to remember to propagate request time
callbacks to any child objects.

:::important Async in Python&lt;=3.10

Any `RunnableLambda`, a `RunnableG...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/callbacks.mdx‚´page_content_summaryÙË## RunnableConfig

Any of the methods that are used to execute the runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) accept a second argument called
`RunnableConfig` ([API Reference](http...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË1. Declaratively using [LangChain Expression Language (LCEL)](/docs/concepts/lcel):

    ```python
    chain = prompt | chat_model | output_parser
    ```

2. Using a [custom Runnable](#custom-runnabl...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËPassing `config` to the `invoke` method is done like so:

```python
some_runnable.invoke(
   some_input, 
   config={
      'run_name': 'my_run', 
      'tags': ['tag1', 'tag2'], 
      'metadata': {'...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚÃNot propagating `RunnableConfig` in Python 3.10 async code can lead to issues where callbacks and configuration values, such as tags and metadata, are not inherited by sub-calls. This manual propagation is necessary because `asyncio` tasks in Python 3.9 and 3.10 do not accept a `context` argument, making automatic propagation impossible. As a result, you may fail to see events emitted from custom runnables or tools, which can hinder functionality.¨query_id¤Q101ªfocus_kind¶chain_execution_result‹¢idÙ@67cecf9235cfb4fa2c9cbc452c169c9336c7d4d2c3291135d1d2fdc8bbdc53f1¢tsËAÚôívK¨trace_id¤Q102­parent_run_idÀ¦run_idÙ$5a59da49-3553-43f4-8038-8157a3ca5b16ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙTDescribe the `@tool` option to hide a parameter while still injecting it at runtime.¨query_id¤Q102§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q102ªfocus_kind¯chain_execution‹¢idÙ@8d448857c5de4e33c3842089c90ead58d83765a4f9c8280fa8ee573d363f1728¢tsËAÚôí†w¨trace_id¤Q102­parent_run_idÙ$5a59da49-3553-43f4-8038-8157a3ca5b16¦run_idÙ$c1e65715-b1b4-40d0-a2aa-4939395f46e4ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙTDescribe the `@tool` option to hide a parameter while still injecting it at runtime.§context¦answer ¨query_id¤Q102¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q102®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:c0b25f8e-2f8f-0de4-9504-0cc2d7e4d0f0ªfocus_kind¯chain_execution‰¢idÙ@2796ff9fe727ae49f8c13dc0d045981fe3252c4402cee92fc0969b5ec905fe52¢tsËAÚôø*¨trace_idÙ$c1e65715-b1b4-40d0-a2aa-4939395f46e4­parent_run_idÙ$5a59da49-3553-43f4-8038-8157a3ca5b16¦run_idÙ$c1e65715-b1b4-40d0-a2aa-4939395f46e4ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string liter...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËThe key attributes that correspond to the tool's **schema**:

- **name**: The name of the tool.
- **description**: A description of what the tool does.
- **args**: Property that returns the JSON schem...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË@tool
async def some_func(..., config: RunnableConfig) -> ...:
    """Tool that does something."""
    # do something with config
    ...

await some_func.ainvoke(..., config={"configurable": {"value"...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).
...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx¨query_id¤Q102ªfocus_kind¶chain_execution_result‹¢idÙ@2a45811edb0ac745b92180cd9810a369bf7cfe377636aa67321a25de82dd8a07¢tsËAÚôø˜·¨trace_id¤Q102­parent_run_idÙ$5a59da49-3553-43f4-8038-8157a3ca5b16¦run_idÙ$61bd46db-4fcb-4b19-88db-cd2c657dced4ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙTDescribe the `@tool` option to hide a parameter while still injecting it at runtime.§context•‚´page_content_summaryÙËYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string liter...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËThe key attributes that correspond to the tool's **schema**:

- **name**: The name of the tool.
- **description**: A description of what the tool does.
- **args**: Property that returns the JSON schem...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË@tool
async def some_func(..., config: RunnableConfig) -> ...:
    """Tool that does something."""
    # do something with config
    ...

await some_func.ainvoke(..., config={"configurable": {"value"...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).
...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx¦answer ¨query_id¤Q102¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q102®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:86765a4c-9bb9-fe52-5cfe-1835332ac391ªfocus_kind¯chain_execution‹¢idÙ@f48dddb3bd57d845d46fb9111348f8405e7b7344616362544f7e0d97821da78a¢tsËAÚôøŒ¨trace_id¤Q102­parent_run_idÙ$61bd46db-4fcb-4b19-88db-cd2c657dced4¦run_idÙ$f02f1d72-6269-45b9-8059-d9030f580e88ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙTDescribe the `@tool` option to hide a parameter while still injecting it at runtime.§contextÚ¶You can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string literal"]** -- Adds a description to the argument that will be exposed in the tool's schema.

### InjectedToolArg

There are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the `InjectedToolArg` annotation, which allows certain parameters to be hidden from the tool's schema.

For example, if a tool requires a `user_id` to be injected dynamically at runtime, it can be structured in this way:

```python
from langchain_core.tools import tool, InjectedToolArg

@tool
def user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:
    """Tool that processes input data."""
    return f"User {user_id} processed {input_data}"
```

See [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's function signature to configure the run time behavior of the tool.

The following type annotations will end up **removing** the argument from the tool's schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.

- **InjectedToolArg**: Value should be injected manually at runtime using `.invoke` or `.ainvoke`.
- **RunnableConfig**: Pass in the RunnableConfig object to the tool.
- **InjectedState**: Pass in the overall state of the LangGraph graph to the tool.
- **InjectedStore**: Pass in the LangGraph store object to the tool.

You can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

The key attributes that correspond to the tool's **schema**:

- **name**: The name of the tool.
- **description**: A description of what the tool does.
- **args**: Property that returns the JSON schema for the tool's arguments.

The key methods to execute the function associated with the **tool**:

- **invoke**: Invokes the tool with the given arguments.
- **ainvoke**: Invokes the tool with the given arguments, asynchronously. Used for [async programming with Langchain](/docs/concepts/async).

## Create tools using the `@tool` decorator

@tool
async def some_func(..., config: RunnableConfig) -> ...:
    """Tool that does something."""
    # do something with config
    ...

await some_func.ainvoke(..., config={"configurable": {"value": "some_value"}})
```

The `config` will not be part of the tool's schema and will be injected at runtime with appropriate values.

:::note
You may need to access the `config` object to manually propagate it to subclass. This happens if you're working with python 3.9 / 3.10 in an [async](/docs/concepts/async) environment and need to manually propagate the `config` object to sub-calls.

Please read [Propagation RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig) for more details to learn how to propagate the `RunnableConfig` down the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).
:::

### InjectedState

## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).

Please see the [API reference for @tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) for more details and review the [how to create custom tools](/docs/how_to/custom_tools/) guide for examples.

## Tool artifacts¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q102®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:86765a4c-9bb9-fe52-5cfe-1835332ac391­checkpoint_nsÙ-generate:86765a4c-9bb9-fe52-5cfe-1835332ac391¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@bd417a819615737e83caa6899f485626372e394caad0a53498d175005af780ac¢tsËAÚôø£Î¨trace_idÙ$f02f1d72-6269-45b9-8059-d9030f580e88­parent_run_idÙ$61bd46db-4fcb-4b19-88db-cd2c657dced4¦run_idÙ$f02f1d72-6269-45b9-8059-d9030f580e88ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ¾messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Describe the `@tool` option to hide a parameter while still injecting it at runtime. \nContext: You can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool\'s schema.\n\n- **Annotated[..., "string literal"]** -- Adds a description to the argument that will be exposed in the tool\'s schema.\n\n### InjectedToolArg\n\nThere are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the `InjectedToolArg` annotation, which allows certain parameters to be hidden from the tool\'s schema.\n\nFor example, if a tool requires a `user_id` to be injected dynamically at runtime, it can be structured in this way:\n\n```python\nfrom langchain_core.tools import tool, InjectedToolArg\n\n@tool\ndef user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:\n    """Tool that processes input data."""\n    return f"User {user_id} processed {input_data}"\n```\n\nSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.\n\n## Special type annotations\n\nThere are a number of special type annotations that can be used in the tool\'s function signature to configure the run time behavior of the tool.\n\nThe following type annotations will end up **removing** the argument from the tool\'s schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.\n\n- **InjectedToolArg**: Value should be injected manually at runtime using `.invoke` or `.ainvoke`.\n- **RunnableConfig**: Pass in the RunnableConfig object to the tool.\n- **InjectedState**: Pass in the overall state of the LangGraph graph to the tool.\n- **InjectedStore**: Pass in the LangGraph store object to the tool.\n\nYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool\'s schema.\n\nThe key attributes that correspond to the tool\'s **schema**:\n\n- **name**: The name of the tool.\n- **description**: A description of what the tool does.\n- **args**: Property that returns the JSON schema for the tool\'s arguments.\n\nThe key methods to execute the function associated with the **tool**:\n\n- **invoke**: Invokes the tool with the given arguments.\n- **ainvoke**: Invokes the tool with the given arguments, asynchronously. Used for [async programming with Langchain](/docs/concepts/async).\n\n## Create tools using the `@tool` decorator\n\n@tool\nasync def some_func(..., config: RunnableConfig) -> ...:\n    """Tool that does something."""\n    # do something with config\n    ...\n\nawait some_func.ainvoke(..., config={"configurable": {"value": "some_value"}})\n```\n\nThe `config` will not be part of the tool\'s schema and will be injected at runtime with appropriate values.\n\n:::note\nYou may need to access the `config` object to manually propagate it to subclass. This happens if you\'re working with python 3.9 / 3.10 in an [async](/docs/concepts/async) environment and need to manually propagate the `config` object to sub-calls.\n\nPlease read [Propagation RunnableConfig](/docs/concepts/runnables/#propagation-of-runnableconfig) for more details to learn how to propagate the `RunnableConfig` down the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).\n:::\n\n### InjectedState\n\n## Configuring the schema\n\nThe `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description\nor parse the function\'s doc-string to infer the schema).\n\nPlease see the [API reference for @tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) for more details and review the [how to create custom tools](/docs/how_to/custom_tools/) guide for examples.\n\n## Tool artifacts \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@f9890ef107e4b5cd51537b8c087481f54e949248bb3393ad323dec324d9ffe40¢tsËAÚôsß¨focus_msÍy§payload‚¤role©assistant§contentÚ˜The `@tool` option allows the use of the `InjectedToolArg` annotation to hide certain parameters from the tool's schema while still injecting them at runtime. This is useful for arguments that should not be exposed to the model, such as a `user_id` that is dynamically provided. By using `InjectedToolArg`, these parameters can be passed manually during invocation without being visible in the tool's schema.ªfocus_kind¬exec_latency¨query_id¤Q102‰¢idÙ@af1f5d61c14e2519a92ac65b47bbec0f80c0a2ce494288380f5e18f0e8688936¢tsËAÚôs$J¨trace_idÙ$61bd46db-4fcb-4b19-88db-cd2c657dced4­parent_run_idÙ$5a59da49-3553-43f4-8038-8157a3ca5b16¦run_idÙ$61bd46db-4fcb-4b19-88db-cd2c657dced4ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ˜The `@tool` option allows the use of the `InjectedToolArg` annotation to hide certain parameters from the tool's schema while still injecting them at runtime. This is useful for arguments that should not be exposed to the model, such as a `user_id` that is dynamically provided. By using `InjectedToolArg`, these parameters can be passed manually during invocation without being visible in the tool's schema.¨query_id¤Q102ªfocus_kind¶chain_execution_result‰¢idÙ@3f8261c5d418520f371f1573b686ee76b6b5a2beba39544eb5e1adb8f43beb8a¢tsËAÚôs-¨trace_idÙ$5a59da49-3553-43f4-8038-8157a3ca5b16­parent_run_idÀ¦run_idÙ$5a59da49-3553-43f4-8038-8157a3ca5b16ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙTDescribe the `@tool` option to hide a parameter while still injecting it at runtime.§context•‚´page_content_summaryÙËYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string liter...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËThe key attributes that correspond to the tool's **schema**:

- **name**: The name of the tool.
- **description**: A description of what the tool does.
- **args**: Property that returns the JSON schem...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË@tool
async def some_func(..., config: RunnableConfig) -> ...:
    """Tool that does something."""
    # do something with config
    ...

await some_func.ainvoke(..., config={"configurable": {"value"...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).
...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx¦answerÚ˜The `@tool` option allows the use of the `InjectedToolArg` annotation to hide certain parameters from the tool's schema while still injecting them at runtime. This is useful for arguments that should not be exposed to the model, such as a `user_id` that is dynamically provided. By using `InjectedToolArg`, these parameters can be passed manually during invocation without being visible in the tool's schema.¨query_id¤Q102ªfocus_kind¶chain_execution_result‹¢idÙ@1d46e6f7574c2ab2dbd72b76d1f71100ec359f693223243e7e2721d7522adcad¢tsËAÚôs6‡¨trace_id¤Q103­parent_run_idÀ¦run_idÙ$4549c525-02b6-42d2-9ccd-caf0607100b1ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ@Which notebook illustrates using an ensemble retriever strategy?¨query_id¤Q103§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q103ªfocus_kind¯chain_execution‹¢idÙ@3cab67d3939b8108c15a4047ba193cf8b3c37cb46c74c64ea324be3c2ab5ea62¢tsËAÚôsC³¨trace_id¤Q103­parent_run_idÙ$4549c525-02b6-42d2-9ccd-caf0607100b1¦run_idÙ$d33acd2c-2003-46dd-a286-11fc44f6f039ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ@Which notebook illustrates using an ensemble retriever strategy?§context¦answer ¨query_id¤Q103¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q103®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:f3b3229e-625b-5c53-a4dd-9b846101123dªfocus_kind¯chain_execution‰¢idÙ@7646e7fedf2867ccc52012f3702a36667df0eb3912bd7373d963637349cb6174¢tsËAÚô‚§+¨trace_idÙ$d33acd2c-2003-46dd-a286-11fc44f6f039­parent_run_idÙ$4549c525-02b6-42d2-9ccd-caf0607100b1¦run_idÙ$d33acd2c-2003-46dd-a286-11fc44f6f039ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we comb...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```

## Advanced retrieval patterns

### Ensemble 

Because the retriever interface is so simple, returning a list of `Do...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.
* See the [Elast...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextu...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small ...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¨query_id¤Q103ªfocus_kind¶chain_execution_result‹¢idÙ@b9bf00725be8294c245c8222c61987a9aaf3117f85d707957ac3eb3d59f5163a¢tsËAÚô‚­÷¨trace_id¤Q103­parent_run_idÙ$4549c525-02b6-42d2-9ccd-caf0607100b1¦run_idÙ$06ab2ca0-c24a-462e-9267-1329369bf8e4ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ@Which notebook illustrates using an ensemble retriever strategy?§context•‚´page_content_summaryÙË```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we comb...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```

## Advanced retrieval patterns

### Ensemble 

Because the retriever interface is so simple, returning a list of `Do...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.
* See the [Elast...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextu...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small ...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¦answer ¨query_id¤Q103¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q103®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:f2b00778-1cdd-7b82-c904-71122813b7faªfocus_kind¯chain_execution‹¢idÙ@211a4d134f4480ea257a7755914ed0d6920d3637db8f57cc70a90425a9d4cda6¢tsËAÚô‚²¨trace_id¤Q103­parent_run_idÙ$06ab2ca0-c24a-462e-9267-1329369bf8e4¦run_idÙ$ee9ca384-5871-4484-8632-c9ef782a4c2bªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ@Which notebook illustrates using an ensemble retriever strategy?§contextÚp```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we combine search results from many retrievers? 
This motivates the concept of re-ranking, which takes the output of multiple retrievers and combines them using a more sophisticated algorithm such as [Reciprocal Rank Fusion (RRF)](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf).

### Source document retention 

Many retrievers utilize some kind of index to make documents easily searchable.
The process of indexing can include a transformation step (e.g., vectorstores often use document splitting). 
Whatever transformation is used, can be very useful to retain a link between the *transformed document* and the original, giving the retriever the ability to return the *original* document.

![Retrieval with full docs](/img/retriever_full_docs.png)

```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```

## Advanced retrieval patterns

### Ensemble 

Because the retriever interface is so simple, returning a list of `Document` objects given a search query, it is possible to combine multiple retrievers using ensembling.
This is particularly useful when you have multiple retrievers that are good at finding different types of relevant documents.
It is easy to create an [ensemble retriever](/docs/how_to/ensemble_retriever/) that combines multiple retrievers with linear weighted scores:

```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

:::info[Further reading]

* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.
* See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.

::: 

### Vector store 

[Vector stores](/docs/concepts/vectorstores/) are a powerful and efficient way to index and retrieve unstructured data. 
A vectorstore can be used as a retriever by calling the `as_retriever()` method.

```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```

## Advanced retrieval patterns

### Ensemble

- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)
- [How to: write a custom retriever class](/docs/how_to/custom_retriever)
- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)
- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)
- [How to: reorder retrieved results to mitigate the "lost in the middle" effect](/docs/how_to/long_context_reorder)
- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)
- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)
- [How to: generate metadata filters](/docs/how_to/self_query)
- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)

![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small chunk size for indexing documents in a vectorstore. 
If you return *only* the chunks as the retrieval result, then the model will have lost the original document context for the chunks. 

LangChain has two different retrievers that can be used to address this challenge. 
The [Multi-Vector](/docs/how_to/multi_vector/) retriever allows the user to use any document transformation (e.g., use an LLM to write a summary of the document) for indexing while retaining linkage to the source document. 
The [ParentDocument](/docs/how_to/parent_document_retriever/) retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q103®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:f2b00778-1cdd-7b82-c904-71122813b7fa­checkpoint_nsÙ-generate:f2b00778-1cdd-7b82-c904-71122813b7fa¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@299f8f8e8e0f6d25c51f60451b7fd0567911a8b3f98a782f05c21984a477fff8¢tsËAÚô‚¸R¨trace_idÙ$ee9ca384-5871-4484-8632-c9ef782a4c2b­parent_run_idÙ$06ab2ca0-c24a-462e-9267-1329369bf8e4¦run_idÙ$ee9ca384-5871-4484-8632-c9ef782a4c2bªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ`messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Which notebook illustrates using an ensemble retriever strategy? \nContext: ```python\n# Initialize the ensemble retriever\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]\n)\n```\n\nWhen ensembling, how do we combine search results from many retrievers? \nThis motivates the concept of re-ranking, which takes the output of multiple retrievers and combines them using a more sophisticated algorithm such as [Reciprocal Rank Fusion (RRF)](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf).\n\n### Source document retention \n\nMany retrievers utilize some kind of index to make documents easily searchable.\nThe process of indexing can include a transformation step (e.g., vectorstores often use document splitting). \nWhatever transformation is used, can be very useful to retain a link between the *transformed document* and the original, giving the retriever the ability to return the *original* document.\n\n![Retrieval with full docs](/img/retriever_full_docs.png)\n\n```python\nvectorstore = MyVectorStore()\nretriever = vectorstore.as_retriever()\n```\n\n## Advanced retrieval patterns\n\n### Ensemble \n\nBecause the retriever interface is so simple, returning a list of `Document` objects given a search query, it is possible to combine multiple retrievers using ensembling.\nThis is particularly useful when you have multiple retrievers that are good at finding different types of relevant documents.\nIt is easy to create an [ensemble retriever](/docs/how_to/ensemble_retriever/) that combines multiple retrievers with linear weighted scores:\n\n```python\n# Initialize the ensemble retriever\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]\n)\n```\n\n:::info[Further reading]\n\n* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.\n* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.\n* See the [Elasticsearch](/docs/integrations/retrievers/elasticsearch_retriever/) retriever integration.\n\n::: \n\n### Vector store \n\n[Vector stores](/docs/concepts/vectorstores/) are a powerful and efficient way to index and retrieve unstructured data. \nA vectorstore can be used as a retriever by calling the `as_retriever()` method.\n\n```python\nvectorstore = MyVectorStore()\nretriever = vectorstore.as_retriever()\n```\n\n## Advanced retrieval patterns\n\n### Ensemble\n\n- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)\n- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)\n- [How to: use contextual compression to compress the data retrieved](/docs/how_to/contextual_compression)\n- [How to: write a custom retriever class](/docs/how_to/custom_retriever)\n- [How to: add similarity scores to retriever results](/docs/how_to/add_scores_retriever)\n- [How to: combine the results from multiple retrievers](/docs/how_to/ensemble_retriever)\n- [How to: reorder retrieved results to mitigate the "lost in the middle" effect](/docs/how_to/long_context_reorder)\n- [How to: generate multiple embeddings per document](/docs/how_to/multi_vector)\n- [How to: retrieve the whole document for a chunk](/docs/how_to/parent_document_retriever)\n- [How to: generate metadata filters](/docs/how_to/self_query)\n- [How to: create a time-weighted retriever](/docs/how_to/time_weighted_vectorstore)\n\n![Retrieval with full docs](/img/retriever_full_docs.png)\n\nThis is particularly useful in AI applications, because it ensures no loss in document context for the model.\nFor example, you may use small chunk size for indexing documents in a vectorstore. \nIf you return *only* the chunks as the retrieval result, then the model will have lost the original document context for the chunks. \n\nLangChain has two different retrievers that can be used to address this challenge. \nThe [Multi-Vector](/docs/how_to/multi_vector/) retriever allows the user to use any document transformation (e.g., use an LLM to write a summary of the document) for indexing while retaining linkage to the source document. \nThe [ParentDocument](/docs/how_to/parent_document_retriever/) retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@793714ad21320dcbd9d4cbe1997eb08b71802c12cfdd94b28f3d86517011612a¢tsËAÚôæ<!¨focus_msÍ§payload‚¤role©assistant§contentÚ«The notebook that illustrates using an ensemble retriever strategy is the one that includes the initialization of the `EnsembleRetriever` with multiple retrievers, such as `bm25_retriever` and `vector_store_retriever`. This is demonstrated in the provided code snippet where the ensemble retriever is created with equal weights for both retrievers. For more details, you can refer to the section on advanced retrieval patterns.ªfocus_kind¬exec_latency¨query_id¤Q103‰¢idÙ@b2737d58d33af5ef6462ef2f1298452d8ef0e2aa10734935786b3cd7ce2bd971¢tsËAÚôæJ¶¨trace_idÙ$06ab2ca0-c24a-462e-9267-1329369bf8e4­parent_run_idÙ$4549c525-02b6-42d2-9ccd-caf0607100b1¦run_idÙ$06ab2ca0-c24a-462e-9267-1329369bf8e4ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ«The notebook that illustrates using an ensemble retriever strategy is the one that includes the initialization of the `EnsembleRetriever` with multiple retrievers, such as `bm25_retriever` and `vector_store_retriever`. This is demonstrated in the provided code snippet where the ensemble retriever is created with equal weights for both retrievers. For more details, you can refer to the section on advanced retrieval patterns.¨query_id¤Q103ªfocus_kind¶chain_execution_result‰¢idÙ@cbccb300d63e155b2c7b8fe4edb58993a8c9bf6772b31dc85aeb307b64654300¢tsËAÚôæY*¨trace_idÙ$4549c525-02b6-42d2-9ccd-caf0607100b1­parent_run_idÀ¦run_idÙ$4549c525-02b6-42d2-9ccd-caf0607100b1ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ@Which notebook illustrates using an ensemble retriever strategy?§context•‚´page_content_summaryÙË```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we comb...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```

## Advanced retrieval patterns

### Ensemble 

Because the retriever interface is so simple, returning a list of `Do...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.
* See the [Elast...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextu...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small ...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¦answerÚ«The notebook that illustrates using an ensemble retriever strategy is the one that includes the initialization of the `EnsembleRetriever` with multiple retrievers, such as `bm25_retriever` and `vector_store_retriever`. This is demonstrated in the provided code snippet where the ensemble retriever is created with equal weights for both retrievers. For more details, you can refer to the section on advanced retrieval patterns.¨query_id¤Q103ªfocus_kind¶chain_execution_result‹¢idÙ@c6b867478cf9a0fa5f3ff1380b5701671a2c5f7291bc3098674c4dbff1e610f7¢tsËAÚôæhD¨trace_id¤Q104­parent_run_idÀ¦run_idÙ$e8026cb2-982c-4cac-bbe2-cd46b85173a8ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ>Give two benefits of the LangChain Expression Language (LCEL).¨query_id¤Q104§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q104ªfocus_kind¯chain_execution‹¢idÙ@0defc7b8b2e650a33130ba71f9b44bb941c486b4c3e6c6cf35066547ab374a86¢tsËAÚôæ~I¨trace_id¤Q104­parent_run_idÙ$e8026cb2-982c-4cac-bbe2-cd46b85173a8¦run_idÙ$50889e78-5e87-41ae-b6fe-089300b595e4ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ>Give two benefits of the LangChain Expression Language (LCEL).§context¦answer ¨query_id¤Q104¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q104®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:8c2e5b1e-c781-1f8e-71e3-2cfb22aae48eªfocus_kind¯chain_execution‰¢idÙ@e5c3e07175e93ae7a3916b978a754e3349c8d8fe657086d8828568d22bea022f¢tsËAÚôµ¨trace_idÙ$50889e78-5e87-41ae-b6fe-089300b595e4­parent_run_idÙ$e8026cb2-982c-4cac-bbe2-cd46b85173a8¦run_idÙ$50889e78-5e87-41ae-b6fe-089300b595e4ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËOther benefits include:

- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)
As your chains get more and more complex, it becomes increasingly important to understand what exactly is ...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËWe often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::n...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË# LangChain Expression Language (LCEL)

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
:::

The **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË## Should I use LCEL?

LCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.

...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¨query_id¤Q104ªfocus_kind¶chain_execution_result‹¢idÙ@8f5ecff69fa37ed48400a4532910c6fc43e894b95480caff0e265e1b9025e7a8¢tsËAÚô)Ã¨trace_id¤Q104­parent_run_idÙ$e8026cb2-982c-4cac-bbe2-cd46b85173a8¦run_idÙ$aa079bc4-8558-4866-8776-2680218fe77bªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ>Give two benefits of the LangChain Expression Language (LCEL).§context•‚´page_content_summaryÙËOther benefits include:

- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)
As your chains get more and more complex, it becomes increasingly important to understand what exactly is ...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËWe often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::n...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË# LangChain Expression Language (LCEL)

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
:::

The **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË## Should I use LCEL?

LCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.

...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¦answer ¨query_id¤Q104¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q104®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:3c7cc4c8-563c-f0f4-4f04-3bb228821326ªfocus_kind¯chain_execution‹¢idÙ@5451d9e7163037c716773e6e57333e50a7ccf036dd9c145a34d6b0cf17cc32a4¢tsËAÚôRì¨trace_id¤Q104­parent_run_idÙ$aa079bc4-8558-4866-8776-2680218fe77b¦run_idÙ$64b3193c-cf6d-4d6b-a435-b0145970b265ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ>Give two benefits of the LangChain Expression Language (LCEL).§contextÚ¾Other benefits include:

- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)
As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.
With LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.
- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.
- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.

## Should I use LCEL?

LCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.

We often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::note
* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.
* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.
* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using LCEL.
:::

## Benefits of LCEL

LangChain optimizes the run-time execution of chains built with LCEL in a number of ways:

# LangChain Expression Language (LCEL)

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
:::

The **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.

This means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.

We often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).

## Should I use LCEL?

LCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.

While we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).

In LangGraph, users define graphs that specify the application's flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.

Here are some guidelines:¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q104®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:3c7cc4c8-563c-f0f4-4f04-3bb228821326­checkpoint_nsÙ-generate:3c7cc4c8-563c-f0f4-4f04-3bb228821326¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@dbafc5c689628544e6101364b3f1c943a1e9d32078b7b1abab246ca8cb180c82¢tsËAÚôo¨trace_idÙ$64b3193c-cf6d-4d6b-a435-b0145970b265­parent_run_idÙ$aa079bc4-8558-4866-8776-2680218fe77b¦run_idÙ$64b3193c-cf6d-4d6b-a435-b0145970b265ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Give two benefits of the LangChain Expression Language (LCEL). \nContext: Other benefits include:\n\n- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\nWith LCEL, **all** steps are automatically logged to [LangSmith](https://docs.smith.langchain.com/) for maximum observability and debuggability.\n- **Standard API**: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\n- [**Deployable with LangServe**](/docs/concepts/architecture#langserve): Chains built with LCEL can be deployed using for production use.\n\n## Should I use LCEL?\n\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\n\nWe often refer to a `Runnable` created using LCEL as a "chain". It\'s important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\n\n:::note\n* The [LCEL cheatsheet](/docs/how_to/lcel_cheatsheet/) shows common patterns that involve the Runnable interface and LCEL expressions.\n* Please see the following list of [how-to guides](/docs/how_to/#langchain-expression-language-lcel) that cover common tasks with LCEL.\n* A list of built-in `Runnables` can be found in the [LangChain Core API Reference](https://python.langchain.com/api_reference/core/runnables.html). Many of these Runnables are useful when composing custom "chains" in LangChain using LCEL.\n:::\n\n## Benefits of LCEL\n\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\n\n# LangChain Expression Language (LCEL)\n\n:::info Prerequisites\n* [Runnable Interface](/docs/concepts/runnables)\n:::\n\nThe **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https://en.wikipedia.org/wiki/Declarative_programming) approach to building new [Runnables](/docs/concepts/runnables) from existing Runnables.\n\nThis means that you describe what *should* happen, rather than *how* it should happen, allowing LangChain to optimize the run-time execution of the chains.\n\nWe often refer to a `Runnable` created using LCEL as a "chain". It\'s important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).\n\n- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a [chat model](/docs/concepts/chat_models) or [llm](/docs/concepts/text_llms) comes out).\n\n## Should I use LCEL?\n\nLCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.\n\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of [LangGraph](/docs/concepts/architecture#langgraph).\n\nIn LangGraph, users define graphs that specify the application\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\n\nHere are some guidelines: \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@7c095ee52ed8cbd84a460ae498b53fc3975bdc5184ccb5bd78737a4bb0d53878¢tsËAÚô ·V¨focus_msÍ§payload‚¤role©assistant§contentÚ„Two benefits of the LangChain Expression Language (LCEL) are its ability to simplify streaming, allowing for incremental output during execution, and its seamless integration with LangSmith for automatic logging of all steps, enhancing observability and debuggability. Additionally, LCEL provides a standard API through the Runnable interface, making it easier to use chains consistently.ªfocus_kind¬exec_latency¨query_id¤Q104‰¢idÙ@6642d70622f2b42e04862c7bbdcc622e8e417ec8d7efd57e5ee85b098e8a3b4c¢tsËAÚô ÍÌ¨trace_idÙ$aa079bc4-8558-4866-8776-2680218fe77b­parent_run_idÙ$e8026cb2-982c-4cac-bbe2-cd46b85173a8¦run_idÙ$aa079bc4-8558-4866-8776-2680218fe77bªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ„Two benefits of the LangChain Expression Language (LCEL) are its ability to simplify streaming, allowing for incremental output during execution, and its seamless integration with LangSmith for automatic logging of all steps, enhancing observability and debuggability. Additionally, LCEL provides a standard API through the Runnable interface, making it easier to use chains consistently.¨query_id¤Q104ªfocus_kind¶chain_execution_result‰¢idÙ@1fff54901de64916036f5ef09d619df39ec3fce955e9f193cd01127e5c691ddd¢tsËAÚô Ş+¨trace_idÙ$e8026cb2-982c-4cac-bbe2-cd46b85173a8­parent_run_idÀ¦run_idÙ$e8026cb2-982c-4cac-bbe2-cd46b85173a8ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ>Give two benefits of the LangChain Expression Language (LCEL).§context•‚´page_content_summaryÙËOther benefits include:

- [**Seamless LangSmith tracing**](https://docs.smith.langchain.com)
As your chains get more and more complex, it becomes increasingly important to understand what exactly is ...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËWe often refer to a `Runnable` created using LCEL as a "chain". It's important to remember that a "chain" is `Runnable` and it implements the full [Runnable Interface](/docs/concepts/runnables).

:::n...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË# LangChain Expression Language (LCEL)

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
:::

The **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË## Should I use LCEL?

LCEL is an [orchestration solution](https://en.wikipedia.org/wiki/Orchestration_(computing)) -- it allows LangChain to handle run-time execution of chains in an optimized way.

...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¦answerÚ„Two benefits of the LangChain Expression Language (LCEL) are its ability to simplify streaming, allowing for incremental output during execution, and its seamless integration with LangSmith for automatic logging of all steps, enhancing observability and debuggability. Additionally, LCEL provides a standard API through the Runnable interface, making it easier to use chains consistently.¨query_id¤Q104ªfocus_kind¶chain_execution_result‹¢idÙ@525e68d7ed47c8ac18f87d9f42373d148aa977e234bc7a94e82075bed1f8d1b6¢tsËAÚô ó”¨trace_id¤Q105­parent_run_idÀ¦run_idÙ$f9b4b963-6bf8-4100-8622-b7890dc67e6cªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙFWhat is the main purpose of the `rate_limiter` argument on ChatModels?¨query_id¤Q105§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q105ªfocus_kind¯chain_execution‹¢idÙ@02e523064e4c4cfa4842adfc09d18cc65577ca64285f7fa3dc02670ec92de123¢tsËAÚô¡%³¨trace_id¤Q105­parent_run_idÙ$f9b4b963-6bf8-4100-8622-b7890dc67e6c¦run_idÙ$a4dc237a-e954-4f5f-b8a1-b9e4f27beb5dªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙFWhat is the main purpose of the `rate_limiter` argument on ChatModels?§context¦answer ¨query_id¤Q105¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q105®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:1609ae62-e427-e7c2-167b-8fc5e0f17507ªfocus_kind¯chain_execution‰¢idÙ@b6e36288e135157c550cabe1ec06fb75907f7692d4d34bad4b45a83c9f7ea2e0¢tsËAÚô´C6¨trace_idÙ$a4dc237a-e954-4f5f-b8a1-b9e4f27beb5d­parent_run_idÙ$f9b4b963-6bf8-4100-8622-b7890dc67e6c¦run_idÙ$a4dc237a-e954-4f5f-b8a1-b9e4f27beb5dªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË| `base_url`     | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.                                ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Chat models

## Overview

Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q105ªfocus_kind¶chain_execution_result‹¢idÙ@08e3341dab2761c51eb38b209ec2f6c15af93e073268d593a4e1a432d0db0e90¢tsËAÚô´Oğ¨trace_id¤Q105­parent_run_idÙ$f9b4b963-6bf8-4100-8622-b7890dc67e6c¦run_idÙ$33d95aa7-3f03-4700-bc5e-56c2f882d6adªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙFWhat is the main purpose of the `rate_limiter` argument on ChatModels?§context•‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË| `base_url`     | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.                                ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Chat models

## Overview

Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answer ¨query_id¤Q105¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q105®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:47a8d9d4-c4be-5629-37c3-f0654175204aªfocus_kind¯chain_execution‹¢idÙ@2d3d1925af56bbd166296dc70675711a7c17a3f841f68dccef96c0f2b56e3bc9¢tsËAÚô´X’¨trace_id¤Q105­parent_run_idÙ$33d95aa7-3f03-4700-bc5e-56c2f882d6ad¦run_idÙ$603758e3-489d-4240-a9bd-a229cad1f98fªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙFWhat is the main purpose of the `rate_limiter` argument on ChatModels?§contextÚ©1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.
2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information.

3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.

| `base_url`     | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.                                                                                                                                                          |
| `rate_limiter` | An optional [BaseRateLimiter](https://python.langchain.com/api_reference/core/rate_limiters/langchain_core.rate_limiters.BaseRateLimiter.html#langchain_core.rate_limiters.BaseRateLimiter) to space out requests to avoid exceeding rate limits.  See [rate-limiting](#rate-limiting) below for more details. |

If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).

The size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.

## Advanced topics
 
### Rate-limiting

Many chat model providers impose a limit on the number of requests that can be made in a given time period.

If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

# Chat models

## Overview

Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.

Modern LLMs are typically accessed through a chat model interface that takes a list of [messages](/docs/concepts/messages) as input and returns a [message](/docs/concepts/messages) as output.

The newest generation of chat models offer additional capabilities:¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q105®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:47a8d9d4-c4be-5629-37c3-f0654175204a­checkpoint_nsÙ-generate:47a8d9d4-c4be-5629-37c3-f0654175204a¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@654280c2019d291efe3192f80f388f9ec07cbf058ee48deaea5708f953dacadb¢tsËAÚô´k¨trace_idÙ$603758e3-489d-4240-a9bd-a229cad1f98f­parent_run_idÙ$33d95aa7-3f03-4700-bc5e-56c2f882d6ad¦run_idÙ$603758e3-489d-4240-a9bd-a229cad1f98fªevent_type¬chain_output¬event_source¥chain§payload§outputsÚlmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What is the main purpose of the `rate_limiter` argument on ChatModels? \nContext: 1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the [how to handle rate limits](/docs/how_to/chat_model_rate_limiting/) for more information on how to use this feature.\n2. Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a `max_retries` parameter that can be used to control the number of retries. See the [standard parameters](#standard-parameters) section for more information.\n\n3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\n\n| `base_url`     | The URL of the API endpoint where requests are sent. This is typically provided by the model\'s provider and is necessary for directing your requests.                                                                                                                                                          |\n| `rate_limiter` | An optional [BaseRateLimiter](https://python.langchain.com/api_reference/core/rate_limiters/langchain_core.rate_limiters.BaseRateLimiter.html#langchain_core.rate_limiters.BaseRateLimiter) to space out requests to avoid exceeding rate limits.  See [rate-limiting](#rate-limiting) below for more details. |\n\nIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can "remember" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the [memory](https://langchain-ai.github.io/langgraph/concepts/memory/).\n\nThe size of the input is measured in [tokens](/docs/concepts/tokens) which are the unit of processing that the model uses.\n\n## Advanced topics\n \n### Rate-limiting\n\nMany chat model providers impose a limit on the number of requests that can be made in a given time period.\n\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n\n# Chat models\n\n## Overview\n\nLarge Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\n\nModern LLMs are typically accessed through a chat model interface that takes a list of [messages](/docs/concepts/messages) as input and returns a [message](/docs/concepts/messages) as output.\n\nThe newest generation of chat models offer additional capabilities: \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@434c1247bc1453edb019fb3bd0134778e5262d7d0df8fa97f9b9dd5b881061ef¢tsËAÚô Ç ¨focus_msÍœ§payload‚¤role©assistant§contentÚlThe main purpose of the `rate_limiter` argument on ChatModels is to control the rate at which requests are made to avoid exceeding rate limits imposed by the model provider. By spacing out requests, it helps prevent hitting these limits, which is particularly useful during benchmarking. This feature allows for more efficient and effective use of the chat models.ªfocus_kind¬exec_latency¨query_id¤Q105‰¢idÙ@71a11f50b1a732fa37d34a67d955bf6acafa9e3f1a3f27f0546a1ecb2679acad¢tsËAÚô Ø8¨trace_idÙ$33d95aa7-3f03-4700-bc5e-56c2f882d6ad­parent_run_idÙ$f9b4b963-6bf8-4100-8622-b7890dc67e6c¦run_idÙ$33d95aa7-3f03-4700-bc5e-56c2f882d6adªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚlThe main purpose of the `rate_limiter` argument on ChatModels is to control the rate at which requests are made to avoid exceeding rate limits imposed by the model provider. By spacing out requests, it helps prevent hitting these limits, which is particularly useful during benchmarking. This feature allows for more efficient and effective use of the chat models.¨query_id¤Q105ªfocus_kind¶chain_execution_result‰¢idÙ@45a617da26f95d073c315f02ef2a6b5bab8c3e889400083756fc7a456b78e3cc¢tsËAÚô àÂ¨trace_idÙ$f9b4b963-6bf8-4100-8622-b7890dc67e6c­parent_run_idÀ¦run_idÙ$f9b4b963-6bf8-4100-8622-b7890dc67e6cªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙFWhat is the main purpose of the `rate_limiter` argument on ChatModels?§context•‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË| `base_url`     | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.                                ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Chat models

## Overview

Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚlThe main purpose of the `rate_limiter` argument on ChatModels is to control the rate at which requests are made to avoid exceeding rate limits imposed by the model provider. By spacing out requests, it helps prevent hitting these limits, which is particularly useful during benchmarking. This feature allows for more efficient and effective use of the chat models.¨query_id¤Q105ªfocus_kind¶chain_execution_result‹¢idÙ@a18ba4f74040e8ab2cfba09f0331306833dc85339e3b069d1898d507a522a1eb¢tsËAÚô éŠ¨trace_id¤Q106­parent_run_idÀ¦run_idÙ$195e5ad7-2cb3-42a8-87d6-22ab5b874ccbªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ<How many tokens is 100 English words approximately equal to?¨query_id¤Q106§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q106ªfocus_kind¯chain_execution‹¢idÙ@e357cc8aba257b4ae47392816d93d207cb04ff9627f5f5a864133a913d3916a1¢tsËAÚô!2¨trace_id¤Q106­parent_run_idÙ$195e5ad7-2cb3-42a8-87d6-22ab5b874ccb¦run_idÙ$572fa2c4-8473-4c56-b9e4-3d94a5ec3629ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ<How many tokens is 100 English words approximately equal to?§context¦answer ¨query_id¤Q106¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q106®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:962895bc-892a-53b6-0ca8-fc6f15a49a16ªfocus_kind¯chain_execution‰¢idÙ@575a0203905ae1f4e5eaca7afcd49e7aaa3b856d9cc13f5b948485c2e9be9d41¢tsËAÚô3r¨trace_idÙ$572fa2c4-8473-4c56-b9e4-3d94a5ec3629­parent_run_idÙ$195e5ad7-2cb3-42a8-87d6-22ab5b874ccb¦run_idÙ$572fa2c4-8473-4c56-b9e4-3d94a5ec3629ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## How tokens correspond to text

Please see this post from [OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) for more details on how tokens are counted and h...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙË# Tokens

Modern large language models (LLMs) are typically based on a transformer architecture that processes a sequence of units known as tokens. Tokens are the fundamental elements that models use ...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙËPlease see the [multimodality](/docs/concepts/multimodality) section for more information on multi-modal inputs and outputs.

## Why not use characters?

Using tokens instead of individual characters ...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙË## How tokens work in language models

The reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, langua...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙË2. **Processing**: The transformer architecture behind these models processes tokens sequentially to predict the next token in a sentence. It does this by analyzing the relationships between tokens, c...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx¨query_id¤Q106ªfocus_kind¶chain_execution_result‹¢idÙ@100a0b7fcbb255c4f6655da8a3c2761f53ead0d7e61c571546e8509357d478af¢tsËAÚô3(¨trace_id¤Q106­parent_run_idÙ$195e5ad7-2cb3-42a8-87d6-22ab5b874ccb¦run_idÙ$6e037a04-600b-46b3-a74b-717049cf563eªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ<How many tokens is 100 English words approximately equal to?§context•‚´page_content_summaryÙË## How tokens correspond to text

Please see this post from [OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) for more details on how tokens are counted and h...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙË# Tokens

Modern large language models (LLMs) are typically based on a transformer architecture that processes a sequence of units known as tokens. Tokens are the fundamental elements that models use ...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙËPlease see the [multimodality](/docs/concepts/multimodality) section for more information on multi-modal inputs and outputs.

## Why not use characters?

Using tokens instead of individual characters ...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙË## How tokens work in language models

The reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, langua...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙË2. **Processing**: The transformer architecture behind these models processes tokens sequentially to predict the next token in a sentence. It does this by analyzing the relationships between tokens, c...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx¦answer ¨query_id¤Q106¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q106®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:85663273-6894-24c5-a7d9-78ef62b19185ªfocus_kind¯chain_execution‹¢idÙ@470a97a2bc0b264908d416a68e407034ac9f3de46dd8a7e13641e1fb4bae0498¢tsËAÚô30Ï¨trace_id¤Q106­parent_run_idÙ$6e037a04-600b-46b3-a74b-717049cf563e¦run_idÙ$0635274a-5059-4f60-8b42-10f437c3bb4eªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ<How many tokens is 100 English words approximately equal to?§contextÚš## How tokens correspond to text

Please see this post from [OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) for more details on how tokens are counted and how they correspond to text.

According to the OpenAI post, the approximate token counts for English text are as follows:

* 1 token ~= 4 chars in English
* 1 token ~= Â¾ words
* 100 tokens ~= 75 words

# Tokens

Modern large language models (LLMs) are typically based on a transformer architecture that processes a sequence of units known as tokens. Tokens are the fundamental elements that models use to break down input and generate output. In this section, we'll discuss what tokens are and how they are used by language models.

## What is a token?

A **token** is the basic unit that a language model reads, processes, and generates. These units can vary based on how the model provider defines them, but in general, they could represent:

* A whole word (e.g., "apple"),
* A part of a word (e.g., "app"),
* Or other linguistic components such as punctuation or spaces.

The way the model tokenizes the input depends on its **tokenizer algorithm**, which converts the input into tokens. Similarly, the modelâ€™s output comes as a stream of tokens, which is then decoded back into human-readable text.

## How tokens work in language models

Please see the [multimodality](/docs/concepts/multimodality) section for more information on multi-modal inputs and outputs.

## Why not use characters?

Using tokens instead of individual characters makes models both more efficient and better at understanding context and grammar. Tokens represent meaningful units, like whole words or parts of words, allowing models to capture language structure more effectively than by processing raw characters. Token-level processing also reduces the number of units the model has to handle, leading to faster computation.

In contrast, character-level processing would require handling a much larger sequence of input, making it harder for the model to learn relationships and context. Tokens enable models to focus on linguistic meaning, making them more accurate and efficient in generating responses.

## How tokens correspond to text

## How tokens work in language models

The reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, language models focus on **tokens**, which represent meaningful linguistic units. Here's how the process works:

1. **Input Tokenization**: When you provide a model with a prompt (e.g., "LangChain is cool!"), the tokenizer algorithm splits the text into tokens. For example, the sentence could be tokenized into parts like `["Lang", "Chain", " is", " cool", "!"]`. Note that token boundaries donâ€™t always align with word boundaries.
    ![](/img/tokenization.png)

2. **Processing**: The transformer architecture behind these models processes tokens sequentially to predict the next token in a sentence. It does this by analyzing the relationships between tokens, capturing context and meaning from the input.
3. **Output Generation**: The model generates new tokens one by one. These output tokens are then decoded back into human-readable text.

Using tokens instead of raw characters allows the model to focus on linguistically meaningful units, which helps it capture grammar, structure, and context more effectively.

## Tokens donâ€™t have to be text

Although tokens are most commonly used to represent text, they donâ€™t have to be limited to textual data. Tokens can also serve as abstract representations of **multi-modal data**, such as:

- **Images**,
- **Audio**,
- **Video**,
- And other types of data.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q106®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:85663273-6894-24c5-a7d9-78ef62b19185­checkpoint_nsÙ-generate:85663273-6894-24c5-a7d9-78ef62b19185¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@0f08d136e87adf25fa235a96081711333693b54fbcba49ebb28a2ed40c14c117¢tsËAÚô3>h¨trace_idÙ$0635274a-5059-4f60-8b42-10f437c3bb4e­parent_run_idÙ$6e037a04-600b-46b3-a74b-717049cf563e¦run_idÙ$0635274a-5059-4f60-8b42-10f437c3bb4eªevent_type¬chain_output¬event_source¥chain§payload§outputsÚomessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: How many tokens is 100 English words approximately equal to? \nContext: ## How tokens correspond to text\n\nPlease see this post from [OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) for more details on how tokens are counted and how they correspond to text.\n\nAccording to the OpenAI post, the approximate token counts for English text are as follows:\n\n* 1 token ~= 4 chars in English\n* 1 token ~= Â¾ words\n* 100 tokens ~= 75 words\n\n# Tokens\n\nModern large language models (LLMs) are typically based on a transformer architecture that processes a sequence of units known as tokens. Tokens are the fundamental elements that models use to break down input and generate output. In this section, we\'ll discuss what tokens are and how they are used by language models.\n\n## What is a token?\n\nA **token** is the basic unit that a language model reads, processes, and generates. These units can vary based on how the model provider defines them, but in general, they could represent:\n\n* A whole word (e.g., "apple"),\n* A part of a word (e.g., "app"),\n* Or other linguistic components such as punctuation or spaces.\n\nThe way the model tokenizes the input depends on its **tokenizer algorithm**, which converts the input into tokens. Similarly, the modelâ€™s output comes as a stream of tokens, which is then decoded back into human-readable text.\n\n## How tokens work in language models\n\nPlease see the [multimodality](/docs/concepts/multimodality) section for more information on multi-modal inputs and outputs.\n\n## Why not use characters?\n\nUsing tokens instead of individual characters makes models both more efficient and better at understanding context and grammar. Tokens represent meaningful units, like whole words or parts of words, allowing models to capture language structure more effectively than by processing raw characters. Token-level processing also reduces the number of units the model has to handle, leading to faster computation.\n\nIn contrast, character-level processing would require handling a much larger sequence of input, making it harder for the model to learn relationships and context. Tokens enable models to focus on linguistic meaning, making them more accurate and efficient in generating responses.\n\n## How tokens correspond to text\n\n## How tokens work in language models\n\nThe reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, language models focus on **tokens**, which represent meaningful linguistic units. Here\'s how the process works:\n\n1. **Input Tokenization**: When you provide a model with a prompt (e.g., "LangChain is cool!"), the tokenizer algorithm splits the text into tokens. For example, the sentence could be tokenized into parts like `["Lang", "Chain", " is", " cool", "!"]`. Note that token boundaries donâ€™t always align with word boundaries.\n    ![](/img/tokenization.png)\n\n2. **Processing**: The transformer architecture behind these models processes tokens sequentially to predict the next token in a sentence. It does this by analyzing the relationships between tokens, capturing context and meaning from the input.\n3. **Output Generation**: The model generates new tokens one by one. These output tokens are then decoded back into human-readable text.\n\nUsing tokens instead of raw characters allows the model to focus on linguistically meaningful units, which helps it capture grammar, structure, and context more effectively.\n\n## Tokens donâ€™t have to be text\n\nAlthough tokens are most commonly used to represent text, they donâ€™t have to be limited to textual data. Tokens can also serve as abstract representations of **multi-modal data**, such as:\n\n- **Images**,\n- **Audio**,\n- **Video**,\n- And other types of data. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@380ac413747ce73b8a8c001e21f3764eb2bf3d9ce13dffff091c39225a188156¢tsËAÚôõ{¨focus_msÍ®§payload‚¤role©assistant§contentÙÊApproximately, 100 English words are equal to about 133 tokens, since 1 token is roughly equivalent to Â¾ of a word. Therefore, if you multiply 100 words by the inverse of Â¾, you get around 133 tokens.ªfocus_kind¬exec_latency¨query_id¤Q106‰¢idÙ@752dbe5c09594796daf894ded4fc01ed2689e5e07b9b4c2415f17aa23d0e9417¢tsËAÚô€$¨trace_idÙ$6e037a04-600b-46b3-a74b-717049cf563e­parent_run_idÙ$195e5ad7-2cb3-42a8-87d6-22ab5b874ccb¦run_idÙ$6e037a04-600b-46b3-a74b-717049cf563eªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙÊApproximately, 100 English words are equal to about 133 tokens, since 1 token is roughly equivalent to Â¾ of a word. Therefore, if you multiply 100 words by the inverse of Â¾, you get around 133 tokens.¨query_id¤Q106ªfocus_kind¶chain_execution_result‰¢idÙ@18d44a99d22524e18e856da187066cff239b78047a546fa63da3ba247073f844¢tsËAÚô€;¨trace_idÙ$195e5ad7-2cb3-42a8-87d6-22ab5b874ccb­parent_run_idÀ¦run_idÙ$195e5ad7-2cb3-42a8-87d6-22ab5b874ccbªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ<How many tokens is 100 English words approximately equal to?§context•‚´page_content_summaryÙË## How tokens correspond to text

Please see this post from [OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) for more details on how tokens are counted and h...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙË# Tokens

Modern large language models (LLMs) are typically based on a transformer architecture that processes a sequence of units known as tokens. Tokens are the fundamental elements that models use ...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙËPlease see the [multimodality](/docs/concepts/multimodality) section for more information on multi-modal inputs and outputs.

## Why not use characters?

Using tokens instead of individual characters ...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙË## How tokens work in language models

The reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, langua...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙË2. **Processing**: The transformer architecture behind these models processes tokens sequentially to predict the next token in a sentence. It does this by analyzing the relationships between tokens, c...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx¦answerÙÊApproximately, 100 English words are equal to about 133 tokens, since 1 token is roughly equivalent to Â¾ of a word. Therefore, if you multiply 100 words by the inverse of Â¾, you get around 133 tokens.¨query_id¤Q106ªfocus_kind¶chain_execution_result‹¢idÙ@c985599895abe9f3844f80b13f742e05d3efec2634ab406aa5b5bc6c4c7fceae¢tsËAÚô€!¨trace_id¤Q107­parent_run_idÀ¦run_idÙ$1bcc0243-2af7-425d-8605-931a8ad7542bªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ6Explain the difference between `invoke` and `ainvoke`.¨query_id¤Q107§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q107ªfocus_kind¯chain_execution‹¢idÙ@1ef5aa2095d035cea8b90d6f3e8a36e03620d2b8038b0676b8bf45a8be4c1d68¢tsËAÚô€0b¨trace_id¤Q107­parent_run_idÙ$1bcc0243-2af7-425d-8605-931a8ad7542b¦run_idÙ$91705fb9-f97b-4757-95e6-d9dedeb01676ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ6Explain the difference between `invoke` and `ainvoke`.§context¦answer ¨query_id¤Q107¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q107®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:05e855fc-0f3a-9f68-7a38-407c2a9fcfa7ªfocus_kind¯chain_execution‰¢idÙ@ccb9dc0ee816f135205a4768404e8a213a6865e3edfe6364c4b838249b70af4e¢tsËAÚôˆù¨trace_idÙ$91705fb9-f97b-4757-95e6-d9dedeb01676­parent_run_idÙ$1bcc0243-2af7-425d-8605-931a8ad7542b¦run_idÙ$91705fb9-f97b-4757-95e6-d9dedeb01676ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËWhen you call the `invoke` (or `ainvoke`) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application. 

Under the...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË## Langchain asynchronous APIs

Many LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.

Typically, any method that may perform I/O operations...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË```python
await some_runnable.ainvoke(some_input)
```

Other components like [Embedding Models](/docs/concepts/embedding_models) and [VectorStore](/docs/concepts/vectorstores) that do not implement th...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙËThe result would be an `AIMessage` containing the model's response in natural language (e.g., "Hello!").
However, if we pass an input *relevant to the tool*, the model should choose to call it:

```py...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙÍ## "Auto-Streaming" Chat Models

LangChain simplifies streaming from [chat models](/docs/concepts/chat_models) by automatically enabling streaming mode in certain cases, even when youâ€™re not explicitl...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¨query_id¤Q107ªfocus_kind¶chain_execution_result‹¢idÙ@acfb852b769d09a133ae938d229f83552d70ea3410059f100c0943ddf747a469¢tsËAÚôšÜ¨trace_id¤Q107­parent_run_idÙ$1bcc0243-2af7-425d-8605-931a8ad7542b¦run_idÙ$ed333059-2a43-43da-970e-770a9d8ef5c9ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ6Explain the difference between `invoke` and `ainvoke`.§context•‚´page_content_summaryÙËWhen you call the `invoke` (or `ainvoke`) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application. 

Under the...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË## Langchain asynchronous APIs

Many LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.

Typically, any method that may perform I/O operations...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË```python
await some_runnable.ainvoke(some_input)
```

Other components like [Embedding Models](/docs/concepts/embedding_models) and [VectorStore](/docs/concepts/vectorstores) that do not implement th...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙËThe result would be an `AIMessage` containing the model's response in natural language (e.g., "Hello!").
However, if we pass an input *relevant to the tool*, the model should choose to call it:

```py...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙÍ## "Auto-Streaming" Chat Models

LangChain simplifies streaming from [chat models](/docs/concepts/chat_models) by automatically enabling streaming mode in certain cases, even when youâ€™re not explicitl...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¦answer ¨query_id¤Q107¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q107®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:29e7cff2-8a31-ff14-a87f-74402fb4e991ªfocus_kind¯chain_execution‹¢idÙ@ce821f265a3b2a4008ca9b8f9502cf85048194e01ea0bccc5cbf570dbe649b8d¢tsËAÚô£˜¨trace_id¤Q107­parent_run_idÙ$ed333059-2a43-43da-970e-770a9d8ef5c9¦run_idÙ$5afdd7a4-8fce-4da0-80a8-5aae6e45b09eªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ6Explain the difference between `invoke` and `ainvoke`.§contextÚ'When you call the `invoke` (or `ainvoke`) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application. 

Under the hood, it'll have `invoke` (or `ainvoke`) use the `stream` (or `astream`) method to generate its output. The result of the invocation will be the same as far as the code that was using `invoke` is concerned; however, while the chat model is being streamed, LangChain will take care of invoking `on_llm_new_token` events in LangChain's [callback system](/docs/concepts/callbacks). These callback events
allow LangGraph `stream`/`astream` and `astream_events` to surface the chat model's output in real-time.

Example:

```python
def node(state):
    ...
    # The code below uses the invoke method, but LangChain will 
    # automatically switch to streaming mode
    # when it detects that the overall 
    # application is being streamed.
    ai_message = model.invoke(state["messages"])
    ...

## Langchain asynchronous APIs

Many LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.

Typically, any method that may perform I/O operations (e.g., making API calls, reading files) will have an asynchronous counterpart.

In LangChain, async implementations are located in the same classes as their synchronous counterparts, with the asynchronous methods having an "a" prefix. For example, the synchronous `invoke` method has an asynchronous counterpart called `ainvoke`.

Many components of LangChain implement the [Runnable Interface](/docs/concepts/runnables), which includes support for asynchronous execution. This means that you can run Runnables asynchronously using the `await` keyword in Python.

```python
await some_runnable.ainvoke(some_input)
```

```python
await some_runnable.ainvoke(some_input)
```

Other components like [Embedding Models](/docs/concepts/embedding_models) and [VectorStore](/docs/concepts/vectorstores) that do not implement the [Runnable Interface](/docs/concepts/runnables) usually still follow the same rule and include the asynchronous version of method in the same class with an "a" prefix.

For example,

```python
await some_vectorstore.aadd_documents(documents)
```

Runnables created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel) can also be run asynchronously as they implement
the full [Runnable Interface](/docs/concepts/runnables).

For more information, please review the [API reference](https://python.langchain.com/api_reference/) for the specific component you are using.

## Delegation to sync methods

The result would be an `AIMessage` containing the model's response in natural language (e.g., "Hello!").
However, if we pass an input *relevant to the tool*, the model should choose to call it:

```python
result = llm_with_tools.invoke("What is 2 multiplied by 3?")
```

As before, the output `result` will be an `AIMessage`. 
But, if the tool was called, `result` will have a `tool_calls` attribute.
This attribute includes everything needed to execute the tool, including the tool name and input arguments:

```
result.tool_calls
{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'xxx', 'type': 'tool_call'}
```

For more details on usage, see our [how-to guides](/docs/how_to/#tools)!

## Tool execution

[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.

## "Auto-Streaming" Chat Models

LangChain simplifies streaming from [chat models](/docs/concepts/chat_models) by automatically enabling streaming mode in certain cases, even when youâ€™re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming `invoke` method but still want to stream the entire application, including intermediate results from the chat model.

### How It Works

When you call the `invoke` (or `ainvoke`) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q107®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:29e7cff2-8a31-ff14-a87f-74402fb4e991­checkpoint_nsÙ-generate:29e7cff2-8a31-ff14-a87f-74402fb4e991¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@3227cb3aaec200335726b4108d1db003a8db4e392e92ac67d8df38745ca8fb04¢tsËAÚô°y¨trace_idÙ$5afdd7a4-8fce-4da0-80a8-5aae6e45b09e­parent_run_idÙ$ed333059-2a43-43da-970e-770a9d8ef5c9¦run_idÙ$5afdd7a4-8fce-4da0-80a8-5aae6e45b09eªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ!messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Explain the difference between `invoke` and `ainvoke`. \nContext: When you call the `invoke` (or `ainvoke`) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application. \n\nUnder the hood, it\'ll have `invoke` (or `ainvoke`) use the `stream` (or `astream`) method to generate its output. The result of the invocation will be the same as far as the code that was using `invoke` is concerned; however, while the chat model is being streamed, LangChain will take care of invoking `on_llm_new_token` events in LangChain\'s [callback system](/docs/concepts/callbacks). These callback events\nallow LangGraph `stream`/`astream` and `astream_events` to surface the chat model\'s output in real-time.\n\nExample:\n\n```python\ndef node(state):\n    ...\n    # The code below uses the invoke method, but LangChain will \n    # automatically switch to streaming mode\n    # when it detects that the overall \n    # application is being streamed.\n    ai_message = model.invoke(state["messages"])\n    ...\n\n## Langchain asynchronous APIs\n\nMany LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.\n\nTypically, any method that may perform I/O operations (e.g., making API calls, reading files) will have an asynchronous counterpart.\n\nIn LangChain, async implementations are located in the same classes as their synchronous counterparts, with the asynchronous methods having an "a" prefix. For example, the synchronous `invoke` method has an asynchronous counterpart called `ainvoke`.\n\nMany components of LangChain implement the [Runnable Interface](/docs/concepts/runnables), which includes support for asynchronous execution. This means that you can run Runnables asynchronously using the `await` keyword in Python.\n\n```python\nawait some_runnable.ainvoke(some_input)\n```\n\n```python\nawait some_runnable.ainvoke(some_input)\n```\n\nOther components like [Embedding Models](/docs/concepts/embedding_models) and [VectorStore](/docs/concepts/vectorstores) that do not implement the [Runnable Interface](/docs/concepts/runnables) usually still follow the same rule and include the asynchronous version of method in the same class with an "a" prefix.\n\nFor example,\n\n```python\nawait some_vectorstore.aadd_documents(documents)\n```\n\nRunnables created using the [LangChain Expression Language (LCEL)](/docs/concepts/lcel) can also be run asynchronously as they implement\nthe full [Runnable Interface](/docs/concepts/runnables).\n\nFor more information, please review the [API reference](https://python.langchain.com/api_reference/) for the specific component you are using.\n\n## Delegation to sync methods\n\nThe result would be an `AIMessage` containing the model\'s response in natural language (e.g., "Hello!").\nHowever, if we pass an input *relevant to the tool*, the model should choose to call it:\n\n```python\nresult = llm_with_tools.invoke("What is 2 multiplied by 3?")\n```\n\nAs before, the output `result` will be an `AIMessage`. \nBut, if the tool was called, `result` will have a `tool_calls` attribute.\nThis attribute includes everything needed to execute the tool, including the tool name and input arguments:\n\n```\nresult.tool_calls\n{\'name\': \'multiply\', \'args\': {\'a\': 2, \'b\': 3}, \'id\': \'xxx\', \'type\': \'tool_call\'}\n```\n\nFor more details on usage, see our [how-to guides](/docs/how_to/#tools)!\n\n## Tool execution\n\n[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.\n\n## "Auto-Streaming" Chat Models\n\nLangChain simplifies streaming from [chat models](/docs/concepts/chat_models) by automatically enabling streaming mode in certain cases, even when youâ€™re not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming `invoke` method but still want to stream the entire application, including intermediate results from the chat model.\n\n### How It Works\n\nWhen you call the `invoke` (or `ainvoke`) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@55aea7ff47019f80102b9ba4983b8329221cacc99fe32f0cc7defaa633d942fe¢tsËAÚôÈ¨focus_msÍæ§payload‚¤role©assistant§contentÚ The `invoke` method is synchronous, while `ainvoke` is its asynchronous counterpart, allowing for non-blocking execution. Both methods can trigger streaming mode in LangChain if the application is set to stream, but `ainvoke` enables the use of the `await` keyword for asynchronous operations. Essentially, the choice between them depends on whether you need synchronous or asynchronous behavior in your application.ªfocus_kind¬exec_latency¨query_id¤Q107‰¢idÙ@1cbcbf530dd1da997ec7f726315f92477277e1544d87f4f4510ea7c0d6b6ff0b¢tsËAÚô×#¨trace_idÙ$ed333059-2a43-43da-970e-770a9d8ef5c9­parent_run_idÙ$1bcc0243-2af7-425d-8605-931a8ad7542b¦run_idÙ$ed333059-2a43-43da-970e-770a9d8ef5c9ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ The `invoke` method is synchronous, while `ainvoke` is its asynchronous counterpart, allowing for non-blocking execution. Both methods can trigger streaming mode in LangChain if the application is set to stream, but `ainvoke` enables the use of the `await` keyword for asynchronous operations. Essentially, the choice between them depends on whether you need synchronous or asynchronous behavior in your application.¨query_id¤Q107ªfocus_kind¶chain_execution_result‰¢idÙ@8c37fd6ca1f7e57e44e2fac8aedb4d44c3ecdc3289fbe726c438b96ad203793d¢tsËAÚôàÆ¨trace_idÙ$1bcc0243-2af7-425d-8605-931a8ad7542b­parent_run_idÀ¦run_idÙ$1bcc0243-2af7-425d-8605-931a8ad7542bªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ6Explain the difference between `invoke` and `ainvoke`.§context•‚´page_content_summaryÙËWhen you call the `invoke` (or `ainvoke`) method on a chat model, LangChain will automatically switch to streaming mode if it detects that you are trying to stream the overall application. 

Under the...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË## Langchain asynchronous APIs

Many LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.

Typically, any method that may perform I/O operations...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË```python
await some_runnable.ainvoke(some_input)
```

Other components like [Embedding Models](/docs/concepts/embedding_models) and [VectorStore](/docs/concepts/vectorstores) that do not implement th...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙËThe result would be an `AIMessage` containing the model's response in natural language (e.g., "Hello!").
However, if we pass an input *relevant to the tool*, the model should choose to call it:

```py...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙÍ## "Auto-Streaming" Chat Models

LangChain simplifies streaming from [chat models](/docs/concepts/chat_models) by automatically enabling streaming mode in certain cases, even when youâ€™re not explicitl...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¦answerÚ The `invoke` method is synchronous, while `ainvoke` is its asynchronous counterpart, allowing for non-blocking execution. Both methods can trigger streaming mode in LangChain if the application is set to stream, but `ainvoke` enables the use of the `await` keyword for asynchronous operations. Essentially, the choice between them depends on whether you need synchronous or asynchronous behavior in your application.¨query_id¤Q107ªfocus_kind¶chain_execution_result‹¢idÙ@0d4dc43b5bb86f28cafd72966353d47e55b6a085a3136e02fb4a04ce3430c4b8¢tsËAÚôí ¨trace_id¤Q108­parent_run_idÀ¦run_idÙ$841a7359-b875-4dfb-b910-beb7f5ab332eªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙLWhat does `batch_as_completed` return with each result to help match inputs?¨query_id¤Q108§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q108ªfocus_kind¯chain_execution‹¢idÙ@568b8a1ec3fe8bb9fa692bf670830dfcd4ef658527240320199b30d1a0349d34¢tsËAÚôÿ3¨trace_id¤Q108­parent_run_idÙ$841a7359-b875-4dfb-b910-beb7f5ab332e¦run_idÙ$619129aa-5486-48c1-9192-8ad3f4252ef3ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙLWhat does `batch_as_completed` return with each result to help match inputs?§context¦answer ¨query_id¤Q108¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q108®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:52b84936-19d4-d71b-5850-ee6f6feb0a1dªfocus_kind¯chain_execution‰¢idÙ@7b4530851efe6ac803d449db5b6441ee10ca0e930e0fbbed1a99f8228aea2a12¢tsËAÚô-¡{¨trace_idÙ$619129aa-5486-48c1-9192-8ad3f4252ef3­parent_run_idÙ$841a7359-b875-4dfb-b910-beb7f5ab332e¦run_idÙ$619129aa-5486-48c1-9192-8ad3f4252ef3ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for user...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q108ªfocus_kind¶chain_execution_result‹¢idÙ@3b170c1eb10c60db32bfdd8e7d9cadcc89df0818f3e554dc17943d180093e193¢tsËAÚô-®³¨trace_id¤Q108­parent_run_idÙ$841a7359-b875-4dfb-b910-beb7f5ab332e¦run_idÙ$b40024d5-5832-4eac-990f-0dd1050b1696ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙLWhat does `batch_as_completed` return with each result to help match inputs?§context•‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for user...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answer ¨query_id¤Q108¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q108®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:dde86287-6af6-2f5d-bba6-feb7840dd500ªfocus_kind¯chain_execution‹¢idÙ@2e7cc11c0fb32c9158638fa68819653f016459599e3749d8135fefe6f43ed772¢tsËAÚô-¶Œ¨trace_id¤Q108­parent_run_idÙ$b40024d5-5832-4eac-990f-0dd1050b1696¦run_idÙ$da712860-839c-4f19-8365-71322e40ee59ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙLWhat does `batch_as_completed` return with each result to help match inputs?§contextÚè<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to process multiple inputs in parallel.

Using these methods can significantly improve performance when needing to process multiple independent inputs, as the
processing can be done in parallel instead of sequentially.

The two batching options are:

* `batch`: Process multiple inputs in parallel, returning results in the same order as the inputs.
* `batch_as_completed`: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.

The default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution.

Some Runnables may provide their own implementations of `batch` and `batch_as_completed` that are optimized for their specific use case (e.g.,
rely on a `batch` API provided by a model provider).

:::note
The async versions of `abatch` and `abatch_as_completed` relies on asyncio's [gather](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather) and [as_completed](https://docs.python.org/3/library/asyncio-task.html#asyncio.as_completed) functions to run the `ainvoke` method in parallel.
:::

:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency` attribute in the `RunnableConfig` dictionary. See the [RunnableConfig](/docs/concepts/runnables/#runnableconfig) for more information.

Chat Models also have a built-in [rate limiter](/docs/concepts/chat_models#rate-limiting) that can be used to control the rate at which requests are made.
:::

### Asynchronous support
<span data-heading-keywords="async-api"></span>

Runnables expose an asynchronous API, allowing them to be called using the `await` syntax in Python. Asynchronous methods can be identified by the "a" prefix (e.g., `ainvoke`, `abatch`, `astream`, `abatch_as_completed`).

Please refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.

## Streaming APIs
<span data-heading-keywords="streaming-api"></span>

## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.
* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.
* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.
* Inspected: Schematic information about Runnable's input, output, and configuration can be accessed.
* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.

Please review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.

<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead to infinite recursion if not handled properly. To prevent this, you can set a `recursion_limit` in the `RunnableConfig` dictionary. This will limit the number of times a Runnable can recurse.

### Setting max concurrency

If using the `batch` or `batch_as_completed` methods, you can set the `max_concurrency` attribute in the `RunnableConfig` dictionary to control the maximum number of parallel calls to make. This can be useful when you want to limit the number of parallel calls to prevent overloading a server or API.


:::tip
If you're trying to rate limit the number of requests made by a **Chat Model**, you can use the built-in [rate limiter](/docs/concepts/chat_models#rate-limiting) instead of setting `max_concurrency`, which will be more effective.¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q108®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:dde86287-6af6-2f5d-bba6-feb7840dd500­checkpoint_nsÙ-generate:dde86287-6af6-2f5d-bba6-feb7840dd500¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@8a418c7cac9e73af27cb094b3eac7a5aa5e379f8735a171d654288153c9d5b89¢tsËAÚô-Ãµ¨trace_idÙ$da712860-839c-4f19-8365-71322e40ee59­parent_run_idÙ$b40024d5-5832-4eac-990f-0dd1050b1696¦run_idÙ$da712860-839c-4f19-8365-71322e40ee59ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚßmessages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: What does `batch_as_completed` return with each result to help match inputs? \nContext: <a id="batch"></a>\n### Optimized parallel execution (batch)\n<span data-heading-keywords="batch"></span>\n\nLangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to process multiple inputs in parallel.\n\nUsing these methods can significantly improve performance when needing to process multiple independent inputs, as the\nprocessing can be done in parallel instead of sequentially.\n\nThe two batching options are:\n\n* `batch`: Process multiple inputs in parallel, returning results in the same order as the inputs.\n* `batch_as_completed`: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.\n\nThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution.\n\nSome Runnables may provide their own implementations of `batch` and `batch_as_completed` that are optimized for their specific use case (e.g.,\nrely on a `batch` API provided by a model provider).\n\n:::note\nThe async versions of `abatch` and `abatch_as_completed` relies on asyncio\'s [gather](https://docs.python.org/3/library/asyncio-task.html#asyncio.gather) and [as_completed](https://docs.python.org/3/library/asyncio-task.html#asyncio.as_completed) functions to run the `ainvoke` method in parallel.\n:::\n\n:::tip\nWhen processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency` attribute in the `RunnableConfig` dictionary. See the [RunnableConfig](/docs/concepts/runnables/#runnableconfig) for more information.\n\nChat Models also have a built-in [rate limiter](/docs/concepts/chat_models#rate-limiting) that can be used to control the rate at which requests are made.\n:::\n\n### Asynchronous support\n<span data-heading-keywords="async-api"></span>\n\nRunnables expose an asynchronous API, allowing them to be called using the `await` syntax in Python. Asynchronous methods can be identified by the "a" prefix (e.g., `ainvoke`, `abatch`, `astream`, `abatch_as_completed`).\n\nPlease refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.\n\n## Streaming APIs\n<span data-heading-keywords="streaming-api"></span>\n\n## Overview of runnable interface\n\nThe Runnable way defines a standard interface that allows a Runnable component to be:\n\n* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input is transformed into an output.\n* [Batched](/docs/how_to/lcel_cheatsheet/#batch-a-runnable): Multiple inputs are efficiently transformed into outputs.\n* [Streamed](/docs/how_to/lcel_cheatsheet/#stream-a-runnable): Outputs are streamed as they are produced.\n* Inspected: Schematic information about Runnable\'s input, output, and configuration can be accessed.\n* Composed: Multiple Runnables can be composed to work together using [the LangChain Expression Language (LCEL)](/docs/concepts/lcel) to create complex pipelines.\n\nPlease review the [LCEL Cheatsheet](/docs/how_to/lcel_cheatsheet) for some common patterns that involve the Runnable interface and LCEL expressions.\n\n<a id="batch"></a>\n### Optimized parallel execution (batch)\n<span data-heading-keywords="batch"></span>\n\n# Do something with the run_id\n```\n\n### Setting recursion limit\n\n:::note\nThis is an advanced feature that is unnecessary for most users.\n:::\n\nSome Runnables may return other Runnables, which can lead to infinite recursion if not handled properly. To prevent this, you can set a `recursion_limit` in the `RunnableConfig` dictionary. This will limit the number of times a Runnable can recurse.\n\n### Setting max concurrency\n\nIf using the `batch` or `batch_as_completed` methods, you can set the `max_concurrency` attribute in the `RunnableConfig` dictionary to control the maximum number of parallel calls to make. This can be useful when you want to limit the number of parallel calls to prevent overloading a server or API.\n\n\n:::tip\nIf you\'re trying to rate limit the number of requests made by a **Chat Model**, you can use the built-in [rate limiter](/docs/concepts/chat_models#rate-limiting) instead of setting `max_concurrency`, which will be more effective. \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@2a9eacdaccbca377b5c62b785df48780d8059a528a79333e3a134e872c7cd5f6¢tsËAÚôÓ ¹¨focus_msÍ
§payload‚¤role©assistant§contentÙü`batch_as_completed` returns results as they complete, and each result includes the input index to help match it back to the original inputs. This allows users to identify which input corresponds to each result, even if the results arrive out of order.ªfocus_kind¬exec_latency¨query_id¤Q108‰¢idÙ@17cda2507393fbb5b908e3e2934d192abd2db0cf472364801b17e294b0adf6a0¢tsËAÚôÓs¨trace_idÙ$b40024d5-5832-4eac-990f-0dd1050b1696­parent_run_idÙ$841a7359-b875-4dfb-b910-beb7f5ab332e¦run_idÙ$b40024d5-5832-4eac-990f-0dd1050b1696ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙü`batch_as_completed` returns results as they complete, and each result includes the input index to help match it back to the original inputs. This allows users to identify which input corresponds to each result, even if the results arrive out of order.¨query_id¤Q108ªfocus_kind¶chain_execution_result‰¢idÙ@9395259e52f13b4cae088b915f2f003f0bce865f30fcbd1d07eb9029ee2f813b¢tsËAÚôÓü¨trace_idÙ$841a7359-b875-4dfb-b910-beb7f5ab332e­parent_run_idÀ¦run_idÙ$841a7359-b875-4dfb-b910-beb7f5ab332eªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙLWhat does `batch_as_completed` return with each result to help match inputs?§context•‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for user...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÙü`batch_as_completed` returns results as they complete, and each result includes the input index to help match it back to the original inputs. This allows users to identify which input corresponds to each result, even if the results arrive out of order.¨query_id¤Q108ªfocus_kind¶chain_execution_result‹¢idÙ@75e829e80519d2030344fc0d50921e5f502ee1ab86124a4fd386fbf21b0e469b¢tsËAÚôÓ!Õ¨trace_id¤Q109­parent_run_idÀ¦run_idÙ$31ae40e0-3197-4456-b468-58d468c7e12cªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ7Why might you choose Chroma over FAISS for prototyping?¨query_id¤Q109§context¦answer ¯serialized_reprÀ¤tags¨metadata¨query_id¤Q109ªfocus_kind¯chain_execution‹¢idÙ@fc0bb490801f46433c9b4ebfaf1764eeb45df6538d74652b0b72e317838ad1d6¢tsËAÚôÓ1w¨trace_id¤Q109­parent_run_idÙ$31ae40e0-3197-4456-b468-58d468c7e12c¦run_idÙ$cfae3d86-82bc-4ecb-8e86-4c3e07d02cc5ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ7Why might you choose Chroma over FAISS for prototyping?§context¦answer ¨query_id¤Q109¯serialized_reprÀ¤tags‘¬graph:step:1¨metadata†¨query_id¤Q109®langgraph_step®langgraph_node¨retrieve²langgraph_triggers‘²branch:to:retrieve®langgraph_path’­__pregel_pull¨retrieve·langgraph_checkpoint_nsÙ-retrieve:4fbce7cc-0487-c1c9-560c-70bfed499c4bªfocus_kind¯chain_execution‰¢idÙ@e5ae69895c17adbca0d61507924b01db800a6a5c4a419d165f32e1109286903a¢tsËAÚôİÃÅ¨trace_idÙ$cfae3d86-82bc-4ecb-8e86-4c3e07d02cc5­parent_run_idÙ$31ae40e0-3197-4456-b468-58d468c7e12c¦run_idÙ$cfae3d86-82bc-4ecb-8e86-4c3e07d02cc5ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËThere are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.


<Tabs>
  <TabIt...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙË:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## ...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË:::info[Further reading]

* See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙËWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal ma...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË| Name                                                                                                              | When to use                                           | Description               ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx¨query_id¤Q109ªfocus_kind¶chain_execution_result‹¢idÙ@c0fd8e6ee6c4dd332069c73b841bf5ed51114ad2d2f5165b67ccd0f05107942e¢tsËAÚôİÑ-¨trace_id¤Q109­parent_run_idÙ$31ae40e0-3197-4456-b468-58d468c7e12c¦run_idÙ$3d8569bc-ccb6-45c0-b7b6-9e1c8a51d048ªevent_type«chain_start¬event_source­Unknown Chain§payload‚¦inputs„¨questionÙ7Why might you choose Chroma over FAISS for prototyping?§context•‚´page_content_summaryÙËThere are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.


<Tabs>
  <TabIt...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙË:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## ...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË:::info[Further reading]

* See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙËWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal ma...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË| Name                                                                                                              | When to use                                           | Description               ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx¦answer ¨query_id¤Q109¯serialized_reprÀ¤tags‘¬graph:step:2¨metadata†¨query_id¤Q109®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:ae18cd3c-a7dc-bd24-0e8e-e069baac45a8ªfocus_kind¯chain_execution‹¢idÙ@747126c55a10a680dfd727dc9bf29eff4ace4dd64b4efcf535a92f77a02170df¢tsËAÚôİÛº¨trace_id¤Q109­parent_run_idÙ$3d8569bc-ccb6-45c0-b7b6-9e1c8a51d048¦run_idÙ$1497d3c5-d795-4df5-8d11-5630d3004d49ªevent_type«chain_start¬event_source²ChatPromptTemplate§payload‚¦inputs‚¨questionÙ7Why might you choose Chroma over FAISS for prototyping?§contextÚHThere are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.


<Tabs>
  <TabItem value="chroma" label="Chroma" default>

This walkthrough uses the `chroma` vector database, which runs on your local machine as a library.

```bash
pip install langchain-chroma
```

```python
from langchain_chroma import Chroma

db = Chroma.from_documents(documents, OpenAIEmbeddings())
```

  </TabItem>
  <TabItem value="faiss" label="FAISS">

This walkthrough uses the `FAISS` vector database, which makes use of the Facebook AI Similarity Search (FAISS) library.

```bash
pip install faiss-cpu
```

```python
from langchain_community.vectorstores import FAISS

db = FAISS.from_documents(documents, OpenAIEmbeddings())
```

  </TabItem>
  <TabItem value="lance" label="Lance">

:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## Conclusion

LangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:
- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.
- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).

:::info[Further reading]

* See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with embeddings.
* See Pinecone's [blog post](https://www.pinecone.io/learn/vector-similarity/) on similarity metrics.
* See OpenAI's [FAQ](https://platform.openai.com/docs/guides/embeddings/faq) on what similarity metric to use with OpenAI embeddings.

:::

### Similarity search

While algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.
As a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches. 
At the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.

| Name                                                                                                              | When to use                                           | Description                                                                                                                                  |
|-------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| [Hybrid search](/docs/integrations/retrievers/pinecone_hybrid_search/)                                            | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. [Paper](https://arxiv.org/abs/2210.11934). |¯serialized_repr§summaryÚ Serialized object too long, starts with: {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'ChatPromptTemplate'], 'kwargs': {'input_variables': ['context', 'question'], 'metadata': {'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, 'messages': [{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'prompts', 'chat', 'HumanMessagePromptTemplate'], 'kwargs': {'prompt': {'lc': 1, 'type': 'constructor', 'id': ['langchain', 'pro...¤tags‘ªseq:step:1¨metadataŠ¨query_id¤Q109®langgraph_step®langgraph_node¨generate²langgraph_triggers‘²branch:to:generate®langgraph_path’­__pregel_pull¨generate·langgraph_checkpoint_nsÙ-generate:ae18cd3c-a7dc-bd24-0e8e-e069baac45a8­checkpoint_nsÙ-generate:ae18cd3c-a7dc-bd24-0e8e-e069baac45a8¬lc_hub_owner£rlm«lc_hub_repoªrag-prompt²lc_hub_commit_hashÙ@50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6eªfocus_kind¯chain_execution‰¢idÙ@12b9ce96179f4c509f82618d6ce779b71f101bb6329941bb391777521eb6e358¢tsËAÚôİç‚¨trace_idÙ$1497d3c5-d795-4df5-8d11-5630d3004d49­parent_run_idÙ$3d8569bc-ccb6-45c0-b7b6-9e1c8a51d048¦run_idÙ$1497d3c5-d795-4df5-8d11-5630d3004d49ªevent_type¬chain_output¬event_source¥chain§payload§outputsÚ"messages=[HumanMessage(content='You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\'t know the answer, just say that you don\'t know. Use three sentences maximum and keep the answer concise.\nQuestion: Why might you choose Chroma over FAISS for prototyping? \nContext: There are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.\n\n\n<Tabs>\n  <TabItem value="chroma" label="Chroma" default>\n\nThis walkthrough uses the `chroma` vector database, which runs on your local machine as a library.\n\n```bash\npip install langchain-chroma\n```\n\n```python\nfrom langchain_chroma import Chroma\n\ndb = Chroma.from_documents(documents, OpenAIEmbeddings())\n```\n\n  </TabItem>\n  <TabItem value="faiss" label="FAISS">\n\nThis walkthrough uses the `FAISS` vector database, which makes use of the Facebook AI Similarity Search (FAISS) library.\n\n```bash\npip install faiss-cpu\n```\n\n```python\nfrom langchain_community.vectorstores import FAISS\n\ndb = FAISS.from_documents(documents, OpenAIEmbeddings())\n```\n\n  </TabItem>\n  <TabItem value="lance" label="Lance">\n\n:::info[Further reading]\n\nSee our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.\n\n:::\n\n## Conclusion\n\nLangChain offers standard interfaces for components that are central to many AI applications, which offers a few specific advantages:\n- **Ease of swapping providers:** It allows you to swap out different component providers without having to change the underlying code.\n- **Advanced features:** It provides common methods for more advanced features, such as [streaming](/docs/concepts/streaming) and [tool calling](/docs/concepts/tool_calling/).\n\n:::info[Further reading]\n\n* See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with embeddings.\n* See Pinecone\'s [blog post](https://www.pinecone.io/learn/vector-similarity/) on similarity metrics.\n* See OpenAI\'s [FAQ](https://platform.openai.com/docs/guides/embeddings/faq) on what similarity metric to use with OpenAI embeddings.\n\n:::\n\n### Similarity search\n\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\nFor example, [maximal marginal relevance](https://python.langchain.com/v0.1/docs/modules/model_io/prompts/example_selectors/mmr/) is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\nAs a second example, some [vector stores](/docs/integrations/retrievers/pinecone_hybrid_search/) offer built-in [hybrid-search](https://docs.pinecone.io/guides/data/understanding-hybrid-search) to combine keyword and semantic similarity search, which marries the benefits of both approaches. \nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with `similarity_search`.\n\n| Name                                                                                                              | When to use                                           | Description                                                                                                                                  |\n|-------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|\n| [Hybrid search](/docs/integrations/retrievers/pinecone_hybrid_search/)                                            | When combining keyword-based and semantic similarity. | Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. [Paper](https://arxiv.org/abs/2210.11934). | \nAnswer:', additional_kwargs={}, response_metadata={})]ªfocus_kind¶chain_execution_result†¢idÙ@6f38907c67d5b364eac8042fa26d7aae8312b1afa7e3bfed86d9cd8053b7d22e¢tsËAÚôPZ[¨focus_msÍû§payload‚¤role©assistant§contentÚyYou might choose Chroma over FAISS for prototyping because Chroma runs entirely on your local machine as a library, making it easier to set up and use without additional dependencies. It also allows for quick integration with LangChain, facilitating rapid development and testing. Additionally, being open-source and free, it can be more accessible for initial experimentation.ªfocus_kind¬exec_latency¨query_id¤Q109‰¢idÙ@25cda5e45ef5978502b156347890622b27ff4b79457afc0647d2001c50bee82f¢tsËAÚôPÎÑ¨trace_idÙ$3d8569bc-ccb6-45c0-b7b6-9e1c8a51d048­parent_run_idÙ$31ae40e0-3197-4456-b468-58d468c7e12c¦run_idÙ$3d8569bc-ccb6-45c0-b7b6-9e1c8a51d048ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚyYou might choose Chroma over FAISS for prototyping because Chroma runs entirely on your local machine as a library, making it easier to set up and use without additional dependencies. It also allows for quick integration with LangChain, facilitating rapid development and testing. Additionally, being open-source and free, it can be more accessible for initial experimentation.¨query_id¤Q109ªfocus_kind¶chain_execution_result‰¢idÙ@af86c445385c7e43ce7b13bbd50498936781e8cb0d8ae39ad813b62e3e62a96a¢tsËAÚôPßç¨trace_idÙ$31ae40e0-3197-4456-b468-58d468c7e12c­parent_run_idÀ¦run_idÙ$31ae40e0-3197-4456-b468-58d468c7e12cªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ7Why might you choose Chroma over FAISS for prototyping?§context•‚´page_content_summaryÙËThere are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.


<Tabs>
  <TabIt...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙË:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## ...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË:::info[Further reading]

* See [this documentation](https://developers.google.com/machine-learning/clustering/dnn-clustering/supervised-similarity) from Google on similarity metrics to consider with ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙËWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.
For example, [maximal ma...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË| Name                                                                                                              | When to use                                           | Description               ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx¦answerÚyYou might choose Chroma over FAISS for prototyping because Chroma runs entirely on your local machine as a library, making it easier to set up and use without additional dependencies. It also allows for quick integration with LangChain, facilitating rapid development and testing. Additionally, being open-source and free, it can be more accessible for initial experimentation.¨query_id¤Q109ªfocus_kind¶chain_execution_result