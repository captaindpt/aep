‰¢idÙ@f4ba04a42b65260b6a89b3508dd8a3d4c615117a417fea970efad1eb825bcbad¢tsËAÚÔS$lœ¨trace_idÙ$f769e53c-9d3e-4994-9d2c-358f7295d275­parent_run_idÙ$57236739-53e0-48a0-a5a9-92b10ccb62d9¦run_idÙ$f769e53c-9d3e-4994-9d2c-358f7295d275ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Why LangChain?

The goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.
While LangChain originally st...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË---
sidebar_position: 0
sidebar_class_name: hidden
---

# Introduction

**LangChain** is a framework for developing applications powered by large language models (LLMs).

LangChain simplifies every st...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/introduction.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## ...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¨query_id¤Q000ªfocus_kind¶chain_execution_result†¢idÙ@eefa56a51cb5b0605e5b9e3f3d387a4c14df2df65ff36dd6cc3f7c54369194d2¢tsËAÚÔS‰¨focus_msÍ)§payload‚¤role©assistant§contentÚ‚LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs). It was created to provide standardized interfaces for various components, making it easier for developers to build, optimize, and deploy AI applications. The goal is to streamline the application lifecycle and facilitate the integration of different models and tools.ªfocus_kind¬exec_latency¨query_id¤Q000‰¢idÙ@9761c5d43ce51dad88cab60b433dbd64d3d5ea00c2b1500a8d909881315b6243¢tsËAÚÔS‰“P¨trace_idÙ$5f7bfb1d-721a-47a5-b191-de533b0b707d­parent_run_idÙ$57236739-53e0-48a0-a5a9-92b10ccb62d9¦run_idÙ$5f7bfb1d-721a-47a5-b191-de533b0b707dªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ‚LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs). It was created to provide standardized interfaces for various components, making it easier for developers to build, optimize, and deploy AI applications. The goal is to streamline the application lifecycle and facilitate the integration of different models and tools.¨query_id¤Q000ªfocus_kind¶chain_execution_result‰¢idÙ@aff91ddf688be9ef8d75e385fb5e5e7e978354c8a7786a312e843b9ab61fc3b1¢tsËAÚÔS‰—§¨trace_idÙ$57236739-53e0-48a0-a5a9-92b10ccb62d9­parent_run_idÀ¦run_idÙ$57236739-53e0-48a0-a5a9-92b10ccb62d9ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ)What is LangChain and why was it created?§context•‚´page_content_summaryÙË# Why LangChain?

The goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.
While LangChain originally st...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË---
sidebar_position: 0
sidebar_class_name: hidden
---

# Introduction

**LangChain** is a framework for developing applications powered by large language models (LLMs).

LangChain simplifies every st...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/introduction.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## ...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answerÚ‚LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs). It was created to provide standardized interfaces for various components, making it easier for developers to build, optimize, and deploy AI applications. The goal is to streamline the application lifecycle and facilitate the integration of different models and tools.¨query_id¤Q000ªfocus_kind¶chain_execution_result‰¢idÙ@017504b60f771c7e3c555040f56de098300f39225a82a864f3426f4bc0f8b752¢tsËAÚÔS•B»¨trace_idÙ$9edfb54c-d589-4174-b8a8-88593a6a42d5­parent_run_idÙ$5bda6112-ebe8-4c99-bc87-a58cd39c7a42¦run_idÙ$9edfb54c-d589-4174-b8a8-88593a6a42d5ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Agents

By themselves, language models can't take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take ...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/agents.mdx‚´page_content_summaryÙË## Legacy agent concept: AgentExecutor

LangChain previously introduced the `AgentExecutor` as a runtime for agents. 
While it served as an excellent starting point, its limitations became apparent wh...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/agents.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## ...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¨query_id¤Q001ªfocus_kind¶chain_execution_result†¢idÙ@831bd8dbea76dcefcfd2cd3a42a1e068dfe6fbf94fc27532ccec4eec78c278b8¢tsËAÚÔT_á"¨focus_msÍ\§payload‚¤role©assistant§contentÚ¯In LangChain, an Agent acts as a system that takes a high-level task and utilizes a language model (LLM) as a reasoning engine to determine and execute the necessary actions. Agents enable more sophisticated interactions by orchestrating complex applications, and LangGraph is recommended for building customizable agents. The previous `AgentExecutor` is being phased out in favor of LangGraph for improved flexibility and control.ªfocus_kind¬exec_latency¨query_id¤Q001‰¢idÙ@b4463b31ee762b9bdc239c0a45137b564e7067a844e209bb5e7a1015e6a7fa25¢tsËAÚÔT_ø8¨trace_idÙ$cdec78d5-0218-4803-840c-b082df2c95d4­parent_run_idÙ$5bda6112-ebe8-4c99-bc87-a58cd39c7a42¦run_idÙ$cdec78d5-0218-4803-840c-b082df2c95d4ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ¯In LangChain, an Agent acts as a system that takes a high-level task and utilizes a language model (LLM) as a reasoning engine to determine and execute the necessary actions. Agents enable more sophisticated interactions by orchestrating complex applications, and LangGraph is recommended for building customizable agents. The previous `AgentExecutor` is being phased out in favor of LangGraph for improved flexibility and control.¨query_id¤Q001ªfocus_kind¶chain_execution_result‰¢idÙ@c93d6e99b548265761ea319d35f661f06e2fab2ad2e2b422577747166705d420¢tsËAÚÔT`,¨trace_idÙ$5bda6112-ebe8-4c99-bc87-a58cd39c7a42­parent_run_idÀ¦run_idÙ$5bda6112-ebe8-4c99-bc87-a58cd39c7a42ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ*Explain the role of an Agent in LangChain.§context•‚´page_content_summaryÙË# Agents

By themselves, language models can't take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take ...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/agents.mdx‚´page_content_summaryÙË## Legacy agent concept: AgentExecutor

LangChain previously introduced the `AgentExecutor` as a runtime for agents. 
While it served as an excellent starting point, its limitations became apparent wh...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/agents.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## ...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answerÚ¯In LangChain, an Agent acts as a system that takes a high-level task and utilizes a language model (LLM) as a reasoning engine to determine and execute the necessary actions. Agents enable more sophisticated interactions by orchestrating complex applications, and LangGraph is recommended for building customizable agents. The previous `AgentExecutor` is being phased out in favor of LangGraph for improved flexibility and control.¨query_id¤Q001ªfocus_kind¶chain_execution_result‰¢idÙ@93e6ca8edf9b95a7a6d23ae7cf96283fafed63c4e812020e7c8692eec6cda5c7¢tsËAÚÔTqä¨trace_idÙ$c791e8bd-2dbf-40d7-9848-b7d5d3407e82­parent_run_idÙ$33b3ae96-5440-40d5-9e28-b374e539659f¦run_idÙ$c791e8bd-2dbf-40d7-9848-b7d5d3407e82ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙÀ```bash
pip install -e .
```

LangGraph, LangSmith SDK, and certain integration packages live outside the main LangChain repo. You can see [all repos here](https://github.com/langchain-ai)....¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx‚´page_content_summaryÙË## Compatibility

LangChain is only compatible with the `asyncio` library, which is distributed as part of the Python standard library. It will not work with other async libraries like `trio` or `curi...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË---
sidebar_position: 2
---

# How to install LangChain packages

The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of
functionality to install.
...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx‚´page_content_summaryÙËAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:

```bash
pip install langchain-community
```

### LangChain experimen...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx‚´page_content_summaryÙË### LangChain core
The `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx¨query_id¤Q002ªfocus_kind¶chain_execution_result†¢idÙ@faf8fb563fbeeb1eaee0e17ef1e174dd59fe43670b6a7814ae137ec92013b0bf¢tsËAÚÔTÓ>Ñ¨focus_msÍü§payload‚¤role©assistant§contentÙ×To install LangChain with extras for tracing, you can use the following command: 

```bash
pip install "langserve[all]"
```

This will install both client and server dependencies necessary for tracing functionality.ªfocus_kind¬exec_latency¨query_id¤Q002‰¢idÙ@58fdaafce258401ed816172d6c9897a55102e988e07f35f730e0e619d2c97e16¢tsËAÚÔTÓH0¨trace_idÙ$88fe07b8-a8a7-444e-a040-4b63c1089351­parent_run_idÙ$33b3ae96-5440-40d5-9e28-b374e539659f¦run_idÙ$88fe07b8-a8a7-444e-a040-4b63c1089351ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙ×To install LangChain with extras for tracing, you can use the following command: 

```bash
pip install "langserve[all]"
```

This will install both client and server dependencies necessary for tracing functionality.¨query_id¤Q002ªfocus_kind¶chain_execution_result‰¢idÙ@47fa0086e04486278098ec4eea21c548c20b4bccfa81f52c9a69ca0ff551a4f6¢tsËAÚÔTÓN½¨trace_idÙ$33b3ae96-5440-40d5-9e28-b374e539659f­parent_run_idÀ¦run_idÙ$33b3ae96-5440-40d5-9e28-b374e539659fªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ3How do I install LangChain with extras for tracing?§context•‚´page_content_summaryÙÀ```bash
pip install -e .
```

LangGraph, LangSmith SDK, and certain integration packages live outside the main LangChain repo. You can see [all repos here](https://github.com/langchain-ai)....¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx‚´page_content_summaryÙË## Compatibility

LangChain is only compatible with the `asyncio` library, which is distributed as part of the Python standard library. It will not work with other async libraries like `trio` or `curi...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË---
sidebar_position: 2
---

# How to install LangChain packages

The LangChain ecosystem is split into different packages, which allow you to choose exactly which pieces of
functionality to install.
...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx‚´page_content_summaryÙËAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:

```bash
pip install langchain-community
```

### LangChain experimen...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx‚´page_content_summaryÙË### LangChain core
The `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx¦answerÙ×To install LangChain with extras for tracing, you can use the following command: 

```bash
pip install "langserve[all]"
```

This will install both client and server dependencies necessary for tracing functionality.¨query_id¤Q002ªfocus_kind¶chain_execution_result‰¢idÙ@3704f9d60899f2742181d80bd2613ca633a142b6bd296abe56865c81caf1a667¢tsËAÚÔTë‘¨trace_idÙ$a1b9b414-139e-43fc-a311-dcaba2b36ab0­parent_run_idÙ$b58bdc90-de56-45a8-af42-4cef6ac68d37¦run_idÙ$a1b9b414-139e-43fc-a311-dcaba2b36ab0ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËFor chains constructed using **LCEL**, the `.stream()` method only streams the output of the final step from the chain. This might be sufficient for some applications, but as you build more complex ch...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË# LangChain Expression Language (LCEL)

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
:::

The **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.

[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain a...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).

For guides on how to do specific tasks with LCEL, check...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¨query_id¤Q003ªfocus_kind¶chain_execution_result†¢idÙ@8c88ff3920832eb4fb4ba055a84c926d9b93010915b86f637fba42cdd2a331d3¢tsËAÚÔUP>l¨focus_msÍ,§payload‚¤role©assistant§contentÚTThe minimal LCEL chain that streams token-by-token can be created using the `.stream()` method, which streams the output of the final step in the chain. This allows for incremental output as the chain executes, optimizing the time-to-first-token. For more complex applications, consider using intermediate values alongside the final output.ªfocus_kind¬exec_latency¨query_id¤Q003‰¢idÙ@a9b3d8a272afeb2cf70a5b7c3347145ba77581ba029cfa519f244dea41a1cc0f¢tsËAÚÔUPb¨trace_idÙ$a5550cf0-aed2-4f96-8015-690231c622de­parent_run_idÙ$b58bdc90-de56-45a8-af42-4cef6ac68d37¦run_idÙ$a5550cf0-aed2-4f96-8015-690231c622deªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚTThe minimal LCEL chain that streams token-by-token can be created using the `.stream()` method, which streams the output of the final step in the chain. This allows for incremental output as the chain executes, optimizing the time-to-first-token. For more complex applications, consider using intermediate values alongside the final output.¨query_id¤Q003ªfocus_kind¶chain_execution_result‰¢idÙ@a50b9969fa6d00057dd49ea2b1a900eb100aa78c7ebaeb0d5006b757b9d87613¢tsËAÚÔUPn¨trace_idÙ$b58bdc90-de56-45a8-af42-4cef6ac68d37­parent_run_idÀ¦run_idÙ$b58bdc90-de56-45a8-af42-4cef6ac68d37ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ8Show the minimal LCEL chain that streams token-by-token.§context•‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËFor chains constructed using **LCEL**, the `.stream()` method only streams the output of the final step from the chain. This might be sufficient for some applications, but as you build more complex ch...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË# LangChain Expression Language (LCEL)

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
:::

The **L**ang**C**hain **E**xpression **L**anguage (LCEL) takes a [declarative](https...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË[**LCEL cheatsheet**](/docs/how_to/lcel_cheatsheet/): For a quick overview of how to use the main LCEL primitives.

[**Migration guide**](/docs/versions/migrating_chains): For migrating legacy chain a...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËIf you are currently using one of these legacy chains, please see [this guide for guidance on how to migrate](/docs/versions/migrating_chains).

For guides on how to do specific tasks with LCEL, check...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¦answerÚTThe minimal LCEL chain that streams token-by-token can be created using the `.stream()` method, which streams the output of the final step in the chain. This allows for incremental output as the chain executes, optimizing the time-to-first-token. For more complex applications, consider using intermediate values alongside the final output.¨query_id¤Q003ªfocus_kind¶chain_execution_result‰¢idÙ@9f14f77bf38afa0bef731e989fc290196980f15f8a11eea5c64e5c5ca97ce2c2¢tsËAÚÔU\ í¨trace_idÙ$c7ab9dcc-3e4e-492a-a1a9-be740f6a3a12­parent_run_idÙ$ab5ba18f-fcc3-4e62-8b17-27b159374afd¦run_idÙ$c7ab9dcc-3e4e-492a-a1a9-be740f6a3a12ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËThere are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.


<Tabs>
  <TabIt...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙË:::

### Similarity search

Given a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over *all* the embedded d...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË```python
from langchain_community.vectorstores import FAISS

db = FAISS.from_documents(documents, OpenAIEmbeddings())
```

  </TabItem>
  <TabItem value="lance" label="Lance">

This notebook shows ho...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙËThese vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.

Vector stores are frequently used to search over unstructured data, ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË:::

### Integrations

LangChain offers many embedding model integrations which you can find [on the embedding models](/docs/integrations/text_embedding/) integrations page.

## Measure similarity

Ea...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx¨query_id¤Q004ªfocus_kind¶chain_execution_result†¢idÙ@20c39e4ee9b6740bff42dd948c5566533c4139160fa958464537dac5fdfdd2f9¢tsËAÚÔUúgZ¨focus_msÍ	 §payload‚¤role©assistant§contentÚBFAISS and Chroma are both vector stores used in LangChain, but they have different underlying technologies. FAISS utilizes the Facebook AI Similarity Search library, while Chroma operates as a local library for vector storage. Both support a `similarity_search` method for retrieving similar documents based on embeddings.ªfocus_kind¬exec_latency¨query_id¤Q004‰¢idÙ@92f55ce9179e3d390ef23cd7386149c7ae159b78e0bde95b21b2ec35272f5409¢tsËAÚÔUúró¨trace_idÙ$48896dbb-e1ce-433e-87d1-9eea59b97fae­parent_run_idÙ$ab5ba18f-fcc3-4e62-8b17-27b159374afd¦run_idÙ$48896dbb-e1ce-433e-87d1-9eea59b97faeªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚBFAISS and Chroma are both vector stores used in LangChain, but they have different underlying technologies. FAISS utilizes the Facebook AI Similarity Search library, while Chroma operates as a local library for vector storage. Both support a `similarity_search` method for retrieving similar documents based on embeddings.¨query_id¤Q004ªfocus_kind¶chain_execution_result‰¢idÙ@13e56745401d26003b15d6e543d7664d7a946f8791119115b4c4545c1f737604¢tsËAÚÔUú~t¨trace_idÙ$ab5ba18f-fcc3-4e62-8b17-27b159374afd­parent_run_idÀ¦run_idÙ$ab5ba18f-fcc3-4e62-8b17-27b159374afdªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ4Compare FAISS and Chroma vector stores in LangChain.§context•‚´page_content_summaryÙËThere are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.


<Tabs>
  <TabIt...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙË:::

### Similarity search

Given a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over *all* the embedded d...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË```python
from langchain_community.vectorstores import FAISS

db = FAISS.from_documents(documents, OpenAIEmbeddings())
```

  </TabItem>
  <TabItem value="lance" label="Lance">

This notebook shows ho...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙËThese vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.

Vector stores are frequently used to search over unstructured data, ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË:::

### Integrations

LangChain offers many embedding model integrations which you can find [on the embedding models](/docs/integrations/text_embedding/) integrations page.

## Measure similarity

Ea...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx¦answerÚBFAISS and Chroma are both vector stores used in LangChain, but they have different underlying technologies. FAISS utilizes the Facebook AI Similarity Search library, while Chroma operates as a local library for vector storage. Both support a `similarity_search` method for retrieving similar documents based on embeddings.¨query_id¤Q004ªfocus_kind¶chain_execution_result‰¢idÙ@2e850460e1afa519058dff4101166c62f2b7a1475232c7c196e1d17db9f0e6c5¢tsËAÚÔVø0¨trace_idÙ$c4e4a79c-0474-486f-b8c6-552f72507833­parent_run_idÙ$e9f27997-25db-4752-b07d-4ac8767e0240¦run_idÙ$c4e4a79c-0474-486f-b8c6-552f72507833ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# How to load Microsoft Office files

The [Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, ...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader

[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning 
b...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË### Prerequisite

An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-s...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË# How to load JSON

[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and trans...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙË- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.
- **[Document loaders](/docs/concepts/document...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/index.mdx¨query_id¤Q005ªfocus_kind¶chain_execution_result†¢idÙ@c49acbe76df2cce4819db06904052c31835108efbd82a984a7ef22e145b795fd¢tsËAÚÔVfÜÒ¨focus_msÍ-§payload‚¤role©assistant§contentÚYTo load a Microsoft Word document into a LangChain pipeline, you can use the `AzureAIDocumentIntelligenceLoader`. First, ensure you have an Azure AI Document Intelligence resource set up, then use the loader with your file path, endpoint, and key to load the document. The code snippet provided in the context demonstrates how to implement this.ªfocus_kind¬exec_latency¨query_id¤Q005‰¢idÙ@c9b71c532e9af13890558fe70886c5e5de9527a4b60059bbbd1aa7ba83f99439¢tsËAÚÔVfå÷¨trace_idÙ$e5c8211b-f23e-4b57-b345-2118a495d4a4­parent_run_idÙ$e9f27997-25db-4752-b07d-4ac8767e0240¦run_idÙ$e5c8211b-f23e-4b57-b345-2118a495d4a4ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚYTo load a Microsoft Word document into a LangChain pipeline, you can use the `AzureAIDocumentIntelligenceLoader`. First, ensure you have an Azure AI Document Intelligence resource set up, then use the loader with your file path, endpoint, and key to load the document. The code snippet provided in the context demonstrates how to implement this.¨query_id¤Q005ªfocus_kind¶chain_execution_result‰¢idÙ@a69d943321211998dc9c894446a3721290230db4cdad545fe5ac92d83ebfd887¢tsËAÚÔVfëò¨trace_idÙ$e9f27997-25db-4752-b07d-4ac8767e0240­parent_run_idÀ¦run_idÙ$e9f27997-25db-4752-b07d-4ac8767e0240ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙCHow can I load a Microsoft Word document into a LangChain pipeline?§context•‚´page_content_summaryÙË# How to load Microsoft Office files

The [Microsoft Office](https://www.office.com/) suite of productivity software includes Microsoft Word, Microsoft Excel, Microsoft PowerPoint, Microsoft Outlook, ...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË## Loading DOCX, XLSX, PPTX with AzureAIDocumentIntelligenceLoader

[Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known as `Azure Form Recognizer`) is machine-learning 
b...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË### Prerequisite

An Azure AI Document Intelligence resource in one of the 3 preview regions: **East US**, **West US2**, **West Europe** - follow [this document](https://learn.microsoft.com/azure/ai-s...¨metadata¦sourceÙX/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_office_file.mdx‚´page_content_summaryÙË# How to load JSON

[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and trans...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙË- **[LangChain Expression Language (LCEL)](/docs/concepts/lcel)**: A syntax for orchestrating LangChain components. Most useful for simpler applications.
- **[Document loaders](/docs/concepts/document...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/index.mdx¦answerÚYTo load a Microsoft Word document into a LangChain pipeline, you can use the `AzureAIDocumentIntelligenceLoader`. First, ensure you have an Azure AI Document Intelligence resource set up, then use the loader with your file path, endpoint, and key to load the document. The code snippet provided in the context demonstrates how to implement this.¨query_id¤Q005ªfocus_kind¶chain_execution_result‰¢idÙ@5884a2edd54426b59e0ab0d3739386b6dbb409903064faf34e29f797d8661d1f¢tsËAÚÔVyøU¨trace_idÙ$a7ffae76-087b-4f3f-848b-455e68ae6274­parent_run_idÙ$8808db0d-880b-42e1-8911-662a55677ee3¦run_idÙ$a7ffae76-087b-4f3f-848b-455e68ae6274ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Relate...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

* `configurable_fields`: This method allows you to configure specific **attribute...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q006ªfocus_kind¶chain_execution_result†¢idÙ@a39cd14baf3ccfba08f928d17341aff84fa45603ef93ffb1ac44cf7448269110¢tsËAÚÔVĞò¨focus_msÍ?§payload‚¤role©assistant§contentÚÌThe Runnable protocol is a standard interface in LangChain that allows components to be invoked, batched, streamed, inspected, and composed, facilitating consistent interaction across various elements. Its advantages include efficient processing of multiple inputs, real-time output streaming, and the ability to create complex pipelines through composition. Additionally, it supports runtime configuration, enabling flexibility in how components are utilized.ªfocus_kind¬exec_latency¨query_id¤Q006‰¢idÙ@93f8863867b11ff0b19ba376bae6ee71acfd23708085d40ef182f58285701dbc¢tsËAÚÔVĞv¨trace_idÙ$f0c00f82-27d3-47a7-9e10-0ae85786ca2d­parent_run_idÙ$8808db0d-880b-42e1-8911-662a55677ee3¦run_idÙ$f0c00f82-27d3-47a7-9e10-0ae85786ca2dªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚÌThe Runnable protocol is a standard interface in LangChain that allows components to be invoked, batched, streamed, inspected, and composed, facilitating consistent interaction across various elements. Its advantages include efficient processing of multiple inputs, real-time output streaming, and the ability to create complex pipelines through composition. Additionally, it supports runtime configuration, enabling flexibility in how components are utilized.¨query_id¤Q006ªfocus_kind¶chain_execution_result‰¢idÙ@88591f7dd1c66b1ec91d4ae70c97f7f9def74da178bb2990bbd8b041ae15d603¢tsËAÚÔVĞô¨trace_idÙ$8808db0d-880b-42e1-8911-662a55677ee3­parent_run_idÀ¦run_idÙ$8808db0d-880b-42e1-8911-662a55677ee3ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ2Describe the Runnable protocol and its advantages.§context•‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Relate...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

* `configurable_fields`: This method allows you to configure specific **attribute...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚÌThe Runnable protocol is a standard interface in LangChain that allows components to be invoked, batched, streamed, inspected, and composed, facilitating consistent interaction across various elements. Its advantages include efficient processing of multiple inputs, real-time output streaming, and the ability to create complex pipelines through composition. Additionally, it supports runtime configuration, enabling flexibility in how components are utilized.¨query_id¤Q006ªfocus_kind¶chain_execution_result‰¢idÙ@fe4aa96de238a727e5cd83e280dd23a55069809cb31cf8b2766beaaa7f5a7ced¢tsËAÚÔVŞüe¨trace_idÙ$3de91934-641a-4371-9913-e1a5179c0e23­parent_run_idÙ$356140ba-3113-42be-b7a1-5e3495235b81¦run_idÙ$3de91934-641a-4371-9913-e1a5179c0e23ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Few-shot prompting

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
:::

## Overview

One of the most effective ways to improve model performance is to give a model examples of
w...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/few_shot_prompting.mdx‚´page_content_summaryÙËThese requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples onl...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/few_shot_prompting.mdx‚´page_content_summaryÙË### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)
:::

## Overview

One common prompting technique for...¨metadata¦sourceÙP/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/example_selectors.mdx‚´page_content_summaryÙË- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily ...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/index.mdx¨query_id¤Q007ªfocus_kind¶chain_execution_result†¢idÙ@fa647e6bd777db40ba868ad0376ce2ae9ea405aad31764e754a320a4c41f898c¢tsËAÚÔWY¨focus_msÍy§payload‚¤role©assistant§contentÚoFew-shot prompting is a technique that enhances model performance by including examples of desired inputs and outputs in the prompt. It is supported by example selectors, which dynamically choose and format relevant examples based on the task at hand. This approach allows models to better understand the expected behavior by providing concrete instances of the task.ªfocus_kind¬exec_latency¨query_id¤Q007‰¢idÙ@78ecc0534fe841dbdf877941792cbe9abf80f6579cd6c305792242debdd36683¢tsËAÚÔWY“¸¨trace_idÙ$9bec48df-5324-44c1-9c35-6ceb5c16a25b­parent_run_idÙ$356140ba-3113-42be-b7a1-5e3495235b81¦run_idÙ$9bec48df-5324-44c1-9c35-6ceb5c16a25bªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚoFew-shot prompting is a technique that enhances model performance by including examples of desired inputs and outputs in the prompt. It is supported by example selectors, which dynamically choose and format relevant examples based on the task at hand. This approach allows models to better understand the expected behavior by providing concrete instances of the task.¨query_id¤Q007ªfocus_kind¶chain_execution_result‰¢idÙ@86dc021b11a59e65de0bbc6ff73b00e7735f325ab6d20342692bb58c26b9bfd0¢tsËAÚÔWY¡<¨trace_idÙ$356140ba-3113-42be-b7a1-5e3495235b81­parent_run_idÀ¦run_idÙ$356140ba-3113-42be-b7a1-5e3495235b81ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ3What is Few-Shot prompting and how is it supported?§context•‚´page_content_summaryÙË# Few-shot prompting

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
:::

## Overview

One of the most effective ways to improve model performance is to give a model examples of
w...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/few_shot_prompting.mdx‚´page_content_summaryÙËThese requirements are model-specific and should be checked for the model you are using. If your model requires ToolMessages after tool calls and/or AIMessages after ToolMessages and your examples onl...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/few_shot_prompting.mdx‚´page_content_summaryÙË### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)
:::

## Overview

One common prompting technique for...¨metadata¦sourceÙP/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/example_selectors.mdx‚´page_content_summaryÙË- **[Output parsers](/docs/concepts/output_parsers)**: Responsible for taking the output of a model and transforming it into a more suitable format for downstream tasks. Output parsers were primarily ...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/index.mdx¦answerÚoFew-shot prompting is a technique that enhances model performance by including examples of desired inputs and outputs in the prompt. It is supported by example selectors, which dynamically choose and format relevant examples based on the task at hand. This approach allows models to better understand the expected behavior by providing concrete instances of the task.¨query_id¤Q007ªfocus_kind¶chain_execution_result‰¢idÙ@7a70dabca8282489f386b1d245ae5d1a7ec0bf88bbb589ddd3cca641a3026ef6¢tsËAÚÔWq£¨trace_idÙ$b1dabdc2-a35d-452a-8308-4429311c27f1­parent_run_idÙ$2a9275f3-0ce4-452b-a44e-551508bbcff9¦run_idÙ$b1dabdc2-a35d-452a-8308-4429311c27f1ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËWhile processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
    -...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙËThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

A full conversation often invo...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË# Chat history

:::info Prerequisites

- [Messages](/docs/concepts/messages)
- [Chat models](/docs/concepts/chat_models)
- [Tool calling](/docs/concepts/tool_calling)
:::

Chat history is a record of ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q008ªfocus_kind¶chain_execution_result†¢idÙ@296969a7975eff84119b651532b52d9c68e010c4bdb4fbad4fcd0287d39a14f1¢tsËAÚÔW» Ï¨focus_msÍŒ§payload‚¤role©assistant§contentÚ?An example of a memory object that stores chat history is a sequence of messages, each associated with roles like "user," "assistant," or "system." This structure helps maintain context and state throughout the conversation. Proper management of this chat history is essential to preserve the correct conversation flow.ªfocus_kind¬exec_latency¨query_id¤Q008‰¢idÙ@0c7b3e448aaa67db4ba7951375b9d163066e354591f84bf8b5dacc7b085bc779¢tsËAÚÔW»½4¨trace_idÙ$3c9775e3-7c97-46db-96a1-ef063a7dfc22­parent_run_idÙ$2a9275f3-0ce4-452b-a44e-551508bbcff9¦run_idÙ$3c9775e3-7c97-46db-96a1-ef063a7dfc22ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ?An example of a memory object that stores chat history is a sequence of messages, each associated with roles like "user," "assistant," or "system." This structure helps maintain context and state throughout the conversation. Proper management of this chat history is essential to preserve the correct conversation flow.¨query_id¤Q008ªfocus_kind¶chain_execution_result‰¢idÙ@357a7f49a2f8b1f253f2ac2d766181b2035332b7d5f7eaa75c11fb1b717d53f8¢tsËAÚÔW»Õ0¨trace_idÙ$2a9275f3-0ce4-452b-a44e-551508bbcff9­parent_run_idÀ¦run_idÙ$2a9275f3-0ce4-452b-a44e-551508bbcff9ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ<Give an example of a memory object that stores chat history.§context•‚´page_content_summaryÙËWhile processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
    -...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙËThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

A full conversation often invo...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË# Chat history

:::info Prerequisites

- [Messages](/docs/concepts/messages)
- [Chat models](/docs/concepts/chat_models)
- [Tool calling](/docs/concepts/tool_calling)
:::

Chat history is a record of ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚ?An example of a memory object that stores chat history is a sequence of messages, each associated with roles like "user," "assistant," or "system." This structure helps maintain context and state throughout the conversation. Proper management of this chat history is essential to preserve the correct conversation flow.¨query_id¤Q008ªfocus_kind¶chain_execution_result‰¢idÙ@94b8195b442f5dd6732a5c5923e571bfeba0b70a70e2dca31882c95a4d9be92d¢tsËAÚÔWÌw…¨trace_idÙ$f08c47b8-8bc9-4f2c-83f2-a1ae9d4c6502­parent_run_idÙ$1ff48e0b-d1d5-48df-9289-b40ce2cb35dc¦run_idÙ$f08c47b8-8bc9-4f2c-83f2-a1ae9d4c6502ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË## [LangGraph](https://langchain-ai.github.io/langgraph)

LangGraph is an extension of LangChain aimed at
building robust and stateful multi-actor applications with LLMs by modeling steps as edges and...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Tracing

<span data-heading-keywords="trace,tracing"></span>

A trace is essentially a series of steps that your application takes to go from input to output.
Traces contain individual steps called ...¨metadata¦sourceÙF/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tracing.mdx‚´page_content_summaryÙË### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process fr...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¨query_id¤Q009ªfocus_kind¶chain_execution_result†¢idÙ@1e57416918270a4b72a285b308f0973faa632f95306bea321fcbef746be94608¢tsËAÚÔX Ù/¨focus_msÍ%§payload‚¤role©assistant§contentÚ;To trace a LangGraph flow in LangSmith, first ensure that your application is integrated with LangSmith for observability. Then, utilize the tracing features to monitor the series of steps (or runs) your application takes from input to output. For detailed guidance, refer to the LangSmith documentation on tracing.ªfocus_kind¬exec_latency¨query_id¤Q009‰¢idÙ@a327fa9fb879e6947cef2920f8e8d203645d8c1127bef2a42a352c989916fbb4¢tsËAÚÔX ï‘¨trace_idÙ$8b988bd0-da53-4a7a-aa20-67ef37785a54­parent_run_idÙ$1ff48e0b-d1d5-48df-9289-b40ce2cb35dc¦run_idÙ$8b988bd0-da53-4a7a-aa20-67ef37785a54ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ;To trace a LangGraph flow in LangSmith, first ensure that your application is integrated with LangSmith for observability. Then, utilize the tracing features to monitor the series of steps (or runs) your application takes from input to output. For detailed guidance, refer to the LangSmith documentation on tracing.¨query_id¤Q009ªfocus_kind¶chain_execution_result‰¢idÙ@815393e6ffbade95764aac20c0c6595360308b37d601470b7c7dc88a8b3f995d¢tsËAÚÔX úH¨trace_idÙ$1ff48e0b-d1d5-48df-9289-b40ce2cb35dc­parent_run_idÀ¦run_idÙ$1ff48e0b-d1d5-48df-9289-b40ce2cb35dcªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ9Outline the steps to trace a LangGraph flow in LangSmith.§context•‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË## [LangGraph](https://langchain-ai.github.io/langgraph)

LangGraph is an extension of LangChain aimed at
building robust and stateful multi-actor applications with LLMs by modeling steps as edges and...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË# Tracing

<span data-heading-keywords="trace,tracing"></span>

A trace is essentially a series of steps that your application takes to go from input to output.
Traces contain individual steps called ...¨metadata¦sourceÙF/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tracing.mdx‚´page_content_summaryÙË### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process fr...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answerÚ;To trace a LangGraph flow in LangSmith, first ensure that your application is integrated with LangSmith for observability. Then, utilize the tracing features to monitor the series of steps (or runs) your application takes from input to output. For detailed guidance, refer to the LangSmith documentation on tracing.¨query_id¤Q009ªfocus_kind¶chain_execution_result‰¢idÙ@ccbfc2685cddfbfc9703fdc13f75b3403a2d795da78932f5b4a5df5aaf9a84ef¢tsËAÚÔX3®Ö¨trace_idÙ$1421b3f7-edda-464c-b750-51d84ec559db­parent_run_idÙ$efaee5e5-26e8-48bf-878f-b3a46d489b4a¦run_idÙ$1421b3f7-edda-464c-b750-51d84ec559dbªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# How to use LangChain with different Pydantic versions

As of the `0.3` release, LangChain uses Pydantic 2 internally. 

Users should install Pydantic 2 and are advised to **avoid** using the `pydant...¨metadata¦sourceÙR/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/pydantic_compatibility.md‚´page_content_summaryÙË## Compatibility

LangChain is only compatible with the `asyncio` library, which is distributed as part of the Python standard library. It will not work with other async libraries like `trio` or `curi...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË## Ecosystem packages

With the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base
classes and abstractions that other packages u...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx‚´page_content_summaryÙËAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:

```bash
pip install langchain-community
```

### LangChain experimen...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx‚´page_content_summaryÙË### LangChain core
The `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx¨query_id¤Q010ªfocus_kind¶chain_execution_result†¢idÙ@916182b8057548c3622f90a822a516c9f94b04410d90a0555d9280affdb1e0e5¢tsËAÚÔXEN¨focus_msÍ§payload‚¤role©assistant§contentÚETo use Pydantic v1 with LangChain, you should pin the version of Pydantic to v1.x. However, it is recommended to use Pydantic 2 instead, as LangChain is designed to work with it and advises against using the `pydantic.v1` namespace. For prior versions of LangChain, refer to the compatibility guide for specific instructions.ªfocus_kind¬exec_latency¨query_id¤Q010‰¢idÙ@a6332bd9c326be2cae8cfa796bbbf138f4a5a5c4679cdab6085bc33f0d2d70ec¢tsËAÚÔXWT¨trace_idÙ$4d2f6f0c-4356-47d3-b35e-612eced9503c­parent_run_idÙ$efaee5e5-26e8-48bf-878f-b3a46d489b4a¦run_idÙ$4d2f6f0c-4356-47d3-b35e-612eced9503cªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚETo use Pydantic v1 with LangChain, you should pin the version of Pydantic to v1.x. However, it is recommended to use Pydantic 2 instead, as LangChain is designed to work with it and advises against using the `pydantic.v1` namespace. For prior versions of LangChain, refer to the compatibility guide for specific instructions.¨query_id¤Q010ªfocus_kind¶chain_execution_result‰¢idÙ@d161284a7e1c25a073615845e6e5b9696ee24215490f62ea12463284e500009b¢tsËAÚÔXbØ¨trace_idÙ$efaee5e5-26e8-48bf-878f-b3a46d489b4a­parent_run_idÀ¦run_idÙ$efaee5e5-26e8-48bf-878f-b3a46d489b4aªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ?What packages must be pinned to use Pydantic v1 with LangChain?§context•‚´page_content_summaryÙË# How to use LangChain with different Pydantic versions

As of the `0.3` release, LangChain uses Pydantic 2 internally. 

Users should install Pydantic 2 and are advised to **avoid** using the `pydant...¨metadata¦sourceÙR/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/pydantic_compatibility.md‚´page_content_summaryÙË## Compatibility

LangChain is only compatible with the `asyncio` library, which is distributed as part of the Python standard library. It will not work with other async libraries like `trio` or `curi...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË## Ecosystem packages

With the exception of the `langsmith` SDK, all packages in the LangChain ecosystem depend on `langchain-core`, which contains base
classes and abstractions that other packages u...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx‚´page_content_summaryÙËAny integrations that haven't been split out into their own packages will live in the `langchain-community` package. Install with:

```bash
pip install langchain-community
```

### LangChain experimen...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx‚´page_content_summaryÙË### LangChain core
The `langchain-core` package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/installation.mdx¦answerÚETo use Pydantic v1 with LangChain, you should pin the version of Pydantic to v1.x. However, it is recommended to use Pydantic 2 instead, as LangChain is designed to work with it and advises against using the `pydantic.v1` namespace. For prior versions of LangChain, refer to the compatibility guide for specific instructions.¨query_id¤Q010ªfocus_kind¶chain_execution_result‰¢idÙ@0f7ab086b85f99d9f285517896266d1960ddd7a52972131ec06118dd076c2be9¢tsËAÚÔXµxF¨trace_idÙ$20612356-3628-4291-8cc3-9d25e268551c­parent_run_idÙ$f0288c11-4975-4a39-8731-c42e203131d6¦run_idÙ$20612356-3628-4291-8cc3-9d25e268551cªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËAt the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you
need OpenAI format for the output as well.

The [conve...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË**Example**: Testing the `convert_langchain_aimessage_to_dict` function to confirm it correctly converts an AI message to a dictionary format:

```python
from langchain_core.messages import AIMessage,...¨metadata¦sourceÙF/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/testing.mdx‚´page_content_summaryÙËFor example, a typical conversation structure might look like this:

1. **User Message**: "Hello, how are you?"
2. **Assistant Message**: "I'm doing well, thank you for asking."
3. **User Message**: "...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË## Key features

This highlights functionality that is core to using LangChain.

- [How to: return structured data from a model](/docs/how_to/structured_output/)
- [How to: use a model to call tools](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¨query_id¤Q011ªfocus_kind¶chain_execution_result†¢idÙ@041d4286412b9f127be63cdb50234d91f2134ed8f638fea84be4cf73dbfb0896¢tsËAÚÔY‚¨focus_msÍë§payload‚¤role©assistant§contentÚÔTool calling in LangChain via OpenAI function messages involves using a standard API to bind tools to models, allowing for the handling of tool call requests and sending results back to the model. The output is structured in LangChain messages, which can be converted to OpenAI format using the `convert_to_openai_messages` utility function if needed. This integration supports various chat model providers and facilitates efficient programming and structured outputs.ªfocus_kind¬exec_latency¨query_id¤Q011‰¢idÙ@8ab49b471b5ab7eace6b76a3fa5cc629450a324acdf22149a0160ee0c477ae39¢tsËAÚÔY‰¨trace_idÙ$d640696c-88b5-4e0e-8cd5-f0df81be17e1­parent_run_idÙ$f0288c11-4975-4a39-8731-c42e203131d6¦run_idÙ$d640696c-88b5-4e0e-8cd5-f0df81be17e1ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚÔTool calling in LangChain via OpenAI function messages involves using a standard API to bind tools to models, allowing for the handling of tool call requests and sending results back to the model. The output is structured in LangChain messages, which can be converted to OpenAI format using the `convert_to_openai_messages` utility function if needed. This integration supports various chat model providers and facilitates efficient programming and structured outputs.¨query_id¤Q011ªfocus_kind¶chain_execution_result‰¢idÙ@44f1896473d0fdb16ef9dc5b2128b44728af054fc94fa3583265c3256ae5121f¢tsËAÚÔYq¨trace_idÙ$f0288c11-4975-4a39-8731-c42e203131d6­parent_run_idÀ¦run_idÙ$f0288c11-4975-4a39-8731-c42e203131d6ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ?Explain tool calling via OpenAI function messages in LangChain.§context•‚´page_content_summaryÙË* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËAt the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you
need OpenAI format for the output as well.

The [conve...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË**Example**: Testing the `convert_langchain_aimessage_to_dict` function to confirm it correctly converts an AI message to a dictionary format:

```python
from langchain_core.messages import AIMessage,...¨metadata¦sourceÙF/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/testing.mdx‚´page_content_summaryÙËFor example, a typical conversation structure might look like this:

1. **User Message**: "Hello, how are you?"
2. **Assistant Message**: "I'm doing well, thank you for asking."
3. **User Message**: "...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË## Key features

This highlights functionality that is core to using LangChain.

- [How to: return structured data from a model](/docs/how_to/structured_output/)
- [How to: use a model to call tools](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answerÚÔTool calling in LangChain via OpenAI function messages involves using a standard API to bind tools to models, allowing for the handling of tool call requests and sending results back to the model. The output is structured in LangChain messages, which can be converted to OpenAI format using the `convert_to_openai_messages` utility function if needed. This integration supports various chat model providers and facilitates efficient programming and structured outputs.¨query_id¤Q011ªfocus_kind¶chain_execution_result‰¢idÙ@7e57a68205488963b0e232c5e37ef84a2714ea319d3818e0bab607bf1b1eda22¢tsËAÚÔY$=>¨trace_idÙ$26f7fecd-39df-45ed-9973-ecee9d82bec9­parent_run_idÙ$9f14952d-2380-4eb5-9459-a0a40d198fa7¦run_idÙ$26f7fecd-39df-45ed-9973-ecee9d82bec9ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Retrieval augmented generation (RAG)

:::info[Prerequisites]

* [Retrieval](/docs/concepts/retrieval/)

:::

## Overview

Retrieval Augmented Generation (RAG) is a powerful technique that enhances [...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/rag.mdx‚´page_content_summaryÙË- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.
- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in s...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/rag.mdx‚´page_content_summaryÙËAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):

```python
from langchain_openai impo...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/rag.mdx‚´page_content_summaryÙË### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)
- [How to: create a custom LL...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË### Serialization
- [How to: save and load LangChain objects](/docs/how_to/serialization)

## Use cases

These guides cover use-case specific details.

### Q&A with RAG

Retrieval Augmented Generation...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¨query_id¤Q012ªfocus_kind¶chain_execution_result†¢idÙ@c423a6d62988cb8dc8aa1b3fba4706b1861d59210b3105f80e8896e00f6f69d3¢tsËAÚÔY†l¨focus_msÍ÷§payload‚¤role©assistant§contentÚ Retrieval-Augmented Generation (RAG) in LangChain works by first receiving a query and then using a retrieval system to search for relevant information. This retrieved information is incorporated into the prompt sent to a language model, which generates a response based on the provided context. This approach allows RAG to utilize up-to-date information and reduce the likelihood of generating inaccurate responses.ªfocus_kind¬exec_latency¨query_id¤Q012‰¢idÙ@07db41cc4171040c745ffb86cf0223136e6f4d71f0313910626cb2d5e7721e1d¢tsËAÚÔY†Ò¨trace_idÙ$866bbdbf-1130-4e18-9ee0-546fef402f12­parent_run_idÙ$9f14952d-2380-4eb5-9459-a0a40d198fa7¦run_idÙ$866bbdbf-1130-4e18-9ee0-546fef402f12ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ Retrieval-Augmented Generation (RAG) in LangChain works by first receiving a query and then using a retrieval system to search for relevant information. This retrieved information is incorporated into the prompt sent to a language model, which generates a response based on the provided context. This approach allows RAG to utilize up-to-date information and reduce the likelihood of generating inaccurate responses.¨query_id¤Q012ªfocus_kind¶chain_execution_result‰¢idÙ@4a43baa65a812f5d99f95f0a070126bc1466d4c7863bcb2b314362cafb3011dc¢tsËAÚÔY†$)¨trace_idÙ$9f14952d-2380-4eb5-9459-a0a40d198fa7­parent_run_idÀ¦run_idÙ$9f14952d-2380-4eb5-9459-a0a40d198fa7ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ@How does Retrieval-Augmented Generation (RAG) work in LangChain?§context•‚´page_content_summaryÙË# Retrieval augmented generation (RAG)

:::info[Prerequisites]

* [Retrieval](/docs/concepts/retrieval/)

:::

## Overview

Retrieval Augmented Generation (RAG) is a powerful technique that enhances [...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/rag.mdx‚´page_content_summaryÙË- **Up-to-date information**: RAG can access and utilize the latest data, keeping responses current.
- **Domain-specific expertise**: With domain-specific knowledge bases, RAG can provide answers in s...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/rag.mdx‚´page_content_summaryÙËAs an example, here's a simple RAG workflow that passes information from a [retriever](/docs/concepts/retrievers/) to a [chat model](/docs/concepts/chat_models/):

```python
from langchain_openai impo...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/rag.mdx‚´page_content_summaryÙË### Custom

All of LangChain components can easily be extended to support your own versions.

- [How to: create a custom chat model class](/docs/how_to/custom_chat_model)
- [How to: create a custom LL...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË### Serialization
- [How to: save and load LangChain objects](/docs/how_to/serialization)

## Use cases

These guides cover use-case specific details.

### Q&A with RAG

Retrieval Augmented Generation...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answerÚ Retrieval-Augmented Generation (RAG) in LangChain works by first receiving a query and then using a retrieval system to search for relevant information. This retrieved information is incorporated into the prompt sent to a language model, which generates a response based on the provided context. This approach allows RAG to utilize up-to-date information and reduce the likelihood of generating inaccurate responses.¨query_id¤Q012ªfocus_kind¶chain_execution_result‰¢idÙ@7c3c5606a76b127698877f72a39704fc796505abe0e533798cf5584ccb83831f¢tsËAÚÔY—L¨trace_idÙ$f4d3891e-748b-4560-a5ed-14cf09de4f03­parent_run_idÙ$d7135103-8cbd-437f-a52d-93da0d307adb¦run_idÙ$f4d3891e-748b-4560-a5ed-14cf09de4f03ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËAt the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you
need OpenAI format for the output as well.

The [conve...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the [seminal BERT paper](https://arxiv.org/abs/1810.04805).
* See Cameron Wolfe's [excellent review](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vecto...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË- `embed_documents`: For embedding multiple texts (documents)
- `embed_query`: For embedding a single text (query)

This distinction is important, as some providers employ different embedding strategi...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË# Embedding models
<span data-heading-keywords="embedding,embeddings"></span>

:::info[Prerequisites]

* [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents....¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q013ªfocus_kind¶chain_execution_result†¢idÙ@c3f5bef74619649246c8bbd6ff96187bf854e0a2582e7bcba5298eb17352f030¢tsËAÚÔZÈï¨focus_msÍš§payload‚¤role©assistant§contentÚ[To create embeddings with OpenAI via LangChain, you can use the following code snippet:

```python
from langchain_openai import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings()
embeddings = embeddings_model.embed_documents(["Hi there!", "Oh, hello!", "What's your name?"])
```

This will generate embeddings for the provided list of strings.ªfocus_kind¬exec_latency¨query_id¤Q013‰¢idÙ@9b5b25e2dfbd832c612b05b2e013eafa325c59c7896c8aae77641435ec7c7a9e¢tsËAÚÔZÛ¨trace_idÙ$1611f355-17fc-4c4e-81bb-7b1e580496e3­parent_run_idÙ$d7135103-8cbd-437f-a52d-93da0d307adb¦run_idÙ$1611f355-17fc-4c4e-81bb-7b1e580496e3ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ[To create embeddings with OpenAI via LangChain, you can use the following code snippet:

```python
from langchain_openai import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings()
embeddings = embeddings_model.embed_documents(["Hi there!", "Oh, hello!", "What's your name?"])
```

This will generate embeddings for the provided list of strings.¨query_id¤Q013ªfocus_kind¶chain_execution_result‰¢idÙ@ce00f0623cef4efa900c7e01bb3c7d62797f97cc063d856ca6c2c971579f15d4¢tsËAÚÔZã¨trace_idÙ$d7135103-8cbd-437f-a52d-93da0d307adb­parent_run_idÀ¦run_idÙ$d7135103-8cbd-437f-a52d-93da0d307adbªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙFProvide a code snippet to create embeddings with OpenAI via LangChain.§context•‚´page_content_summaryÙËAt the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you
need OpenAI format for the output as well.

The [conve...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the [seminal BERT paper](https://arxiv.org/abs/1810.04805).
* See Cameron Wolfe's [excellent review](https://cameronrwolfe.substack.com/p/the-basics-of-ai-powered-vecto...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË- `embed_documents`: For embedding multiple texts (documents)
- `embed_query`: For embedding a single text (query)

This distinction is important, as some providers employ different embedding strategi...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË# Embedding models
<span data-heading-keywords="embedding,embeddings"></span>

:::info[Prerequisites]

* [Documents](https://python.langchain.com/api_reference/core/documents/langchain_core.documents....¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚ[To create embeddings with OpenAI via LangChain, you can use the following code snippet:

```python
from langchain_openai import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings()
embeddings = embeddings_model.embed_documents(["Hi there!", "Oh, hello!", "What's your name?"])
```

This will generate embeddings for the provided list of strings.¨query_id¤Q013ªfocus_kind¶chain_execution_result‰¢idÙ@fd253bc13b50cd4c970102f27bf0accae363d6380c66a0eaadb00c7ea6fa3ea0¢tsËAÚÔZ,,ö¨trace_idÙ$2fe05d70-5bad-4f70-ab62-aab6d7033783­parent_run_idÙ$02cb26dc-10d0-470a-89f2-2022295b549d¦run_idÙ$2fe05d70-5bad-4f70-ab62-aab6d7033783ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Callbacks

:::note Prerequisites
- [Runnable interface](/docs/concepts/runnables)
:::

LangChain provides a callback system that allows you to hook into the various stages of your LLM application. T...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/callbacks.mdx‚´page_content_summaryÙË| Event            | Event Trigger                               | Associated Method     |
|------------------|---------------------------------------------|-----------------------|
| Chat model start...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/callbacks.mdx‚´page_content_summaryÙË### Callbacks

[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application's execution.

- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_run...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË- **"values"**: Emit all values of the [state](https://langchain-ai.github.io/langgraph/concepts/low_level/) for each step.
- **"updates"**: Emit only the node name(s) and updates that were returned b...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË### Multimodal

- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)
- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)


### Agents

:::note

For in ...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¨query_id¤Q014ªfocus_kind¶chain_execution_result†¢idÙ@cdd6082abf117a97ccd86868f948f8450da3bdcad530979cdabc6a81b0299527¢tsËAÚÔZÂCE¨focus_msÍ	(§payload‚¤role©assistant§contentÙ£Three callback events emitted during an LLM call are: "Chat model start" (on_chat_model_start), "LLM start" (on_llm_start), and "LLM new token" (on_llm_new_token).ªfocus_kind¬exec_latency¨query_id¤Q014‰¢idÙ@492493eb98c68718d5a53c8ba733839afd0a768f4f98115727da0dc4064516b2¢tsËAÚÔZÂ[O¨trace_idÙ$26a8d31e-9872-462b-9044-534412f46c32­parent_run_idÙ$02cb26dc-10d0-470a-89f2-2022295b549d¦run_idÙ$26a8d31e-9872-462b-9044-534412f46c32ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙ£Three callback events emitted during an LLM call are: "Chat model start" (on_chat_model_start), "LLM start" (on_llm_start), and "LLM new token" (on_llm_new_token).¨query_id¤Q014ªfocus_kind¶chain_execution_result‰¢idÙ@dd05dd5ecf0709a4100e6f3009ccd8e73aa1b1a0f1845538d4d26ee51a466b09¢tsËAÚÔZÂs`¨trace_idÙ$02cb26dc-10d0-470a-89f2-2022295b549d­parent_run_idÀ¦run_idÙ$02cb26dc-10d0-470a-89f2-2022295b549dªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ6List three callback events emitted during an LLM call.§context•‚´page_content_summaryÙË# Callbacks

:::note Prerequisites
- [Runnable interface](/docs/concepts/runnables)
:::

LangChain provides a callback system that allows you to hook into the various stages of your LLM application. T...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/callbacks.mdx‚´page_content_summaryÙË| Event            | Event Trigger                               | Associated Method     |
|------------------|---------------------------------------------|-----------------------|
| Chat model start...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/callbacks.mdx‚´page_content_summaryÙË### Callbacks

[Callbacks](/docs/concepts/callbacks) allow you to hook into the various stages of your LLM application's execution.

- [How to: pass in callbacks at runtime](/docs/how_to/callbacks_run...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË- **"values"**: Emit all values of the [state](https://langchain-ai.github.io/langgraph/concepts/low_level/) for each step.
- **"updates"**: Emit only the node name(s) and updates that were returned b...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË### Multimodal

- [How to: pass multimodal data directly to models](/docs/how_to/multimodal_inputs/)
- [How to: use multimodal prompts](/docs/how_to/multimodal_prompts/)


### Agents

:::note

For in ...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answerÙ£Three callback events emitted during an LLM call are: "Chat model start" (on_chat_model_start), "LLM start" (on_llm_start), and "LLM new token" (on_llm_new_token).¨query_id¤Q014ªfocus_kind¶chain_execution_result‰¢idÙ@6d11c62fec578b1399f56c8da9a89f0822ef6548b4547596c568a97e777b5d4b¢tsËAÚÔZÏÊ:¨trace_idÙ$6166339a-022d-4921-982f-fa53c6f063e6­parent_run_idÙ$acb44514-2b01-4763-927b-50b99f64064c¦run_idÙ$6166339a-022d-4921-982f-fa53c6f063e6ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË```python
await some_runnable.ainvoke(some_input)
```

Other components like [Embedding Models](/docs/concepts/embedding_models) and [VectorStore](/docs/concepts/vectorstores) that do not implement th...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Relate...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q015ªfocus_kind¶chain_execution_result†¢idÙ@822309597eba67fa67742a89a28b88784cd55c56096b217e04fba6548aeaaa1f¢tsËAÚÔ[h¶¨focus_msÍÛ§payload‚¤role©assistant§contentÚ›The AsyncRunnable interface is used to enable asynchronous execution of tasks within LangChain components, allowing for efficient handling of multiple requests concurrently. This is particularly beneficial in server environments where reducing latency and improving throughput is essential. By using AsyncRunnable, developers can leverage parallel execution and ensure that their applications remain responsive.ªfocus_kind¬exec_latency¨query_id¤Q015‰¢idÙ@4040e825d9f97ebf465765bb8995b769324858a866442b95474a90ca623bd830¢tsËAÚÔ[t\¨trace_idÙ$bb2e6c3a-01c3-428d-972e-13d040845b68­parent_run_idÙ$acb44514-2b01-4763-927b-50b99f64064c¦run_idÙ$bb2e6c3a-01c3-428d-972e-13d040845b68ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ›The AsyncRunnable interface is used to enable asynchronous execution of tasks within LangChain components, allowing for efficient handling of multiple requests concurrently. This is particularly beneficial in server environments where reducing latency and improving throughput is essential. By using AsyncRunnable, developers can leverage parallel execution and ensure that their applications remain responsive.¨query_id¤Q015ªfocus_kind¶chain_execution_result‰¢idÙ@94e03b8f31553933e8fe4812e8bdd627ff806413faba1ac0457f106cba1f1ea5¢tsËAÚÔ[…w¨trace_idÙ$acb44514-2b01-4763-927b-50b99f64064c­parent_run_idÀ¦run_idÙ$acb44514-2b01-4763-927b-50b99f64064cªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ.Why would you use the AsyncRunnable interface?§context•‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË```python
await some_runnable.ainvoke(some_input)
```

Other components like [Embedding Models](/docs/concepts/embedding_models) and [VectorStore](/docs/concepts/vectorstores) that do not implement th...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.

:::info Relate...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚ›The AsyncRunnable interface is used to enable asynchronous execution of tasks within LangChain components, allowing for efficient handling of multiple requests concurrently. This is particularly beneficial in server environments where reducing latency and improving throughput is essential. By using AsyncRunnable, developers can leverage parallel execution and ensure that their applications remain responsive.¨query_id¤Q015ªfocus_kind¶chain_execution_result‰¢idÙ@b356624f93e9c1d54f654ee0937313ceb6ee21c3cc58aa06c01c7e3344b3eb87¢tsËAÚÔ[4qÎ¨trace_idÙ$b13b2b39-1120-420a-8757-8d227d6f2f13­parent_run_idÙ$487c45ce-ba0b-4693-b5b6-eea4d7487711¦run_idÙ$b13b2b39-1120-420a-8757-8d227d6f2f13ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing t...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/evaluation.mdx‚´page_content_summaryÙË### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process fr...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË:::

### Integrations

LangChain offers many embedding model integrations which you can find [on the embedding models](/docs/integrations/text_embedding/) integrations page.

## Measure similarity

Ea...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build compl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¨query_id¤Q016ªfocus_kind¶chain_execution_result†¢idÙ@17d650f8e5bcf62c96cc51afe1cf72a638d4b19cc64bad4c8379f15d9a132477¢tsËAÚÔ[¯¯¼¨focus_msÍ„§payload‚¤role©assistant§contentÚThe retrieved context does not specify the exact metrics that LangChain's evaluation module computes out-of-the-box. It mentions that LangSmith provides an evaluation framework to define metrics and run evaluations, but does not list specific metrics. Therefore, I don't know the answer.ªfocus_kind¬exec_latency¨query_id¤Q016‰¢idÙ@01f58fa3011a742d42af01795229936f05124377a8c484134382ec4b1dec8f5b¢tsËAÚÔ[¯Î¨trace_idÙ$eb690241-c50e-4d01-a041-49cac733b8c2­parent_run_idÙ$487c45ce-ba0b-4693-b5b6-eea4d7487711¦run_idÙ$eb690241-c50e-4d01-a041-49cac733b8c2ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚThe retrieved context does not specify the exact metrics that LangChain's evaluation module computes out-of-the-box. It mentions that LangSmith provides an evaluation framework to define metrics and run evaluations, but does not list specific metrics. Therefore, I don't know the answer.¨query_id¤Q016ªfocus_kind¶chain_execution_result‰¢idÙ@c5d4559e177ee5bf0b38ad787fed4e210afd27b4a05eb4e1ad453a9bf0f4cb6d¢tsËAÚÔ[¯ßÚ¨trace_idÙ$487c45ce-ba0b-4693-b5b6-eea4d7487711­parent_run_idÀ¦run_idÙ$487c45ce-ba0b-4693-b5b6-eea4d7487711ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙCWhat metrics does LangChain's evaluation module compute out-of-box?§context•‚´page_content_summaryÙË## LangSmith

LangSmith allows you to closely trace, monitor and evaluate your LLM application.
It seamlessly integrates with LangChain, and you can use it to inspect and debug individual steps of you...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË# Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.
It involves testing t...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/evaluation.mdx‚´page_content_summaryÙË### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process fr...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË:::

### Integrations

LangChain offers many embedding model integrations which you can find [on the embedding models](/docs/integrations/text_embedding/) integrations page.

## Measure similarity

Ea...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/embedding_models.mdx‚´page_content_summaryÙË:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build compl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answerÚThe retrieved context does not specify the exact metrics that LangChain's evaluation module computes out-of-the-box. It mentions that LangSmith provides an evaluation framework to define metrics and run evaluations, but does not list specific metrics. Therefore, I don't know the answer.¨query_id¤Q016ªfocus_kind¶chain_execution_result‰¢idÙ@46ea19db103dfbdbca72ef795a67e11c31e05173b14c40478846acf1c43bf36a¢tsËAÚÔ[Åœ¨trace_idÙ$80325cc8-b661-4547-a0c1-492ca52159dd­parent_run_idÙ$28571e30-80ec-4f98-aa02-6cf2718b39b5¦run_idÙ$80325cc8-b661-4547-a0c1-492ca52159ddªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË:::

### Text-structured based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. 
We can leverage this inherent structure to inform our splitting strategy,...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙËExample implementation using LangChain's `CharacterTextSplitter` with token-based splitting:

```python
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË# Text splitters
<span data-heading-keywords="text splitter,text splitting"></span>

:::info[Prerequisites]

* [Documents](/docs/concepts/retrievers/#interface)
* Tokenization(/docs/concepts/tokens)
:...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the how-to guide for [recursive text splitting](/docs/how_to/recursive_text_splitter/).

:::

### Document-structured based

Some documents have an inherent structure, ...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙËNow, the next question is *how* to split the documents into chunks! There are several strategies, each with its own advantages.

:::info[Further reading]
* See Greg Kamradt's [chunkviz](https://chunkv...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx¨query_id¤Q017ªfocus_kind¶chain_execution_result†¢idÙ@095b1875baf2feec0d56945c8055d3bccf7158af225360107174b0335a04beb2¢tsËAÚÔ\0÷Œ¨focus_msÍŒ§payload‚¤role©assistant§contentÚàTo chunk text using `RecursiveCharacterTextSplitter`, you can follow this example: 

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)
texts = text_splitter.split_text(document)
```

This code initializes the text splitter with a specified chunk size and overlap, then splits the provided document into manageable chunks while preserving the natural structure of the text.ªfocus_kind¬exec_latency¨query_id¤Q017‰¢idÙ@319ff25986ea0486122dc78083d041d29b1b4169e1ef1bfe5f30e6d196103cd3¢tsËAÚÔ\12¨trace_idÙ$8d62d9ad-f7ab-438b-a124-13914beaf10d­parent_run_idÙ$28571e30-80ec-4f98-aa02-6cf2718b39b5¦run_idÙ$8d62d9ad-f7ab-438b-a124-13914beaf10dªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚàTo chunk text using `RecursiveCharacterTextSplitter`, you can follow this example: 

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)
texts = text_splitter.split_text(document)
```

This code initializes the text splitter with a specified chunk size and overlap, then splits the provided document into manageable chunks while preserving the natural structure of the text.¨query_id¤Q017ªfocus_kind¶chain_execution_result‰¢idÙ@6fbf8146a6da336ccbc25fce9927d537fa22aa47a04bfa64bccecc06330ff953¢tsËAÚÔ\1İ¨trace_idÙ$28571e30-80ec-4f98-aa02-6cf2718b39b5­parent_run_idÀ¦run_idÙ$28571e30-80ec-4f98-aa02-6cf2718b39b5ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙCDemonstrate how to chunk text using RecursiveCharacterTextSplitter.§context•‚´page_content_summaryÙË:::

### Text-structured based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. 
We can leverage this inherent structure to inform our splitting strategy,...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙËExample implementation using LangChain's `CharacterTextSplitter` with token-based splitting:

```python
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË# Text splitters
<span data-heading-keywords="text splitter,text splitting"></span>

:::info[Prerequisites]

* [Documents](/docs/concepts/retrievers/#interface)
* Tokenization(/docs/concepts/tokens)
:...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË:::info[Further reading]

* See the how-to guide for [recursive text splitting](/docs/how_to/recursive_text_splitter/).

:::

### Document-structured based

Some documents have an inherent structure, ...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙËNow, the next question is *how* to split the documents into chunks! There are several strategies, each with its own advantages.

:::info[Further reading]
* See Greg Kamradt's [chunkviz](https://chunkv...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx¦answerÚàTo chunk text using `RecursiveCharacterTextSplitter`, you can follow this example: 

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)
texts = text_splitter.split_text(document)
```

This code initializes the text splitter with a specified chunk size and overlap, then splits the provided document into manageable chunks while preserving the natural structure of the text.¨query_id¤Q017ªfocus_kind¶chain_execution_result‰¢idÙ@8ebb92179415993d89df11dce46f5620963f03bfaaf94f0402a210038dfb0658¢tsËAÚÔ\D
¨trace_idÙ$ee55c58b-82f1-422c-8834-824e307a82b3­parent_run_idÙ$e9e2a52f-a399-4afb-83ae-3bbdda5930ef¦run_idÙ$ee55c58b-82f1-422c-8834-824e307a82b3ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## langchain

The main `langchain` package contains chains and retrieval strategies that make up an application's cognitive architecture.
These are NOT third-party integrations.
All chains, agents, an...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/architecture.mdx‚´page_content_summaryÙË| `stop`         | Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.                    ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Async programming with langchain

:::info Prerequisites
* [Runnable interface](/docs/concepts/runnables)
* [asyncio](https://docs.python.org/3/library/asyncio.html)
:::

LLM based applications often...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Langchain asynchronous APIs

Many LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.

Typically, any method that may perform I/O operations...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx¨query_id¤Q018ªfocus_kind¶chain_execution_result†¢idÙ@ae4ebf99fc376327c4f81f505b4eafa6904b30d577deff002bd43577435c00ac¢tsËAÚÔ\“§.¨focus_msÍÚ§payload‚¤role©assistant§contentÙøThe environment variable that controls OpenAI API retries in LangChain is `max_retries`. This variable specifies the maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.ªfocus_kind¬exec_latency¨query_id¤Q018‰¢idÙ@a33b297c523c6ac5887a68d3fa598c1f455fb915ff3751048fe3723efa1ec077¢tsËAÚÔ\“´p¨trace_idÙ$e7178711-55d1-4970-95ea-890b4867e7be­parent_run_idÙ$e9e2a52f-a399-4afb-83ae-3bbdda5930ef¦run_idÙ$e7178711-55d1-4970-95ea-890b4867e7beªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙøThe environment variable that controls OpenAI API retries in LangChain is `max_retries`. This variable specifies the maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.¨query_id¤Q018ªfocus_kind¶chain_execution_result‰¢idÙ@70e06601b598a49917b154e1afcb9c14948dc4f0257c3c7eb489a1c9e40a987a¢tsËAÚÔ\“¹ş¨trace_idÙ$e9e2a52f-a399-4afb-83ae-3bbdda5930ef­parent_run_idÀ¦run_idÙ$e9e2a52f-a399-4afb-83ae-3bbdda5930efªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙCWhat environment variable controls OpenAI API retries in LangChain?§context•‚´page_content_summaryÙË## langchain

The main `langchain` package contains chains and retrieval strategies that make up an application's cognitive architecture.
These are NOT third-party integrations.
All chains, agents, an...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/architecture.mdx‚´page_content_summaryÙË| `stop`         | Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.                    ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Async programming with langchain

:::info Prerequisites
* [Runnable interface](/docs/concepts/runnables)
* [asyncio](https://docs.python.org/3/library/asyncio.html)
:::

LLM based applications often...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Langchain asynchronous APIs

Many LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.

Typically, any method that may perform I/O operations...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx¦answerÙøThe environment variable that controls OpenAI API retries in LangChain is `max_retries`. This variable specifies the maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.¨query_id¤Q018ªfocus_kind¶chain_execution_result‰¢idÙ@64b54769180f423450ff72106a82ad9b63c5fbda5f870ae3934cd344e2e8c78d¢tsËAÚÔ\£Ì>¨trace_idÙ$312ffbae-313f-4693-bd5a-5a9916fd8c42­parent_run_idÙ$0f0e5cb6-e64e-4aa7-82a0-ae4b624ad6a8¦run_idÙ$312ffbae-313f-4693-bd5a-5a9916fd8c42ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Usage

The key-value store interface in LangChain is used primarily for:

1. Caching [embeddings](/docs/concepts/embedding_models) via [CachedBackedEmbeddings](https://python.langchain.com/api_refe...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/key_value_stores.mdx‚´page_content_summaryÙË# Key-value stores

## Overview

LangChain provides a key-value store interface for storing and retrieving data.

LangChain includes a [`BaseStore`](https://python.langchain.com/api_reference/core/sto...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/key_value_stores.mdx‚´page_content_summaryÙËFor chains constructed using **LCEL**, the `.stream()` method only streams the output of the final step from the chain. This might be sufficient for some applications, but as you build more complex ch...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË* [How to cache embeddings guide](/docs/how_to/caching_embeddings/).
* [How to retriever using multiple vectors per document](/docs/how_to/custom_retriever/).

## Interface

All [`BaseStores`](https:/...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/key_value_stores.mdx¨query_id¤Q019ªfocus_kind¶chain_execution_result†¢idÙ@e665e383906c573cf0a77f2df51156d83854d7ba0933a4c81d765b145e32b89c¢tsËAÚÔ]ˆùı¨focus_msÍû§payload‚¤role©assistant§contentÚ—Key-value stores can be used to cache intermediate outputs in LangChain by storing the results of each step in a chain, allowing for efficient retrieval without recomputation. This is particularly useful in complex chains where intermediate values may be needed alongside the final output. The `BaseStore` interface facilitates the storage and retrieval of these outputs, simplifying the management of data.ªfocus_kind¬exec_latency¨query_id¤Q019‰¢idÙ@0e072dbf5ad800f8e8a4b0058a8415fdaf11664ca3a5278813b190a3f391a9d4¢tsËAÚÔ]‰
À¨trace_idÙ$379d9f18-374a-4249-85e0-fb8cc1a592ff­parent_run_idÙ$0f0e5cb6-e64e-4aa7-82a0-ae4b624ad6a8¦run_idÙ$379d9f18-374a-4249-85e0-fb8cc1a592ffªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ—Key-value stores can be used to cache intermediate outputs in LangChain by storing the results of each step in a chain, allowing for efficient retrieval without recomputation. This is particularly useful in complex chains where intermediate values may be needed alongside the final output. The `BaseStore` interface facilitates the storage and retrieval of these outputs, simplifying the management of data.¨query_id¤Q019ªfocus_kind¶chain_execution_result‰¢idÙ@67dc0b729199f2ac1216f987749c676dd75410f3c33e97df552d6d0900697c0b¢tsËAÚÔ]‰D¨trace_idÙ$0f0e5cb6-e64e-4aa7-82a0-ae4b624ad6a8­parent_run_idÀ¦run_idÙ$0f0e5cb6-e64e-4aa7-82a0-ae4b624ad6a8ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙDHow can key-value stores be used to cache intermediate chain output?§context•‚´page_content_summaryÙË## Usage

The key-value store interface in LangChain is used primarily for:

1. Caching [embeddings](/docs/concepts/embedding_models) via [CachedBackedEmbeddings](https://python.langchain.com/api_refe...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/key_value_stores.mdx‚´page_content_summaryÙË# Key-value stores

## Overview

LangChain provides a key-value store interface for storing and retrieving data.

LangChain includes a [`BaseStore`](https://python.langchain.com/api_reference/core/sto...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/key_value_stores.mdx‚´page_content_summaryÙËFor chains constructed using **LCEL**, the `.stream()` method only streams the output of the final step from the chain. This might be sufficient for some applications, but as you build more complex ch...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË* [How to cache embeddings guide](/docs/how_to/caching_embeddings/).
* [How to retriever using multiple vectors per document](/docs/how_to/custom_retriever/).

## Interface

All [`BaseStores`](https:/...¨metadata¦sourceÙO/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/key_value_stores.mdx¦answerÚ—Key-value stores can be used to cache intermediate outputs in LangChain by storing the results of each step in a chain, allowing for efficient retrieval without recomputation. This is particularly useful in complex chains where intermediate values may be needed alongside the final output. The `BaseStore` interface facilitates the storage and retrieval of these outputs, simplifying the management of data.¨query_id¤Q019ªfocus_kind¶chain_execution_result‰¢idÙ@33b9b928ebc2943c83d00b76703bbe35ca6b2bd4c1459240c94020adf3c424d1¢tsËAÚÔ]œ?È¨trace_idÙ$901dfbc2-d9b9-4d0a-8946-fa4731fae120­parent_run_idÙ$7c4cb63f-ba4e-4e23-b3db-98c4bf15e5d6¦run_idÙ$901dfbc2-d9b9-4d0a-8946-fa4731fae120ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Multimodality

## Overview

**Multimodality** refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various compo...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË:::info Pre-requisites
* [Chat models](/docs/concepts/chat_models)
* [Messages](/docs/concepts/messages)
:::
 
LangChain supports multimodal data as input to chat models:

1. Following provider-specif...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË### What kind of multimodality is supported?

#### Inputs

Some models can accept multimodal inputs, such as images, audio, video, or files.
The types of multimodal inputs supported depend on the mode...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙËThe newest generation of chat models offer additional capabilities:

* [Tool calling](/docs/concepts/tool_calling): Many popular chat models offer a native [tool calling](/docs/concepts/tool_calling) ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË- **Images**,
- **Audio**,
- **Video**,
- And other types of data.

At the time of writing, virtually no models support **multi-modal output**, and only a few models can handle **multi-modal inputs** ...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx¨query_id¤Q020ªfocus_kind¶chain_execution_result†¢idÙ@bc0f285130657ef3dae4c397567937e9e4487a775366e118c37f77b5844cfc57¢tsËAÚÔ^1Ùı¨focus_msÍ	§payload‚¤role©assistant§contentÚïMultimodality in LangChain allows chat models to process various data types, such as text, images, audio, and video. For example, you can pass an image URL along with a text prompt to a chat model using a structured message format, like this:

```python
message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the weather in this image:"},
        {
            "type": "image",
            "source_type": "url",
            "url": "https://...",
        },
    ],
)
```ªfocus_kind¬exec_latency¨query_id¤Q020‰¢idÙ@733e36e02d64440e4836d6b812d6320c0a558bbdfa4006e71c1d5e392c9eba46¢tsËAÚÔ^1èÑ¨trace_idÙ$f0418f22-97f1-4aef-b299-c2ac9557648e­parent_run_idÙ$7c4cb63f-ba4e-4e23-b3db-98c4bf15e5d6¦run_idÙ$f0418f22-97f1-4aef-b299-c2ac9557648eªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚïMultimodality in LangChain allows chat models to process various data types, such as text, images, audio, and video. For example, you can pass an image URL along with a text prompt to a chat model using a structured message format, like this:

```python
message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the weather in this image:"},
        {
            "type": "image",
            "source_type": "url",
            "url": "https://...",
        },
    ],
)
```¨query_id¤Q020ªfocus_kind¶chain_execution_result‰¢idÙ@53b0a1ffa9b820a2c414f2b887a6df4e5610882db25884c4a9c587f5e9ff8672¢tsËAÚÔ^1ñ=¨trace_idÙ$7c4cb63f-ba4e-4e23-b3db-98c4bf15e5d6­parent_run_idÀ¦run_idÙ$7c4cb63f-ba4e-4e23-b3db-98c4bf15e5d6ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ=Explain "Multimodality" support in LangChain with an example.§context•‚´page_content_summaryÙË# Multimodality

## Overview

**Multimodality** refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. Multimodality can appear in various compo...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË:::info Pre-requisites
* [Chat models](/docs/concepts/chat_models)
* [Messages](/docs/concepts/messages)
:::
 
LangChain supports multimodal data as input to chat models:

1. Following provider-specif...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙË### What kind of multimodality is supported?

#### Inputs

Some models can accept multimodal inputs, such as images, audio, video, or files.
The types of multimodal inputs supported depend on the mode...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/multimodality.mdx‚´page_content_summaryÙËThe newest generation of chat models offer additional capabilities:

* [Tool calling](/docs/concepts/tool_calling): Many popular chat models offer a native [tool calling](/docs/concepts/tool_calling) ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË- **Images**,
- **Audio**,
- **Video**,
- And other types of data.

At the time of writing, virtually no models support **multi-modal output**, and only a few models can handle **multi-modal inputs** ...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx¦answerÚïMultimodality in LangChain allows chat models to process various data types, such as text, images, audio, and video. For example, you can pass an image URL along with a text prompt to a chat model using a structured message format, like this:

```python
message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the weather in this image:"},
        {
            "type": "image",
            "source_type": "url",
            "url": "https://...",
        },
    ],
)
```¨query_id¤Q020ªfocus_kind¶chain_execution_result‰¢idÙ@200396021aada8cbd0fb4d3715d2ecf601294edd6cce3fd05bb5e49c23f78625¢tsËAÚÔ^DN¨trace_idÙ$384741fd-573f-4ee4-b089-896ce25e2d78­parent_run_idÙ$455918c5-1250-476a-bb13-05f6c5529ae2¦run_idÙ$384741fd-573f-4ee4-b089-896ce25e2d78ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË:::note

These docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.

:::

## Architecture

The LangChain framework consists o...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/introduction.mdx‚´page_content_summaryÙË# Why LangChain?

The goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.
While LangChain originally st...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## ...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¨query_id¤Q021ªfocus_kind¶chain_execution_result†¢idÙ@191cf5a9118996d01191d122d8d9e0f696cdb446ebd5279665dd49f93e33d876¢tsËAÚÔ^¯Eà¨focus_msÍ†§payload‚¤role©assistant§contentÚäThe LangChain architecture diagram conveys the structure and components of the LangChain ecosystem, highlighting its evolution from a single package to a comprehensive framework for building AI applications. It emphasizes the standardized interfaces for various components, which facilitate easier integration and swapping of different providers. Additionally, it showcases features like orchestration, observability, and advanced functionalities that enhance application development.ªfocus_kind¬exec_latency¨query_id¤Q021‰¢idÙ@176c338fad9d665229bc075add08024c8018c0cf3c39e326a30379052d0f6e2d¢tsËAÚÔ^¯j¶¨trace_idÙ$cc63dd45-44d4-4a3b-920e-fcc7741a1652­parent_run_idÙ$455918c5-1250-476a-bb13-05f6c5529ae2¦run_idÙ$cc63dd45-44d4-4a3b-920e-fcc7741a1652ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚäThe LangChain architecture diagram conveys the structure and components of the LangChain ecosystem, highlighting its evolution from a single package to a comprehensive framework for building AI applications. It emphasizes the standardized interfaces for various components, which facilitate easier integration and swapping of different providers. Additionally, it showcases features like orchestration, observability, and advanced functionalities that enhance application development.¨query_id¤Q021ªfocus_kind¶chain_execution_result‰¢idÙ@b7d6a7fd0121cc329f0bf7680b4284cff3129b93ef7d2bdcd5811e658d82d4b2¢tsËAÚÔ^¯qÖ¨trace_idÙ$455918c5-1250-476a-bb13-05f6c5529ae2­parent_run_idÀ¦run_idÙ$455918c5-1250-476a-bb13-05f6c5529ae2ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ<What is the LangChain architecture diagram trying to convey?§context•‚´page_content_summaryÙË:::note

These docs focus on the Python LangChain library. [Head here](https://js.langchain.com) for docs on the JavaScript LangChain library.

:::

## Architecture

The LangChain framework consists o...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/introduction.mdx‚´page_content_summaryÙË# Why LangChain?

The goal of `langchain` the Python package and LangChain the company is to make it as easy as possible for developers to build applications that reason.
While LangChain originally st...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË:::info[Further reading]

See our video playlist on [LangSmith tracing and evaluations](https://youtube.com/playlist?list=PLfaIDFEXuae0um8Fj0V4dHG37fGFU8Q5S&feature=shared) for more details.

:::

## ...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answerÚäThe LangChain architecture diagram conveys the structure and components of the LangChain ecosystem, highlighting its evolution from a single package to a comprehensive framework for building AI applications. It emphasizes the standardized interfaces for various components, which facilitate easier integration and swapping of different providers. Additionally, it showcases features like orchestration, observability, and advanced functionalities that enhance application development.¨query_id¤Q021ªfocus_kind¶chain_execution_result‰¢idÙ@9abed3136e681d6ff88df64711819279c938af08853888e3808c5bf15db43a0b¢tsËAÚÔ^Ãä¨trace_idÙ$0bd0dbee-da78-46e8-8a3f-ed8a2b8b774e­parent_run_idÙ$459dd65e-d6a0-43d2-9895-5d46fb1b233a¦run_idÙ$0bd0dbee-da78-46e8-8a3f-ed8a2b8b774eªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)
:::

## Overview

One common prompting technique for...¨metadata¦sourceÙP/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/example_selectors.mdx‚´page_content_summaryÙË### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË### Messages

[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.

- [How to: trim messages](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË| Name                                                                                                      | When to use                                                                               ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË| Name                                     | When to Use                                                                                                                          | Description         ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx¨query_id¤Q022ªfocus_kind¶chain_execution_result†¢idÙ@78f793e5483061bcd0a462fb4c0129b6a46d8fc8ff52fc273eca4786e1ced00e¢tsËAÚÔ_-hä¨focus_msÍ{§payload‚¤role©assistant§contentÚ¥Two example selectors are "length-based" and "semantic similarity." Use the length-based selector when you want to ensure that the examples fit within a specific character or word limit, while the semantic similarity selector is ideal for selecting examples that closely match the context or intent of the current prompt. These selectors help tailor the few-shot examples to improve the performance of the language model.ªfocus_kind¬exec_latency¨query_id¤Q022‰¢idÙ@956c96a54633e6d097ebed612682ce4513c57ffb0b1e451a79fce9ade6d5793a¢tsËAÚÔ_-r¨trace_idÙ$45463120-364c-4306-9100-5123bba0061f­parent_run_idÙ$459dd65e-d6a0-43d2-9895-5d46fb1b233a¦run_idÙ$45463120-364c-4306-9100-5123bba0061fªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ¥Two example selectors are "length-based" and "semantic similarity." Use the length-based selector when you want to ensure that the examples fit within a specific character or word limit, while the semantic similarity selector is ideal for selecting examples that closely match the context or intent of the current prompt. These selectors help tailor the few-shot examples to improve the performance of the language model.¨query_id¤Q022ªfocus_kind¶chain_execution_result‰¢idÙ@c72beb22cd12079b5a59757a7a6b9c043d33ee8f4ccb0c3489fb990c21eb1171¢tsËAÚÔ_-yJ¨trace_idÙ$459dd65e-d6a0-43d2-9895-5d46fb1b233a­parent_run_idÀ¦run_idÙ$459dd65e-d6a0-43d2-9895-5d46fb1b233aªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ3Provide two example selectors and when to use them.§context•‚´page_content_summaryÙË# Example selectors

:::note Prerequisites

- [Chat models](/docs/concepts/chat_models/)
- [Few-shot prompting](/docs/concepts/few_shot_prompting/)
:::

## Overview

One common prompting technique for...¨metadata¦sourceÙP/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/example_selectors.mdx‚´page_content_summaryÙË### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË### Messages

[Messages](/docs/concepts/messages) are the input and output of chat models. They have some `content` and a `role`, which describes the source of the message.

- [How to: trim messages](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË| Name                                                                                                      | When to use                                                                               ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË| Name                                     | When to Use                                                                                                                          | Description         ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx¦answerÚ¥Two example selectors are "length-based" and "semantic similarity." Use the length-based selector when you want to ensure that the examples fit within a specific character or word limit, while the semantic similarity selector is ideal for selecting examples that closely match the context or intent of the current prompt. These selectors help tailor the few-shot examples to improve the performance of the language model.¨query_id¤Q022ªfocus_kind¶chain_execution_result‰¢idÙ@83d05d65eee42da74100340b46b8dfb44c108a9f68d7850e77b0d5210af3ec6f¢tsËAÚÔ_? Í¨trace_idÙ$cc9cc7f1-8263-41cc-9bd5-5e5a0487dba9­parent_run_idÙ$46db8269-7bd9-415b-bc4e-c24e35affa31¦run_idÙ$cc9cc7f1-8263-41cc-9bd5-5e5a0487dba9ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Output parsers

<span data-heading-keywords="output parser"></span>

:::note

The information here refers to parsers that take a text output from a model try to parse it into a more structured repre...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/output_parsers.mdx‚´page_content_summaryÙsFor specifics on how to use output parsers, see the [relevant how-to guides here](/docs/how_to/#output-parsers)....¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/output_parsers.mdx‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËHere are some guidelines:

* If you are making a single LLM call, you don't need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.
* If you have a simple chain (e.g....¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË### LLMs

What LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.

- [How to: cache model responses](/docs/how_to/llm_cachin...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¨query_id¤Q023ªfocus_kind¶chain_execution_result†¢idÙ@e5857e826c38cb7735a44fe8e81d3c1984fd8016ae10670d9dc4cff6f8901a8b¢tsËAÚÔ_™~N¨focus_msÍ§payload‚¤role©assistant§contentÚ…Output parsers simplify post-processing in LCEL by transforming the raw output from a model into a more structured format suitable for downstream tasks. They are particularly useful for generating structured data or normalizing outputs from chat models and LLMs. However, it is recommended to use function or tool calling for automatic handling instead of relying solely on output parsing.ªfocus_kind¬exec_latency¨query_id¤Q023‰¢idÙ@ddfc5a806162a6970810e5b0c455b9b6e3d2987fa17677d29fb42c31da5355c6¢tsËAÚÔ_™Œ—¨trace_idÙ$3d77c075-3dbe-4304-bd8e-dd8e03680d90­parent_run_idÙ$46db8269-7bd9-415b-bc4e-c24e35affa31¦run_idÙ$3d77c075-3dbe-4304-bd8e-dd8e03680d90ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ…Output parsers simplify post-processing in LCEL by transforming the raw output from a model into a more structured format suitable for downstream tasks. They are particularly useful for generating structured data or normalizing outputs from chat models and LLMs. However, it is recommended to use function or tool calling for automatic handling instead of relying solely on output parsing.¨query_id¤Q023ªfocus_kind¶chain_execution_result‰¢idÙ@22aaa239eb74110855123365b12cae8f3dd77f2aaf37b3510597bdc4e319c31f¢tsËAÚÔ_™šA¨trace_idÙ$46db8269-7bd9-415b-bc4e-c24e35affa31­parent_run_idÀ¦run_idÙ$46db8269-7bd9-415b-bc4e-c24e35affa31ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ6How do OutputParsers simplify post-processing in LCEL?§context•‚´page_content_summaryÙË# Output parsers

<span data-heading-keywords="output parser"></span>

:::note

The information here refers to parsers that take a text output from a model try to parse it into a more structured repre...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/output_parsers.mdx‚´page_content_summaryÙsFor specifics on how to use output parsers, see the [relevant how-to guides here](/docs/how_to/#output-parsers)....¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/output_parsers.mdx‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙËHere are some guidelines:

* If you are making a single LLM call, you don't need LCEL; instead call the underlying [chat model](/docs/concepts/chat_models) directly.
* If you have a simple chain (e.g....¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË### LLMs

What LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.

- [How to: cache model responses](/docs/how_to/llm_cachin...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx¦answerÚ…Output parsers simplify post-processing in LCEL by transforming the raw output from a model into a more structured format suitable for downstream tasks. They are particularly useful for generating structured data or normalizing outputs from chat models and LLMs. However, it is recommended to use function or tool calling for automatic handling instead of relying solely on output parsing.¨query_id¤Q023ªfocus_kind¶chain_execution_result‰¢idÙ@518caa0d47bc604ae9c2bce930e976b9aea954b82ee8b15bd2cbe7315711c0a8¢tsËAÚÔ_«0¨trace_idÙ$bd1aff39-8ccd-42c5-a2db-1799d1631c89­parent_run_idÙ$c923d95a-de3b-42ca-b23b-6400f379380e¦run_idÙ$bd1aff39-8ccd-42c5-a2db-1799d1631c89ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Callbacks

:::note Prerequisites
- [Runnable interface](/docs/concepts/runnables)
:::

LangChain provides a callback system that allows you to hook into the various stages of your LLM application. T...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/callbacks.mdx‚´page_content_summaryÙË# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applic...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËfor chunk in compiled_graph.stream(..., mode="messages"): 
    ...
```
## Async Programming

LangChain offers both synchronous (sync) and asynchronous (async) versions of many of its methods. The asyn...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËIn this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.

## What to stream in LLM...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËFor writing custom data to the stream, please see the following resources:

* If using LangGraph, see [how to stream custom data](https://langchain-ai.github.io/langgraph/how-tos/streaming-content/).
...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¨query_id¤Q024ªfocus_kind¶chain_execution_result†¢idÙ@678f72d112f800e68b537ff2a6b87273eeaafa2abacb5a9fa072e6704ea9c009¢tsËAÚÔ`(¿¨focus_msÍ›§payload‚¤role©assistant§contentÚ•Streaming chunk callbacks allow for the progressive display of output from LLMs as they generate responses, enhancing user experience by providing immediate feedback. In contrast, token callbacks focus on handling individual tokens as they are produced, which can be useful for more granular control over the output. Both methods aim to improve responsiveness and reduce perceived latency in applications.ªfocus_kind¬exec_latency¨query_id¤Q024‰¢idÙ@ec1596443cfd2e0f7e4e6b89e796a698a82834968f9cd7f61a1584e861da9f6f¢tsËAÚÔ`4¹¨trace_idÙ$e0503236-3252-4470-af82-760aeba687e2­parent_run_idÙ$c923d95a-de3b-42ca-b23b-6400f379380e¦run_idÙ$e0503236-3252-4470-af82-760aeba687e2ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ•Streaming chunk callbacks allow for the progressive display of output from LLMs as they generate responses, enhancing user experience by providing immediate feedback. In contrast, token callbacks focus on handling individual tokens as they are produced, which can be useful for more granular control over the output. Both methods aim to improve responsiveness and reduce perceived latency in applications.¨query_id¤Q024ªfocus_kind¶chain_execution_result‰¢idÙ@0ae59c19546658a54e64f0b7e8608b7b3b4337b779e03304aeb27a83949012a8¢tsËAÚÔ`>y¨trace_idÙ$c923d95a-de3b-42ca-b23b-6400f379380e­parent_run_idÀ¦run_idÙ$c923d95a-de3b-42ca-b23b-6400f379380eªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ:Describe streaming chunk callbacks versus token callbacks.§context•‚´page_content_summaryÙË# Callbacks

:::note Prerequisites
- [Runnable interface](/docs/concepts/runnables)
:::

LangChain provides a callback system that allows you to hook into the various stages of your LLM application. T...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/callbacks.mdx‚´page_content_summaryÙË# Streaming

:::info Prerequisites
* [Runnable Interface](/docs/concepts/runnables)
* [Chat Models](/docs/concepts/chat_models)
:::

**Streaming** is crucial for enhancing the responsiveness of applic...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËfor chunk in compiled_graph.stream(..., mode="messages"): 
    ...
```
## Async Programming

LangChain offers both synchronous (sync) and asynchronous (async) versions of many of its methods. The asyn...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËIn this guide, we'll discuss streaming in LLM applications and explore how LangChain's streaming APIs facilitate real-time output from various components in your application.

## What to stream in LLM...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËFor writing custom data to the stream, please see the following resources:

* If using LangGraph, see [how to stream custom data](https://langchain-ai.github.io/langgraph/how-tos/streaming-content/).
...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¦answerÚ•Streaming chunk callbacks allow for the progressive display of output from LLMs as they generate responses, enhancing user experience by providing immediate feedback. In contrast, token callbacks focus on handling individual tokens as they are produced, which can be useful for more granular control over the output. Both methods aim to improve responsiveness and reduce perceived latency in applications.¨query_id¤Q024ªfocus_kind¶chain_execution_result‰¢idÙ@d2d7dbbcc054424e7397384619d45f3256ed311c5c8b77447a099d8b0350c7b7¢tsËAÚÔ`×Ô¨trace_idÙ$83058829-c290-4ca9-a637-4dc5208f4c0f­parent_run_idÙ$9aa0d093-d866-44fc-ac41-90f16dd4cc99¦run_idÙ$83058829-c290-4ca9-a637-4dc5208f4c0fªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËWhile processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
    -...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙËThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

A full conversation often invo...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q025ªfocus_kind¶chain_execution_result†¢idÙ@1408d5b25e0dc31e20bbfc728b8a219ba305e7676a47317cda2bed131162912c¢tsËAÚÔ`€É ¨focus_msÍ¤§payload‚¤role©assistant§contentÚzThe `ConversationBufferMemory` class is designed to manage and preserve the structure of chat history during conversations. It should be used when implementing memory in chat models to ensure that the conversation follows the correct sequence of user and assistant messages, especially when tool calls are involved. This helps maintain coherence and context within the dialogue.ªfocus_kind¬exec_latency¨query_id¤Q025‰¢idÙ@0f188e82668bf367d9ed2bfbec081d1f6025ae5d9dc853c92434bdd4009ecfc8¢tsËAÚÔ`€Ú¡¨trace_idÙ$7a2a9fe8-0ac2-48c1-a58d-05a9a4637b9c­parent_run_idÙ$9aa0d093-d866-44fc-ac41-90f16dd4cc99¦run_idÙ$7a2a9fe8-0ac2-48c1-a58d-05a9a4637b9cªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚzThe `ConversationBufferMemory` class is designed to manage and preserve the structure of chat history during conversations. It should be used when implementing memory in chat models to ensure that the conversation follows the correct sequence of user and assistant messages, especially when tool calls are involved. This helps maintain coherence and context within the dialogue.¨query_id¤Q025ªfocus_kind¶chain_execution_result‰¢idÙ@7c7272adc0fc9e8a480c45eafdbcf4dacbff2c4d6af3434d84f341740a08cfec¢tsËAÚÔ`€ãu¨trace_idÙ$9aa0d093-d866-44fc-ac41-90f16dd4cc99­parent_run_idÀ¦run_idÙ$9aa0d093-d866-44fc-ac41-90f16dd4cc99ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙWWhat is the purpose of the `ConversationBufferMemory` class and when should it be used?§context•‚´page_content_summaryÙËWhile processing chat history, it's essential to preserve a correct conversation structure. 

Key guidelines for managing chat history:

- The conversation should follow one of these structures:
    -...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙËThe **assistant** may respond directly to the user or if configured with tools request that a [tool](/docs/concepts/tool_calling) be invoked to perform a specific task.

A full conversation often invo...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Related resources

- [How to trim messages](/docs/how_to/trim_messages/)
- [Memory guide](https://langchain-ai.github.io/langgraph/concepts/memory/) for information on implementing short-term and l...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_history.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚzThe `ConversationBufferMemory` class is designed to manage and preserve the structure of chat history during conversations. It should be used when implementing memory in chat models to ensure that the conversation follows the correct sequence of user and assistant messages, especially when tool calls are involved. This helps maintain coherence and context within the dialogue.¨query_id¤Q025ªfocus_kind¶chain_execution_result‰¢idÙ@89c381b57ac7ad70755148d00060dd520861a82f6d421017517a5a03545932b2¢tsËAÚÔ`”=¨trace_idÙ$340f65d8-914c-4d4e-a11b-9622192c82ab­parent_run_idÙ$a0aa5c44-da56-4265-a74c-fbcaf4ee7844¦run_idÙ$340f65d8-914c-4d4e-a11b-9622192c82abªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËExample implementation using LangChain's `CharacterTextSplitter` with token-based splitting:

```python
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË:::

### Text-structured based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. 
We can leverage this inherent structure to inform our splitting strategy,...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË# Text splitters
<span data-heading-keywords="text splitter,text splitting"></span>

:::info[Prerequisites]

* [Documents](/docs/concepts/retrievers/#interface)
* Tokenization(/docs/concepts/tokens)
:...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙËNow, the next question is *how* to split the documents into chunks! There are several strategies, each with its own advantages.

:::info[Further reading]
* See Greg Kamradt's [chunkviz](https://chunkv...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË## How tokens work in language models

The reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, langua...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx¨query_id¤Q026ªfocus_kind¶chain_execution_result†¢idÙ@a6fc898adfe9ea6765e8bba860a4ebc9e943639d4519157913d806e247241e05¢tsËAÚÔ`â³¯¨focus_msÍË§payload‚¤role©assistant§contentÚ˜The LangChain text splitter that allows you to split documents by tokens is the `CharacterTextSplitter` with a token-based approach. You would choose it over a character splitter because it aligns better with how language models process text, focusing on meaningful linguistic units rather than arbitrary character counts. This can improve the quality of text representations used in downstream applications.ªfocus_kind¬exec_latency¨query_id¤Q026‰¢idÙ@37caaabba7f3b08bec70f2ca736c9c421353538b5abebce01ffe31fe38366ea1¢tsËAÚÔ`âÁº¨trace_idÙ$ad9c831c-b1e9-4b10-9f38-66f2a2e00e3b­parent_run_idÙ$a0aa5c44-da56-4265-a74c-fbcaf4ee7844¦run_idÙ$ad9c831c-b1e9-4b10-9f38-66f2a2e00e3bªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ˜The LangChain text splitter that allows you to split documents by tokens is the `CharacterTextSplitter` with a token-based approach. You would choose it over a character splitter because it aligns better with how language models process text, focusing on meaningful linguistic units rather than arbitrary character counts. This can improve the quality of text representations used in downstream applications.¨query_id¤Q026ªfocus_kind¶chain_execution_result‰¢idÙ@61b3492052cfee0c54b62f1f6bf2bf2893e880a9bfd7a7f3c272c121f0d456fa¢tsËAÚÔ`âÈ~¨trace_idÙ$a0aa5c44-da56-4265-a74c-fbcaf4ee7844­parent_run_idÀ¦run_idÙ$a0aa5c44-da56-4265-a74c-fbcaf4ee7844ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙwWhich LangChain text splitter lets you split documents by tokens and why would you choose it over a character splitter?§context•‚´page_content_summaryÙËExample implementation using LangChain's `CharacterTextSplitter` with token-based splitting:

```python
from langchain_text_splitters import CharacterTextSplitter
text_splitter = CharacterTextSplitter...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË:::

### Text-structured based

Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. 
We can leverage this inherent structure to inform our splitting strategy,...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË# Text splitters
<span data-heading-keywords="text splitter,text splitting"></span>

:::info[Prerequisites]

* [Documents](/docs/concepts/retrievers/#interface)
* Tokenization(/docs/concepts/tokens)
:...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙËNow, the next question is *how* to split the documents into chunks! There are several strategies, each with its own advantages.

:::info[Further reading]
* See Greg Kamradt's [chunkviz](https://chunkv...¨metadata¦sourceÙM/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/text_splitters.mdx‚´page_content_summaryÙË## How tokens work in language models

The reason language models use tokens is tied to how they understand and predict language. Rather than processing characters or entire sentences directly, langua...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx¦answerÚ˜The LangChain text splitter that allows you to split documents by tokens is the `CharacterTextSplitter` with a token-based approach. You would choose it over a character splitter because it aligns better with how language models process text, focusing on meaningful linguistic units rather than arbitrary character counts. This can improve the quality of text representations used in downstream applications.¨query_id¤Q026ªfocus_kind¶chain_execution_result‰¢idÙ@4a67a50342df517ce7020bbce9a40dc01ff23e5fcf028d996f78a84ececf76f9¢tsËAÚÔ`ô|Ù¨trace_idÙ$93499b62-e49e-4d12-ae24-4cc4acde3d6a­parent_run_idÙ$b4403c7b-5db6-45e1-a1f4-2e42635d1478¦run_idÙ$93499b62-e49e-4d12-ae24-4cc4acde3d6aªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË:::info[Further reading]

* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.
* See the [Elast...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```

## Advanced retrieval patterns

### Ensemble 

Because the retriever interface is so simple, returning a list of `Do...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we comb...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextu...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small ...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¨query_id¤Q027ªfocus_kind¶chain_execution_result†¢idÙ@a30a4a88c3a4dd7ea7a24a04b63497e7ed4b8d860c6ddcf6ed0b48c408c5af57¢tsËAÚÔaM‡ç¨focus_msÍm§payload‚¤role©assistant§contentÚÇ`VectorStoreRetriever` focuses on retrieving documents based on vector similarity, while `EnsembleRetriever` combines multiple retrievers to improve the overall retrieval performance by leveraging their strengths. Additionally, `EnsembleRetriever` can apply weighted scoring to the results from different retrievers, whereas `VectorStoreRetriever` operates independently. This makes `EnsembleRetriever` more versatile in handling diverse types of queries.ªfocus_kind¬exec_latency¨query_id¤Q027‰¢idÙ@76c678784dd54c89cf6f813ae4ef2f78c06b9cb059e16c788d489dd63780f3c1¢tsËAÚÔaMš¨trace_idÙ$08cab91f-94a4-44d4-85d8-ee1e3fa62760­parent_run_idÙ$b4403c7b-5db6-45e1-a1f4-2e42635d1478¦run_idÙ$08cab91f-94a4-44d4-85d8-ee1e3fa62760ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚÇ`VectorStoreRetriever` focuses on retrieving documents based on vector similarity, while `EnsembleRetriever` combines multiple retrievers to improve the overall retrieval performance by leveraging their strengths. Additionally, `EnsembleRetriever` can apply weighted scoring to the results from different retrievers, whereas `VectorStoreRetriever` operates independently. This makes `EnsembleRetriever` more versatile in handling diverse types of queries.¨query_id¤Q027ªfocus_kind¶chain_execution_result‰¢idÙ@e64822ad54651a26927d49d345b33cd8a92039795afa4b4186182845ffa975bd¢tsËAÚÔaM£ƒ¨trace_idÙ$b4403c7b-5db6-45e1-a1f4-2e42635d1478­parent_run_idÀ¦run_idÙ$b4403c7b-5db6-45e1-a1f4-2e42635d1478ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙLList two differences between `VectorStoreRetriever` and `EnsembleRetriever`.§context•‚´page_content_summaryÙË:::info[Further reading]

* See the [BM25](/docs/integrations/retrievers/bm25/) retriever integration.
* See the [TF-IDF](/docs/integrations/retrievers/tf_idf/) retriever integration.
* See the [Elast...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```

## Advanced retrieval patterns

### Ensemble 

Because the retriever interface is so simple, returning a list of `Do...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we comb...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË- [How to: use a vector store to retrieve data](/docs/how_to/vectorstore_retriever)
- [How to: generate multiple queries to retrieve data for](/docs/how_to/MultiQueryRetriever)
- [How to: use contextu...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small ...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¦answerÚÇ`VectorStoreRetriever` focuses on retrieving documents based on vector similarity, while `EnsembleRetriever` combines multiple retrievers to improve the overall retrieval performance by leveraging their strengths. Additionally, `EnsembleRetriever` can apply weighted scoring to the results from different retrievers, whereas `VectorStoreRetriever` operates independently. This makes `EnsembleRetriever` more versatile in handling diverse types of queries.¨query_id¤Q027ªfocus_kind¶chain_execution_result‰¢idÙ@412fbec2e9fb50a0fcbf04df28c5062300fd0a7b590b78be4959685d941a6bb1¢tsËAÚÔa`ŸE¨trace_idÙ$db48a3e0-0454-4590-9f67-be758760be4a­parent_run_idÙ$6dfd1642-4e8e-47ff-b4f0-967eb610bf4e¦run_idÙ$db48a3e0-0454-4590-9f67-be758760be4aªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙÊIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

You have a few options to deal with rate limits:...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q028ªfocus_kind¶chain_execution_result†¢idÙ@1ba31602b938579f5cd20ce32db8a4b830a5584dafd1fcc295780c461d4f3ba4¢tsËAÚÔaºƒs¨focus_msÍ{§payload‚¤role©assistant§contentÚSTo configure a chat model to automatically retry on rate limiting errors, you can set the `max_retries` parameter to control the number of retries. Additionally, implement a waiting mechanism that increases the wait time after each subsequent rate limit error. This approach helps manage requests effectively while adhering to rate limits.ªfocus_kind¬exec_latency¨query_id¤Q028‰¢idÙ@2f0a3931eb8d5be7d632d612ad3da8f8247aec5ee1edaea3df594d98bc3ed517¢tsËAÚÔaºŸÔ¨trace_idÙ$037b520f-cf17-43b1-b65a-95ae621701b1­parent_run_idÙ$6dfd1642-4e8e-47ff-b4f0-967eb610bf4e¦run_idÙ$037b520f-cf17-43b1-b65a-95ae621701b1ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚSTo configure a chat model to automatically retry on rate limiting errors, you can set the `max_retries` parameter to control the number of retries. Additionally, implement a waiting mechanism that increases the wait time after each subsequent rate limit error. This approach helps manage requests effectively while adhering to rate limits.¨query_id¤Q028ªfocus_kind¶chain_execution_result‰¢idÙ@5d641b7a7623a84d9b17eeeccc3aa95593eebd1822649ea9adba12420d6a3396¢tsËAÚÔaº§×¨trace_idÙ$6dfd1642-4e8e-47ff-b4f0-967eb610bf4e­parent_run_idÀ¦run_idÙ$6dfd1642-4e8e-47ff-b4f0-967eb610bf4eªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙQHow can you configure a chat model to automatically retry on ratelimiting errors?§context•‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙ‘3. Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited....¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙÊIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

You have a few options to deal with rate limits:...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚSTo configure a chat model to automatically retry on rate limiting errors, you can set the `max_retries` parameter to control the number of retries. Additionally, implement a waiting mechanism that increases the wait time after each subsequent rate limit error. This approach helps manage requests effectively while adhering to rate limits.¨query_id¤Q028ªfocus_kind¶chain_execution_result‰¢idÙ@e9b7bfee9c1188f4b776e41cd40c4c6b9de521b581cf6a710c973db0b4fbd5e0¢tsËAÚÔaĞL¹¨trace_idÙ$7169a027-e4d1-468c-89c9-400dc1061259­parent_run_idÙ$8806b236-fd8f-4bd8-82f0-c318bc05b9e2¦run_idÙ$7169a027-e4d1-468c-89c9-400dc1061259ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string liter...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).
...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Key concepts

- Tools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.
- Create tools using the [@tool](https://python.langchain.com/api_reference/cor...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË@tool
def user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:
    """Tool that processes input data."""
    return f"User {user_id} processed {input_data}"
```

Annotating the `user_...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx¨query_id¤Q029ªfocus_kind¶chain_execution_result†¢idÙ@5b2cfe9d122a3214c7e2eb3604c51fd85a354846f733a1392d3fad8f3bf4b046¢tsËAÚÔbAó¨focus_msÍî§payload‚¤role©assistant§contentÚŸ`InjectedToolArg` allows certain parameters to be passed to a tool at runtime without being exposed in the tool's schema, ensuring that sensitive or dynamic values are not accessible to the model. For example, a `user_id` can be injected dynamically into a tool that processes user-specific data, keeping it hidden from the tool's schema. This is useful for maintaining security and control over runtime parameters.ªfocus_kind¬exec_latency¨query_id¤Q029‰¢idÙ@20fea9e8d56af9e879feda35792c19c4ed82670406496f7d75319f0794b5f4ab¢tsËAÚÔbB¨trace_idÙ$357fce08-995e-44dd-942f-beb10ae10ed3­parent_run_idÙ$8806b236-fd8f-4bd8-82f0-c318bc05b9e2¦run_idÙ$357fce08-995e-44dd-942f-beb10ae10ed3ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚŸ`InjectedToolArg` allows certain parameters to be passed to a tool at runtime without being exposed in the tool's schema, ensuring that sensitive or dynamic values are not accessible to the model. For example, a `user_id` can be injected dynamically into a tool that processes user-specific data, keeping it hidden from the tool's schema. This is useful for maintaining security and control over runtime parameters.¨query_id¤Q029ªfocus_kind¶chain_execution_result‰¢idÙ@675c11e22833c12fbe427d9fdebdb1448cfe0dc212c24eeee2ad37ca51ce8433¢tsËAÚÔbB¨trace_idÙ$8806b236-fd8f-4bd8-82f0-c318bc05b9e2­parent_run_idÀ¦run_idÙ$8806b236-fd8f-4bd8-82f0-c318bc05b9e2ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙJExplain how `InjectedToolArg` affects a tool schema and give one use case.§context•‚´page_content_summaryÙËYou can also use the `Annotated` type with a string literal to provide a **description** for the corresponding argument that **WILL** be exposed in the tool's schema.

- **Annotated[..., "string liter...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).
...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Key concepts

- Tools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.
- Create tools using the [@tool](https://python.langchain.com/api_reference/cor...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË@tool
def user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:
    """Tool that processes input data."""
    return f"User {user_id} processed {input_data}"
```

Annotating the `user_...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx¦answerÚŸ`InjectedToolArg` allows certain parameters to be passed to a tool at runtime without being exposed in the tool's schema, ensuring that sensitive or dynamic values are not accessible to the model. For example, a `user_id` can be injected dynamically into a tool that processes user-specific data, keeping it hidden from the tool's schema. This is useful for maintaining security and control over runtime parameters.¨query_id¤Q029ªfocus_kind¶chain_execution_result‰¢idÙ@ade831c931c56c9966b16d5cf007826f4ceb4f2f4721dad4b9eca97f6754babb¢tsËAÚÔbRIÃ¨trace_idÙ$dee4c918-6aed-471e-ae4d-0483b78a4a90­parent_run_idÙ$79f48209-de46-4a1a-b4af-c632bd26023f¦run_idÙ$dee4c918-6aed-471e-ae4d-0483b78a4a90ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Create tools using the `@tool` decorator

The recommended way to create tools is using the [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) deco...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË1. Declaratively using [LangChain Expression Language (LCEL)](/docs/concepts/lcel):

    ```python
    chain = prompt | chat_model | output_parser
    ```

2. Using a [custom Runnable](#custom-runnabl...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË![Conceptual overview of tool calling](/img/tool_calling_concept.png)

## Key concepts 

**(1) Tool Creation:** Use the [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.too...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙËThe key attributes that correspond to the tool's **schema**:

- **name**: The name of the tool.
- **description**: A description of what the tool does.
- **args**: Property that returns the JSON schem...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).
...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx¨query_id¤Q030ªfocus_kind¶chain_execution_result†¢idÙ@4634ae9c94acc55801b70c05ef94a51f958ee1421826786ce32df8f21e46e2ed¢tsËAÚÔb¥ÆŸ¨focus_msÍ§payload‚¤role©assistant§contentÚThe recommended decorator for quickly turning a Python function into a LangChain Tool is the `@tool` decorator. This decorator simplifies the process of tool creation and should be used in most cases. It allows you to easily associate a function with its schema to create a tool.ªfocus_kind¬exec_latency¨query_id¤Q030‰¢idÙ@730d81b5c8dfea49b84d04c4aae69f50ac44c8c60a5bebd1230eeceecee12735¢tsËAÚÔb¥çÁ¨trace_idÙ$30fd4164-d7f3-4746-86f3-3612c1c2adf3­parent_run_idÙ$79f48209-de46-4a1a-b4af-c632bd26023f¦run_idÙ$30fd4164-d7f3-4746-86f3-3612c1c2adf3ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚThe recommended decorator for quickly turning a Python function into a LangChain Tool is the `@tool` decorator. This decorator simplifies the process of tool creation and should be used in most cases. It allows you to easily associate a function with its schema to create a tool.¨query_id¤Q030ªfocus_kind¶chain_execution_result‰¢idÙ@b6a13657cc4cbd7ec8dcdbc3fd761b603294915182539a1a53b50da2c06dbd67¢tsËAÚÔb¥õ9¨trace_idÙ$79f48209-de46-4a1a-b4af-c632bd26023f­parent_run_idÀ¦run_idÙ$79f48209-de46-4a1a-b4af-c632bd26023fªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ[Which decorator is recommended for quickly turning a Python function into a LangChain Tool?§context•‚´page_content_summaryÙË## Create tools using the `@tool` decorator

The recommended way to create tools is using the [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.tools.convert.tool.html) deco...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË1. Declaratively using [LangChain Expression Language (LCEL)](/docs/concepts/lcel):

    ```python
    chain = prompt | chat_model | output_parser
    ```

2. Using a [custom Runnable](#custom-runnabl...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË![Conceptual overview of tool calling](/img/tool_calling_concept.png)

## Key concepts 

**(1) Tool Creation:** Use the [@tool](https://python.langchain.com/api_reference/core/tools/langchain_core.too...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙËThe key attributes that correspond to the tool's **schema**:

- **name**: The name of the tool.
- **description**: A description of what the tool does.
- **args**: Property that returns the JSON schem...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).
...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx¦answerÚThe recommended decorator for quickly turning a Python function into a LangChain Tool is the `@tool` decorator. This decorator simplifies the process of tool creation and should be used in most cases. It allows you to easily associate a function with its schema to create a tool.¨query_id¤Q030ªfocus_kind¶chain_execution_result‰¢idÙ@4c31d854ede50ba4bd8d9d6bb5be93924d586803c0fbb97dfc1109238553247c¢tsËAÚÔbµ€g¨trace_idÙ$3e7030ab-686b-487c-a1f8-fc0a66ef4e14­parent_run_idÙ$34bcb376-a10f-4366-8076-e8a93f3cb23e¦run_idÙ$3e7030ab-686b-487c-a1f8-fc0a66ef4e14ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for user...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¨query_id¤Q031ªfocus_kind¶chain_execution_result†¢idÙ@f24a99ede9274cdfe55d791d58767cd1283e6dcb237f1020b9bec5c58092bc2b¢tsËAÚÔc#†^¨focus_msÍ´§payload‚¤role©assistant§contentÚµTo batch inputs through a Runnable for better throughput, you can use the built-in `batch` or `batch_as_completed` API, which processes multiple inputs in parallel. This approach significantly improves performance by allowing independent inputs to be processed simultaneously, rather than sequentially. Additionally, you can control the maximum number of parallel calls by setting the `max_concurrency` attribute in the `RunnableConfig`.ªfocus_kind¬exec_latency¨query_id¤Q031‰¢idÙ@0b3b08ba8cfa685216f66fe6bfc4e588d3445050a0117a40acff80c40a999a00¢tsËAÚÔc#É¨trace_idÙ$ade5d6f8-2927-4a76-bd89-8958162eef31­parent_run_idÙ$34bcb376-a10f-4366-8076-e8a93f3cb23e¦run_idÙ$ade5d6f8-2927-4a76-bd89-8958162eef31ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚµTo batch inputs through a Runnable for better throughput, you can use the built-in `batch` or `batch_as_completed` API, which processes multiple inputs in parallel. This approach significantly improves performance by allowing independent inputs to be processed simultaneously, rather than sequentially. Additionally, you can control the maximum number of parallel calls by setting the `max_concurrency` attribute in the `RunnableConfig`.¨query_id¤Q031ªfocus_kind¶chain_execution_result‰¢idÙ@7a6f7d9fee57c28cef3cc03e3d69f51cacd29c1d52b3b0bf8eaacdc3a3f8fb2c¢tsËAÚÔc#–š¨trace_idÙ$34bcb376-a10f-4366-8076-e8a93f3cb23e­parent_run_idÀ¦run_idÙ$34bcb376-a10f-4366-8076-e8a93f3cb23eªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙFDescribe how to batch inputs through a Runnable for better throughput.§context•‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËThe default implementation of `batch` and `batch_as_completed` use a thread pool executor to run the `invoke` method in parallel. This allows for efficient parallel execution without the need for user...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË- **Optimized parallel execution**: Run Runnables in parallel using [RunnableParallel](#runnableparallel) or run multiple inputs through a given chain in parallel using the [Runnable Batch API](/docs/...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx¦answerÚµTo batch inputs through a Runnable for better throughput, you can use the built-in `batch` or `batch_as_completed` API, which processes multiple inputs in parallel. This approach significantly improves performance by allowing independent inputs to be processed simultaneously, rather than sequentially. Additionally, you can control the maximum number of parallel calls by setting the `max_concurrency` attribute in the `RunnableConfig`.¨query_id¤Q031ªfocus_kind¶chain_execution_result‰¢idÙ@c9fa8fb906cb2572d0b3ac5712f7c710dc7e0ab720770c7d4bbe90f3b52861dc¢tsËAÚÔc7ã’¨trace_idÙ$6b52d122-f5d5-4506-ac5d-0a420f4b7275­parent_run_idÙ$b88cbbc1-1edd-4ba3-a071-4c9630759f33¦run_idÙ$6b52d122-f5d5-4506-ac5d-0a420f4b7275ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË| max_concurrency | Maximum number of parallel calls to make (e.g., used by batch).                            |
| recursion_limit | Maximum number of times a call can recurse (e.g., used by Runnables...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## RunnableConfig

Any of the methods that are used to execute the runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) accept a second argument called
`RunnableConfig` ([API Reference](http...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q032ªfocus_kind¶chain_execution_result†¢idÙ@aecf1c5f389239de78a33af5e06ca1f4c68d39bb07b2d1ee8d78c8b0d56d4189¢tsËAÚÔcw`Ñ¨focus_msÍß§payload‚¤role©assistant§contentÙâThe parameter in `RunnableConfig` that controls the maximum number of parallel calls during batching is `max_concurrency`. This attribute allows you to limit the number of parallel calls to prevent overloading a server or API.ªfocus_kind¬exec_latency¨query_id¤Q032‰¢idÙ@ef6a59cdea3dc88af2927b52b6b47b1ead74b5bff0f6c5b2e55f18cf30568140¢tsËAÚÔcw¥¨trace_idÙ$06d5a7cf-8a2c-425f-b401-f4571ac1fe80­parent_run_idÙ$b88cbbc1-1edd-4ba3-a071-4c9630759f33¦run_idÙ$06d5a7cf-8a2c-425f-b401-f4571ac1fe80ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙâThe parameter in `RunnableConfig` that controls the maximum number of parallel calls during batching is `max_concurrency`. This attribute allows you to limit the number of parallel calls to prevent overloading a server or API.¨query_id¤Q032ªfocus_kind¶chain_execution_result‰¢idÙ@14bb269fd1b92da705d8b6a5cb55573befa2201fc6f2c424b7d8b293033424cb¢tsËAÚÔcw¬i¨trace_idÙ$b88cbbc1-1edd-4ba3-a071-4c9630759f33­parent_run_idÀ¦run_idÙ$b88cbbc1-1edd-4ba3-a071-4c9630759f33ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙaWhat parameter in `RunnableConfig` controls the maximum number of parallel calls during batching?§context•‚´page_content_summaryÙË| max_concurrency | Maximum number of parallel calls to make (e.g., used by batch).                            |
| recursion_limit | Maximum number of times a call can recurse (e.g., used by Runnables...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## RunnableConfig

Any of the methods that are used to execute the runnable (e.g., `invoke`, `batch`, `stream`, `astream_events`) accept a second argument called
`RunnableConfig` ([API Reference](http...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË<a id="batch"></a>
### Optimized parallel execution (batch)
<span data-heading-keywords="batch"></span>

LangChain Runnables offer a built-in `batch` (and `batch_as_completed`) API that allow you to p...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÙâThe parameter in `RunnableConfig` that controls the maximum number of parallel calls during batching is `max_concurrency`. This attribute allows you to limit the number of parallel calls to prevent overloading a server or API.¨query_id¤Q032ªfocus_kind¶chain_execution_result‰¢idÙ@5a9269aebe8e3462af10c5acf7f9fdc3fba0279589d6d56d951030aaccbe6725¢tsËAÚÔc0Â¨trace_idÙ$3cf7ea9e-a5bc-4f0f-b7e5-9b16da2a6c8f­parent_run_idÙ$2f88ef2f-238b-458d-810c-abfe35833bc0¦run_idÙ$3cf7ea9e-a5bc-4f0f-b7e5-9b16da2a6c8fªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË### JSON mode

In addition to tool calling, some model providers support a feature called `JSON mode`. 
This supports JSON schema definition as input and enforces the model to produce a conforming JSO...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙË![Diagram of with structured output](/img/with_structured_output.png)

This both binds the schema to the model as a tool and parses the output to the specified output schema. 

```python
# Bind the sc...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙË# Structured outputs

## Overview 

For many applications, such as chatbots, models need to respond to users directly in natural language. 
However, there are scenarios where we need models to output ...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË```python
# Tool creation
tools = [my_tool]
# Tool binding
model_with_tools = model.bind_tools(tools)
```

Similarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/)...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¨query_id¤Q033ªfocus_kind¶chain_execution_result†¢idÙ@a4dc284cca8706d273b0597f7cac7611ab9f929e0f113def8c4ff3de8b9c32c0¢tsËAÚÔcòM¨focus_msÍ&§payload‚¤role©assistant§contentÚ³An example of using `with_structured_output` to get JSON back from a chat model is as follows:

```python
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o").with_structured_output(method="json_mode")
ai_msg = model.invoke("Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]")
```

This would return a JSON object like `{'random_ints': [45, 67, 12, 34, 89, 23, 78, 56, 90, 11]}`.ªfocus_kind¬exec_latency¨query_id¤Q033‰¢idÙ@81fedfe48f4f3134ba8f80786b2cdb54e4abe2e2cb1797d83831b280b1f6dce9¢tsËAÚÔcòÚ¨trace_idÙ$392ea8b3-bb28-4039-a517-6c3f23361d98­parent_run_idÙ$2f88ef2f-238b-458d-810c-abfe35833bc0¦run_idÙ$392ea8b3-bb28-4039-a517-6c3f23361d98ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ³An example of using `with_structured_output` to get JSON back from a chat model is as follows:

```python
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o").with_structured_output(method="json_mode")
ai_msg = model.invoke("Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]")
```

This would return a JSON object like `{'random_ints': [45, 67, 12, 34, 89, 23, 78, 56, 90, 11]}`.¨query_id¤Q033ªfocus_kind¶chain_execution_result‰¢idÙ@8c4c6b497f9f812faac35b84e7e9c2b65738954811059b6ee54e25a554e97131¢tsËAÚÔcò(¢¨trace_idÙ$2f88ef2f-238b-458d-810c-abfe35833bc0­parent_run_idÀ¦run_idÙ$2f88ef2f-238b-458d-810c-abfe35833bc0ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙUGive an example of using `with_structured_output` to get JSON back from a chat model.§context•‚´page_content_summaryÙË### JSON mode

In addition to tool calling, some model providers support a feature called `JSON mode`. 
This supports JSON schema definition as input and enforces the model to produce a conforming JSO...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙË![Diagram of with structured output](/img/with_structured_output.png)

This both binds the schema to the model as a tool and parses the output to the specified output schema. 

```python
# Bind the sc...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙË# Structured outputs

## Overview 

For many applications, such as chatbots, models need to respond to users directly in natural language. 
However, there are scenarios where we need models to output ...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/structured_outputs.mdx‚´page_content_summaryÙË## Structured outputs

Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely
useful for information extraction tasks. P...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË```python
# Tool creation
tools = [my_tool]
# Tool binding
model_with_tools = model.bind_tools(tools)
```

Similarly, getting models to produce [structured outputs](/docs/concepts/structured_outputs/)...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answerÚ³An example of using `with_structured_output` to get JSON back from a chat model is as follows:

```python
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o").with_structured_output(method="json_mode")
ai_msg = model.invoke("Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]")
```

This would return a JSON object like `{'random_ints': [45, 67, 12, 34, 89, 23, 78, 56, 90, 11]}`.¨query_id¤Q033ªfocus_kind¶chain_execution_result‰¢idÙ@8a941b96e14d8550b2171c04f4079e69f1e05f40b7b758d23df784b77cbce340¢tsËAÚÔdM¨trace_idÙ$e976f9fc-7571-440d-b06e-12da0f2b1943­parent_run_idÙ$7ad16231-054e-451a-999b-971d258fdbf6¦run_idÙ$e976f9fc-7571-440d-b06e-12da0f2b1943ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Callbacks

:::note Prerequisites
- [Runnable interface](/docs/concepts/runnables)
:::

LangChain provides a callback system that allows you to hook into the various stages of your LLM application. T...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/callbacks.mdx‚´page_content_summaryÙË- **In LCEL Pipelines:**
Streaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙÍ#### Usage with LCEL

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the outp...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËPlease refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.

## Streaming APIs
<span data-heading-keywords="streaming-api"></span>

Streaming is critical in ma...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q034ªfocus_kind¶chain_execution_result†¢idÙ@9a1faf1126ed2db423df18eba514f201a2e5ddbac8e39bcb799510decf25c9b5¢tsËAÚÔd\œ®¨focus_msÍM§payload‚¤role©assistant§contentÚATo stream intermediate events from a Runnable, you would use the `astream_events` API. This allows access to custom data and intermediate outputs from LLM applications built with LangChain's Expression Language (LCEL). Additionally, you can also use the async `astream` method for streaming outputs as they are generated.ªfocus_kind¬exec_latency¨query_id¤Q034‰¢idÙ@bd6ada5da9db9e7554c6a8f763339c107f86b8a1d8dd38c6da1c44d229233145¢tsËAÚÔd\¸=¨trace_idÙ$af53b1df-af90-4621-ae38-5ef93b857bba­parent_run_idÙ$7ad16231-054e-451a-999b-971d258fdbf6¦run_idÙ$af53b1df-af90-4621-ae38-5ef93b857bbaªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚATo stream intermediate events from a Runnable, you would use the `astream_events` API. This allows access to custom data and intermediate outputs from LLM applications built with LangChain's Expression Language (LCEL). Additionally, you can also use the async `astream` method for streaming outputs as they are generated.¨query_id¤Q034ªfocus_kind¶chain_execution_result‰¢idÙ@8d32c4d02a41eb74c02c23dfa18026bae87965e09cdce60546f344f4333c5e09¢tsËAÚÔd\Ãä¨trace_idÙ$7ad16231-054e-451a-999b-971d258fdbf6­parent_run_idÀ¦run_idÙ$7ad16231-054e-451a-999b-971d258fdbf6ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙJWhat callback would you use to stream intermediate events from a Runnable?§context•‚´page_content_summaryÙË# Callbacks

:::note Prerequisites
- [Runnable interface](/docs/concepts/runnables)
:::

LangChain provides a callback system that allows you to hook into the various stages of your LLM application. T...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/callbacks.mdx‚´page_content_summaryÙË- **In LCEL Pipelines:**
Streaming updates from an [LCEL](/docs/concepts/lcel) pipeline involves capturing progress from individual **sub-runnables**. For example, as different steps or components of ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙÍ#### Usage with LCEL

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the outp...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËPlease refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.

## Streaming APIs
<span data-heading-keywords="streaming-api"></span>

Streaming is critical in ma...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚATo stream intermediate events from a Runnable, you would use the `astream_events` API. This allows access to custom data and intermediate outputs from LLM applications built with LangChain's Expression Language (LCEL). Additionally, you can also use the async `astream` method for streaming outputs as they are generated.¨query_id¤Q034ªfocus_kind¶chain_execution_result‰¢idÙ@21f7f61ab2e4edcb1a0710404fcbdb329851f9978b7cd22095d68cdfbf759660¢tsËAÚÔdyİ¹¨trace_idÙ$7679317c-164f-4472-9c03-910a03bdb704­parent_run_idÙ$5a79c16f-3ee5-410f-8800-0d858df18655¦run_idÙ$7679317c-164f-4472-9c03-910a03bdb704ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËMany of the key methods of chat models operate on [messages](/docs/concepts/messages) as input and return messages as output.

Chat models offer a standard set of parameters that can be used to config...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË### Inputs and outputs

Modern LLMs are typically accessed through a chat model interface that takes [messages](/docs/concepts/messages) as input and returns [messages](/docs/concepts/messages) as out...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËSome important things to note:

- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration fo...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË| `temperature`  | Controls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.  ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q035ªfocus_kind¶chain_execution_result†¢idÙ@8dd63cb8d98718377c40d5e1af03508cf72591670c53842e18756bcd3c6a85c2¢tsËAÚÔeà¨focus_msÍ
§payload‚¤role©assistant§contentÚTThe two standard parameters that influence generation randomness and length in chat models are `temperature` and `max_tokens`. The `temperature` controls the randomness of the model's output, while `max_tokens` limits the total number of tokens in the response. Together, they help shape the creativity and length of the generated messages.ªfocus_kind¬exec_latency¨query_id¤Q035‰¢idÙ@a8ecaffba7d9134603adef5e97912a6dc7f7486520910e3a6bbdfab4e8a05d5d¢tsËAÚÔeê5¨trace_idÙ$4dc94429-18be-44e1-ac5a-468f224e2c24­parent_run_idÙ$5a79c16f-3ee5-410f-8800-0d858df18655¦run_idÙ$4dc94429-18be-44e1-ac5a-468f224e2c24ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚTThe two standard parameters that influence generation randomness and length in chat models are `temperature` and `max_tokens`. The `temperature` controls the randomness of the model's output, while `max_tokens` limits the total number of tokens in the response. Together, they help shape the creativity and length of the generated messages.¨query_id¤Q035ªfocus_kind¶chain_execution_result‰¢idÙ@c383b8d045b15e795ce214bbb11b0e027df621c8a5972596fdfa12b3e0eaecef¢tsËAÚÔeù ¨trace_idÙ$5a79c16f-3ee5-410f-8800-0d858df18655­parent_run_idÀ¦run_idÙ$5a79c16f-3ee5-410f-8800-0d858df18655ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙXWhich two standard parameters influence generation randomness and length in chat models?§context•‚´page_content_summaryÙËMany of the key methods of chat models operate on [messages](/docs/concepts/messages) as input and return messages as output.

Chat models offer a standard set of parameters that can be used to config...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË### Inputs and outputs

Modern LLMs are typically accessed through a chat model interface that takes [messages](/docs/concepts/messages) as input and returns [messages](/docs/concepts/messages) as out...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËSome important things to note:

- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration fo...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË| `temperature`  | Controls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.  ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚTThe two standard parameters that influence generation randomness and length in chat models are `temperature` and `max_tokens`. The `temperature` controls the randomness of the model's output, while `max_tokens` limits the total number of tokens in the response. Together, they help shape the creativity and length of the generated messages.¨query_id¤Q035ªfocus_kind¶chain_execution_result‰¢idÙ@689b515b7c3a9800d6a9dbb99f522907d1d1ddcf1be65f7581d07bb86c2c7306¢tsËAÚÔe++í¨trace_idÙ$bc43cdba-0693-4973-920d-2098f0dd66b1­parent_run_idÙ$fce39679-ea4c-4e8d-8ba4-9a548f351efd¦run_idÙ$bc43cdba-0693-4973-920d-2098f0dd66b1ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËThere are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.


<Tabs>
  <TabIt...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙË# How to create and query vector stores

:::info
Head to [Integrations](/docs/integrations/vectorstores/) for documentation on built-in integrations with 3rd-party vector stores.
:::

One of the most ...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙËThese vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.

Vector stores are frequently used to search over unstructured data, ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË```python
from langchain_community.vectorstores import FAISS

db = FAISS.from_documents(documents, OpenAIEmbeddings())
```

  </TabItem>
  <TabItem value="lance" label="Lance">

This notebook shows ho...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙËBefore using the vectorstore at all, we need to load some data and initialize an embedding model.

We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.

```python
import os
import get...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx¨query_id¤Q036ªfocus_kind¶chain_execution_result†¢idÙ@1114a1aae7c11c7bf5a1eccfecfc3a26d4af441742354bb76e8624af324b94b7¢tsËAÚÔeWÙD¨focus_msÍ¸§payload‚¤role©assistant§contentÙvThe provided context does not specify how to persist a FAISS vector store to disk. Therefore, I don't know the answer.ªfocus_kind¬exec_latency¨query_id¤Q036‰¢idÙ@4433a5dc8727caefae841619b8904d153d28ce7ee3e86a13bbf5b12f6c62782c¢tsËAÚÔeWßç¨trace_idÙ$35a7ef7e-b8c5-458a-bc2a-5c87836a16f1­parent_run_idÙ$fce39679-ea4c-4e8d-8ba4-9a548f351efd¦run_idÙ$35a7ef7e-b8c5-458a-bc2a-5c87836a16f1ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙvThe provided context does not specify how to persist a FAISS vector store to disk. Therefore, I don't know the answer.¨query_id¤Q036ªfocus_kind¶chain_execution_result‰¢idÙ@19d46cd4a746ad9e065f1757770addfbf8adba409b76e8ab050cb08815c39a5d¢tsËAÚÔeWå†¨trace_idÙ$fce39679-ea4c-4e8d-8ba4-9a548f351efd­parent_run_idÀ¦run_idÙ$fce39679-ea4c-4e8d-8ba4-9a548f351efdªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ0How do you persist a FAISS vector store to disk?§context•‚´page_content_summaryÙËThere are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.


<Tabs>
  <TabIt...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙË# How to create and query vector stores

:::info
Head to [Integrations](/docs/integrations/vectorstores/) for documentation on built-in integrations with 3rd-party vector stores.
:::

One of the most ...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙËThese vectors, called [embeddings](/docs/concepts/embedding_models/), capture the semantic meaning of data that has been embedded.

Vector stores are frequently used to search over unstructured data, ...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/vectorstores.mdx‚´page_content_summaryÙË```python
from langchain_community.vectorstores import FAISS

db = FAISS.from_documents(documents, OpenAIEmbeddings())
```

  </TabItem>
  <TabItem value="lance" label="Lance">

This notebook shows ho...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx‚´page_content_summaryÙËBefore using the vectorstore at all, we need to load some data and initialize an embedding model.

We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.

```python
import os
import get...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/vectorstores.mdx¦answerÙvThe provided context does not specify how to persist a FAISS vector store to disk. Therefore, I don't know the answer.¨query_id¤Q036ªfocus_kind¶chain_execution_result‰¢idÙ@fdd9c8886e3dfa9990800ff23444ec38000efb1d4117f90b52017df1401545fa¢tsËAÚÔefÚè¨trace_idÙ$42ba3c8e-5c2f-45a8-b4d3-47d2eddb91bb­parent_run_idÙ$6017ce13-e826-4c09-af3a-46bfdad26517¦run_idÙ$42ba3c8e-5c2f-45a8-b4d3-47d2eddb91bbªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙÍ#### Usage with LCEL

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the outp...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËThe [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming b...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËPlease refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.

## Streaming APIs
<span data-heading-keywords="streaming-api"></span>

Streaming is critical in ma...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::note
In addition, there is a **legacy** async [astream_log](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runna...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¨query_id¤Q037ªfocus_kind¶chain_execution_result†¢idÙ@9f2b405e98a5ab8e65f9bfae68ebf48fbc745e61d0b22e9d247fd20b8dd957ad¢tsËAÚÔeÏOf¨focus_msÍ_§payload‚¤role©assistant§contentÚ}The `stream()` method is synchronous and yields output chunks as they are produced, allowing for real-time processing in a blocking manner. In contrast, the `astream()` method is asynchronous, designed for non-blocking workflows, enabling real-time streaming in asynchronous code. Both methods facilitate incremental output from Runnables, but they differ in their execution model.ªfocus_kind¬exec_latency¨query_id¤Q037‰¢idÙ@a48980e670928fd3239dca7b8c8821492c45bd9468298c42d3ba7e96577469c7¢tsËAÚÔeÏbÔ¨trace_idÙ$7154d157-95ea-4552-b5ef-0017658a8ca8­parent_run_idÙ$6017ce13-e826-4c09-af3a-46bfdad26517¦run_idÙ$7154d157-95ea-4552-b5ef-0017658a8ca8ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ}The `stream()` method is synchronous and yields output chunks as they are produced, allowing for real-time processing in a blocking manner. In contrast, the `astream()` method is asynchronous, designed for non-blocking workflows, enabling real-time streaming in asynchronous code. Both methods facilitate incremental output from Runnables, but they differ in their execution model.¨query_id¤Q037ªfocus_kind¶chain_execution_result‰¢idÙ@d335f6b1ee2e8399bf3091136e0d3db53dca57964510b2c798132e150cca7411¢tsËAÚÔeÏq ¨trace_idÙ$6017ce13-e826-4c09-af3a-46bfdad26517­parent_run_idÀ¦run_idÙ$6017ce13-e826-4c09-af3a-46bfdad26517ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙLExplain the difference between `stream` and `astream` methods on a Runnable.§context•‚´page_content_summaryÙË1. sync [stream](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runnable.stream) and async [astream](https://python...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙÍ#### Usage with LCEL

If you compose multiple Runnables using [LangChainâ€™s Expression Language (LCEL)](/docs/concepts/lcel), the `stream()` and `astream()` methods will, by convention, stream the outp...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËThe [asynchronous version](/docs/concepts/async), `astream()`, works similarly but is designed for non-blocking workflows. You can use it in asynchronous code to achieve the same real-time streaming b...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙËPlease refer to the [Async Programming with LangChain](/docs/concepts/async) guide for more details.

## Streaming APIs
<span data-heading-keywords="streaming-api"></span>

Streaming is critical in ma...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::note
In addition, there is a **legacy** async [astream_log](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#langchain_core.runnables.base.Runna...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¦answerÚ}The `stream()` method is synchronous and yields output chunks as they are produced, allowing for real-time processing in a blocking manner. In contrast, the `astream()` method is asynchronous, designed for non-blocking workflows, enabling real-time streaming in asynchronous code. Both methods facilitate incremental output from Runnables, but they differ in their execution model.¨query_id¤Q037ªfocus_kind¶chain_execution_result‰¢idÙ@9b986476e4a14af8cfe6af197dfc405714a58d864c575e9595b65385d4cc39db¢tsËAÚÔeâÆ½¨trace_idÙ$18707b9c-75ef-4217-b2a2-0f9425938c45­parent_run_idÙ$6e677793-dbd6-4581-a107-0f20443faa5d¦run_idÙ$18707b9c-75ef-4217-b2a2-0f9425938c45ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËThe newest generation of chat models offer additional capabilities:

* [Tool calling](/docs/concepts/tool_calling): Many popular chat models offer a native [tool calling](/docs/concepts/tool_calling) ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Key features

This highlights functionality that is core to using LangChain.

- [How to: return structured data from a model](/docs/how_to/structured_output/)
- [How to: use a model to call tools](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¨query_id¤Q038ªfocus_kind¶chain_execution_result†¢idÙ@cf0b2edaec7d19ca5504de08d05e7344da2a010dc61b7cd9ced5595a5a982623¢tsËAÚÔfıI#¨focus_msÍ=§payload‚¤role©assistant§contentÚ\LangChain provides example selectors that help in selecting the correct few-shot examples to pass to the prompt, ensuring that the prompts fit within an LLM's context window. These selectors can choose examples based on various criteria, such as length or semantic similarity. This functionality aids in optimizing the input for the language model.ªfocus_kind¬exec_latency¨query_id¤Q038‰¢idÙ@697036f61b1c45f1532b93c2139bc0d41f31a4b9ede908dd991b9a37b8c12ad1¢tsËAÚÔfıšş¨trace_idÙ$9a30ee7d-a19b-4143-af88-6ad98a8da128­parent_run_idÙ$6e677793-dbd6-4581-a107-0f20443faa5d¦run_idÙ$9a30ee7d-a19b-4143-af88-6ad98a8da128ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ\LangChain provides example selectors that help in selecting the correct few-shot examples to pass to the prompt, ensuring that the prompts fit within an LLM's context window. These selectors can choose examples based on various criteria, such as length or semantic similarity. This functionality aids in optimizing the input for the language model.¨query_id¤Q038ªfocus_kind¶chain_execution_result‰¢idÙ@48ae068b0c80025f3ff85f85741e4d593cd53adc7da673bd3179aea4a9b03262¢tsËAÚÔfı¥µ¨trace_idÙ$6e677793-dbd6-4581-a107-0f20443faa5d­parent_run_idÀ¦run_idÙ$6e677793-dbd6-4581-a107-0f20443faa5dªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙYWhat feature does LangChain provide to ensure prompts fit within an LLM's context window?§context•‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË### Example selectors

[Example Selectors](/docs/concepts/example_selectors) are responsible for selecting the correct few shot examples to pass to the prompt.

- [How to: use example selectors](/docs...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËThe newest generation of chat models offer additional capabilities:

* [Tool calling](/docs/concepts/tool_calling): Many popular chat models offer a native [tool calling](/docs/concepts/tool_calling) ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## Key features

This highlights functionality that is core to using LangChain.

- [How to: return structured data from a model](/docs/how_to/structured_output/)
- [How to: use a model to call tools](...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## Features

There are several primary needs that LangChain aims to address:

1. **Standardized component interfaces:** The growing number of [models](/docs/integrations/chat/) and [related components...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx¦answerÚ\LangChain provides example selectors that help in selecting the correct few-shot examples to pass to the prompt, ensuring that the prompts fit within an LLM's context window. These selectors can choose examples based on various criteria, such as length or semantic similarity. This functionality aids in optimizing the input for the language model.¨query_id¤Q038ªfocus_kind¶chain_execution_result‰¢idÙ@15d2344f8b1ee58c3d0e74f772e10a8034317636b7e7a1ca3cde7880c2969c03¢tsËAÚÔg¶|¨trace_idÙ$92d88770-7d8f-46c9-9027-45cee5665294­parent_run_idÙ$2671eadb-b2d1-4700-afa2-cb00f686afbf¦run_idÙ$92d88770-7d8f-46c9-9027-45cee5665294ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# Messages

:::info Prerequisites
- [Chat Models](/docs/concepts/chat_models)
:::

## Overview

Messages are the unit of communication in [chat models](/docs/concepts/chat_models). They are used to re...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙËFor example, a typical conversation structure might look like this:

1. **User Message**: "Hello, how are you?"
2. **Assistant Message**: "I'm doing well, thank you for asking."
3. **User Message**: "...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙËThe five main message types are:

- [SystemMessage](#systemmessage): corresponds to **system** role
- [HumanMessage](#humanmessage): corresponds to **user** role
- [AIMessage](#aimessage): corresponds...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË### Inputs and outputs

Modern LLMs are typically accessed through a chat model interface that takes [messages](/docs/concepts/messages) as input and returns [messages](/docs/concepts/messages) as out...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## What is inside a message?

A message typically consists of the following pieces of information:

- **Role**: The role of the message (e.g., "user", "assistant").
- **Content**: The content of the m...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx¨query_id¤Q039ªfocus_kind¶chain_execution_result†¢idÙ@83e1317f512d0a479dd3fa5219a52d47f94d5437ada051dead3341c5ea0ea0f9¢tsËAÚÔgMÓ9¨focus_msÍø§payload‚¤role©assistant§contentÙÁThree message roles supported by LangChain chat messages are **system**, **user** (or **human**), and **assistant** (or **AI**). Additionally, there are roles for streaming responses and tools.ªfocus_kind¬exec_latency¨query_id¤Q039‰¢idÙ@275241e2ad67faa9edf07d0bc4f47bfad75ac3166511bfae2042b562ddec8fd7¢tsËAÚÔgMğõ¨trace_idÙ$5e206c94-7f3e-4a29-8e8e-c6f5dcbeb549­parent_run_idÙ$2671eadb-b2d1-4700-afa2-cb00f686afbf¦run_idÙ$5e206c94-7f3e-4a29-8e8e-c6f5dcbeb549ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙÁThree message roles supported by LangChain chat messages are **system**, **user** (or **human**), and **assistant** (or **AI**). Additionally, there are roles for streaming responses and tools.¨query_id¤Q039ªfocus_kind¶chain_execution_result‰¢idÙ@97e75e85187241c75d4e8213ca6b4e6122c33dbb44b8575e7f25a1a362daadde¢tsËAÚÔgN y¨trace_idÙ$2671eadb-b2d1-4700-afa2-cb00f686afbf­parent_run_idÀ¦run_idÙ$2671eadb-b2d1-4700-afa2-cb00f686afbfªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙ>Name three message roles supported by LangChain chat messages.§context•‚´page_content_summaryÙË# Messages

:::info Prerequisites
- [Chat Models](/docs/concepts/chat_models)
:::

## Overview

Messages are the unit of communication in [chat models](/docs/concepts/chat_models). They are used to re...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙËFor example, a typical conversation structure might look like this:

1. **User Message**: "Hello, how are you?"
2. **Assistant Message**: "I'm doing well, thank you for asking."
3. **User Message**: "...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙËThe five main message types are:

- [SystemMessage](#systemmessage): corresponds to **system** role
- [HumanMessage](#humanmessage): corresponds to **user** role
- [AIMessage](#aimessage): corresponds...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx‚´page_content_summaryÙË### Inputs and outputs

Modern LLMs are typically accessed through a chat model interface that takes [messages](/docs/concepts/messages) as input and returns [messages](/docs/concepts/messages) as out...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## What is inside a message?

A message typically consists of the following pieces of information:

- **Role**: The role of the message (e.g., "user", "assistant").
- **Content**: The content of the m...¨metadata¦sourceÙG/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/messages.mdx¦answerÙÁThree message roles supported by LangChain chat messages are **system**, **user** (or **human**), and **assistant** (or **AI**). Additionally, there are roles for streaming responses and tools.¨query_id¤Q039ªfocus_kind¶chain_execution_result‰¢idÙ@bc4fbde0bb63ecc2566ef3b28e0b8d12f7802f9b0e351ca2c45dbfcd428dc540¢tsËAÚÔgfü¨trace_idÙ$ed354f68-5f9f-44f2-9c17-3cee3677ca7f­parent_run_idÙ$80988a68-6cd7-425c-9cdb-4a454ab2841f¦run_idÙ$ed354f68-5f9f-44f2-9c17-3cee3677ca7fªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we comb...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small ...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË```python
documents = my_retriever.invoke("What is the meaning of life?")
```

## Orchestration 

While standardization for individual components is useful, we've increasingly seen that developers wan...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```

## Advanced retrieval patterns

### Ensemble 

Because the retriever interface is so simple, returning a list of `Do...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË# Retrievers

<span data-heading-keywords="retriever,retrievers"></span>

:::info[Prerequisites]

* [Vector stores](/docs/concepts/vectorstores/)
* [Embeddings](/docs/concepts/embedding_models/)
* [Te...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¨query_id¤Q040ªfocus_kind¶chain_execution_result†¢idÙ@29c089231da10cfd1ca1a4faa97bcb8d7ec94d711d999d74499e517f225f1a1b¢tsËAÚÔg¿ø^¨focus_msÍt§payload‚¤role©assistant§contentÚ”To combine documents returned by multiple retrievers before passing them to an LLM, you can use an ensemble retriever that merges results from different retrievers with weighted scores. This approach allows for re-ranking of the documents based on their relevance, ensuring that the best results are prioritized. Additionally, retaining links to the original documents helps maintain context for the LLM.ªfocus_kind¬exec_latency¨query_id¤Q040‰¢idÙ@857831edab8032290f9a7ecc2ad4b32913016bf55293a3cda5814874bf018b70¢tsËAÚÔgÀÄ¨trace_idÙ$76c0cf9e-d109-4147-8fa1-4efa2e843b97­parent_run_idÙ$80988a68-6cd7-425c-9cdb-4a454ab2841f¦run_idÙ$76c0cf9e-d109-4147-8fa1-4efa2e843b97ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ”To combine documents returned by multiple retrievers before passing them to an LLM, you can use an ensemble retriever that merges results from different retrievers with weighted scores. This approach allows for re-ranking of the documents based on their relevance, ensuring that the best results are prioritized. Additionally, retaining links to the original documents helps maintain context for the LLM.¨query_id¤Q040ªfocus_kind¶chain_execution_result‰¢idÙ@e4d53bd4ed2e99147ae9e1702f85872536832ffa8b56a767092fa8c961260f4c¢tsËAÚÔgÀv¨trace_idÙ$80988a68-6cd7-425c-9cdb-4a454ab2841f­parent_run_idÀ¦run_idÙ$80988a68-6cd7-425c-9cdb-4a454ab2841fªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙJHow can you combine docs returned by a Retriever before passing to an LLM?§context•‚´page_content_summaryÙË```python
# Initialize the ensemble retriever
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5]
)
```

When ensembling, how do we comb...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË![Retrieval with full docs](/img/retriever_full_docs.png)

This is particularly useful in AI applications, because it ensures no loss in document context for the model.
For example, you may use small ...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË```python
documents = my_retriever.invoke("What is the meaning of life?")
```

## Orchestration 

While standardization for individual components is useful, we've increasingly seen that developers wan...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË```python
vectorstore = MyVectorStore()
retriever = vectorstore.as_retriever()
```

## Advanced retrieval patterns

### Ensemble 

Because the retriever interface is so simple, returning a list of `Do...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx‚´page_content_summaryÙË# Retrievers

<span data-heading-keywords="retriever,retrievers"></span>

:::info[Prerequisites]

* [Vector stores](/docs/concepts/vectorstores/)
* [Embeddings](/docs/concepts/embedding_models/)
* [Te...¨metadata¦sourceÙI/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrievers.mdx¦answerÚ”To combine documents returned by multiple retrievers before passing them to an LLM, you can use an ensemble retriever that merges results from different retrievers with weighted scores. This approach allows for re-ranking of the documents based on their relevance, ensuring that the best results are prioritized. Additionally, retaining links to the original documents helps maintain context for the LLM.¨query_id¤Q040ªfocus_kind¶chain_execution_result‰¢idÙ@9d41468bcd0a3ec0641c6dca7de61781d6edce270aa5c01ae04c20b70dce6605¢tsËAÚÔgÎ‘¸¨trace_idÙ$49dd6c13-90b7-4938-b97f-b7277946ecab­parent_run_idÙ$0c01da01-7845-4ada-a09f-307cc756ab3f¦run_idÙ$49dd6c13-90b7-4938-b97f-b7277946ecabªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË| `base_url`     | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.                                ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙÊIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

You have a few options to deal with rate limits:...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q041ªfocus_kind¶chain_execution_result†¢idÙ@4f90831805a968a3c675349fa3344786fbcb06137add249a1b42da915f40f0dd¢tsËAÚÔhE¢M¨focus_msÍC§payload‚¤role©assistant§contentÚ–The `rate_limiter` parameter accepts an instance of `BaseRateLimiter`, which is used to space out requests to avoid exceeding rate limits imposed by the API provider. This is useful for managing the frequency of requests, especially during benchmarking or when handling potential rate limit errors. By controlling the request rate, it helps ensure smoother interactions with the API without hitting limits.ªfocus_kind¬exec_latency¨query_id¤Q041‰¢idÙ@2b226b5ed5101980298c6b257a573119ab1f9d635cb61e11f6f0ab81365d1ba1¢tsËAÚÔhE²¨trace_idÙ$73b05276-1171-457d-a4ad-2ae684c3b7be­parent_run_idÙ$0c01da01-7845-4ada-a09f-307cc756ab3f¦run_idÙ$73b05276-1171-457d-a4ad-2ae684c3b7beªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ–The `rate_limiter` parameter accepts an instance of `BaseRateLimiter`, which is used to space out requests to avoid exceeding rate limits imposed by the API provider. This is useful for managing the frequency of requests, especially during benchmarking or when handling potential rate limit errors. By controlling the request rate, it helps ensure smoother interactions with the API without hitting limits.¨query_id¤Q041ªfocus_kind¶chain_execution_result‰¢idÙ@3dd7c34f57772b2d683ebe23536d5eeabad0696d23740831ffc98ed8b6e8b9ab¢tsËAÚÔhEºV¨trace_idÙ$0c01da01-7845-4ada-a09f-307cc756ab3f­parent_run_idÀ¦run_idÙ$0c01da01-7845-4ada-a09f-307cc756ab3fªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙCWhat does the `rate_limiter` parameter accept and why is it useful?§context•‚´page_content_summaryÙË| `base_url`     | The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.                                ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË1. Try to avoid hitting rate limits by spacing out requests: Chat models accept a `rate_limiter` parameter that can be provided during initialization. This parameter is used to control the rate at whi...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙÊIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.

You have a few options to deal with rate limits:...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË# Do something with the run_id
```

### Setting recursion limit

:::note
This is an advanced feature that is unnecessary for most users.
:::

Some Runnables may return other Runnables, which can lead ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚ–The `rate_limiter` parameter accepts an instance of `BaseRateLimiter`, which is used to space out requests to avoid exceeding rate limits imposed by the API provider. This is useful for managing the frequency of requests, especially during benchmarking or when handling potential rate limit errors. By controlling the request rate, it helps ensure smoother interactions with the API without hitting limits.¨query_id¤Q041ªfocus_kind¶chain_execution_result‰¢idÙ@40f027c38e5c163670cccbff5adbce73aaf69df540110e1f74978f0e432d739e¢tsËAÚÔhRÛe¨trace_idÙ$0adcc3ee-9b36-43f7-9eb2-bdb83de65272­parent_run_idÙ$aba2c94b-6167-4f1f-a76f-9f3e16aa3c0a¦run_idÙ$0adcc3ee-9b36-43f7-9eb2-bdb83de65272ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË# Chat models

## Overview

Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËmodel = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË## Streaming APIs

LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), includin...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. LangGraph's [StreamWriter](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamWriter) can be used to write custom data that will surface through **stream** and **astrea...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¨query_id¤Q042ªfocus_kind¶chain_execution_result†¢idÙ@acded0902fe7bf56e613c297567ac5a9f87bc893fabeac25d6a83f25bd7cdb5c¢tsËAÚÔhâ%Ñ¨focus_msÍ½§payload‚¤role©assistant§contentÚÄHereâ€™s a code snippet to stream tokens from an OpenAI chat model using LCEL:

```python
model = ChatAnthropic(model="claude-3-sonnet-20240229")
prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for event in chain.astream_events({"topic": "parrot"}):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(event, end="|", flush=True)
```ªfocus_kind¬exec_latency¨query_id¤Q042‰¢idÙ@8cccc0fc6e12b71baa55f82e0610b389caa06c65f8d8157a654b70f53cc90ab9¢tsËAÚÔhâ®§¨trace_idÙ$a1187c05-e46a-46da-9359-91da49617bca­parent_run_idÙ$aba2c94b-6167-4f1f-a76f-9f3e16aa3c0a¦run_idÙ$a1187c05-e46a-46da-9359-91da49617bcaªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚÄHereâ€™s a code snippet to stream tokens from an OpenAI chat model using LCEL:

```python
model = ChatAnthropic(model="claude-3-sonnet-20240229")
prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for event in chain.astream_events({"topic": "parrot"}):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(event, end="|", flush=True)
```¨query_id¤Q042ªfocus_kind¶chain_execution_result‰¢idÙ@09503543813ef94bf3e87c65890e690f3aa1614d36743e5388e2e832e1e60965¢tsËAÚÔhâ¸ö¨trace_idÙ$aba2c94b-6167-4f1f-a76f-9f3e16aa3c0a­parent_run_idÀ¦run_idÙ$aba2c94b-6167-4f1f-a76f-9f3e16aa3c0aªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙMProvide a code snippet to stream tokens from an OpenAI chat model using LCEL.§context•‚´page_content_summaryÙË- **Simplify streaming**: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(...¨metadata¦sourceÙC/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/lcel.mdx‚´page_content_summaryÙË# Chat models

## Overview

Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËmodel = ChatAnthropic(model="claude-3-sonnet-20240229")

prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË## Streaming APIs

LangChain has two main APIs for streaming output in real-time. These APIs are supported by any component that implements the [Runnable Interface](/docs/concepts/runnables), includin...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx‚´page_content_summaryÙË1. LangGraph's [StreamWriter](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.StreamWriter) can be used to write custom data that will surface through **stream** and **astrea...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¦answerÚÄHereâ€™s a code snippet to stream tokens from an OpenAI chat model using LCEL:

```python
model = ChatAnthropic(model="claude-3-sonnet-20240229")
prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for event in chain.astream_events({"topic": "parrot"}):
    kind = event["event"]
    if kind == "on_chat_model_stream":
        print(event, end="|", flush=True)
```¨query_id¤Q042ªfocus_kind¶chain_execution_result‰¢idÙ@46429cfef65ac7d27dcea5b8d8629eed21224285e5d6154031e04afc9340990f¢tsËAÚÔhò¨trace_idÙ$a8392183-ed21-415b-896d-b0bebd2265bd­parent_run_idÙ$52dbf3cc-c88c-4df3-83a9-dcd5b10203d1¦run_idÙ$a8392183-ed21-415b-896d-b0bebd2265bdªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build compl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process fr...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## How tokens correspond to text

Please see this post from [OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) for more details on how tokens are counted and h...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙË- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.
- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts t...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/index.mdx¨query_id¤Q043ªfocus_kind¶chain_execution_result†¢idÙ@a3712116d0432adbc8573e6dff2cb887b14027d84d656f66e647443efdd9a7e6¢tsËAÚÔi'=¨focus_msÍÔ§payload‚¤role©assistant§content­I don't know.ªfocus_kind¬exec_latency¨query_id¤Q043‰¢idÙ@bb9e10852abaa527be302310d27a6c53123411c10eb09ced796680d693bf4de3¢tsËAÚÔi5&¨trace_idÙ$6b8930bd-8912-46df-a1f8-5a003a8a087a­parent_run_idÙ$52dbf3cc-c88c-4df3-83a9-dcd5b10203d1¦run_idÙ$6b8930bd-8912-46df-a1f8-5a003a8a087aªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answer­I don't know.¨query_id¤Q043ªfocus_kind¶chain_execution_result‰¢idÙ@b2956c2d9ced4418f9deb8f50be1104ccf9e29f4dd2ee281a85140821101fcfa¢tsËAÚÔi;q¨trace_idÙ$52dbf3cc-c88c-4df3-83a9-dcd5b10203d1­parent_run_idÀ¦run_idÙ$52dbf3cc-c88c-4df3-83a9-dcd5b10203d1ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙGWhich notebook demonstrates how to track token usage costs per request?§context•‚´page_content_summaryÙËIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the contex...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË:::info[Further reading]

Have a look at our free course, [Introduction to LangGraph](https://academy.langchain.com/courses/intro-to-langgraph), to learn more about how to use LangGraph to build compl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙË### Evaluation
<span data-heading-keywords="evaluation,evaluate"></span>

Evaluating performance is a vital part of building LLM-powered applications.
LangSmith helps with every step of the process fr...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## How tokens correspond to text

Please see this post from [OpenAI](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them) for more details on how tokens are counted and h...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tokens.mdx‚´page_content_summaryÙË- **[Tokens](/docs/concepts/tokens)**: The basic unit that a language model reads, processes, and generates under the hood.
- **[Tool artifacts](/docs/concepts/tools#tool-artifacts)**: Add artifacts t...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/index.mdx¦answer­I don't know.¨query_id¤Q043ªfocus_kind¶chain_execution_result‰¢idÙ@bfb078c01a661a843a3aec0a6e5c966bf9942675e93450250b846e5e1ffda159¢tsËAÚÔi#O]¨trace_idÙ$1aa85320-90e6-4a12-b1c6-05a642d9158b­parent_run_idÙ$5734a8c0-d8e0-4d47-ae24-d8467b215963¦run_idÙ$1aa85320-90e6-4a12-b1c6-05a642d9158bªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Tool artifacts

**Tools** are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that w...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to mo...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).
...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Key concepts

- Tools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.
- Create tools using the [@tool](https://python.langchain.com/api_reference/cor...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx¨query_id¤Q044ªfocus_kind¶chain_execution_result†¢idÙ@8c8a337d1a7324505005a553a132e47e892e576e2a5f79586ccc0c8414ea81c1¢tsËAÚÔi¡ø4¨focus_msÍ¹§payload‚¤role©assistant§contentÚÁTool artifacts are outputs from a tool's execution that can be accessed by downstream components without being exposed to the model itself. They can be returned from a tool using a specific response format, such as `response_format="content_and_artifact"`, which allows the tool to send both a message for the model and the artifact itself. This enables the model to receive relevant metadata while keeping the full output accessible for other uses.ªfocus_kind¬exec_latency¨query_id¤Q044‰¢idÙ@a89945d6a9015937b784b4546c280c07fd8a719dc0fba55a424d292c5642dc41¢tsËAÚÔi¢
|¨trace_idÙ$cc632ebd-2888-4573-9f41-f1ae351f7087­parent_run_idÙ$5734a8c0-d8e0-4d47-ae24-d8467b215963¦run_idÙ$cc632ebd-2888-4573-9f41-f1ae351f7087ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚÁTool artifacts are outputs from a tool's execution that can be accessed by downstream components without being exposed to the model itself. They can be returned from a tool using a specific response format, such as `response_format="content_and_artifact"`, which allows the tool to send both a message for the model and the artifact itself. This enables the model to receive relevant metadata while keeping the full output accessible for other uses.¨query_id¤Q044ªfocus_kind¶chain_execution_result‰¢idÙ@9c91a42cdf229effaed8b7c54751e8e79ab1d8d3f5f0bb26de8f04347b9db044¢tsËAÚÔi¢Q¨trace_idÙ$5734a8c0-d8e0-4d47-ae24-d8467b215963­parent_run_idÀ¦run_idÙ$5734a8c0-d8e0-4d47-ae24-d8467b215963ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙAWhat are tool artifacts and how can they be returned from a tool?§context•‚´page_content_summaryÙË## Tool artifacts

**Tools** are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that w...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to mo...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙËSee [how to return artifacts from tools](/docs/how_to/tool_artifacts/) for more details.

## Special type annotations

There are a number of special type annotations that can be used in the tool's fun...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Configuring the schema

The `@tool` decorator offers additional options to configure the schema of the tool (e.g., modify name, description
or parse the function's doc-string to infer the schema).
...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx‚´page_content_summaryÙË## Key concepts

- Tools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.
- Create tools using the [@tool](https://python.langchain.com/api_reference/cor...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tools.mdx¦answerÚÁTool artifacts are outputs from a tool's execution that can be accessed by downstream components without being exposed to the model itself. They can be returned from a tool using a specific response format, such as `response_format="content_and_artifact"`, which allows the tool to send both a message for the model and the artifact itself. This enables the model to receive relevant metadata while keeping the full output accessible for other uses.¨query_id¤Q044ªfocus_kind¶chain_execution_result‰¢idÙ@e351d0be56e9f1ae60fb109e74748ab33dc53e785a716700ecca23fcbf3800c0¢tsËAÚÔi¯q¨trace_idÙ$6b939192-b45f-43c0-989c-4954041582df­parent_run_idÙ$d26f3960-7de0-4dc4-af8a-18c217cbb199¦run_idÙ$6b939192-b45f-43c0-989c-4954041582dfªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË```python
await some_runnable.ainvoke(some_input)
```

Other components like [Embedding Models](/docs/concepts/embedding_models) and [VectorStore](/docs/concepts/vectorstores) that do not implement th...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Langchain asynchronous APIs

Many LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.

Typically, any method that may perform I/O operations...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q045ªfocus_kind¶chain_execution_result†¢idÙ@1e5960682c68fd0d5d7c232731290ac350b7da17059ef0e4cec063cd804be521¢tsËAÚÔiü™o¨focus_msÍ´§payload‚¤role©assistant§contentÚ#To asynchronously invoke a Runnable and await the result, you can use the `await` keyword followed by the `ainvoke` method of the Runnable. For example, you would write `await some_runnable.ainvoke(some_input)`. This allows you to execute the Runnable asynchronously and wait for the result.ªfocus_kind¬exec_latency¨query_id¤Q045‰¢idÙ@f7894d1de0e3b7ffefe6938c558f14b78715eaae2f93311e7ad86326c7ca7720¢tsËAÚÔiü©'¨trace_idÙ$31a30b42-a722-4e4e-882d-391ad6930f31­parent_run_idÙ$d26f3960-7de0-4dc4-af8a-18c217cbb199¦run_idÙ$31a30b42-a722-4e4e-882d-391ad6930f31ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ#To asynchronously invoke a Runnable and await the result, you can use the `await` keyword followed by the `ainvoke` method of the Runnable. For example, you would write `await some_runnable.ainvoke(some_input)`. This allows you to execute the Runnable asynchronously and wait for the result.¨query_id¤Q045ªfocus_kind¶chain_execution_result‰¢idÙ@36c438e68035d4e66681a3ca117d42365aa8e5b7b89204622de1d9f962815a45¢tsËAÚÔiü³u¨trace_idÙ$d26f3960-7de0-4dc4-af8a-18c217cbb199­parent_run_idÀ¦run_idÙ$d26f3960-7de0-4dc4-af8a-18c217cbb199ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙEExplain how to asynchronously invoke a Runnable and await the result.§context•‚´page_content_summaryÙË```python
await some_runnable.ainvoke(some_input)
```

Other components like [Embedding Models](/docs/concepts/embedding_models) and [VectorStore](/docs/concepts/vectorstores) that do not implement th...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Langchain asynchronous APIs

Many LangChain APIs are designed to be asynchronous, allowing you to build efficient and responsive applications.

Typically, any method that may perform I/O operations...¨metadata¦sourceÙD/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/async.mdx‚´page_content_summaryÙË# Runnable interface

The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as [language models](/docs/concepts/chat_models), [...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË:::tip
When processing a large number of inputs using `batch` or `batch_as_completed`, users may want to control the maximum number of parallel calls. This can be done by setting the `max_concurrency`...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚ#To asynchronously invoke a Runnable and await the result, you can use the `await` keyword followed by the `ainvoke` method of the Runnable. For example, you would write `await some_runnable.ainvoke(some_input)`. This allows you to execute the Runnable asynchronously and wait for the result.¨query_id¤Q045ªfocus_kind¶chain_execution_result‰¢idÙ@9c907f620c9c934b0d9618839b0cee2ef9d3e2115be4b110209877295478b549¢tsËAÚÔj=[¨trace_idÙ$48d17ec1-ce66-4b61-818f-e277bcfb3e8a­parent_run_idÙ$7fedf5f2-9be0-4dfd-a10f-f2fde4127833¦run_idÙ$48d17ec1-ce66-4b61-818f-e277bcfb3e8aªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## [LangGraph](https://langchain-ai.github.io/langgraph)

LangGraph is an extension of LangChain aimed at
building robust and stateful multi-actor applications with LLMs by modeling steps as edges and...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙËRefer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.

## Orchestration

Get started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble La...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocati...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË#### Usage with LangGraph

[LangGraph](/docs/concepts/architecture#langgraph) compiled graphs are [Runnables](/docs/concepts/runnables) and support the standard streaming APIs.

When using the *stream...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¨query_id¤Q046ªfocus_kind¶chain_execution_result†¢idÙ@0fdef47d16466dc1a99567a2182e702a0d6470eadabac8e1385cfda448540e7e¢tsËAÚÔj\˜k¨focus_msÍ¨§payload‚¤role©assistant§contentÙşTo understand LangGraph node construction, you should read the "LangGraph how-to guides" and the "LangGraph concepts" documentation. These resources provide detailed instructions and explanations on building and orchestrating applications with LangGraph.ªfocus_kind¬exec_latency¨query_id¤Q046‰¢idÙ@62f5ead4f1a9e708def6e5e16f67911bf896eac06987a7fb4b582a05e6d07038¢tsËAÚÔj\¤©¨trace_idÙ$3f9dc1c2-7cdd-4366-9add-bc08a7675290­parent_run_idÙ$7fedf5f2-9be0-4dfd-a10f-f2fde4127833¦run_idÙ$3f9dc1c2-7cdd-4366-9add-bc08a7675290ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÙşTo understand LangGraph node construction, you should read the "LangGraph how-to guides" and the "LangGraph concepts" documentation. These resources provide detailed instructions and explanations on building and orchestrating applications with LangGraph.¨query_id¤Q046ªfocus_kind¶chain_execution_result‰¢idÙ@0b4ede48347c9aa424ced53344303c2248d17ea91a8ffce25474ccf388f8e1c9¢tsËAÚÔj\±¨trace_idÙ$7fedf5f2-9be0-4dfd-a10f-f2fde4127833­parent_run_idÀ¦run_idÙ$7fedf5f2-9be0-4dfd-a10f-f2fde4127833ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙNWhich two documents should you read to understand LangGraph node construction?§context•‚´page_content_summaryÙË## [LangGraph](https://langchain-ai.github.io/langgraph)

LangGraph is an extension of LangChain aimed at
building robust and stateful multi-actor applications with LLMs by modeling steps as edges and...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË[LangGraph](https://langchain-ai.github.io/langgraph/concepts/high_level/) makes it possible to orchestrate complex applications (e.g., [agents](/docs/concepts/agents/)) and provide features like incl...¨metadata¦sourceÙL/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/why_langchain.mdx‚´page_content_summaryÙËRefer to the [how-to guides](/docs/how_to) for more detail on using all LangChain components.

## Orchestration

Get started using [LangGraph](https://langchain-ai.github.io/langgraph/) to assemble La...¨metadata¦sourceÙE/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/tutorials/index.mdx‚´page_content_summaryÙË- [How to: chain runnables](/docs/how_to/sequence)
- [How to: stream runnables](/docs/how_to/streaming)
- [How to: invoke runnables in parallel](/docs/how_to/parallel/)
- [How to: add default invocati...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË#### Usage with LangGraph

[LangGraph](/docs/concepts/architecture#langgraph) compiled graphs are [Runnables](/docs/concepts/runnables) and support the standard streaming APIs.

When using the *stream...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/streaming.mdx¦answerÙşTo understand LangGraph node construction, you should read the "LangGraph how-to guides" and the "LangGraph concepts" documentation. These resources provide detailed instructions and explanations on building and orchestrating applications with LangGraph.¨query_id¤Q046ªfocus_kind¶chain_execution_result‰¢idÙ@40279aa8722ab465fa437f1b73428b8565bc3fbea9153485710a4cd018fbad93¢tsËAÚÔjj%‰¨trace_idÙ$edfe2613-7dbf-4c6b-b67b-9019b0c07ab3­parent_run_idÙ$90643bdc-cf43-4281-8918-3aca51755aca¦run_idÙ$edfe2613-7dbf-4c6b-b67b-9019b0c07ab3ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Tool execution

[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.

[LangGraph]...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙË- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to mo...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

* `configurable_fields`: This method allows you to configure specific **attribute...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¨query_id¤Q047ªfocus_kind¶chain_execution_result†¢idÙ@f6262f57f7f69fc792b1fa5279ba95470fff9b8397f3d406ccc718b301e91888¢tsËAÚÔjÁ+¨focus_msÍN§payload‚¤role©assistant§contentÚrTo convert a Runnable into a Tool for agent use, you implement the Runnable interface, allowing the tool to be invoked directly using `tool.invoke(args)`. Additionally, you can utilize pre-built components like ToolNode from LangGraph to facilitate tool invocation on behalf of the user. For detailed guidance, refer to the how-to guide on converting Runnables to tools.ªfocus_kind¬exec_latency¨query_id¤Q047‰¢idÙ@bd7da039dfa42054e3875cd8a13242eebb306af77f580f04c06a0ce7a83877ca¢tsËAÚÔjÁ7Î¨trace_idÙ$7c2603a5-e4b7-4194-82a7-4cf5a6386147­parent_run_idÙ$90643bdc-cf43-4281-8918-3aca51755aca¦run_idÙ$7c2603a5-e4b7-4194-82a7-4cf5a6386147ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚrTo convert a Runnable into a Tool for agent use, you implement the Runnable interface, allowing the tool to be invoked directly using `tool.invoke(args)`. Additionally, you can utilize pre-built components like ToolNode from LangGraph to facilitate tool invocation on behalf of the user. For detailed guidance, refer to the how-to guide on converting Runnables to tools.¨query_id¤Q047ªfocus_kind¶chain_execution_result‰¢idÙ@7fe6be19fd8b0ca65752c5523e827dca21ce41d9b93700cf46c17f0a13c4be97¢tsËAÚÔjÁ=¸¨trace_idÙ$90643bdc-cf43-4281-8918-3aca51755aca­parent_run_idÀ¦run_idÙ$90643bdc-cf43-4281-8918-3aca51755acaªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙHDescribe the process of converting a Runnable into a Tool for agent use.§context•‚´page_content_summaryÙË## Tool execution

[Tools](/docs/concepts/tools/) implement the [Runnable](/docs/concepts/runnables/) interface, which means that they can be invoked (e.g., `tool.invoke(args)`) directly.

[LangGraph]...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/tool_calling.mdx‚´page_content_summaryÙË- [How to: return artifacts from a tool](/docs/how_to/tool_artifacts/)
- [How to: convert Runnables to tools](/docs/how_to/convert_runnable_to_tool)
- [How to: add ad-hoc tool calling capability to mo...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË## Overview of runnable interface

The Runnable way defines a standard interface that allows a Runnable component to be:

* [Invoked](/docs/how_to/lcel_cheatsheet/#invoke-a-runnable): A single input i...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙË## Configurable runnables

:::note
This is an advanced feature that is unnecessary for most users.

It helps with configuration of large "chains" created using the [LangChain Expression Language (LCEL...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx‚´page_content_summaryÙËTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:

* `configurable_fields`: This method allows you to configure specific **attribute...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/runnables.mdx¦answerÚrTo convert a Runnable into a Tool for agent use, you implement the Runnable interface, allowing the tool to be invoked directly using `tool.invoke(args)`. Additionally, you can utilize pre-built components like ToolNode from LangGraph to facilitate tool invocation on behalf of the user. For detailed guidance, refer to the how-to guide on converting Runnables to tools.¨query_id¤Q047ªfocus_kind¶chain_execution_result‰¢idÙ@fdc044a0cc2fb9f1b1b3a76f948cc1827d52d58b7bfb74d5905a0cf3154e36aa¢tsËAÚÔjÍÑÈ¨trace_idÙ$e0a1cfce-1a5a-4b45-a449-ff575cf91546­parent_run_idÙ$9a24bf08-6974-466b-8b56-f810234585cd¦run_idÙ$e0a1cfce-1a5a-4b45-a449-ff575cf91546ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË## Integrations

LangChain has many chat model integrations that allow you to use a wide variety of models from different providers.

These integrations are one of two types:

1. **Official models**: ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËPlease review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.

:::note
Models that do **not** include the prefix "Chat" in their name or include "LLM" as a suff...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## langchain-community

This package contains third-party integrations that are maintained by the LangChain community.
Key integration packages are separated out (see above).
This contains integration...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/architecture.mdx‚´page_content_summaryÙË* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËSome important things to note:

- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration fo...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¨query_id¤Q048ªfocus_kind¶chain_execution_result†¢idÙ@f111144bf7d146fb0eab0a38e63c0cb1df5e8acf7e1165dbff1bacdf227b26ec¢tsËAÚÔk00¨focus_msÍ§payload‚¤role©assistant§contentÚThe Python package that contains community-contributed chat model integrations is called `langchain-community`. This package includes various third-party integrations maintained by the LangChain community. For official models, you would look into the `langchain-<provider>` packages.ªfocus_kind¬exec_latency¨query_id¤Q048‰¢idÙ@d132cf05f993a381acd0c974468361b57b76524995235906bfd2a32c3227bf82¢tsËAÚÔk0¨trace_idÙ$e16a425f-6307-46dc-9be2-f2891d99c0a7­parent_run_idÙ$9a24bf08-6974-466b-8b56-f810234585cd¦run_idÙ$e16a425f-6307-46dc-9be2-f2891d99c0a7ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚThe Python package that contains community-contributed chat model integrations is called `langchain-community`. This package includes various third-party integrations maintained by the LangChain community. For official models, you would look into the `langchain-<provider>` packages.¨query_id¤Q048ªfocus_kind¶chain_execution_result‰¢idÙ@33469986514ec52f1452635e39005e57cd66097e3f1ea4baf4f5403df1ec5a81¢tsËAÚÔk0™ ¨trace_idÙ$9a24bf08-6974-466b-8b56-f810234585cd­parent_run_idÀ¦run_idÙ$9a24bf08-6974-466b-8b56-f810234585cdªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙKWhat Python package contains community-contributed chat model integrations?§context•‚´page_content_summaryÙË## Integrations

LangChain has many chat model integrations that allow you to use a wide variety of models from different providers.

These integrations are one of two types:

1. **Official models**: ...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËPlease review the [chat model integrations](/docs/integrations/chat/) for a list of supported models.

:::note
Models that do **not** include the prefix "Chat" in their name or include "LLM" as a suff...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË## langchain-community

This package contains third-party integrations that are maintained by the LangChain community.
Key integration packages are separated out (see above).
This contains integration...¨metadata¦sourceÙK/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/architecture.mdx‚´page_content_summaryÙË* Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see [chat model integrations](/docs/...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙËSome important things to note:

- Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration fo...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx¦answerÚThe Python package that contains community-contributed chat model integrations is called `langchain-community`. This package includes various third-party integrations maintained by the LangChain community. For official models, you would look into the `langchain-<provider>` packages.¨query_id¤Q048ªfocus_kind¶chain_execution_result‰¢idÙ@d791972efa94e56786f34654e106629da356ddf1e08212ff2cae96124d697e1c¢tsËAÚÔkDYK¨trace_idÙ$4ca0c91b-5db4-4759-92f6-57cbacb4335f­parent_run_idÙ$00980f07-6821-4447-8928-b19d5bda7097¦run_idÙ$4ca0c91b-5db4-4759-92f6-57cbacb4335fªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙËA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to conver...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË### LLMs

What LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.

- [How to: cache model responses](/docs/how_to/llm_cachin...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË| [Self Query](/docs/how_to/self_query/)   | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË1. **Query Clarification**: Models can rephrase ambiguous or poorly worded queries for clarity.
2. **Semantic Understanding**: They can capture the intent behind a query, going beyond literal keyword ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx¨query_id¤Q049ªfocus_kind¶chain_execution_result†¢idÙ@8e7ed81c8a4c84128a2140b6b51da0105553aa351c7e2fac4ba977c84355b753¢tsËAÚÔk™}¿¨focus_msÍ1§payload‚¤role©assistant§contentÚ£Semantic caching can improve LLM response times by storing responses based on the meaning of the input rather than exact matches, which reduces the number of requests to the model provider. This approach relies on an embedding model to convert text into vector representations for better semantic understanding. However, the effectiveness of semantic caching can vary depending on the context and nature of the queries.ªfocus_kind¬exec_latency¨query_id¤Q049‰¢idÙ@ab61961e2bc3cd15fd7692965d55280a68b3d21a8955f5828623ac329f806de1¢tsËAÚÔk™æ¨trace_idÙ$f1caf9a7-3e5f-4c24-bbd3-5fbe5e168cc9­parent_run_idÙ$00980f07-6821-4447-8928-b19d5bda7097¦run_idÙ$f1caf9a7-3e5f-4c24-bbd3-5fbe5e168cc9ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚ£Semantic caching can improve LLM response times by storing responses based on the meaning of the input rather than exact matches, which reduces the number of requests to the model provider. This approach relies on an embedding model to convert text into vector representations for better semantic understanding. However, the effectiveness of semantic caching can vary depending on the context and nature of the queries.¨query_id¤Q049ªfocus_kind¶chain_execution_result‰¢idÙ@e7296191761db3bfe15272b12fa9be28ac8f8ad5039dd6cb97bbb958c23a33ec¢tsËAÚÔk™‰¨trace_idÙ$00980f07-6821-4447-8928-b19d5bda7097­parent_run_idÀ¦run_idÙ$00980f07-6821-4447-8928-b19d5bda7097ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙYHow can semantic caching improve LLM response times and what extra model does it rely on?§context•‚´page_content_summaryÙËA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an [embedding model](/docs/concepts/embedding_models) to conver...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË### Caching

Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of...¨metadata¦sourceÙJ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/chat_models.mdx‚´page_content_summaryÙË### LLMs

What LangChain calls [LLMs](/docs/concepts/text_llms) are older forms of language models that take a string in and output a string.

- [How to: cache model responses](/docs/how_to/llm_cachin...¨metadata¦sourceÙB/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/index.mdx‚´page_content_summaryÙË| [Self Query](/docs/how_to/self_query/)   | If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. | This uses an LLM to ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx‚´page_content_summaryÙË1. **Query Clarification**: Models can rephrase ambiguous or poorly worded queries for clarity.
2. **Semantic Understanding**: They can capture the intent behind a query, going beyond literal keyword ...¨metadata¦sourceÙH/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/concepts/retrieval.mdx¦answerÚ£Semantic caching can improve LLM response times by storing responses based on the meaning of the input rather than exact matches, which reduces the number of requests to the model provider. This approach relies on an embedding model to convert text into vector representations for better semantic understanding. However, the effectiveness of semantic caching can vary depending on the context and nature of the queries.¨query_id¤Q049ªfocus_kind¶chain_execution_result‰¢idÙ@61c47930c14c98642887d5deafb7eb77ad55818ed4615475bcf64a58388c0a71¢tsËAÚÔk¥_ÿ¨trace_idÙ$61fb99ee-4287-40cf-8e64-a6e2d521036d­parent_run_idÙ$effe135c-28aa-485a-86bc-62b6135521f0¦run_idÙ$61fb99ee-4287-40cf-8e64-a6e2d521036dªevent_type¬chain_output¬event_source¥chain§payload§outputs‚§context•‚´page_content_summaryÙË# How to load JSON

[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and trans...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙË### JSON Lines file

If you want to load documents from a JSON Lines file, you pass `json_lines=True`
and specify `jq_schema` to extract `page_content` from a single JSON object.

```python
file_path ...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙËIt uses the `jq` python package. Check out this [manual](https://stedolan.github.io/jq/manual/#Basicfilters) for a detailed documentation of the `jq` syntax.

Here we will demonstrate: 

- How to load...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙËdata = loader.load()
```

```python
pprint(data)
```

```output
    [Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_c...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙËdata = loader.load()
```

```python
pprint(data)
```

```output
    [Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx¨query_id¤Q050ªfocus_kind¶chain_execution_result†¢idÙ@ded68a27e8e81eeca6a37095cba9d12c024510054be3822d3f47ccecf5441d20¢tsËAÚÔl ¨focus_msÍˆ§payload‚¤role©assistant§contentÚYou would use the `JSONLoader` from LangChain for processing JSON Lines files. When initializing the loader, set `json_lines=True` and specify a `jq_schema` to extract the desired content. This allows you to convert JSONL data into LangChain Document objects effectively.ªfocus_kind¬exec_latency¨query_id¤Q050‰¢idÙ@60d4673348af7c43ea282d9c6668e724a51efcf7ab72fc1a1ae7a5f2d56a3a95¢tsËAÚÔl )¨trace_idÙ$1ab2155c-5d38-41e8-a559-e00e45da2353­parent_run_idÙ$effe135c-28aa-485a-86bc-62b6135521f0¦run_idÙ$1ab2155c-5d38-41e8-a559-e00e45da2353ªevent_type¬chain_output¬event_source¥chain§payload§outputs‚¦answerÚYou would use the `JSONLoader` from LangChain for processing JSON Lines files. When initializing the loader, set `json_lines=True` and specify a `jq_schema` to extract the desired content. This allows you to convert JSONL data into LangChain Document objects effectively.¨query_id¤Q050ªfocus_kind¶chain_execution_result‰¢idÙ@2ce232b6d6e88f177ec546b41512c584b3149a7b88c5e8d40abb237d340f4535¢tsËAÚÔl /8¨trace_idÙ$effe135c-28aa-485a-86bc-62b6135521f0­parent_run_idÀ¦run_idÙ$effe135c-28aa-485a-86bc-62b6135521f0ªevent_type¬chain_output¬event_source¥chain§payload§outputs„¨questionÙDWhich document loader would you use for processing JSON lines files?§context•‚´page_content_summaryÙË# How to load JSON

[JSON (JavaScript Object Notation)](https://en.wikipedia.org/wiki/JSON) is an open standard file format and data interchange format that uses human-readable text to store and trans...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙË### JSON Lines file

If you want to load documents from a JSON Lines file, you pass `json_lines=True`
and specify `jq_schema` to extract `page_content` from a single JSON object.

```python
file_path ...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙËIt uses the `jq` python package. Check out this [manual](https://stedolan.github.io/jq/manual/#Basicfilters) for a detailed documentation of the `jq` syntax.

Here we will demonstrate: 

- How to load...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙËdata = loader.load()
```

```python
pprint(data)
```

```output
    [Document(page_content='Bye!', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook_c...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx‚´page_content_summaryÙËdata = loader.load()
```

```python
pprint(data)
```

```output
    [Document(page_content='User 2', metadata={'source': 'langchain/docs/modules/indexes/document_loaders/examples/example_data/facebook...¨metadata¦sourceÙQ/Users/manirashahmadi/ccode/aep/aep-sdk/docs/docs/how_to/document_loader_json.mdx¦answerÚYou would use the `JSONLoader` from LangChain for processing JSON Lines files. When initializing the loader, set `json_lines=True` and specify a `jq_schema` to extract the desired content. This allows you to convert JSONL data into LangChain Document objects effectively.¨query_id¤Q050ªfocus_kind¶chain_execution_result‰¢idÙ@77ff26d3315140922fa5ef6747db8dac95c95ed7f52357fd71f2e5b7ed145b70¢tsËAÚÔlï¨trace_idÙ$6cb1ad47-4aa9-4c57-a369-53253e1cbb46­parent_run_idÙ$4ff74a10-ded9-41d3-afd2-0575758eb25f¦run_idÙ$6cb1ad47-4aa9-4c57-a369-53253e1cbb46ªevent_type«chain_error¬event_source¥chain§payload¥error ªfocus_kind¥error‰¢idÙ@7f338c251531fbb351b77ca1b33c16fed5f2d45bd93a3f86c6b5c3068b52ac8b¢tsËAÚÔl*E¨trace_idÙ$4ff74a10-ded9-41d3-afd2-0575758eb25f­parent_run_idÀ¦run_idÙ$4ff74a10-ded9-41d3-afd2-0575758eb25fªevent_type«chain_error¬event_source¥chain§payload¥error ªfocus_kind¥error