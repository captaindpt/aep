†¢idÙ@78239cc8b9aa1ae7169ca43512478dc707dc9edbd8fb52b07d7fd06365bc955f¢tsËAÚÓõƒ‰3¨focus_msÍÌ§payload‚¤role©assistant§contentÚ…LangChain is a framework designed to simplify the development of applications powered by large language models (LLMs). It was created to provide standardized interfaces for various components, making it easier for developers to build, optimize, and deploy AI applications. The goal is to streamline the application lifecycle and facilitate the integration of different models and services.ªfocus_kind¬exec_latency¨query_id¤Q000†¢idÙ@58615d9af8fe685cccc8218751fa9c2ba0a44c468c0253f11e4b4d752c530650¢tsËAÚÓõ÷)¦¨focus_msÍ§payload‚¤role©assistant§contentÚdIn LangChain, an Agent acts as a system that takes a high-level task and utilizes a language model (LLM) as a reasoning engine to determine and execute the necessary actions. Agents enable more complex and customizable interactions compared to using LLMs alone. LangGraph is recommended for building these agents, offering enhanced flexibility and control.ªfocus_kind¬exec_latency¨query_id¤Q001†¢idÙ@faf8fb563fbeeb1eaee0e17ef1e174dd59fe43670b6a7814ae137ec92013b0bf¢tsËAÚÓök¤Ë¨focus_msÍ^§payload‚¤role©assistant§contentÙ×To install LangChain with extras for tracing, you can use the following command: 

```bash
pip install "langserve[all]"
```

This will install both client and server dependencies necessary for tracing functionality.ªfocus_kind¬exec_latency¨query_id¤Q002†¢idÙ@2e0dbe4351f37164a569edf88ad846ff2e876eda0e5a4c7bda6569248da18b30¢tsËAÚÓöËí%¨focus_msÍÀ§payload‚¤role©assistant§contentÚXThe minimal LCEL chain that streams token-by-token can be constructed using the `.stream()` method, which streams the output of the final step in the chain. This allows for incremental output as the chain executes, optimizing the time-to-first-token. For more complex applications, consider using intermediate values alongside the final output.ªfocus_kind¬exec_latency¨query_id¤Q003†¢idÙ@20c39e4ee9b6740bff42dd948c5566533c4139160fa958464537dac5fdfdd2f9¢tsËAÚÓ÷E/ä¨focus_msÍú§payload‚¤role©assistant§contentÚBFAISS and Chroma are both vector stores used in LangChain, but they have different underlying technologies. FAISS utilizes the Facebook AI Similarity Search library, while Chroma operates as a local library for vector storage. Both support a `similarity_search` method for retrieving similar documents based on embeddings.ªfocus_kind¬exec_latency¨query_id¤Q004†¢idÙ@c49acbe76df2cce4819db06904052c31835108efbd82a984a7ef22e145b795fd¢tsËAÚÓ÷á3œ¨focus_msÍ	§payload‚¤role©assistant§contentÚYTo load a Microsoft Word document into a LangChain pipeline, you can use the `AzureAIDocumentIntelligenceLoader`. First, ensure you have an Azure AI Document Intelligence resource set up, then use the loader with your file path, endpoint, and key to load the document. The code snippet provided in the context demonstrates how to implement this.ªfocus_kind¬exec_latency¨query_id¤Q005†¢idÙ@a39cd14baf3ccfba08f928d17341aff84fa45603ef93ffb1ac44cf7448269110¢tsËAÚÓøPÈë¨focus_msÍğ§payload‚¤role©assistant§contentÚÌThe Runnable protocol is a standard interface in LangChain that allows components to be invoked, batched, streamed, inspected, and composed, facilitating consistent interaction across various elements. Its advantages include efficient processing of multiple inputs, real-time output streaming, and the ability to create complex pipelines through composition. Additionally, it supports runtime configuration, enabling flexibility in how components are utilized.ªfocus_kind¬exec_latency¨query_id¤Q006†¢idÙ@e53ff6ed14d58f4263e231c2524a1567c47381b9aee39a5e9fd41f6cd13ca4e9¢tsËAÚÓø±°À¨focus_msÍ§payload‚¤role©assistant§contentÚ‰Few-shot prompting is a technique that enhances model performance by providing examples of desired inputs and outputs within the prompt. It is supported by example selectors, which dynamically choose relevant examples based on the context, ensuring that the model receives clear and informative guidance. This approach is particularly effective for improving task execution in language models.ªfocus_kind¬exec_latency¨query_id¤Q007†¢idÙ@433358658c730fceec485dfaedaf9206ff42318867ee4f096af328717c97193b¢tsËAÚÓù.’û¨focus_msÍ¤§payload‚¤role©assistant§contentÚ2An example of a memory object that stores chat history is a sequence of messages that includes roles such as "user," "assistant," and "system." This structure helps maintain context and state throughout the conversation. Proper management of this chat history is essential to ensure a coherent interaction.ªfocus_kind¬exec_latency¨query_id¤Q008†¢idÙ@1e57416918270a4b72a285b308f0973faa632f95306bea321fcbef746be94608¢tsËAÚÓù½¦¨focus_msÍâ§payload‚¤role©assistant§contentÚ;To trace a LangGraph flow in LangSmith, first ensure that your application is integrated with LangSmith for observability. Then, utilize the tracing features to monitor the series of steps (or runs) your application takes from input to output. For detailed guidance, refer to the LangSmith documentation on tracing.ªfocus_kind¬exec_latency¨query_id¤Q009†¢idÙ@916182b8057548c3622f90a822a516c9f94b04410d90a0555d9280affdb1e0e5¢tsËAÚÓúC
p¨focus_msÍ§payload‚¤role©assistant§contentÚETo use Pydantic v1 with LangChain, you should pin the version of Pydantic to v1.x. However, it is recommended to use Pydantic 2 instead, as LangChain is designed to work with it and advises against using the `pydantic.v1` namespace. For prior versions of LangChain, refer to the compatibility guide for specific instructions.ªfocus_kind¬exec_latency¨query_id¤Q010†¢idÙ@041d4286412b9f127be63cdb50234d91f2134ed8f638fea84be4cf73dbfb0896¢tsËAÚÓúÊŞÂ¨focus_msÍ§payload‚¤role©assistant§contentÚÔTool calling in LangChain via OpenAI function messages involves using a standard API to bind tools to models, allowing for the handling of tool call requests and sending results back to the model. The output is structured in LangChain messages, which can be converted to OpenAI format using the `convert_to_openai_messages` utility function if needed. This integration supports various chat model providers and facilitates efficient programming and structured outputs.ªfocus_kind¬exec_latency¨query_id¤Q011†¢idÙ@4fbb4ad95d7895e68d9c865bdda3ade1e1eaa8996205ef42a7fb11fca16d37e8¢tsËAÚÓûB)¨focus_msÍ§payload‚¤role©assistant§contentÚ‘Retrieval-Augmented Generation (RAG) in LangChain works by first receiving an input query and then using a retrieval system to search for relevant information. This retrieved information is incorporated into the prompt sent to the language model, which generates a response based on the provided context. This approach allows RAG to utilize up-to-date information and reduce inaccuracies in responses.ªfocus_kind¬exec_latency¨query_id¤Q012†¢idÙ@f9483467633c9c8e2111fcd87786125679946876e68e6f2810f5dbf8f713763c¢tsËAÚÓûÂó¨focus_msÍl§payload‚¤role©assistant§contentÚ›To create embeddings with OpenAI via LangChain, you can use the following code snippet:

```python
from langchain_openai import OpenAIEmbeddings
embeddings_model = OpenAIEmbeddings()
embeddings = embeddings_model.embed_documents(["Hi there!", "Oh, hello!", "What's your name?"])
```

This will generate embeddings for the provided list of strings. For a single query, you can use `embed_query` method similarly.ªfocus_kind¬exec_latency¨query_id¤Q013†¢idÙ@cdd6082abf117a97ccd86868f948f8450da3bdcad530979cdabc6a81b0299527¢tsËAÚÓü2/¨focus_msÍĞ§payload‚¤role©assistant§contentÙ£Three callback events emitted during an LLM call are: "Chat model start" (on_chat_model_start), "LLM start" (on_llm_start), and "LLM new token" (on_llm_new_token).ªfocus_kind¬exec_latency¨query_id¤Q014†¢idÙ@ab4faa83aa3339a35d0df51c713ad28d73ac2f1633f4b6127c0d8853802139b8¢tsËAÚÓüwÁô¨focus_msÍ‚§payload‚¤role©assistant§contentÚ¢The AsyncRunnable interface is used to enable asynchronous execution of tasks within LangChain components, allowing for efficient handling of multiple requests concurrently. This is particularly beneficial in server environments where reducing latency and improving throughput is essential. By using AsyncRunnable, developers can leverage optimized parallel execution and maintain responsiveness in their applications.ªfocus_kind¬exec_latency¨query_id¤Q015†¢idÙ@17d650f8e5bcf62c96cc51afe1cf72a638d4b19cc64bad4c8379f15d9a132477¢tsËAÚÓüâÁä¨focus_msÍó§payload‚¤role©assistant§contentÚThe retrieved context does not specify the exact metrics that LangChain's evaluation module computes out-of-the-box. It mentions that LangSmith provides an evaluation framework to define metrics and run evaluations, but does not list specific metrics. Therefore, I don't know the answer.ªfocus_kind¬exec_latency¨query_id¤Q016†¢idÙ@095b1875baf2feec0d56945c8055d3bccf7158af225360107174b0335a04beb2¢tsËAÚÓıUmø¨focus_msÍ÷§payload‚¤role©assistant§contentÚàTo chunk text using `RecursiveCharacterTextSplitter`, you can follow this example: 

```python
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)
texts = text_splitter.split_text(document)
```

This code initializes the text splitter with a specified chunk size and overlap, then splits the provided document into manageable chunks while preserving the natural structure of the text.ªfocus_kind¬exec_latency¨query_id¤Q017†¢idÙ@ae4ebf99fc376327c4f81f505b4eafa6904b30d577deff002bd43577435c00ac¢tsËAÚÓı¸Êl¨focus_msÍ½§payload‚¤role©assistant§contentÙøThe environment variable that controls OpenAI API retries in LangChain is `max_retries`. This variable specifies the maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.ªfocus_kind¬exec_latency¨query_id¤Q018†¢idÙ@f173bb4f76f81c2642f53d761ae626c3e24ce737e0b1f08a3202dc65b0652412¢tsËAÚÓş¬MP¨focus_msÍA§payload‚¤role©assistant§contentÚŸKey-value stores can be used to cache intermediate outputs in LangChain by storing the results of each step in a chain, allowing for efficient retrieval and reuse of these outputs. This is particularly useful in complex chains where intermediate values may be needed alongside the final output. By using the `mset` and `mget` methods, users can easily manage the storage and retrieval of these intermediate results.ªfocus_kind¬exec_latency¨query_id¤Q019†¢idÙ@bc0f285130657ef3dae4c397567937e9e4487a775366e118c37f77b5844cfc57¢tsËAÚÓÿYg¨focus_msÍ	=§payload‚¤role©assistant§contentÚïMultimodality in LangChain allows chat models to process various data types, such as text, images, audio, and video. For example, you can pass an image URL along with a text prompt to a chat model using a structured message format, like this:

```python
message = HumanMessage(
    content=[
        {"type": "text", "text": "Describe the weather in this image:"},
        {
            "type": "image",
            "source_type": "url",
            "url": "https://...",
        },
    ],
)
```ªfocus_kind¬exec_latency¨query_id¤Q020†¢idÙ@191cf5a9118996d01191d122d8d9e0f696cdb446ebd5279665dd49f93e33d876¢tsËAÚÓÿ¾íé¨focus_msÍ1§payload‚¤role©assistant§contentÚäThe LangChain architecture diagram conveys the structure and components of the LangChain ecosystem, highlighting its evolution from a single package to a comprehensive framework for building AI applications. It emphasizes the standardized interfaces for various components, which facilitate easier integration and swapping of different providers. Additionally, it showcases features like orchestration, observability, and advanced functionalities that enhance application development.ªfocus_kind¬exec_latency¨query_id¤Q021†¢idÙ@a21b252f59267e89a071988e030fc152b797f27d74f6d23189e6b88446ae4c8c¢tsËAÚÔ 4•å¨focus_msÍ%§payload‚¤role©assistant§contentÚ¬Two example selectors are "length-based selection" and "semantic similarity selection." Use length-based selection when you want to ensure that the examples fit within a specific character or word limit, while semantic similarity selection is useful when you need examples that closely match the context or intent of the prompt. These selectors help tailor the few-shot examples to improve the performance of the language model.ªfocus_kind¬exec_latency¨query_id¤Q022†¢idÙ@e5857e826c38cb7735a44fe8e81d3c1984fd8016ae10670d9dc4cff6f8901a8b¢tsËAÚÔ œO¨focus_msÍ4§payload‚¤role©assistant§contentÚ…Output parsers simplify post-processing in LCEL by transforming the raw output from a model into a more structured format suitable for downstream tasks. They are particularly useful for generating structured data or normalizing outputs from chat models and LLMs. However, it is recommended to use function or tool calling for automatic handling instead of relying solely on output parsing.ªfocus_kind¬exec_latency¨query_id¤Q023†¢idÙ@678f72d112f800e68b537ff2a6b87273eeaafa2abacb5a9fa072e6704ea9c009¢tsËAÚÔ  ¨focus_msÍ¢§payload‚¤role©assistant§contentÚ•Streaming chunk callbacks allow for the progressive display of output from LLMs as they generate responses, enhancing user experience by providing immediate feedback. In contrast, token callbacks focus on handling individual tokens as they are produced, which can be useful for more granular control over the output. Both methods aim to improve responsiveness and reduce perceived latency in applications.ªfocus_kind¬exec_latency¨query_id¤Q024†¢idÙ@9523791848c1dee52a9aa1040861f6efaae1035ce8758be3125b8d501e623197¢tsËAÚÔŒK®¨focus_msÍñ§payload‚¤role©assistant§contentÚ–The `ConversationBufferMemory` class is designed to manage and preserve the correct structure of chat history during conversations. It should be used when implementing memory in chat models to ensure that the conversation follows the required message patterns and remains coherent. This is particularly important for maintaining context and facilitating effective interactions between users and assistants.ªfocus_kind¬exec_latency¨query_id¤Q025†¢idÙ@5f776f8e5c1dcf985946b179a254f095a0e99fe7c576db0827bf085e93ae60c3¢tsËAÚÔûz¼¨focus_msÍÎ§payload‚¤role©assistant§contentÚ“The LangChain text splitter that allows you to split documents by tokens is the `CharacterTextSplitter` with a token-based approach. You would choose it over a character splitter because it aligns better with how language models process text, focusing on meaningful linguistic units rather than just character counts. This can improve the quality of text representations used in downstream applications.ªfocus_kind¬exec_latency¨query_id¤Q026†¢idÙ@a30a4a88c3a4dd7ea7a24a04b63497e7ed4b8d860c6ddcf6ed0b48c408c5af57¢tsËAÚÔ¶Ùğ¨focus_msÍ	u§payload‚¤role©assistant§contentÚÇ`VectorStoreRetriever` focuses on retrieving documents based on vector similarity, while `EnsembleRetriever` combines multiple retrievers to improve the overall retrieval performance by leveraging their strengths. Additionally, `EnsembleRetriever` can apply weighted scoring to the results from different retrievers, whereas `VectorStoreRetriever` operates independently. This makes `EnsembleRetriever` more versatile in handling diverse types of queries.ªfocus_kind¬exec_latency¨query_id¤Q027†¢idÙ@1ba31602b938579f5cd20ce32db8a4b830a5584dafd1fcc295780c461d4f3ba4¢tsËAÚÔÿ¼Ï¨focus_msÍ§payload‚¤role©assistant§contentÚSTo configure a chat model to automatically retry on rate limiting errors, you can set the `max_retries` parameter to control the number of retries. Additionally, implement a waiting mechanism that increases the wait time after each subsequent rate limit error. This approach helps manage requests effectively while adhering to rate limits.ªfocus_kind¬exec_latency¨query_id¤Q028†¢idÙ@d4cb136fb14128410d5211975d3e798901030a39b127a75fdd70f4e012aa91c6¢tsËAÚÔn¿º¨focus_msÍ™§payload‚¤role©assistant§contentÚŸ`InjectedToolArg` allows certain parameters to be passed to a tool at runtime without being exposed in the tool's schema, ensuring that sensitive or dynamic values are not controlled by the model. For example, a `user_id` can be injected dynamically into a tool that processes user-specific data, keeping it hidden from the tool's schema. This is useful for maintaining security and control over runtime parameters.ªfocus_kind¬exec_latency¨query_id¤Q029†¢idÙ@f52f61ba2fd70c42e62bbb9e307564a187d798dc88202ace0ff0ec19e0f7d3aa¢tsËAÚÔĞÅD¨focus_msÍ§payload‚¤role©assistant§contentÚThe recommended decorator for quickly turning a Python function into a LangChain Tool is the `@tool` decorator. It simplifies the process of tool creation and should be used in most cases. You can decorate a function with `@tool` to create a tool that implements the Tool Interface.ªfocus_kind¬exec_latency¨query_id¤Q030†¢idÙ@080faa053effbe4bbec87da98899044ce773f60dbaae0fc965a1b9252e966fb0¢tsËAÚÔ`»g¨focus_msÍß§payload‚¤role©assistant§contentÚÎTo batch inputs through a Runnable for better throughput, you can use the built-in `batch` or `batch_as_completed` API, which allows for processing multiple inputs in parallel. The `batch` method returns results in the same order as inputs, while `batch_as_completed` returns results as they complete, potentially out of order. Additionally, you can control the maximum number of parallel calls by setting the `max_concurrency` attribute in the `RunnableConfig`.ªfocus_kind¬exec_latency¨query_id¤Q031†¢idÙ@aecf1c5f389239de78a33af5e06ca1f4c68d39bb07b2d1ee8d78c8b0d56d4189¢tsËAÚÔ²=h¨focus_msÍ§§payload‚¤role©assistant§contentÙâThe parameter in `RunnableConfig` that controls the maximum number of parallel calls during batching is `max_concurrency`. This attribute allows you to limit the number of parallel calls to prevent overloading a server or API.ªfocus_kind¬exec_latency¨query_id¤Q032†¢idÙ@a4dc284cca8706d273b0597f7cac7611ab9f929e0f113def8c4ff3de8b9c32c0¢tsËAÚÔ&›Ë¨focus_msÍR§payload‚¤role©assistant§contentÚ³An example of using `with_structured_output` to get JSON back from a chat model is as follows:

```python
from langchain_openai import ChatOpenAI
model = ChatOpenAI(model="gpt-4o").with_structured_output(method="json_mode")
ai_msg = model.invoke("Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]")
```

This would return a JSON object like `{'random_ints': [45, 67, 12, 34, 89, 23, 78, 56, 90, 11]}`.ªfocus_kind¬exec_latency¨query_id¤Q033†¢idÙ@9a1faf1126ed2db423df18eba514f201a2e5ddbac8e39bcb799510decf25c9b5¢tsËAÚÔ“…Z¨focus_msÍX§payload‚¤role©assistant§contentÚATo stream intermediate events from a Runnable, you would use the `astream_events` API. This allows access to custom data and intermediate outputs from LLM applications built with LangChain's Expression Language (LCEL). Additionally, you can also use the async `astream` method for streaming outputs as they are generated.ªfocus_kind¬exec_latency¨query_id¤Q034†¢idÙ@8dd63cb8d98718377c40d5e1af03508cf72591670c53842e18756bcd3c6a85c2¢tsËAÚÔoÁ¨focus_msÍ§payload‚¤role©assistant§contentÚTThe two standard parameters that influence generation randomness and length in chat models are `temperature` and `max_tokens`. The `temperature` controls the randomness of the model's output, while `max_tokens` limits the total number of tokens in the response. Together, they help shape the creativity and length of the generated messages.ªfocus_kind¬exec_latency¨query_id¤Q035†¢idÙ@1114a1aae7c11c7bf5a1eccfecfc3a26d4af441742354bb76e8624af324b94b7¢tsËAÚÔGÏF¨focus_msÍ§payload‚¤role©assistant§contentÙvThe provided context does not specify how to persist a FAISS vector store to disk. Therefore, I don't know the answer.ªfocus_kind¬exec_latency¨query_id¤Q036†¢idÙ@c00646277e013f1cd1f3f5b091d1f9bbe9915b1013e8eb3de1f3371dd2a765a1¢tsËAÚÔı]§¨focus_msÍÅ§payload‚¤role©assistant§contentÚÊThe `stream()` method is synchronous and yields output chunks as they are produced, allowing for real-time processing in a blocking manner. In contrast, the `astream()` method is asynchronous, designed for non-blocking workflows, enabling the same real-time streaming behavior without blocking the execution of other code. Both methods are used to stream outputs from Runnables, but their usage depends on whether the workflow is synchronous or asynchronous.ªfocus_kind¬exec_latency¨query_id¤Q037†¢idÙ@9ff42e37d77c91d76801c3a0e273855efb0a61bb43bd9457063e0cab188a53d4¢tsËAÚÔá>¨focus_msÍ#§payload‚¤role©assistant§contentÚwLangChain provides example selectors that help in selecting the correct few-shot examples to fit within an LLM's context window. These selectors can choose examples based on various criteria, such as length or semantic similarity, ensuring that the prompts remain within the model's constraints. This feature aids in optimizing the input for better performance and relevance.ªfocus_kind¬exec_latency¨query_id¤Q038†¢idÙ@464b41254130bc857c99c395f510c8429936991ac140897b1943cf8794c71926¢tsËAÚÔ=b¢¨focus_msÍ\§payload‚¤role©assistant§contentÚThree message roles supported by LangChain chat messages are **system**, **user** (or **human**), and **assistant** (or **AI**). Additionally, there are roles for streaming responses and tools. These roles help distinguish the types of messages in a conversation.ªfocus_kind¬exec_latency¨query_id¤Q039†¢idÙ@25de4b34d23207279cea946dd00c83b49c1800f03a6e5ce529af27afae9edc8b¢tsËAÚÔ´]`¨focus_msÍ*§payload‚¤role©assistant§contentÚšTo combine documents returned by multiple retrievers before passing them to an LLM, you can use an ensemble retriever that merges the results based on weighted scores. This approach allows you to leverage the strengths of different retrievers, ensuring a more comprehensive set of documents. Additionally, re-ranking techniques like Reciprocal Rank Fusion can be applied to further refine the combined results.ªfocus_kind¬exec_latency¨query_id¤Q040†¢idÙ@40af339be08c6548ef20e185b186dc5be65f866693febabd8ac79d527a29b677¢tsËAÚÔ	7Zû¨focus_msÍÕ§payload‚¤role©assistant§contentÚ½The `rate_limiter` parameter accepts an instance of `BaseRateLimiter`, which is used to space out requests to avoid exceeding rate limits imposed by the API provider. This is useful for managing the frequency of requests, especially during benchmarking or when handling potential rate limit errors. By controlling the rate of requests, it helps ensure smoother interactions with the API and reduces the likelihood of receiving rate limit errors.ªfocus_kind¬exec_latency¨query_id¤Q041†¢idÙ@21f3c82c8227cc39b48823476a23fee2d31f73355b2388ff7bcc94c00c4cde02¢tsËAÚÔ	¼¬Î¨focus_msÍP§payload‚¤role©assistant§contentÚ´Hereâ€™s a code snippet to stream tokens from an OpenAI chat model using LCEL:

```python
model = ChatAnthropic(model="claude-3-sonnet-20240229")
prompt = ChatPromptTemplate.from_template("tell me a joke about {topic}")
parser = StrOutputParser()
chain = prompt | model | parser

async for event in chain.astream_events({"topic": "parrot"}):
    if event["event"] == "on_chat_model_stream":
        print(event, end="|", flush=True)
```ªfocus_kind¬exec_latency¨query_id¤Q042†¢idÙ@a3712116d0432adbc8573e6dff2cb887b14027d84d656f66e647443efdd9a7e6¢tsËAÚÔ	òl¨focus_msÍ§payload‚¤role©assistant§content­I don't know.ªfocus_kind¬exec_latency¨query_id¤Q043†¢idÙ@c9e94e41c43b5ad4e8df1954fdf6353934aa8638c37e890e39d09792d6c8d527¢tsËAÚÔ
†ÁÂ¨focus_msÍš§payload‚¤role©assistant§contentÚ¸Tool artifacts are outputs from a tool's execution that can be accessed by downstream components without being exposed to the model itself. They can be returned from a tool using a specific response format, such as `response_format="content_and_artifact"`, which allows both a message for the model and the artifact to be returned. This enables the model to receive relevant metadata while keeping the full output accessible for other uses.ªfocus_kind¬exec_latency¨query_id¤Q044†¢idÙ@1e5960682c68fd0d5d7c232731290ac350b7da17059ef0e4cec063cd804be521¢tsËAÚÔ
ÿŞ©¨focus_msÍE§payload‚¤role©assistant§contentÚ#To asynchronously invoke a Runnable and await the result, you can use the `await` keyword followed by the `ainvoke` method of the Runnable. For example, you would write `await some_runnable.ainvoke(some_input)`. This allows you to execute the Runnable asynchronously and wait for the result.ªfocus_kind¬exec_latency¨query_id¤Q045†¢idÙ@0fdef47d16466dc1a99567a2182e702a0d6470eadabac8e1385cfda448540e7e¢tsËAÚÔ\T]¨focus_msÍµ§payload‚¤role©assistant§contentÙşTo understand LangGraph node construction, you should read the "LangGraph how-to guides" and the "LangGraph concepts" documentation. These resources provide detailed instructions and explanations on building and orchestrating applications with LangGraph.ªfocus_kind¬exec_latency¨query_id¤Q046†¢idÙ@f6262f57f7f69fc792b1fa5279ba95470fff9b8397f3d406ccc718b301e91888¢tsËAÚÔÇ¶)¨focus_msÍÏ§payload‚¤role©assistant§contentÚrTo convert a Runnable into a Tool for agent use, you implement the Runnable interface, allowing the tool to be invoked directly using `tool.invoke(args)`. Additionally, you can utilize pre-built components like ToolNode from LangGraph to facilitate tool invocation on behalf of the user. For detailed guidance, refer to the how-to guide on converting Runnables to tools.ªfocus_kind¬exec_latency¨query_id¤Q047†¢idÙ@f111144bf7d146fb0eab0a38e63c0cb1df5e8acf7e1165dbff1bacdf227b26ec¢tsËAÚÔ5Åã¨focus_msÍê§payload‚¤role©assistant§contentÚThe Python package that contains community-contributed chat model integrations is called `langchain-community`. This package includes various third-party integrations maintained by the LangChain community. For official models, you would look into the `langchain-<provider>` packages.ªfocus_kind¬exec_latency¨query_id¤Q048†¢idÙ@fe89bd6b9f6f8743cd6414f7c84b25245c487fadf18d3a68f2423c32bf8fd809¢tsËAÚÔ˜uæ¨focus_msÍ§payload‚¤role©assistant§contentÚsSemantic caching can improve LLM response times by storing responses based on the meaning of the input rather than exact matches, which reduces the number of requests to the model provider. This approach relies on an embedding model to convert text into vector representations for effective caching. However, it may not always accurately capture the meaning of the input.ªfocus_kind¬exec_latency¨query_id¤Q049†¢idÙ@ded68a27e8e81eeca6a37095cba9d12c024510054be3822d3f47ccecf5441d20¢tsËAÚÔô O¨focus_msÍ§payload‚¤role©assistant§contentÚYou would use the `JSONLoader` from LangChain for processing JSON Lines files. When initializing the loader, set `json_lines=True` and specify a `jq_schema` to extract the desired content. This allows you to convert JSONL data into LangChain Document objects effectively.ªfocus_kind¬exec_latency¨query_id¤Q050†¢idÙ@10fed025565c7e3a54738a8ed287c178608501576be2839a8466fe7f7456ae6b¢tsËAÚÔ†×¨focus_msÍ§payload‚¤role©assistant§contentÚaAn `OutputParser` is a component that takes the output from a model and transforms it into a more structured format suitable for downstream tasks. It is particularly useful for generating structured data or normalizing outputs from chat models and LLMs. However, with the advent of function or tool calling, the reliance on output parsing has decreased.ªfocus_kind¬exec_latency¨query_id¤Q051†¢idÙ@65c791da05f3714fc6464b345bfb6abca39c1d45b833e221949bb015b452a488¢tsËAÚÔ5™ö¨focus_msÍ	§payload‚¤role©assistant§contentÚ¨An example selector that prioritizes shorter examples could be one that selects examples based on their token length. This selector is useful in scenarios where prompt length is a concern, as shorter examples can help reduce costs and latency while still providing relevant context for the model. By focusing on brevity, it ensures that the model receives concise and clear examples without overwhelming it with information.ªfocus_kind¬exec_latency¨query_id¤Q052†¢idÙ@68edc5a368ff304c6a8dce26844e134bcdd6d1ceea1f8f07cc3359ac59c6d269¢tsËAÚÔw+N¨focus_msÍ4§payload‚¤role©assistant§contentÙ«The environment variable that can override the OpenAI API base URL in LangChain is `base_url`. This variable specifies the URL of the API endpoint where requests are sent.ªfocus_kind¬exec_latency¨query_id¤Q053†¢idÙ@2dbd905c23636faf35ad62721d278f1749a60eef782edbbb095fc269ca890be7¢tsËAÚÔøf¨focus_msÍª§payload‚¤role©assistant§contentÚ}To attach custom metadata to a Runnable run in LangSmith, use the `run_name`, `tags`, and `metadata` attributes in the `RunnableConfig` dictionary. The `run_name` sets a custom name for the run, while `tags` and `metadata` allow you to specify lists and dictionaries of custom values, respectively. These attributes will be visible in LangSmith for tracking and debugging purposes.ªfocus_kind¬exec_latency¨query_id¤Q054†¢idÙ@829b982446710e2706ced26a73208afd7bb4bec688ceb4b822a6462ccd662aaa¢tsËAÚÔ\4µ¨focus_msÍ§payload‚¤role©assistant§contentÚoThe `configurable_fields` helper enables the configuration of specific attributes in a Runnable, such as the `temperature` of a chat model. This feature is particularly useful for creating advanced chains that require runtime adjustments or experimentation with different parameters. It simplifies the process of managing complex configurations in deployed Runnables.ªfocus_kind¬exec_latency¨query_id¤Q055†¢idÙ@c4f6db29478d1b9f63b1657de8f1d295c3eae71114df09cda9668655471e96c3¢tsËAÚÔ­Ğ¨focus_msÍŞ§payload‚¤role©assistant§contentÙËTwo ChatModel parameters that help avoid exceeding rate limits are `rate_limiter`, which controls the rate of requests, and `max_retries`, which determines the number of retries after a rate limit error.ªfocus_kind¬exec_latency¨query_id¤Q056†¢idÙ@c9ceb1b2c17e38d19f9dd10969a6b46489250c3c9396510ba507304c745df01f¢tsËAÚÔ§Ê¨focus_msÍò§payload‚¤role©assistant§contentÚJTo specify a custom UUID for a Runnable run, you can pass it as a key-value pair in the `config` dictionary when invoking the Runnable. The `run_id` must be a valid UUID string and unique for each run. For example, you can generate a UUID using `uuid.uuid4()` and include it in the `config` like this: `config={'run_id': run_id}`.ªfocus_kind¬exec_latency¨query_id¤Q057†¢idÙ@e11ce449e22a6f65dc51996b050bedbf3ca15396fb54ecb0c5669a6fbac72c46¢tsËAÚÔŸ	ÿ¨focus_msÍN§payload‚¤role©assistant§contentÚşThe `batch` method processes multiple inputs in parallel and returns results in the same order as the inputs, while `batch_as_completed` also processes inputs in parallel but returns results as they complete, which may be out of order. Both methods improve performance by allowing parallel execution, but the choice between them depends on whether maintaining input order is important. If order is not a concern, `batch_as_completed` can be more efficient as it returns results immediately when they are ready.ªfocus_kind¬exec_latency¨query_id¤Q058†¢idÙ@05434ecdca76515d75bf5488faef58994a0c0220fad9cdb3a93fb4ad946054d9¢tsËAÚÔ=!¨focus_msÍA§payload‚¤role©assistant§contentÚÏLangChain, through its integration with LangSmith, assists in evaluating generated answers by providing an evaluation framework that allows developers to define metrics and test their applications against curated datasets. It also enables tracking of results over time and supports automated evaluations as part of continuous integration processes. This systematic approach ensures that LLM applications meet quality standards and fulfill their intended purposes.ªfocus_kind¬exec_latency¨query_id¤Q059†¢idÙ@a9a7236ce5db7feb7c3d5b4d0a363327ca7f9b684fff2e0b5b010a5b5b58f556¢tsËAÚÔq½A¨focus_msÍ\§payload‚¤role©assistant§contentÙ®LangChain's built-in evaluation module supports metrics such as accuracy and latency. These metrics help assess the performance and effectiveness of LLM-powered applications.ªfocus_kind¬exec_latency¨query_id¤Q060†¢idÙ@d3dcc9f96a22b607c22f6f3f1c0130039b01490d4aabae84139471a4ce3d8e0e¢tsËAÚÔÚË¨focus_msÍ*§payload‚¤role©assistant§contentÚ>The LangChain concept that allows combining multiple Runnables with the `|` operator is the LangChain Expression Language (LCEL). This approach enables the creation of chains by describing what should happen rather than how, optimizing runtime execution. Runnables created using LCEL are often referred to as "chains."ªfocus_kind¬exec_latency¨query_id¤Q061†¢idÙ@a69b4680d112d89c12be199659d450e936dfd57b770937075a7a9db2f743f2ae¢tsËAÚÔ‹Š¨focus_msÍ	N§payload‚¤role©assistant§contentÚğTo set up tool streaming for incrementally passing results back to the model, you need to implement a streaming mechanism that captures intermediate outputs from the tool as they are generated. This can be done by integrating streaming APIs that allow real-time updates, either from the LLM outputs or from specific steps in your workflow, such as LangGraph nodes or LCEL pipelines. By doing so, you provide users with immediate feedback and a clearer understanding of the application's progress.ªfocus_kind¬exec_latency¨query_id¤Q062†¢idÙ@bf6f22e3481ad6f07eed2cc5802b8e5bafe502ec9349bc3c2b7afc473f44a3f1¢tsËAÚÔòG€¨focus_msÍI§payload‚¤role©assistant§contentÚThe benefit of using `RunnableLambda` over subclassing a Runnable is that it simplifies the creation of custom Runnables for straightforward transformations without the complexity and potential errors associated with subclassing. `RunnableLambda` is specifically designed for simple use cases, making it easier to implement and maintain. In contrast, subclassing can be more complex and error-prone.ªfocus_kind¬exec_latency¨query_id¤Q063†¢idÙ@4899bfde8e3f3ae67ab57f8e6b08a1a299348df0b0d189e30c9618bf8a5f8fdc¢tsËAÚÔToÎ¨focus_msÍ§payload‚¤role©assistant§contentÚ%The two documents that explain how to build a streaming question answering chain with citations are "How to: stream" and "How to: return citations." These guides provide specific instructions for implementing streaming capabilities and managing citation outputs in a question answering system.ªfocus_kind¬exec_latency¨query_id¤Q064†¢idÙ@4e714f82e2cf6769056880626a0cb8332dc32566a7ff93a491d77d7db8962d3e¢tsËAÚÔ¶…(¨focus_msÍç§payload‚¤role©assistant§contentÚThe provided context does not specify how to measure the BLEU score in LangChain evaluations. It discusses evaluation processes and tools like LangSmith but does not mention BLEU specifically. Therefore, I don't know how to measure BLEU score in LangChain evaluations.ªfocus_kind¬exec_latency¨query_id¤Q065†¢idÙ@843ecda9acdb344644a5e92b5a87af4ca1346dacc0e1a3e202e75f12c5f8ca23¢tsËAÚÔ%_¨focus_msÍ÷§payload‚¤role©assistant§contentÚcThe `ConversationBufferWindowMemory` keeps a record of the conversation history, including messages exchanged between the user and the assistant. It manages this history to ensure the conversation structure is preserved while adhering to the model's context window limitations. This allows the assistant to maintain context and continuity in the dialogue.ªfocus_kind¬exec_latency¨query_id¤Q066†¢idÙ@a76971c2ff5ea4002de92d2ceddac32c46f693307ded7ac717bf7d5a7309d785¢tsËAÚÔ™©Í¨focus_msÍX§payload‚¤role©assistant§contentÚThe loader designed specifically for PDF files in LangChain is the `AzureAIDocumentIntelligenceLoader`. This loader utilizes Azure's Document Intelligence service to extract content from PDFs and other document formats. It can return the content in various formats, including markdown.ªfocus_kind¬exec_latency¨query_id¤Q067†¢idÙ@b0e9422a1bd1be870631c5afe33679eab5ac84a4e162071779ff97bc8cd313f8¢tsËAÚÔş¨focus_msÍ!§payload‚¤role©assistant§contentÚMThe concept guide that covers the standard streaming APIs exposed by Runnables is the [Streaming Conceptual Guide](/docs/concepts/streaming). This guide provides details on how to stream in LangChain. For more information on async programming, you can also refer to the [Async Programming with LangChain](/docs/concepts/async) guide.ªfocus_kind¬exec_latency¨query_id¤Q068†¢idÙ@123ca23133f591925714dd1fbca573ea653d44699f236505f39a0bf7e6ce0213¢tsËAÚÔ¼-Ë¨focus_msÍ	/§payload‚¤role©assistant§contentÚÁThe advantage of using `LangServe` with Runnables is that it allows for the deployment of configurable Runnables, enabling users to experiment with different parameters and models at runtime. This flexibility simplifies the process of adjusting settings, such as temperature in chat models, without needing to modify the underlying code. Additionally, it enhances the ability to compose complex chains using the LangChain Expression Language (LCEL).ªfocus_kind¬exec_latency¨query_id¤Q069†¢idÙ@7429d2380bf35ccefa54d63341272585217e14ca97165f70e737dd79cf8f086d¢tsËAÚÔ+œI¨focus_msÍÒ§payload‚¤role©assistant§contentÚ{Chat models support multimodal inputs by accepting various data types such as text, images, audio, and video, depending on the model provider. Users can pass these inputs using content blocks that specify the type and corresponding data, like URLs for images. For specific implementation details, users can refer to the integration tables and how-to guides provided by LangChain.ªfocus_kind¬exec_latency¨query_id¤Q070†¢idÙ@121a649b6cfe76504dcbe3f6131e4e84c38702fc06da07f35cc72b23cbeadf21¢tsËAÚÔ’hé¨focus_msÍ§payload‚¤role©assistant§contentÙÿTo recover from rate limiting errors, you should wait a certain amount of time before retrying the request, increasing the wait time with each subsequent error. Additionally, you can use the `max_retries` parameter to control the number of retry attempts.ªfocus_kind¬exec_latency¨query_id¤Q071†¢idÙ@119a213f629ef386b4ddaf5a7ad8b004501a5ce932337fab601faea415b613e9¢tsËAÚÔğ!O¨focus_msÍ¡§payload‚¤role©assistant§contentÚQThe parameter that allows a Runnable to stream events as JSON for UIs is the `astream_events` API. This API provides access to custom data and intermediate outputs from applications built with the LangChain Expression Language (LCEL). It enables real-time streaming of results, enhancing user experience by providing incremental updates.ªfocus_kind¬exec_latency¨query_id¤Q072†¢idÙ@3ac90f853ebe215b6c124295d53334541b7c506343ee4519852668e912a2c6e4¢tsËAÚÔs ¥¨focus_msÍw§payload‚¤role©assistant§contentÚITo embed text using a local Hugging Face model, you can use the following code snippet:

```python
from langchain_huggingface import HuggingFaceEmbeddings
embeddings_model = HuggingFaceEmbeddings()
embeddings = embeddings_model.embed_documents(["Your text here."])
```

This will create embeddings for the provided list of texts.ªfocus_kind¬exec_latency¨query_id¤Q073†¢idÙ@abe9b05750c6f138ccc8910cfa7e127a607a563b49847cfc7eca875dc51fbd36¢tsËAÚÔê­G¨focus_msÍ4§payload‚¤role©assistant§contentÚ“The `ContextualCompressionRetriever` reduces context windows by allowing the use of smaller chunk sizes for indexing documents while retaining the linkage to the original document. This ensures that even when only chunks are retrieved, the model does not lose the overall context of the document. By maintaining this connection, it effectively compresses the data while preserving essential information.ªfocus_kind¬exec_latency¨query_id¤Q074†¢idÙ@7b8b2a4380815a8030c6da619906da453c66d07f90abcc9c1be9cde3c78093dc¢tsËAÚÔUwG¨focus_msÍ§payload‚¤role©assistant§contentÚYou should consult the guide titled "How to configure runtime chain internals" for configuring runtime chain internals like temperature. This guide provides detailed information on how to adjust specific attributes in a Runnable. You can find it [here](https://docs/how_to/configure).ªfocus_kind¬exec_latency¨query_id¤Q075†¢idÙ@ccafd2ad56d8b253707d07a9bc6efe53bf584065649bc68dca34b2fe47ffe626¢tsËAÚÔçS£¨focus_msÍ"§payload‚¤role©assistant§contentÚ5The selector that uses Maximal Marginal Relevance (MMR) to diversify examples is the "Example Selector by Maximal Marginal Relevance." This selector is designed to ensure a more diverse set of results by re-ranking after the initial similarity search. It helps avoid returning similar and redundant documents.ªfocus_kind¬exec_latency¨query_id¤Q076†¢idÙ@f240e11aa88c28cf9e8bd7815c4e3a88f41c9698b9f6e4ed4653268c22a92e50¢tsËAÚÔg;`¨focus_msÍ`§payload‚¤role©assistant§contentÚ˜The recommended way to expose tool schemas to chat models using OpenAI function calling is to bind the schema as a tool to the model. This can be done using the `bind_tools` method, allowing the model to call the tool and ensure its response conforms to the tool's schema. For example, you can create a model instance and bind the schema like this: `model_with_tools = model.bind_tools([ResponseFormatter])`.ªfocus_kind¬exec_latency¨query_id¤Q077†¢idÙ@e89f47ec5629611a7016956b83ce02128a7d94879bdd5ff9f078c13ba50c0522¢tsËAÚÔæÂ¨focus_msÍ§§payload‚¤role©assistant§contentÚîThe advantage of using `batch_as_completed` for long-running tasks is that it allows for efficient parallel processing of multiple inputs, returning results as they complete rather than in a fixed order. This can significantly reduce overall latency, especially for I/O-bound tasks, as it enables the system to handle results as soon as they are available. Additionally, it simplifies thread management for users, allowing them to focus on the task rather than the underlying execution details.ªfocus_kind¬exec_latency¨query_id¤Q078†¢idÙ@14f5060988121f58afb935568be06367d8f2b50998783c3aed895ec6c7160b53¢tsËAÚÔ<² ¨focus_msÍ8§payload‚¤role©assistant§contentÙ‰The two files that outline strategies to trim messages to fit the context window are "How to trim messages" and "How to filter messages."ªfocus_kind¬exec_latency¨query_id¤Q079†¢idÙ@00e914f9112bd0b61dd0ce94409bbd593ccbf4d04f813105adfaea19257fe646¢tsËAÚÔ³Ú¨focus_msÍá§payload‚¤role©assistant§contentÚœThe `max_concurrency` field does not control the rate at which requests are made to the model provider; that is managed by the `rate_limiter` parameter. Additionally, it does not affect the number of retries allowed after hitting a rate limit, which is controlled by the `max_retries` parameter. Essentially, `max_concurrency` is focused on parallel calls, while rate limiting and retries are handled separately.ªfocus_kind¬exec_latency¨query_id¤Q080†¢idÙ@24838a1982cb6de72654be349f16ba57676a13f0af5df211bc39c51320806cc9¢tsËAÚÔ(x¨focus_msÍ§payload‚¤role©assistant§contentÙßThe document that explains how to call tools in parallel with OpenAI is titled "How to: disable parallel tool calling." You can find it in the context provided. For more details, refer to the specific guide on tool calling.ªfocus_kind¬exec_latency¨query_id¤Q081†¢idÙ@4e1ca8c8a442c0d0832eb182e4b3d8e4bfb66ef927da5f02a6dba494a55ce581¢tsËAÚÔx¢Æ¨focus_msÍ÷§payload‚¤role©assistant§contentÚ¿`RunnableLambda` is designed for simple transformations where streaming is not required, while `RunnableGenerator` is intended for more complex transformations that involve streaming. This distinction allows developers to choose the appropriate runnable based on the complexity and requirements of their processing logic. In summary, use `RunnableLambda` for straightforward tasks and `RunnableGenerator` when streaming capabilities are necessary.ªfocus_kind¬exec_latency¨query_id¤Q082†¢idÙ@ac2cd32fc51ef6c6b4d31127eabbeb8a7fe187208aa688900f925f02297eea29¢tsËAÚÔÄS¨focus_msÍ&§payload‚¤role©assistant§contentÙ‹The retrieved context does not specify which vector store integration supports time weighted relevance. Therefore, I don't know the answer.ªfocus_kind¬exec_latency¨query_id¤Q083†¢idÙ@3bd2f0d786a6202efa72a629c1ca711176230fa72ac035f1866c5cae99541376¢tsËAÚÔ8Ş©¨focus_msÍ§payload‚¤role©assistant§contentÚ]To use `run_name` for easier debugging in LangSmith, set it as a custom string in the `RunnableConfig` dictionary for your run. This name will appear in logs and help identify the run, making it easier to trace and monitor your LLM application. Remember that `run_name` is not inherited by sub-calls, so you may need to set it for each relevant run.ªfocus_kind¬exec_latency¨query_id¤Q084†¢idÙ@05b6fe6348d210e4d57a41b62de70b3176c995d02f79e5414055bdea40dcea45¢tsËAÚÔ²à¨focus_msÍû§payload‚¤role©assistant§contentÚTo initialize `AzureAIDocumentIntelligenceLoader`, you must provide the `<endpoint>` and `<key>` parameters. These are essential for connecting to the Azure AI Document Intelligence resource. Additionally, you will also need to specify the `file_path` and optionally the `api_model`.ªfocus_kind¬exec_latency¨query_id¤Q085†¢idÙ@44bdf94c8b6d16dcccdca5ac162f8987e222285b8ad446bd19a14f33cd150ef4¢tsËAÚÔ1¬:¨focus_msÍ§payload‚¤role©assistant§contentÚÀThe main benefit of `asam_stream` events over `astream` is that it allows access to custom data and intermediate outputs from LLM applications built with LangChain's Expression Language (LCEL). This provides more granular control and insight into the streaming process, which can be particularly useful for debugging or enhancing interactivity. In contrast, `astream` focuses on streaming outputs without the additional event handling capabilities.ªfocus_kind¬exec_latency¨query_id¤Q086†¢idÙ@c08026897fd3060a1e53c61f37745e5af8eb3638dd492b6e3b7b61ff2aa60c4f¢tsËAÚÔ¥„f¨focus_msÍ*§payload‚¤role©assistant§contentÚkTo specify hidden arguments in a Tool so the model cannot set them, you can use the `InjectedToolArg` annotation. This allows certain parameters to be injected manually at runtime without being exposed in the tool's schema. Other special annotations like `RunnableConfig`, `InjectedState`, and `InjectedStore` can also be used to remove arguments from the schema.ªfocus_kind¬exec_latency¨query_id¤Q087†¢idÙ@cbee694cbfca6a1a35f17f71848c6da769965bb6d0b5f39d0710594496787cc5¢tsËAÚÔÉ`¨focus_msÍİ§payload‚¤role©assistant§contentÚThe guide that demonstrates building a SQL retriever over a large database is the "Q&A over SQL + CSV" tutorial. This guide focuses on using LLMs for question answering over tabular data, including SQL databases. For more details, you can refer to the specific tutorial mentioned.ªfocus_kind¬exec_latency¨query_id¤Q088†¢idÙ@313e0bf2a09744c1dbaec3b5e55a175593d20618e67aaeed889cc60dbfb6ced7¢tsËAÚÔ]ùŒ¨focus_msÍu§payload‚¤role©assistant§contentÚSemantic caching is unlikely to hit after the first chat turn because conversations typically do not start with the exact same message or sequence of messages. As interactions progress, the inputs become more varied and context-specific, making it difficult for the cache to find a match. Additionally, relying on exact inputs for caching limits its effectiveness in dynamic conversational scenarios.ªfocus_kind¬exec_latency¨query_id¤Q089†¢idÙ@20eb95e39f2117fb52954756e7e054a37379502ea03d460b0c0901a40b6aebd9¢tsËAÚÔúV~¨focus_msÍ§payload‚¤role©assistant§contentÚA use case for `RecursiveCharacterTextSplitter` is to preprocess large documents for natural language processing tasks by breaking them into smaller, manageable chunks while preserving the hierarchical structure of the text. This ensures that larger units like paragraphs remain intact, maintaining semantic coherence, which is beneficial for applications such as text retrieval or summarization. For example, it can be used to split a lengthy article into smaller sections without losing the context of each paragraph.ªfocus_kind¬exec_latency¨query_id¤Q090†¢idÙ@6e39d17136707aa47de02133c65e58ec1655a174775a0e8f05f40d8df7ced9cf¢tsËAÚÔO‚ì¨focus_msÍ9§payload‚¤role©assistant§contentÚTwo methods to stream output from a ChatModel are `stream()` and `astream()`. The `stream()` method is synchronous, while `astream()` is asynchronous and designed for non-blocking workflows. Both methods allow for real-time streaming of output as it is generated by the model.ªfocus_kind¬exec_latency¨query_id¤Q091†¢idÙ@939e6c8bd06f9fedc6dea79aad6071f017cd5169543e43b8a220f1e56e32d97e¢tsËAÚÔ±ë¨focus_msÍÇ§payload‚¤role©assistant§contentÚ6The document that teaches you to build a hybrid search retriever combining keyword and vector is titled "How to: use hybrid vector and keyword retrieval." You can find more details in the provided link to the guide. This resource explains how to effectively combine both approaches for improved search results.ªfocus_kind¬exec_latency¨query_id¤Q092†¢idÙ@2fcea393f72f7657305546f8b65c56bb166753d2a686d2cc34c5dc2cae51f450¢tsËAÚÔ .“¨focus_msÍf§payload‚¤role©assistant§contentÚuThe purpose of `Annotated` type hints in tool schemas is to provide descriptions for arguments that will be exposed in the tool's schema. This allows for better documentation and understanding of the tool's parameters when they are presented to users or models. Additionally, it helps in configuring the runtime behavior of the tool by clarifying the role of each argument.ªfocus_kind¬exec_latency¨query_id¤Q093†¢idÙ@b9bb68914ad4f7990c3ab5b9b08e9a66130b8d0dce3830bce05875e330482040¢tsËAÚÔ ª›b¨focus_msÍà§payload‚¤role©assistant§contentÚªCaching chat model responses is challenging because exact inputs are unlikely to repeat after the initial interactions, making cache hits rare. Additionally, semantic caching, which relies on understanding the meaning of inputs, introduces dependencies on other models and may not accurately capture input meanings. While caching can be beneficial for frequently asked questions, its complexity requires careful consideration.ªfocus_kind¬exec_latency¨query_id¤Q094†¢idÙ@ad18c6612a4ee6f529bed7f25404823ebdba41de31661ea52e494aae00cc52b1¢tsËAÚÔ!+¢^¨focus_msÍ §payload‚¤role©assistant§contentÙçThe two documents that explain trimming long documents before embedding are "How to trim messages" and "Text Splitters." These resources provide guidance on effectively managing document lengths for better processing and embedding.ªfocus_kind¬exec_latency¨query_id¤Q095†¢idÙ@f502d4ad44fda5c1a9e483bfae1ed642dc0e2b2809c29ab4ef5bc6d40befa734¢tsËAÚÔ!¾°¨focus_msÍõ§payload‚¤role©assistant§contentÚ/The class that enables you to get structured JSON output directly from LLMs without a parser is the `JsonOutputParser`. It returns a JSON object as specified and can be used with a Pydantic model to ensure the output conforms to that model. This makes it a reliable option for obtaining structured data.ªfocus_kind¬exec_latency¨query_id¤Q096†¢idÙ@a7995adb07b5a8aee8eed941bc1b248bc4b3baf5c12d3e78a1cc8758a33e288d¢tsËAÚÔ"*[¯¨focus_msÍ~§payload‚¤role©assistant§contentÚ%Three built-in message field keys aside from `content` are `role`, `id`, and `name`. The `role` distinguishes between different types of messages, while `id` and `name` serve as additional metadata. These keys help in organizing and understanding the context of the messages in a conversation.ªfocus_kind¬exec_latency¨query_id¤Q097†¢idÙ@c33e6080c22241fd56262217c772ec3bd21e46628f2baca519df234465e7fd6d¢tsËAÚÔ"“œÌ¨focus_msÍ™§payload‚¤role©assistant§contentÚkThe first step when creating a custom Runnable for streaming transformations is to choose between `RunnableLambda` for simple transformations or `RunnableGenerator` for more complex transformations that require streaming. It's important to avoid subclassing Runnables, as this can be complex and error-prone. Instead, use the appropriate type based on your needs.ªfocus_kind¬exec_latency¨query_id¤Q098†¢idÙ@2d36f13a7b147691e55e23c13d3711a76727a99dd4859c60facc3978c8fe9816¢tsËAÚÔ#(çâ¨focus_msÍû§payload‚¤role©assistant§contentÚ®LangChain differentiates official and community chat model integrations based on their support and source. Official models are supported by LangChain and/or the model provider, found in the `langchain-<provider>` packages, while community models are contributed and supported by the community, located in the `langchain-community` package. This distinction helps users identify the level of support and reliability for each model.ªfocus_kind¬exec_latency¨query_id¤Q099†¢idÙ@59f43d9a09c35cb2b3174deed2ea7ed5341fd0fd65d206206cfd09b90cf62a56¢tsËAÚÔ#zXG¨focus_msÍ9§payload‚¤role©assistant§contentÚThe two sources that explain creating example selectors based on semantic similarity are:  
1. [How to: select examples by semantic similarity](/docs/how_to/example_selectors_similarity)  
2. [How to: select examples by semantic ngram overlap](/docs/how_to/example_selectors_ngram)ªfocus_kind¬exec_latency¨query_id¤Q100†¢idÙ@f498c3b7b4457891b076066b4db4c4f5d882d5b272ce298d7833cdd12b765c09¢tsËAÚÔ$¶x¨focus_msÍµ§payload‚¤role©assistant§contentÚÃNot propagating `RunnableConfig` in Python 3.10 async code can lead to issues where callbacks and configuration values, such as tags and metadata, are not inherited by sub-calls. This manual propagation is necessary because `asyncio` tasks in Python 3.9 and 3.10 do not accept a `context` argument, making automatic propagation impossible. As a result, you may fail to see events emitted from custom runnables or tools, which can hinder functionality.ªfocus_kind¬exec_latency¨query_id¤Q101†¢idÙ@f9890ef107e4b5cd51537b8c087481f54e949248bb3393ad323dec324d9ffe40¢tsËAÚÔ$¶M¨focus_msÍä§payload‚¤role©assistant§contentÚ˜The `@tool` option allows the use of the `InjectedToolArg` annotation to hide certain parameters from the tool's schema while still injecting them at runtime. This is useful for arguments that should not be exposed to the model, such as a `user_id` that is dynamically provided. By using `InjectedToolArg`, these parameters can be passed manually during invocation without being visible in the tool's schema.ªfocus_kind¬exec_latency¨query_id¤Q102†¢idÙ@55c63c747897ee9efcb6e2442fdb90beefc326fd25f5d0c063de06946e399a07¢tsËAÚÔ%95ß¨focus_msÍÁ§payload‚¤role©assistant§contentÚ†The notebook that illustrates using an ensemble retriever strategy is the one that includes the initialization of the `EnsembleRetriever` with multiple retrievers, as shown in the provided code snippet. This example combines a `bm25_retriever` and a `vector_store_retriever` with equal weights. For further details, you can refer to the documentation on how to create an ensemble retriever.ªfocus_kind¬exec_latency¨query_id¤Q103†¢idÙ@7c095ee52ed8cbd84a460ae498b53fc3975bdc5184ccb5bd78737a4bb0d53878¢tsËAÚÔ%Ê¥n¨focus_msÍß§payload‚¤role©assistant§contentÚ„Two benefits of the LangChain Expression Language (LCEL) are its ability to simplify streaming, allowing for incremental output during execution, and its seamless integration with LangSmith for automatic logging of all steps, enhancing observability and debuggability. Additionally, LCEL provides a standard API through the Runnable interface, making it easier to use chains consistently.ªfocus_kind¬exec_latency¨query_id¤Q104†¢idÙ@434c1247bc1453edb019fb3bd0134778e5262d7d0df8fa97f9b9dd5b881061ef¢tsËAÚÔ&F	9¨focus_msÍY§payload‚¤role©assistant§contentÚlThe main purpose of the `rate_limiter` argument on ChatModels is to control the rate at which requests are made to avoid exceeding rate limits imposed by the model provider. By spacing out requests, it helps prevent hitting these limits, which is particularly useful during benchmarking. This feature allows for more efficient and effective use of the chat models.ªfocus_kind¬exec_latency¨query_id¤Q105†¢idÙ@956293de684c1b0eb0592f25456ece04894dcba89817b21a5fc51bef10995779¢tsËAÚÔ&¥÷è¨focus_msÍÓ§payload‚¤role©assistant§contentÙØ100 English words is approximately equal to 133 tokens. This is based on the approximation that 1 token is roughly equivalent to Â¾ of a word. Therefore, 100 words would translate to about 100 / 0.75 = 133.33 tokens.ªfocus_kind¬exec_latency¨query_id¤Q106†¢idÙ@7406c4f091baa42a1fe39acb68d5ee5b8d861aa56c418255b20c487c9bac5bb5¢tsËAÚÔ'7“¨focus_msÍŒ§payload‚¤role©assistant§contentÚzThe `invoke` method is synchronous, while `ainvoke` is its asynchronous counterpart, allowing for non-blocking execution. Both methods can trigger streaming mode in LangChain if the application is set to stream, but `ainvoke` enables the use of the `await` keyword for asynchronous operations. Essentially, use `invoke` for synchronous calls and `ainvoke` for asynchronous ones.ªfocus_kind¬exec_latency¨query_id¤Q107†¢idÙ@fde7638954169dca7ce5cbd654aaf9dc32caa02b7e06fa27bb0218b4359e8e37¢tsËAÚÔ'X´á¨focus_msÍ §payload‚¤role©assistant§contentÙû`batch_as_completed` returns results as they complete, and each result includes the input index to help match it back to the original input. This allows users to identify which input corresponds to each result, even if the results arrive out of order.ªfocus_kind¬exec_latency¨query_id¤Q108†¢idÙ@fe4b6da0a4507f6714e852cf7dd997f6e301819e6a88cf1336703db38f3323d1¢tsËAÚÔ'Ó‹¨focus_msÍV§payload‚¤role©assistant§contentÚvYou might choose Chroma over FAISS for prototyping because Chroma runs entirely on your local machine as a library, making it easier to set up and use without additional dependencies. It also allows for quick integration with LangChain, facilitating rapid development and testing. Additionally, being open-source and free enhances its accessibility for prototyping purposes.ªfocus_kind¬exec_latency¨query_id¤Q109