{"ts": 1747974674.0166872, "query_id": "Q000", "question": "What is LangChain and why was it created?", "retrieved_items": [{"doc_source": "concepts/why_langchain.mdx", "score": 0.6256128549575806}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7753462791442871}, {"doc_source": "introduction.mdx", "score": 0.8179372549057007}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.862593412399292}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8778152465820312}, {"doc_source": "how_to/index.mdx", "score": 0.8905954957008362}, {"doc_source": "concepts/architecture.mdx", "score": 0.8992823362350464}, {"doc_source": "concepts/lcel.mdx", "score": 0.9277485609054565}, {"doc_source": "introduction.mdx", "score": 0.9306665062904358}, {"doc_source": "concepts/lcel.mdx", "score": 0.9347594976425171}, {"doc_source": "concepts/architecture.mdx", "score": 0.9381152391433716}, {"doc_source": "tutorials/index.mdx", "score": 0.9413684010505676}, {"doc_source": "concepts/async.mdx", "score": 0.9587855339050293}, {"doc_source": "introduction.mdx", "score": 0.9641822576522827}, {"doc_source": "how_to/installation.mdx", "score": 0.9663776159286499}]}
{"ts": 1747974675.5346508, "query_id": "Q001", "question": "Explain the role of an Agent in LangChain.", "retrieved_items": [{"doc_source": "concepts/agents.mdx", "score": 0.762130856513977}, {"doc_source": "concepts/agents.mdx", "score": 0.8566524982452393}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9449858665466309}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0098626613616943}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.01024329662323}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0199079513549805}, {"doc_source": "how_to/index.mdx", "score": 1.021437644958496}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.041858434677124}, {"doc_source": "introduction.mdx", "score": 1.046122670173645}, {"doc_source": "concepts/architecture.mdx", "score": 1.0863351821899414}, {"doc_source": "concepts/async.mdx", "score": 1.0927753448486328}, {"doc_source": "concepts/index.mdx", "score": 1.0976085662841797}, {"doc_source": "concepts/lcel.mdx", "score": 1.1028136014938354}, {"doc_source": "introduction.mdx", "score": 1.1086885929107666}, {"doc_source": "concepts/lcel.mdx", "score": 1.11927330493927}]}
{"ts": 1747974677.639953, "query_id": "Q002", "question": "How do I install LangChain with extras for tracing?", "retrieved_items": [{"doc_source": "how_to/installation.mdx", "score": 0.8146346211433411}, {"doc_source": "concepts/async.mdx", "score": 0.854292094707489}, {"doc_source": "how_to/installation.mdx", "score": 0.8707662224769592}, {"doc_source": "how_to/installation.mdx", "score": 0.881988525390625}, {"doc_source": "how_to/installation.mdx", "score": 0.9093832969665527}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9152989387512207}, {"doc_source": "how_to/installation.mdx", "score": 0.9299374222755432}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9447572231292725}, {"doc_source": "tutorials/index.mdx", "score": 0.9480108022689819}, {"doc_source": "tutorials/index.mdx", "score": 0.9626230001449585}, {"doc_source": "how_to/index.mdx", "score": 0.9628430604934692}, {"doc_source": "how_to/index.mdx", "score": 0.9877479672431946}, {"doc_source": "concepts/tracing.mdx", "score": 0.9976584911346436}, {"doc_source": "concepts/architecture.mdx", "score": 0.9982905387878418}, {"doc_source": "introduction.mdx", "score": 1.0017629861831665}]}
{"ts": 1747974678.938158, "query_id": "Q003", "question": "Show the minimal LCEL chain that streams token-by-token.", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.5404735207557678}, {"doc_source": "concepts/streaming.mdx", "score": 0.7818113565444946}, {"doc_source": "concepts/lcel.mdx", "score": 0.9595978856086731}, {"doc_source": "how_to/index.mdx", "score": 0.981630802154541}, {"doc_source": "concepts/lcel.mdx", "score": 0.9824298620223999}, {"doc_source": "concepts/lcel.mdx", "score": 0.9857434034347534}, {"doc_source": "concepts/lcel.mdx", "score": 1.011744499206543}, {"doc_source": "concepts/lcel.mdx", "score": 1.0125576257705688}, {"doc_source": "how_to/index.mdx", "score": 1.0156954526901245}, {"doc_source": "concepts/streaming.mdx", "score": 1.0306841135025024}, {"doc_source": "concepts/lcel.mdx", "score": 1.0463629961013794}, {"doc_source": "concepts/streaming.mdx", "score": 1.0568232536315918}, {"doc_source": "concepts/streaming.mdx", "score": 1.0825427770614624}, {"doc_source": "concepts/lcel.mdx", "score": 1.132508397102356}, {"doc_source": "concepts/streaming.mdx", "score": 1.1389080286026}]}
{"ts": 1747974680.298922, "query_id": "Q004", "question": "Compare FAISS and Chroma vector stores in LangChain.", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 0.806323766708374}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.037609338760376}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.0529485940933228}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0543752908706665}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0763309001922607}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.095676064491272}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1125597953796387}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1496846675872803}, {"doc_source": "concepts/multimodality.mdx", "score": 1.1517837047576904}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1747734546661377}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1776008605957031}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.1974241733551025}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.2417325973510742}, {"doc_source": "concepts/lcel.mdx", "score": 1.2423663139343262}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.2602388858795166}]}
{"ts": 1747974682.445011, "query_id": "Q005", "question": "How can I load a Microsoft Word document into a LangChain pipeline?", "retrieved_items": [{"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.763249933719635}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.8708325028419495}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.0561538934707642}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0572929382324219}, {"doc_source": "concepts/index.mdx", "score": 1.0581622123718262}, {"doc_source": "introduction.mdx", "score": 1.063763976097107}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0754098892211914}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.0757726430892944}, {"doc_source": "tutorials/index.mdx", "score": 1.0889432430267334}, {"doc_source": "how_to/index.mdx", "score": 1.0924122333526611}, {"doc_source": "tutorials/index.mdx", "score": 1.129474401473999}, {"doc_source": "tutorials/index.mdx", "score": 1.1388264894485474}, {"doc_source": "concepts/lcel.mdx", "score": 1.143405556678772}, {"doc_source": "how_to/index.mdx", "score": 1.1449612379074097}, {"doc_source": "how_to/index.mdx", "score": 1.1471027135849}]}
{"ts": 1747974683.846813, "query_id": "Q006", "question": "Describe the Runnable protocol and its advantages.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.7724624276161194}, {"doc_source": "concepts/runnables.mdx", "score": 0.8455075025558472}, {"doc_source": "concepts/runnables.mdx", "score": 0.8832784295082092}, {"doc_source": "concepts/runnables.mdx", "score": 0.9488407373428345}, {"doc_source": "concepts/runnables.mdx", "score": 0.9549291133880615}, {"doc_source": "concepts/lcel.mdx", "score": 0.9705426096916199}, {"doc_source": "concepts/runnables.mdx", "score": 1.0508990287780762}, {"doc_source": "concepts/lcel.mdx", "score": 1.0747566223144531}, {"doc_source": "concepts/runnables.mdx", "score": 1.0764281749725342}, {"doc_source": "concepts/runnables.mdx", "score": 1.0801348686218262}, {"doc_source": "concepts/runnables.mdx", "score": 1.112895131111145}, {"doc_source": "concepts/lcel.mdx", "score": 1.1278302669525146}, {"doc_source": "concepts/lcel.mdx", "score": 1.1315538883209229}, {"doc_source": "concepts/runnables.mdx", "score": 1.1356472969055176}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1439578533172607}]}
{"ts": 1747974685.1971788, "query_id": "Q007", "question": "What is Few-Shot prompting and how is it supported?", "retrieved_items": [{"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.6539869904518127}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.8982607126235962}, {"doc_source": "how_to/index.mdx", "score": 1.0801829099655151}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.08315908908844}, {"doc_source": "concepts/index.mdx", "score": 1.0852382183074951}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0978506803512573}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1167761087417603}, {"doc_source": "how_to/index.mdx", "score": 1.1301103830337524}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1818643808364868}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2026060819625854}, {"doc_source": "how_to/index.mdx", "score": 1.2045385837554932}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2113205194473267}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2270116806030273}, {"doc_source": "how_to/index.mdx", "score": 1.2281379699707031}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2336554527282715}]}
{"ts": 1747974686.735032, "query_id": "Q008", "question": "Give an example of a memory object that stores chat history.", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.0518193244934082}, {"doc_source": "concepts/chat_history.mdx", "score": 1.0831236839294434}, {"doc_source": "concepts/chat_history.mdx", "score": 1.0965120792388916}, {"doc_source": "concepts/chat_history.mdx", "score": 1.107028603553772}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1658220291137695}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2140223979949951}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2281112670898438}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2292814254760742}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2391186952590942}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2478481531143188}, {"doc_source": "concepts/messages.mdx", "score": 1.254511833190918}, {"doc_source": "concepts/chat_models.mdx", "score": 1.255309820175171}, {"doc_source": "concepts/streaming.mdx", "score": 1.2613943815231323}, {"doc_source": "concepts/index.mdx", "score": 1.2691402435302734}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2692381143569946}]}
{"ts": 1747974687.971564, "query_id": "Q009", "question": "Outline the steps to trace a LangGraph flow in LangSmith.", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8301337957382202}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8385686278343201}, {"doc_source": "how_to/index.mdx", "score": 0.8489181399345398}, {"doc_source": "concepts/tracing.mdx", "score": 0.9181725382804871}, {"doc_source": "how_to/index.mdx", "score": 0.9691957235336304}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9921094179153442}, {"doc_source": "introduction.mdx", "score": 1.0403468608856201}, {"doc_source": "concepts/streaming.mdx", "score": 1.0460822582244873}, {"doc_source": "tutorials/index.mdx", "score": 1.0698779821395874}, {"doc_source": "how_to/index.mdx", "score": 1.0905511379241943}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0946513414382935}, {"doc_source": "concepts/streaming.mdx", "score": 1.106436014175415}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1459667682647705}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1461951732635498}, {"doc_source": "how_to/installation.mdx", "score": 1.1682076454162598}]}
{"ts": 1747974689.4747388, "query_id": "Q010", "question": "What packages must be pinned to use Pydantic v1 with LangChain?", "retrieved_items": [{"doc_source": "how_to/pydantic_compatibility.md", "score": 0.6679131388664246}, {"doc_source": "concepts/async.mdx", "score": 0.9406142830848694}, {"doc_source": "how_to/installation.mdx", "score": 0.9444892406463623}, {"doc_source": "how_to/installation.mdx", "score": 0.9477394819259644}, {"doc_source": "how_to/installation.mdx", "score": 0.951897382736206}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9903054237365723}, {"doc_source": "how_to/installation.mdx", "score": 0.9912256002426147}, {"doc_source": "how_to/installation.mdx", "score": 0.9954094886779785}, {"doc_source": "how_to/installation.mdx", "score": 1.0310624837875366}, {"doc_source": "concepts/architecture.mdx", "score": 1.063117504119873}, {"doc_source": "concepts/architecture.mdx", "score": 1.116708517074585}, {"doc_source": "concepts/runnables.mdx", "score": 1.1506597995758057}, {"doc_source": "concepts/async.mdx", "score": 1.1581151485443115}, {"doc_source": "introduction.mdx", "score": 1.1684556007385254}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1736489534378052}]}
{"ts": 1747974691.2981062, "query_id": "Q011", "question": "Explain tool calling via OpenAI function messages in LangChain.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.744651198387146}, {"doc_source": "concepts/messages.mdx", "score": 0.7672620415687561}, {"doc_source": "concepts/testing.mdx", "score": 0.8310785293579102}, {"doc_source": "concepts/messages.mdx", "score": 0.8540018200874329}, {"doc_source": "how_to/index.mdx", "score": 0.8576446771621704}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.8632838726043701}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8920662999153137}, {"doc_source": "concepts/testing.mdx", "score": 0.8942639231681824}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9014481902122498}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9068634510040283}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9132900238037109}, {"doc_source": "concepts/messages.mdx", "score": 0.9183686971664429}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9187717437744141}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9202892780303955}, {"doc_source": "concepts/tools.mdx", "score": 0.9247482419013977}]}
{"ts": 1747974692.75177, "query_id": "Q012", "question": "How does Retrieval-Augmented Generation (RAG) work in LangChain?", "retrieved_items": [{"doc_source": "concepts/rag.mdx", "score": 0.5998701453208923}, {"doc_source": "concepts/rag.mdx", "score": 0.8095791339874268}, {"doc_source": "concepts/rag.mdx", "score": 0.9054796099662781}, {"doc_source": "how_to/index.mdx", "score": 0.9584709405899048}, {"doc_source": "how_to/index.mdx", "score": 0.9606523513793945}, {"doc_source": "concepts/rag.mdx", "score": 0.9874659776687622}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0007789134979248}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0547834634780884}, {"doc_source": "concepts/retrieval.mdx", "score": 1.066786289215088}, {"doc_source": "concepts/rag.mdx", "score": 1.0728881359100342}, {"doc_source": "concepts/retrievers.mdx", "score": 1.095107078552246}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1009864807128906}, {"doc_source": "concepts/index.mdx", "score": 1.10440993309021}, {"doc_source": "how_to/index.mdx", "score": 1.1051747798919678}, {"doc_source": "introduction.mdx", "score": 1.1093780994415283}]}
{"ts": 1747974694.560425, "query_id": "Q013", "question": "Provide a code snippet to create embeddings with OpenAI via LangChain.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 0.807744562625885}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8198244571685791}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8584426641464233}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8886377811431885}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9258445501327515}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9356765151023865}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9412320852279663}, {"doc_source": "introduction.mdx", "score": 0.9481805562973022}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9868213534355164}, {"doc_source": "introduction.mdx", "score": 0.987270712852478}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0095010995864868}, {"doc_source": "how_to/index.mdx", "score": 1.0241724252700806}, {"doc_source": "how_to/installation.mdx", "score": 1.0267212390899658}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0315845012664795}, {"doc_source": "how_to/installation.mdx", "score": 1.0330944061279297}]}
{"ts": 1747974696.309215, "query_id": "Q014", "question": "List three callback events emitted during an LLM call.", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 0.8811360597610474}, {"doc_source": "concepts/callbacks.mdx", "score": 0.997875452041626}, {"doc_source": "how_to/index.mdx", "score": 1.0631370544433594}, {"doc_source": "concepts/streaming.mdx", "score": 1.1637694835662842}, {"doc_source": "how_to/index.mdx", "score": 1.1788558959960938}, {"doc_source": "concepts/streaming.mdx", "score": 1.217941164970398}, {"doc_source": "concepts/callbacks.mdx", "score": 1.233821988105774}, {"doc_source": "how_to/index.mdx", "score": 1.2729060649871826}, {"doc_source": "concepts/chat_models.mdx", "score": 1.289503812789917}, {"doc_source": "concepts/lcel.mdx", "score": 1.2895355224609375}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2918074131011963}, {"doc_source": "concepts/callbacks.mdx", "score": 1.3094990253448486}, {"doc_source": "concepts/streaming.mdx", "score": 1.3112527132034302}, {"doc_source": "concepts/streaming.mdx", "score": 1.312583088874817}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3160536289215088}]}
{"ts": 1747974697.5014222, "query_id": "Q015", "question": "Why would you use the AsyncRunnable interface?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9024970531463623}, {"doc_source": "concepts/async.mdx", "score": 0.9286519289016724}, {"doc_source": "concepts/runnables.mdx", "score": 0.9566218256950378}, {"doc_source": "concepts/lcel.mdx", "score": 0.9969758987426758}, {"doc_source": "concepts/runnables.mdx", "score": 1.0043003559112549}, {"doc_source": "concepts/runnables.mdx", "score": 1.063917636871338}, {"doc_source": "concepts/async.mdx", "score": 1.0713164806365967}, {"doc_source": "concepts/runnables.mdx", "score": 1.08419930934906}, {"doc_source": "concepts/runnables.mdx", "score": 1.0921388864517212}, {"doc_source": "concepts/async.mdx", "score": 1.112335443496704}, {"doc_source": "concepts/runnables.mdx", "score": 1.1137404441833496}, {"doc_source": "concepts/callbacks.mdx", "score": 1.1396353244781494}, {"doc_source": "concepts/streaming.mdx", "score": 1.1413309574127197}, {"doc_source": "concepts/runnables.mdx", "score": 1.142803430557251}, {"doc_source": "concepts/runnables.mdx", "score": 1.145635724067688}]}
{"ts": 1747974698.8132648, "query_id": "Q016", "question": "What metrics does LangChain's evaluation module compute out-of-box?", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8663620948791504}, {"doc_source": "concepts/evaluation.mdx", "score": 0.9073658585548401}, {"doc_source": "how_to/index.mdx", "score": 0.9129538536071777}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9283957481384277}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9346678256988525}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9462305307388306}, {"doc_source": "how_to/index.mdx", "score": 0.9599090814590454}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9808598160743713}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9956388473510742}, {"doc_source": "how_to/index.mdx", "score": 1.0374228954315186}, {"doc_source": "concepts/lcel.mdx", "score": 1.0416944026947021}, {"doc_source": "concepts/streaming.mdx", "score": 1.0569852590560913}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0617809295654297}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0649542808532715}, {"doc_source": "how_to/index.mdx", "score": 1.065202236175537}]}
{"ts": 1747974700.26066, "query_id": "Q017", "question": "Demonstrate how to chunk text using RecursiveCharacterTextSplitter.", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.8124167919158936}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9811102151870728}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0012919902801514}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.002683162689209}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.018438458442688}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.055332064628601}, {"doc_source": "how_to/index.mdx", "score": 1.0790477991104126}, {"doc_source": "how_to/index.mdx", "score": 1.096566915512085}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1525919437408447}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1681221723556519}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2836463451385498}, {"doc_source": "concepts/streaming.mdx", "score": 1.3952429294586182}, {"doc_source": "concepts/lcel.mdx", "score": 1.411849021911621}, {"doc_source": "concepts/streaming.mdx", "score": 1.4183542728424072}, {"doc_source": "how_to/index.mdx", "score": 1.4398785829544067}]}
{"ts": 1747974702.2014358, "query_id": "Q018", "question": "What environment variable controls OpenAI API retries in LangChain?", "retrieved_items": [{"doc_source": "concepts/architecture.mdx", "score": 1.1664249897003174}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1724159717559814}, {"doc_source": "concepts/async.mdx", "score": 1.1811460256576538}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1881568431854248}, {"doc_source": "concepts/async.mdx", "score": 1.200913429260254}, {"doc_source": "how_to/installation.mdx", "score": 1.2060489654541016}, {"doc_source": "concepts/async.mdx", "score": 1.2172553539276123}, {"doc_source": "concepts/messages.mdx", "score": 1.2186334133148193}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2190780639648438}, {"doc_source": "concepts/async.mdx", "score": 1.219377040863037}, {"doc_source": "concepts/async.mdx", "score": 1.2266381978988647}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2283799648284912}, {"doc_source": "how_to/installation.mdx", "score": 1.2287760972976685}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2382664680480957}, {"doc_source": "concepts/streaming.mdx", "score": 1.2398751974105835}]}
{"ts": 1747974703.456987, "query_id": "Q019", "question": "How can key-value stores be used to cache intermediate chain output?", "retrieved_items": [{"doc_source": "concepts/key_value_stores.mdx", "score": 0.9602024555206299}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.0030235052108765}, {"doc_source": "concepts/streaming.mdx", "score": 1.1442111730575562}, {"doc_source": "concepts/lcel.mdx", "score": 1.1699233055114746}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.262619972229004}, {"doc_source": "concepts/lcel.mdx", "score": 1.302964210510254}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3147226572036743}, {"doc_source": "concepts/lcel.mdx", "score": 1.3161532878875732}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3171077966690063}, {"doc_source": "concepts/lcel.mdx", "score": 1.3233036994934082}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3438587188720703}, {"doc_source": "concepts/lcel.mdx", "score": 1.3458898067474365}, {"doc_source": "concepts/lcel.mdx", "score": 1.353567361831665}, {"doc_source": "concepts/streaming.mdx", "score": 1.3565329313278198}, {"doc_source": "how_to/index.mdx", "score": 1.3574204444885254}]}
{"ts": 1747974704.933463, "query_id": "Q020", "question": "Explain \"Multimodality\" support in LangChain with an example.", "retrieved_items": [{"doc_source": "concepts/multimodality.mdx", "score": 0.6335165500640869}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7257100343704224}, {"doc_source": "concepts/multimodality.mdx", "score": 0.8760230541229248}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9223558306694031}, {"doc_source": "concepts/tokens.mdx", "score": 0.9275288581848145}, {"doc_source": "concepts/index.mdx", "score": 0.9438586235046387}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9459598064422607}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9563775658607483}, {"doc_source": "how_to/index.mdx", "score": 0.9741711616516113}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.982769250869751}, {"doc_source": "concepts/lcel.mdx", "score": 0.9885106682777405}, {"doc_source": "how_to/index.mdx", "score": 0.9891414046287537}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9974358677864075}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0055335760116577}, {"doc_source": "concepts/streaming.mdx", "score": 1.0111913681030273}]}
{"ts": 1747974706.449825, "query_id": "Q021", "question": "What is the LangChain architecture diagram trying to convey?", "retrieved_items": [{"doc_source": "introduction.mdx", "score": 0.8043962717056274}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8249566555023193}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8385835289955139}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8471784591674805}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8669542670249939}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.883878231048584}, {"doc_source": "introduction.mdx", "score": 0.8907108902931213}, {"doc_source": "introduction.mdx", "score": 0.8952263593673706}, {"doc_source": "how_to/index.mdx", "score": 0.9196272492408752}, {"doc_source": "concepts/lcel.mdx", "score": 0.9287299513816833}, {"doc_source": "concepts/architecture.mdx", "score": 0.9341284036636353}, {"doc_source": "concepts/index.mdx", "score": 0.9423681497573853}, {"doc_source": "tutorials/index.mdx", "score": 0.9548076391220093}, {"doc_source": "concepts/architecture.mdx", "score": 0.9797672033309937}, {"doc_source": "concepts/index.mdx", "score": 0.9869052171707153}]}
{"ts": 1747974708.055978, "query_id": "Q022", "question": "Provide two example selectors and when to use them.", "retrieved_items": [{"doc_source": "concepts/example_selectors.mdx", "score": 0.8741787075996399}, {"doc_source": "how_to/index.mdx", "score": 1.0563435554504395}, {"doc_source": "how_to/index.mdx", "score": 1.176438808441162}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2142183780670166}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2195253372192383}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.321842908859253}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3478730916976929}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.348643183708191}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3767101764678955}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3856935501098633}, {"doc_source": "how_to/index.mdx", "score": 1.4001023769378662}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.4052684307098389}, {"doc_source": "concepts/index.mdx", "score": 1.4129656553268433}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.4284651279449463}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.440999984741211}]}
{"ts": 1747974709.71395, "query_id": "Q023", "question": "How do OutputParsers simplify post-processing in LCEL?", "retrieved_items": [{"doc_source": "concepts/output_parsers.mdx", "score": 0.9453810453414917}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.9979862570762634}, {"doc_source": "concepts/lcel.mdx", "score": 1.0187896490097046}, {"doc_source": "concepts/lcel.mdx", "score": 1.0716336965560913}, {"doc_source": "how_to/index.mdx", "score": 1.089523434638977}, {"doc_source": "how_to/index.mdx", "score": 1.112616777420044}, {"doc_source": "concepts/index.mdx", "score": 1.188816785812378}, {"doc_source": "concepts/lcel.mdx", "score": 1.2002034187316895}, {"doc_source": "concepts/lcel.mdx", "score": 1.2073615789413452}, {"doc_source": "how_to/index.mdx", "score": 1.2138314247131348}, {"doc_source": "concepts/lcel.mdx", "score": 1.231001853942871}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2362818717956543}, {"doc_source": "concepts/streaming.mdx", "score": 1.2389752864837646}, {"doc_source": "concepts/streaming.mdx", "score": 1.2614846229553223}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2665055990219116}]}
{"ts": 1747974711.287248, "query_id": "Q024", "question": "Describe streaming chunk callbacks versus token callbacks.", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 1.0603013038635254}, {"doc_source": "concepts/streaming.mdx", "score": 1.064807415008545}, {"doc_source": "concepts/streaming.mdx", "score": 1.110206127166748}, {"doc_source": "concepts/streaming.mdx", "score": 1.1239291429519653}, {"doc_source": "concepts/streaming.mdx", "score": 1.1326398849487305}, {"doc_source": "concepts/streaming.mdx", "score": 1.136669635772705}, {"doc_source": "concepts/streaming.mdx", "score": 1.1373907327651978}, {"doc_source": "concepts/streaming.mdx", "score": 1.150660753250122}, {"doc_source": "concepts/lcel.mdx", "score": 1.153358817100525}, {"doc_source": "concepts/streaming.mdx", "score": 1.169683575630188}, {"doc_source": "concepts/streaming.mdx", "score": 1.1782993078231812}, {"doc_source": "concepts/runnables.mdx", "score": 1.1833964586257935}, {"doc_source": "concepts/callbacks.mdx", "score": 1.1855170726776123}, {"doc_source": "concepts/streaming.mdx", "score": 1.1943708658218384}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.200619101524353}]}
{"ts": 1747974712.887427, "query_id": "Q025", "question": "What is the purpose of the `ConversationBufferMemory` class and when should it be used?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.1947553157806396}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2198344469070435}, {"doc_source": "concepts/chat_models.mdx", "score": 1.236875057220459}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2725176811218262}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3233171701431274}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3517541885375977}, {"doc_source": "concepts/messages.mdx", "score": 1.3558262586593628}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3740462064743042}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3750489950180054}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3800193071365356}, {"doc_source": "how_to/index.mdx", "score": 1.3867335319519043}, {"doc_source": "concepts/chat_models.mdx", "score": 1.390106201171875}, {"doc_source": "concepts/streaming.mdx", "score": 1.3923567533493042}, {"doc_source": "concepts/messages.mdx", "score": 1.4017802476882935}, {"doc_source": "concepts/index.mdx", "score": 1.40336275100708}]}
{"ts": 1747974714.3816268, "query_id": "Q026", "question": "Which LangChain text splitter lets you split documents by tokens and why would you choose it over a character splitter?", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.6631693840026855}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.7730101346969604}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.8015227317810059}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.8804733753204346}, {"doc_source": "concepts/tokens.mdx", "score": 0.8910840153694153}, {"doc_source": "concepts/lcel.mdx", "score": 0.9239540100097656}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9463688135147095}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9710425138473511}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9804661273956299}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0275704860687256}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.038142204284668}, {"doc_source": "concepts/index.mdx", "score": 1.0393987894058228}, {"doc_source": "introduction.mdx", "score": 1.0479034185409546}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0491197109222412}, {"doc_source": "how_to/index.mdx", "score": 1.057189702987671}]}
{"ts": 1747974716.535274, "query_id": "Q027", "question": "List two differences between `VectorStoreRetriever` and `EnsembleRetriever`.", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.7439137697219849}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8023593425750732}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8832237720489502}, {"doc_source": "how_to/index.mdx", "score": 1.0252223014831543}, {"doc_source": "concepts/retrievers.mdx", "score": 1.069690465927124}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1026475429534912}, {"doc_source": "concepts/retrievers.mdx", "score": 1.117879033088684}, {"doc_source": "how_to/index.mdx", "score": 1.1326682567596436}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1536221504211426}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.156407356262207}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1670334339141846}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2473747730255127}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2545826435089111}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2578637599945068}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2652268409729004}]}
{"ts": 1747974718.7693172, "query_id": "Q028", "question": "How can you configure a chat model to automatically retry on ratelimiting errors?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5972751379013062}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7220752239227295}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9777392745018005}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0452275276184082}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0813367366790771}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1290087699890137}, {"doc_source": "concepts/chat_models.mdx", "score": 1.152199149131775}, {"doc_source": "concepts/streaming.mdx", "score": 1.1571519374847412}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1601169109344482}, {"doc_source": "concepts/chat_models.mdx", "score": 1.174391746520996}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.197850227355957}, {"doc_source": "concepts/chat_models.mdx", "score": 1.202385663986206}, {"doc_source": "concepts/runnables.mdx", "score": 1.2067663669586182}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2157764434814453}, {"doc_source": "concepts/rag.mdx", "score": 1.2160371541976929}]}
{"ts": 1747974720.464122, "query_id": "Q029", "question": "Explain how `InjectedToolArg` affects a tool schema and give one use case.", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.6153978109359741}, {"doc_source": "concepts/tools.mdx", "score": 0.6412104368209839}, {"doc_source": "concepts/tools.mdx", "score": 0.9970866441726685}, {"doc_source": "concepts/tools.mdx", "score": 1.0167717933654785}, {"doc_source": "concepts/tools.mdx", "score": 1.017867088317871}, {"doc_source": "concepts/tools.mdx", "score": 1.0189578533172607}, {"doc_source": "concepts/tools.mdx", "score": 1.0457717180252075}, {"doc_source": "concepts/tools.mdx", "score": 1.0774390697479248}, {"doc_source": "concepts/tools.mdx", "score": 1.0918209552764893}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0983365774154663}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1132431030273438}, {"doc_source": "concepts/tools.mdx", "score": 1.1138862371444702}, {"doc_source": "concepts/tools.mdx", "score": 1.1251200437545776}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.126418113708496}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1502273082733154}]}
{"ts": 1747974722.642967, "query_id": "Q030", "question": "Which decorator is recommended for quickly turning a Python function into a LangChain Tool?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8511143922805786}, {"doc_source": "concepts/runnables.mdx", "score": 0.9628751277923584}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.9698956608772278}, {"doc_source": "concepts/tools.mdx", "score": 1.0076549053192139}, {"doc_source": "concepts/tools.mdx", "score": 1.0291279554367065}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.030137538909912}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.033600926399231}, {"doc_source": "concepts/async.mdx", "score": 1.0533051490783691}, {"doc_source": "concepts/tools.mdx", "score": 1.0608904361724854}, {"doc_source": "concepts/async.mdx", "score": 1.0751806497573853}, {"doc_source": "concepts/lcel.mdx", "score": 1.0759527683258057}, {"doc_source": "concepts/tools.mdx", "score": 1.0763226747512817}, {"doc_source": "concepts/async.mdx", "score": 1.0827867984771729}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.105096459388733}, {"doc_source": "concepts/testing.mdx", "score": 1.1077194213867188}]}
{"ts": 1747974724.8705752, "query_id": "Q031", "question": "Describe how to batch inputs through a Runnable for better throughput.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6072465181350708}, {"doc_source": "concepts/runnables.mdx", "score": 0.7850796580314636}, {"doc_source": "concepts/runnables.mdx", "score": 0.9014024138450623}, {"doc_source": "concepts/runnables.mdx", "score": 0.9243550896644592}, {"doc_source": "concepts/lcel.mdx", "score": 0.9339388012886047}, {"doc_source": "concepts/runnables.mdx", "score": 1.04087233543396}, {"doc_source": "concepts/runnables.mdx", "score": 1.0663765668869019}, {"doc_source": "concepts/async.mdx", "score": 1.117976188659668}, {"doc_source": "concepts/runnables.mdx", "score": 1.1304409503936768}, {"doc_source": "concepts/runnables.mdx", "score": 1.1371331214904785}, {"doc_source": "concepts/runnables.mdx", "score": 1.138059377670288}, {"doc_source": "concepts/runnables.mdx", "score": 1.1657243967056274}, {"doc_source": "concepts/runnables.mdx", "score": 1.168076992034912}, {"doc_source": "concepts/runnables.mdx", "score": 1.1727207899093628}, {"doc_source": "concepts/runnables.mdx", "score": 1.1837236881256104}]}
{"ts": 1747974726.800024, "query_id": "Q032", "question": "What parameter in `RunnableConfig` controls the maximum number of parallel calls during batching?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8541287779808044}, {"doc_source": "concepts/runnables.mdx", "score": 0.8996992707252502}, {"doc_source": "concepts/runnables.mdx", "score": 0.9033241868019104}, {"doc_source": "concepts/runnables.mdx", "score": 0.9460441470146179}, {"doc_source": "concepts/runnables.mdx", "score": 0.9969255924224854}, {"doc_source": "concepts/lcel.mdx", "score": 1.0378303527832031}, {"doc_source": "concepts/runnables.mdx", "score": 1.069619059562683}, {"doc_source": "concepts/runnables.mdx", "score": 1.093639850616455}, {"doc_source": "concepts/runnables.mdx", "score": 1.1011420488357544}, {"doc_source": "concepts/runnables.mdx", "score": 1.1349854469299316}, {"doc_source": "concepts/runnables.mdx", "score": 1.1443865299224854}, {"doc_source": "concepts/runnables.mdx", "score": 1.157991886138916}, {"doc_source": "concepts/runnables.mdx", "score": 1.174830436706543}, {"doc_source": "concepts/runnables.mdx", "score": 1.2125577926635742}, {"doc_source": "concepts/lcel.mdx", "score": 1.2187567949295044}]}
{"ts": 1747974727.760381, "query_id": "Q033", "question": "Give an example of using `with_structured_output` to get JSON back from a chat model.", "retrieved_items": [{"doc_source": "concepts/structured_outputs.mdx", "score": 0.780985414981842}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8988903760910034}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.9392427206039429}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9613547325134277}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9789652824401855}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.9875693321228027}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9999458193778992}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.0024919509887695}, {"doc_source": "concepts/retrieval.mdx", "score": 1.010955810546875}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0281002521514893}, {"doc_source": "concepts/chat_models.mdx", "score": 1.047870397567749}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.0824040174484253}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.11580491065979}, {"doc_source": "concepts/index.mdx", "score": 1.117809772491455}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.118018388748169}]}
{"ts": 1747974730.241123, "query_id": "Q034", "question": "What callback would you use to stream intermediate events from a Runnable?", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 0.8934335708618164}, {"doc_source": "concepts/streaming.mdx", "score": 0.9890404343605042}, {"doc_source": "concepts/streaming.mdx", "score": 1.008561611175537}, {"doc_source": "concepts/streaming.mdx", "score": 1.0113866329193115}, {"doc_source": "concepts/runnables.mdx", "score": 1.0397053956985474}, {"doc_source": "concepts/runnables.mdx", "score": 1.0663822889328003}, {"doc_source": "concepts/runnables.mdx", "score": 1.095582365989685}, {"doc_source": "concepts/callbacks.mdx", "score": 1.097243309020996}, {"doc_source": "concepts/streaming.mdx", "score": 1.0973433256149292}, {"doc_source": "concepts/streaming.mdx", "score": 1.1348118782043457}, {"doc_source": "concepts/runnables.mdx", "score": 1.1392388343811035}, {"doc_source": "concepts/streaming.mdx", "score": 1.1488618850708008}, {"doc_source": "concepts/streaming.mdx", "score": 1.1525089740753174}, {"doc_source": "concepts/streaming.mdx", "score": 1.1597548723220825}, {"doc_source": "concepts/runnables.mdx", "score": 1.1612067222595215}]}
{"ts": 1747974731.923465, "query_id": "Q035", "question": "Which two standard parameters influence generation randomness and length in chat models?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.9568188190460205}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0018939971923828}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0070934295654297}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0433233976364136}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0653163194656372}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1297802925109863}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1473336219787598}, {"doc_source": "concepts/chat_models.mdx", "score": 1.150551199913025}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1539814472198486}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1684939861297607}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1787173748016357}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1839427947998047}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1947615146636963}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2134618759155273}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2171735763549805}]}
{"ts": 1747974733.5654469, "query_id": "Q036", "question": "How do you persist a FAISS vector store to disk?", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 1.1070287227630615}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.1651984453201294}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1919969320297241}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.1963629722595215}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.2159278392791748}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2597166299819946}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2649574279785156}, {"doc_source": "concepts/multimodality.mdx", "score": 1.27949059009552}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2846163511276245}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2856913805007935}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2997119426727295}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.3221923112869263}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.337691307067871}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.346938133239746}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.3684360980987549}]}
{"ts": 1747974734.739624, "query_id": "Q037", "question": "Explain the difference between `stream` and `astream` methods on a Runnable.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.7147122025489807}, {"doc_source": "concepts/streaming.mdx", "score": 0.750545859336853}, {"doc_source": "concepts/streaming.mdx", "score": 0.7568495273590088}, {"doc_source": "concepts/runnables.mdx", "score": 0.7943127751350403}, {"doc_source": "concepts/streaming.mdx", "score": 0.8236989974975586}, {"doc_source": "concepts/streaming.mdx", "score": 0.8399219512939453}, {"doc_source": "concepts/runnables.mdx", "score": 0.880516529083252}, {"doc_source": "concepts/runnables.mdx", "score": 1.0105375051498413}, {"doc_source": "concepts/streaming.mdx", "score": 1.0276570320129395}, {"doc_source": "concepts/runnables.mdx", "score": 1.055921196937561}, {"doc_source": "concepts/runnables.mdx", "score": 1.1137189865112305}, {"doc_source": "concepts/async.mdx", "score": 1.1150667667388916}, {"doc_source": "concepts/streaming.mdx", "score": 1.1174039840698242}, {"doc_source": "concepts/streaming.mdx", "score": 1.1288747787475586}, {"doc_source": "concepts/runnables.mdx", "score": 1.1471431255340576}]}
{"ts": 1747974736.3659651, "query_id": "Q038", "question": "What feature does LangChain provide to ensure prompts fit within an LLM's context window?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.8542962074279785}, {"doc_source": "how_to/index.mdx", "score": 0.9134602546691895}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9302247762680054}, {"doc_source": "how_to/index.mdx", "score": 0.9465752840042114}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9496963024139404}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9639006853103638}, {"doc_source": "concepts/lcel.mdx", "score": 0.9696375131607056}, {"doc_source": "how_to/index.mdx", "score": 0.9765609502792358}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9805574417114258}, {"doc_source": "concepts/streaming.mdx", "score": 0.9845702052116394}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9860134720802307}, {"doc_source": "concepts/index.mdx", "score": 0.9907078146934509}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9966773986816406}, {"doc_source": "tutorials/index.mdx", "score": 0.9972293376922607}, {"doc_source": "concepts/streaming.mdx", "score": 1.0045666694641113}]}
{"ts": 1747974739.3259, "query_id": "Q039", "question": "Name three message roles supported by LangChain chat messages.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 0.7468531727790833}, {"doc_source": "concepts/messages.mdx", "score": 0.7780396342277527}, {"doc_source": "concepts/messages.mdx", "score": 0.8878283500671387}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9012303948402405}, {"doc_source": "concepts/messages.mdx", "score": 0.9069714546203613}, {"doc_source": "concepts/messages.mdx", "score": 0.9362462759017944}, {"doc_source": "concepts/messages.mdx", "score": 0.9395574331283569}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0092933177947998}, {"doc_source": "concepts/multimodality.mdx", "score": 1.016806960105896}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0168626308441162}, {"doc_source": "how_to/index.mdx", "score": 1.0289459228515625}, {"doc_source": "concepts/messages.mdx", "score": 1.041947603225708}, {"doc_source": "concepts/messages.mdx", "score": 1.053396224975586}, {"doc_source": "concepts/streaming.mdx", "score": 1.0550904273986816}, {"doc_source": "concepts/index.mdx", "score": 1.0617893934249878}]}
{"ts": 1747974740.587546, "query_id": "Q040", "question": "How can you combine docs returned by a Retriever before passing to an LLM?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.8409014940261841}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8478938341140747}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9801164865493774}, {"doc_source": "concepts/retrievers.mdx", "score": 1.003244400024414}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0324559211730957}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0521926879882812}, {"doc_source": "how_to/index.mdx", "score": 1.0557212829589844}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0585412979125977}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0690066814422607}, {"doc_source": "concepts/rag.mdx", "score": 1.124057412147522}, {"doc_source": "concepts/rag.mdx", "score": 1.1355570554733276}, {"doc_source": "concepts/retrieval.mdx", "score": 1.138745903968811}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1419570446014404}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1451324224472046}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1566014289855957}]}
{"ts": 1747974742.242534, "query_id": "Q041", "question": "What does the `rate_limiter` parameter accept and why is it useful?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.9187584519386292}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9408445954322815}, {"doc_source": "concepts/chat_models.mdx", "score": 1.071192741394043}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1764020919799805}, {"doc_source": "concepts/runnables.mdx", "score": 1.2194461822509766}, {"doc_source": "concepts/runnables.mdx", "score": 1.270388126373291}, {"doc_source": "concepts/runnables.mdx", "score": 1.3254506587982178}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3378390073776245}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3581678867340088}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3739701509475708}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3982034921646118}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4094321727752686}, {"doc_source": "concepts/runnables.mdx", "score": 1.413726806640625}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4161417484283447}, {"doc_source": "concepts/runnables.mdx", "score": 1.4327988624572754}]}
{"ts": 1747974744.785044, "query_id": "Q042", "question": "Provide a code snippet to stream tokens from an OpenAI chat model using LCEL.", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.8580672740936279}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9569725394248962}, {"doc_source": "concepts/streaming.mdx", "score": 0.9773334264755249}, {"doc_source": "concepts/streaming.mdx", "score": 0.9781379699707031}, {"doc_source": "concepts/streaming.mdx", "score": 0.9933494329452515}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0009372234344482}, {"doc_source": "concepts/streaming.mdx", "score": 1.0149884223937988}, {"doc_source": "concepts/streaming.mdx", "score": 1.015478491783142}, {"doc_source": "concepts/streaming.mdx", "score": 1.0285007953643799}, {"doc_source": "concepts/lcel.mdx", "score": 1.0335556268692017}, {"doc_source": "concepts/messages.mdx", "score": 1.034729242324829}, {"doc_source": "concepts/streaming.mdx", "score": 1.0400614738464355}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0528016090393066}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0541335344314575}, {"doc_source": "concepts/rag.mdx", "score": 1.0626581907272339}]}
{"ts": 1747974746.564917, "query_id": "Q043", "question": "Which notebook demonstrates how to track token usage costs per request?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 1.2758994102478027}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.286210298538208}, {"doc_source": "how_to/index.mdx", "score": 1.3035249710083008}, {"doc_source": "concepts/tokens.mdx", "score": 1.3319091796875}, {"doc_source": "concepts/index.mdx", "score": 1.332702875137329}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3333897590637207}, {"doc_source": "concepts/tokens.mdx", "score": 1.3349742889404297}, {"doc_source": "introduction.mdx", "score": 1.3474395275115967}, {"doc_source": "how_to/index.mdx", "score": 1.350388526916504}, {"doc_source": "tutorials/index.mdx", "score": 1.3521878719329834}, {"doc_source": "how_to/index.mdx", "score": 1.3534271717071533}, {"doc_source": "concepts/tokens.mdx", "score": 1.3602854013442993}, {"doc_source": "concepts/index.mdx", "score": 1.3629999160766602}, {"doc_source": "tutorials/index.mdx", "score": 1.365742564201355}, {"doc_source": "how_to/index.mdx", "score": 1.366734504699707}]}
{"ts": 1747974747.4515781, "query_id": "Q044", "question": "What are tool artifacts and how can they be returned from a tool?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.7273582220077515}, {"doc_source": "how_to/index.mdx", "score": 0.8020932078361511}, {"doc_source": "concepts/tools.mdx", "score": 0.8544261455535889}, {"doc_source": "concepts/tools.mdx", "score": 0.9141151309013367}, {"doc_source": "concepts/tools.mdx", "score": 1.0447263717651367}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1082346439361572}, {"doc_source": "how_to/index.mdx", "score": 1.1097279787063599}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.131062626838684}, {"doc_source": "concepts/messages.mdx", "score": 1.1470674276351929}, {"doc_source": "concepts/tools.mdx", "score": 1.1604117155075073}, {"doc_source": "concepts/tools.mdx", "score": 1.1638593673706055}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1663814783096313}, {"doc_source": "concepts/tools.mdx", "score": 1.1773717403411865}, {"doc_source": "concepts/tools.mdx", "score": 1.1785392761230469}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1901845932006836}]}
{"ts": 1747974749.412946, "query_id": "Q045", "question": "Explain how to asynchronously invoke a Runnable and await the result.", "retrieved_items": [{"doc_source": "concepts/async.mdx", "score": 0.8696340322494507}, {"doc_source": "concepts/runnables.mdx", "score": 0.9418246150016785}, {"doc_source": "concepts/async.mdx", "score": 1.006432294845581}, {"doc_source": "concepts/runnables.mdx", "score": 1.0402286052703857}, {"doc_source": "concepts/runnables.mdx", "score": 1.0633937120437622}, {"doc_source": "concepts/runnables.mdx", "score": 1.0664507150650024}, {"doc_source": "concepts/runnables.mdx", "score": 1.1049065589904785}, {"doc_source": "concepts/lcel.mdx", "score": 1.144421935081482}, {"doc_source": "concepts/lcel.mdx", "score": 1.1547610759735107}, {"doc_source": "concepts/runnables.mdx", "score": 1.1653153896331787}, {"doc_source": "concepts/runnables.mdx", "score": 1.1715314388275146}, {"doc_source": "concepts/runnables.mdx", "score": 1.1743805408477783}, {"doc_source": "concepts/runnables.mdx", "score": 1.1981539726257324}, {"doc_source": "concepts/lcel.mdx", "score": 1.2030324935913086}, {"doc_source": "concepts/runnables.mdx", "score": 1.203944444656372}]}
{"ts": 1747974751.2114801, "query_id": "Q046", "question": "Which two documents should you read to understand LangGraph node construction?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.8433270454406738}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9650079011917114}, {"doc_source": "tutorials/index.mdx", "score": 0.9924482107162476}, {"doc_source": "how_to/index.mdx", "score": 1.0161128044128418}, {"doc_source": "concepts/streaming.mdx", "score": 1.0292083024978638}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0589890480041504}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0643954277038574}, {"doc_source": "introduction.mdx", "score": 1.0982979536056519}, {"doc_source": "introduction.mdx", "score": 1.100066900253296}, {"doc_source": "concepts/index.mdx", "score": 1.1131185293197632}, {"doc_source": "introduction.mdx", "score": 1.1280392408370972}, {"doc_source": "how_to/index.mdx", "score": 1.1306922435760498}, {"doc_source": "tutorials/index.mdx", "score": 1.134656310081482}, {"doc_source": "concepts/index.mdx", "score": 1.1410754919052124}, {"doc_source": "concepts/retrieval.mdx", "score": 1.142289638519287}]}
{"ts": 1747974754.670157, "query_id": "Q047", "question": "Describe the process of converting a Runnable into a Tool for agent use.", "retrieved_items": [{"doc_source": "concepts/tool_calling.mdx", "score": 0.8755960464477539}, {"doc_source": "how_to/index.mdx", "score": 0.9115573763847351}, {"doc_source": "concepts/runnables.mdx", "score": 1.0066299438476562}, {"doc_source": "concepts/runnables.mdx", "score": 1.0175738334655762}, {"doc_source": "concepts/runnables.mdx", "score": 1.0319454669952393}, {"doc_source": "concepts/runnables.mdx", "score": 1.0383288860321045}, {"doc_source": "concepts/tools.mdx", "score": 1.0552089214324951}, {"doc_source": "how_to/index.mdx", "score": 1.0552383661270142}, {"doc_source": "concepts/runnables.mdx", "score": 1.0682363510131836}, {"doc_source": "concepts/tools.mdx", "score": 1.0734639167785645}, {"doc_source": "concepts/tools.mdx", "score": 1.11476469039917}, {"doc_source": "concepts/runnables.mdx", "score": 1.1228768825531006}, {"doc_source": "concepts/runnables.mdx", "score": 1.1273595094680786}, {"doc_source": "concepts/tools.mdx", "score": 1.1284960508346558}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1601788997650146}]}
{"ts": 1747974756.76912, "query_id": "Q048", "question": "What Python package contains community-contributed chat model integrations?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7932825684547424}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9217169880867004}, {"doc_source": "concepts/architecture.mdx", "score": 0.9720602035522461}, {"doc_source": "concepts/chat_models.mdx", "score": 0.991050660610199}, {"doc_source": "concepts/chat_models.mdx", "score": 1.017716407775879}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0466560125350952}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0605427026748657}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0620002746582031}, {"doc_source": "concepts/architecture.mdx", "score": 1.0700914859771729}, {"doc_source": "concepts/index.mdx", "score": 1.076059341430664}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0799974203109741}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0933806896209717}, {"doc_source": "how_to/installation.mdx", "score": 1.0948734283447266}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0949597358703613}, {"doc_source": "how_to/installation.mdx", "score": 1.1038950681686401}]}
{"ts": 1747974759.079232, "query_id": "Q049", "question": "How can semantic caching improve LLM response times and what extra model does it rely on?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7451983690261841}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9085593223571777}, {"doc_source": "how_to/index.mdx", "score": 1.0320791006088257}, {"doc_source": "concepts/retrieval.mdx", "score": 1.097856879234314}, {"doc_source": "concepts/retrieval.mdx", "score": 1.108012318611145}, {"doc_source": "how_to/index.mdx", "score": 1.1667096614837646}, {"doc_source": "how_to/index.mdx", "score": 1.171334981918335}, {"doc_source": "concepts/chat_models.mdx", "score": 1.181129813194275}, {"doc_source": "concepts/rag.mdx", "score": 1.1874606609344482}, {"doc_source": "concepts/lcel.mdx", "score": 1.1928619146347046}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.196015477180481}, {"doc_source": "concepts/rag.mdx", "score": 1.1987911462783813}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2026046514511108}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2224340438842773}, {"doc_source": "concepts/streaming.mdx", "score": 1.2235573530197144}]}
{"ts": 1747974761.007885, "query_id": "Q050", "question": "Which document loader would you use for processing JSON lines files?", "retrieved_items": [{"doc_source": "how_to/document_loader_json.mdx", "score": 0.6945728063583374}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.8332122564315796}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.8511616587638855}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.9374269247055054}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.9546530246734619}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.0603286027908325}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0620708465576172}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.0653338432312012}, {"doc_source": "how_to/index.mdx", "score": 1.0682519674301147}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.07631516456604}, {"doc_source": "how_to/index.mdx", "score": 1.0867478847503662}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.1033943891525269}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.1606346368789673}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.1673837900161743}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2411608695983887}]}
{"ts": 1747974762.128173, "query_id": "Q051", "question": "What is an `OutputParser` and why is it useful in LCEL chains?", "retrieved_items": [{"doc_source": "concepts/output_parsers.mdx", "score": 0.666566014289856}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.8717368245124817}, {"doc_source": "how_to/index.mdx", "score": 0.8749825358390808}, {"doc_source": "concepts/lcel.mdx", "score": 0.9573459625244141}, {"doc_source": "concepts/index.mdx", "score": 0.9734790325164795}, {"doc_source": "how_to/index.mdx", "score": 0.9745916724205017}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.9897770285606384}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.9972451329231262}, {"doc_source": "concepts/lcel.mdx", "score": 1.0475847721099854}, {"doc_source": "concepts/streaming.mdx", "score": 1.050242304801941}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.0783095359802246}, {"doc_source": "concepts/lcel.mdx", "score": 1.087519645690918}, {"doc_source": "concepts/streaming.mdx", "score": 1.0896022319793701}, {"doc_source": "concepts/lcel.mdx", "score": 1.093246340751648}, {"doc_source": "concepts/lcel.mdx", "score": 1.1054003238677979}]}
{"ts": 1747974764.224253, "query_id": "Q052", "question": "Give an example selector that prioritizes shorter examples and describe its use case.", "retrieved_items": [{"doc_source": "concepts/example_selectors.mdx", "score": 0.7915245890617371}, {"doc_source": "how_to/index.mdx", "score": 0.8988745212554932}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0417726039886475}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0673996210098267}, {"doc_source": "how_to/index.mdx", "score": 1.071521282196045}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1145079135894775}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2265288829803467}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2373075485229492}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2497491836547852}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2873811721801758}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2900316715240479}, {"doc_source": "concepts/index.mdx", "score": 1.2927888631820679}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2983436584472656}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.359760046005249}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.370274543762207}]}
{"ts": 1747974765.871914, "query_id": "Q053", "question": "Which environment variable can override OpenAI API base URL in LangChain?", "retrieved_items": [{"doc_source": "how_to/installation.mdx", "score": 1.117238163948059}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1247845888137817}, {"doc_source": "concepts/messages.mdx", "score": 1.1472233533859253}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1501657962799072}, {"doc_source": "how_to/installation.mdx", "score": 1.1661661863327026}, {"doc_source": "concepts/architecture.mdx", "score": 1.1807626485824585}, {"doc_source": "introduction.mdx", "score": 1.1884618997573853}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1907936334609985}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1981210708618164}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1984342336654663}, {"doc_source": "introduction.mdx", "score": 1.2037458419799805}, {"doc_source": "concepts/architecture.mdx", "score": 1.214874505996704}, {"doc_source": "concepts/async.mdx", "score": 1.2155683040618896}, {"doc_source": "concepts/async.mdx", "score": 1.2264128923416138}, {"doc_source": "concepts/async.mdx", "score": 1.2305322885513306}]}
{"ts": 1747974766.628788, "query_id": "Q054", "question": "Describe how to attach custom metadata to a Runnable run for later inspection in LangSmith.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8650907874107361}, {"doc_source": "concepts/runnables.mdx", "score": 1.063724398612976}, {"doc_source": "concepts/runnables.mdx", "score": 1.0840266942977905}, {"doc_source": "concepts/runnables.mdx", "score": 1.0896906852722168}, {"doc_source": "concepts/runnables.mdx", "score": 1.0987436771392822}, {"doc_source": "concepts/runnables.mdx", "score": 1.108872652053833}, {"doc_source": "how_to/index.mdx", "score": 1.1295170783996582}, {"doc_source": "concepts/tools.mdx", "score": 1.1362147331237793}, {"doc_source": "concepts/runnables.mdx", "score": 1.151036262512207}, {"doc_source": "how_to/index.mdx", "score": 1.1560078859329224}, {"doc_source": "concepts/runnables.mdx", "score": 1.156150460243225}, {"doc_source": "tutorials/index.mdx", "score": 1.1598234176635742}, {"doc_source": "concepts/runnables.mdx", "score": 1.1604278087615967}, {"doc_source": "concepts/runnables.mdx", "score": 1.1727242469787598}, {"doc_source": "concepts/runnables.mdx", "score": 1.1823261976242065}]}
{"ts": 1747974768.285849, "query_id": "Q055", "question": "What does the `configurable_fields` helper enable for advanced chains?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9237215518951416}, {"doc_source": "concepts/runnables.mdx", "score": 1.1059658527374268}, {"doc_source": "concepts/runnables.mdx", "score": 1.129848837852478}, {"doc_source": "concepts/runnables.mdx", "score": 1.2372381687164307}, {"doc_source": "concepts/lcel.mdx", "score": 1.2803070545196533}, {"doc_source": "concepts/runnables.mdx", "score": 1.2866963148117065}, {"doc_source": "concepts/lcel.mdx", "score": 1.291886806488037}, {"doc_source": "concepts/lcel.mdx", "score": 1.2932255268096924}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.301077127456665}, {"doc_source": "concepts/lcel.mdx", "score": 1.3022758960723877}, {"doc_source": "concepts/lcel.mdx", "score": 1.3103312253952026}, {"doc_source": "concepts/lcel.mdx", "score": 1.3288748264312744}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3338690996170044}, {"doc_source": "concepts/runnables.mdx", "score": 1.335625171661377}, {"doc_source": "how_to/index.mdx", "score": 1.3409390449523926}]}
{"ts": 1747974769.876342, "query_id": "Q056", "question": "Name two ChatModel parameters that help avoid exceeding rate limits.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5763458013534546}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7863516807556152}, {"doc_source": "concepts/chat_models.mdx", "score": 0.825593113899231}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9939647316932678}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9982819557189941}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0520739555358887}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0629053115844727}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0733782052993774}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0872578620910645}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0920438766479492}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0973937511444092}, {"doc_source": "concepts/chat_models.mdx", "score": 1.101324439048767}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1098026037216187}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1320072412490845}, {"doc_source": "concepts/index.mdx", "score": 1.1390801668167114}]}
{"ts": 1747974771.337516, "query_id": "Q057", "question": "How do you specify a custom UUID for a Runnable run?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9111743569374084}, {"doc_source": "concepts/runnables.mdx", "score": 1.0183892250061035}, {"doc_source": "concepts/runnables.mdx", "score": 1.0532180070877075}, {"doc_source": "concepts/runnables.mdx", "score": 1.0823194980621338}, {"doc_source": "concepts/runnables.mdx", "score": 1.111802101135254}, {"doc_source": "concepts/runnables.mdx", "score": 1.1242324113845825}, {"doc_source": "concepts/runnables.mdx", "score": 1.1606931686401367}, {"doc_source": "concepts/runnables.mdx", "score": 1.1608774662017822}, {"doc_source": "concepts/runnables.mdx", "score": 1.1709871292114258}, {"doc_source": "concepts/runnables.mdx", "score": 1.177156925201416}, {"doc_source": "concepts/runnables.mdx", "score": 1.1943646669387817}, {"doc_source": "concepts/runnables.mdx", "score": 1.1971125602722168}, {"doc_source": "concepts/runnables.mdx", "score": 1.1978585720062256}, {"doc_source": "concepts/runnables.mdx", "score": 1.2037638425827026}, {"doc_source": "concepts/runnables.mdx", "score": 1.2134336233139038}]}
{"ts": 1747974773.125449, "query_id": "Q058", "question": "Compare `batch` and `batch_as_completed` when processing multiple inputs.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8145785331726074}, {"doc_source": "concepts/runnables.mdx", "score": 0.8470011353492737}, {"doc_source": "concepts/runnables.mdx", "score": 1.0513825416564941}, {"doc_source": "concepts/runnables.mdx", "score": 1.1103897094726562}, {"doc_source": "concepts/lcel.mdx", "score": 1.2726666927337646}, {"doc_source": "concepts/streaming.mdx", "score": 1.2928855419158936}, {"doc_source": "concepts/runnables.mdx", "score": 1.3113112449645996}, {"doc_source": "concepts/streaming.mdx", "score": 1.3245251178741455}, {"doc_source": "concepts/runnables.mdx", "score": 1.3437190055847168}, {"doc_source": "concepts/streaming.mdx", "score": 1.34983491897583}, {"doc_source": "concepts/async.mdx", "score": 1.3590542078018188}, {"doc_source": "concepts/runnables.mdx", "score": 1.3688406944274902}, {"doc_source": "concepts/streaming.mdx", "score": 1.3700839281082153}, {"doc_source": "concepts/lcel.mdx", "score": 1.3838773965835571}, {"doc_source": "concepts/streaming.mdx", "score": 1.3962123394012451}]}
{"ts": 1747974775.19487, "query_id": "Q059", "question": "How can LangChain assist with evaluating generated answers automatically?", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8681085109710693}, {"doc_source": "concepts/evaluation.mdx", "score": 0.9115369915962219}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9273431301116943}, {"doc_source": "how_to/index.mdx", "score": 0.9304296374320984}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9342418909072876}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.941401481628418}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9677556157112122}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9747974276542664}, {"doc_source": "how_to/index.mdx", "score": 0.993353545665741}, {"doc_source": "how_to/index.mdx", "score": 1.0075433254241943}, {"doc_source": "introduction.mdx", "score": 1.0214751958847046}, {"doc_source": "how_to/index.mdx", "score": 1.0237374305725098}, {"doc_source": "introduction.mdx", "score": 1.0372201204299927}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0457044839859009}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0484427213668823}]}
{"ts": 1747974776.911159, "query_id": "Q060", "question": "Provide two metrics supported by LangChain's built in evaluation module.", "retrieved_items": [{"doc_source": "concepts/evaluation.mdx", "score": 0.8831498026847839}, {"doc_source": "how_to/index.mdx", "score": 0.8950956463813782}, {"doc_source": "tutorials/index.mdx", "score": 0.9040983319282532}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9286414980888367}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9697948694229126}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9783912897109985}, {"doc_source": "how_to/index.mdx", "score": 0.9851574897766113}, {"doc_source": "concepts/streaming.mdx", "score": 1.0263952016830444}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0552783012390137}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0677473545074463}, {"doc_source": "concepts/index.mdx", "score": 1.0679476261138916}, {"doc_source": "how_to/index.mdx", "score": 1.0760440826416016}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0782742500305176}, {"doc_source": "concepts/architecture.mdx", "score": 1.0941060781478882}, {"doc_source": "concepts/lcel.mdx", "score": 1.1150919198989868}]}
{"ts": 1747974777.780942, "query_id": "Q061", "question": "Which LangChain concept allows combining multiple Runnables with the `|` operator?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.6602378487586975}, {"doc_source": "concepts/runnables.mdx", "score": 0.7449436783790588}, {"doc_source": "concepts/lcel.mdx", "score": 0.7530964612960815}, {"doc_source": "concepts/lcel.mdx", "score": 0.763640284538269}, {"doc_source": "how_to/index.mdx", "score": 0.7690602540969849}, {"doc_source": "concepts/runnables.mdx", "score": 0.7852416038513184}, {"doc_source": "concepts/lcel.mdx", "score": 0.7863801717758179}, {"doc_source": "concepts/lcel.mdx", "score": 0.7923846244812012}, {"doc_source": "concepts/async.mdx", "score": 0.8220717906951904}, {"doc_source": "concepts/lcel.mdx", "score": 0.844659686088562}, {"doc_source": "concepts/runnables.mdx", "score": 0.8499869108200073}, {"doc_source": "concepts/streaming.mdx", "score": 0.8546198606491089}, {"doc_source": "concepts/streaming.mdx", "score": 0.8547351360321045}, {"doc_source": "concepts/lcel.mdx", "score": 0.8575356006622314}, {"doc_source": "concepts/runnables.mdx", "score": 0.8635910749435425}]}
{"ts": 1747974779.393279, "query_id": "Q062", "question": "Explain how to set up tool streaming so that tool results are incrementally passed back to the model.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.9547368884086609}, {"doc_source": "concepts/streaming.mdx", "score": 0.9633723497390747}, {"doc_source": "concepts/streaming.mdx", "score": 1.0007637739181519}, {"doc_source": "concepts/streaming.mdx", "score": 1.0245555639266968}, {"doc_source": "concepts/streaming.mdx", "score": 1.0504353046417236}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0511970520019531}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0673325061798096}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0933940410614014}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1050078868865967}, {"doc_source": "how_to/index.mdx", "score": 1.1092802286148071}, {"doc_source": "concepts/streaming.mdx", "score": 1.1165802478790283}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1197713613510132}, {"doc_source": "concepts/streaming.mdx", "score": 1.1251208782196045}, {"doc_source": "how_to/index.mdx", "score": 1.1289114952087402}, {"doc_source": "how_to/index.mdx", "score": 1.134570837020874}]}
{"ts": 1747974781.1035242, "query_id": "Q063", "question": "What is the benefit of using `RunnableLambda` over subclassing a Runnable?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8218463659286499}, {"doc_source": "concepts/runnables.mdx", "score": 0.971828818321228}, {"doc_source": "concepts/runnables.mdx", "score": 0.9909914135932922}, {"doc_source": "concepts/lcel.mdx", "score": 0.9929336309432983}, {"doc_source": "concepts/runnables.mdx", "score": 1.035704493522644}, {"doc_source": "concepts/runnables.mdx", "score": 1.0421428680419922}, {"doc_source": "concepts/runnables.mdx", "score": 1.0544610023498535}, {"doc_source": "concepts/lcel.mdx", "score": 1.0581932067871094}, {"doc_source": "concepts/runnables.mdx", "score": 1.082933783531189}, {"doc_source": "concepts/lcel.mdx", "score": 1.1029776334762573}, {"doc_source": "concepts/lcel.mdx", "score": 1.1064671277999878}, {"doc_source": "concepts/lcel.mdx", "score": 1.1222094297409058}, {"doc_source": "concepts/lcel.mdx", "score": 1.1645139455795288}, {"doc_source": "concepts/runnables.mdx", "score": 1.1718106269836426}, {"doc_source": "concepts/lcel.mdx", "score": 1.194213628768921}]}
{"ts": 1747974782.497864, "query_id": "Q064", "question": "Which two documents together explain how to build a streaming question answering chain with citations?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.988341748714447}, {"doc_source": "how_to/index.mdx", "score": 1.0464513301849365}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0590620040893555}, {"doc_source": "tutorials/index.mdx", "score": 1.0654561519622803}, {"doc_source": "how_to/index.mdx", "score": 1.073103666305542}, {"doc_source": "concepts/rag.mdx", "score": 1.077179193496704}, {"doc_source": "concepts/streaming.mdx", "score": 1.0912611484527588}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0939664840698242}, {"doc_source": "concepts/streaming.mdx", "score": 1.095635175704956}, {"doc_source": "how_to/index.mdx", "score": 1.1048394441604614}, {"doc_source": "concepts/retrieval.mdx", "score": 1.107347011566162}, {"doc_source": "how_to/index.mdx", "score": 1.1311707496643066}, {"doc_source": "tutorials/index.mdx", "score": 1.1387468576431274}, {"doc_source": "concepts/streaming.mdx", "score": 1.1561075448989868}, {"doc_source": "how_to/index.mdx", "score": 1.1588279008865356}]}
{"ts": 1747974784.124513, "query_id": "Q065", "question": "How do you measure BLEU score in LangChain evaluations?", "retrieved_items": [{"doc_source": "concepts/embedding_models.mdx", "score": 0.9988753795623779}, {"doc_source": "tutorials/index.mdx", "score": 1.0170812606811523}, {"doc_source": "concepts/evaluation.mdx", "score": 1.0848697423934937}, {"doc_source": "how_to/index.mdx", "score": 1.0970184803009033}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1301233768463135}, {"doc_source": "concepts/messages.mdx", "score": 1.1464849710464478}, {"doc_source": "tutorials/index.mdx", "score": 1.1493760347366333}, {"doc_source": "how_to/index.mdx", "score": 1.1501708030700684}, {"doc_source": "how_to/index.mdx", "score": 1.164204478263855}, {"doc_source": "how_to/index.mdx", "score": 1.1771526336669922}, {"doc_source": "introduction.mdx", "score": 1.1810085773468018}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1813998222351074}, {"doc_source": "concepts/lcel.mdx", "score": 1.191497802734375}, {"doc_source": "how_to/index.mdx", "score": 1.1945518255233765}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1955022811889648}]}
{"ts": 1747974785.19024, "query_id": "Q066", "question": "What does the `ConversationBufferWindowMemory` keep in memory?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.1609647274017334}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1824370622634888}, {"doc_source": "concepts/chat_history.mdx", "score": 1.231811761856079}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2468056678771973}, {"doc_source": "concepts/chat_models.mdx", "score": 1.274263858795166}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2775980234146118}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2917637825012207}, {"doc_source": "concepts/messages.mdx", "score": 1.3126062154769897}, {"doc_source": "concepts/index.mdx", "score": 1.3625441789627075}, {"doc_source": "concepts/index.mdx", "score": 1.3812934160232544}, {"doc_source": "concepts/messages.mdx", "score": 1.4071483612060547}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.408811092376709}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4142247438430786}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4145214557647705}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4158587455749512}]}
{"ts": 1747974786.220937, "query_id": "Q067", "question": "Name a loader designed specifically for PDF files in LangChain.", "retrieved_items": [{"doc_source": "concepts/document_loaders.mdx", "score": 0.964363694190979}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.008927583694458}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0582906007766724}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.094031572341919}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0985198020935059}, {"doc_source": "how_to/index.mdx", "score": 1.1510967016220093}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.1550931930541992}, {"doc_source": "concepts/index.mdx", "score": 1.1572778224945068}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1970720291137695}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.2025426626205444}, {"doc_source": "concepts/retrievers.mdx", "score": 1.204717993736267}, {"doc_source": "concepts/lcel.mdx", "score": 1.2074899673461914}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2296643257141113}, {"doc_source": "how_to/index.mdx", "score": 1.2303788661956787}, {"doc_source": "introduction.mdx", "score": 1.2351627349853516}]}
{"ts": 1747974787.4562502, "query_id": "Q068", "question": "Which concept guide covers the standard streaming APIs exposed by Runnables?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.7736462354660034}, {"doc_source": "concepts/runnables.mdx", "score": 0.8441465497016907}, {"doc_source": "concepts/runnables.mdx", "score": 0.8498862981796265}, {"doc_source": "concepts/runnables.mdx", "score": 0.8666174411773682}, {"doc_source": "concepts/streaming.mdx", "score": 0.8680585622787476}, {"doc_source": "concepts/runnables.mdx", "score": 0.8796613812446594}, {"doc_source": "concepts/streaming.mdx", "score": 0.890842854976654}, {"doc_source": "concepts/runnables.mdx", "score": 0.9343417882919312}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9432806372642517}, {"doc_source": "concepts/index.mdx", "score": 0.9438942670822144}, {"doc_source": "concepts/streaming.mdx", "score": 0.9541480541229248}, {"doc_source": "concepts/streaming.mdx", "score": 0.9877237677574158}, {"doc_source": "concepts/lcel.mdx", "score": 1.003112554550171}, {"doc_source": "concepts/streaming.mdx", "score": 1.0087604522705078}, {"doc_source": "concepts/streaming.mdx", "score": 1.0172847509384155}]}
{"ts": 1747974788.786538, "query_id": "Q069", "question": "What is the advantage of using `LangServe` with Runnables?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.821479082107544}, {"doc_source": "concepts/lcel.mdx", "score": 0.8782547116279602}, {"doc_source": "concepts/runnables.mdx", "score": 0.8921812176704407}, {"doc_source": "concepts/runnables.mdx", "score": 0.9145112633705139}, {"doc_source": "concepts/runnables.mdx", "score": 0.9167919158935547}, {"doc_source": "concepts/lcel.mdx", "score": 0.9368176460266113}, {"doc_source": "concepts/lcel.mdx", "score": 0.9407743215560913}, {"doc_source": "concepts/runnables.mdx", "score": 0.9597000479698181}, {"doc_source": "concepts/async.mdx", "score": 0.9737913608551025}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9827975034713745}, {"doc_source": "concepts/architecture.mdx", "score": 0.9978174567222595}, {"doc_source": "concepts/runnables.mdx", "score": 1.0009390115737915}, {"doc_source": "concepts/index.mdx", "score": 1.0072418451309204}, {"doc_source": "concepts/async.mdx", "score": 1.0305473804473877}, {"doc_source": "concepts/runnables.mdx", "score": 1.0315890312194824}]}
{"ts": 1747974790.267494, "query_id": "Q070", "question": "Explain how ChatModels support multimodal inputs.", "retrieved_items": [{"doc_source": "concepts/multimodality.mdx", "score": 0.591133713722229}, {"doc_source": "concepts/multimodality.mdx", "score": 0.6077048778533936}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7256899476051331}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7810487151145935}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7820441722869873}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7924947738647461}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8061254024505615}, {"doc_source": "concepts/chat_models.mdx", "score": 0.839788556098938}, {"doc_source": "concepts/tokens.mdx", "score": 0.8889898061752319}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8930299878120422}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8985527157783508}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9237212538719177}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9428342580795288}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9541950225830078}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9543265104293823}]}
{"ts": 1747974792.1108952, "query_id": "Q071", "question": "What two steps are recommended to recover from ratelimiting errors?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.824700117111206}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9089843034744263}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1987733840942383}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2090424299240112}, {"doc_source": "concepts/retrieval.mdx", "score": 1.4205083847045898}, {"doc_source": "concepts/runnables.mdx", "score": 1.446563959121704}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4578031301498413}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4748451709747314}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.4796034097671509}, {"doc_source": "concepts/rag.mdx", "score": 1.5077643394470215}, {"doc_source": "concepts/retrieval.mdx", "score": 1.5166168212890625}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.525590181350708}, {"doc_source": "concepts/runnables.mdx", "score": 1.5352545976638794}, {"doc_source": "concepts/streaming.mdx", "score": 1.5451881885528564}, {"doc_source": "how_to/index.mdx", "score": 1.5489709377288818}]}
{"ts": 1747974793.346453, "query_id": "Q072", "question": "Which parameter allows a Runnable to stream events as JSON for UIs?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 1.0529756546020508}, {"doc_source": "concepts/streaming.mdx", "score": 1.066117286682129}, {"doc_source": "concepts/streaming.mdx", "score": 1.0809471607208252}, {"doc_source": "concepts/streaming.mdx", "score": 1.0862126350402832}, {"doc_source": "concepts/runnables.mdx", "score": 1.089738368988037}, {"doc_source": "concepts/runnables.mdx", "score": 1.1111892461776733}, {"doc_source": "concepts/runnables.mdx", "score": 1.125075101852417}, {"doc_source": "concepts/streaming.mdx", "score": 1.1306912899017334}, {"doc_source": "concepts/streaming.mdx", "score": 1.1317343711853027}, {"doc_source": "concepts/streaming.mdx", "score": 1.1325639486312866}, {"doc_source": "concepts/runnables.mdx", "score": 1.1376022100448608}, {"doc_source": "concepts/streaming.mdx", "score": 1.1416127681732178}, {"doc_source": "concepts/runnables.mdx", "score": 1.1419658660888672}, {"doc_source": "concepts/runnables.mdx", "score": 1.1532281637191772}, {"doc_source": "concepts/streaming.mdx", "score": 1.1546235084533691}]}
{"ts": 1747974794.7881432, "query_id": "Q073", "question": "Provide a code snippet to embed text with a local HuggingFace model.", "retrieved_items": [{"doc_source": "concepts/embedding_models.mdx", "score": 1.007082223892212}, {"doc_source": "how_to/embed_text.mdx", "score": 1.02755868434906}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0612881183624268}, {"doc_source": "how_to/embed_text.mdx", "score": 1.063272476196289}, {"doc_source": "concepts/multimodality.mdx", "score": 1.0713725090026855}, {"doc_source": "concepts/multimodality.mdx", "score": 1.0833637714385986}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1118022203445435}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1273235082626343}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1461272239685059}, {"doc_source": "how_to/index.mdx", "score": 1.154701590538025}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.157551884651184}, {"doc_source": "how_to/embed_text.mdx", "score": 1.16350519657135}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.16484534740448}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1771366596221924}, {"doc_source": "concepts/multimodality.mdx", "score": 1.1800943613052368}]}
{"ts": 1747974796.514134, "query_id": "Q074", "question": "How does the `ContextualCompressionRetriever` reduce context windows?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 1.1378002166748047}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2801166772842407}, {"doc_source": "how_to/index.mdx", "score": 1.300673484802246}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3162928819656372}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3545430898666382}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.3702077865600586}, {"doc_source": "concepts/rag.mdx", "score": 1.4050337076187134}, {"doc_source": "concepts/rag.mdx", "score": 1.40959894657135}, {"doc_source": "concepts/rag.mdx", "score": 1.4117631912231445}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4130438566207886}, {"doc_source": "concepts/retrieval.mdx", "score": 1.423211932182312}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4324990510940552}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4369711875915527}, {"doc_source": "concepts/lcel.mdx", "score": 1.4374020099639893}, {"doc_source": "concepts/retrievers.mdx", "score": 1.4433979988098145}]}
{"ts": 1747974798.131549, "query_id": "Q075", "question": "What guide should you consult for configuring runtime chain internals like temperature?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 1.1227996349334717}, {"doc_source": "concepts/runnables.mdx", "score": 1.1303682327270508}, {"doc_source": "how_to/index.mdx", "score": 1.1874008178710938}, {"doc_source": "tutorials/index.mdx", "score": 1.2215877771377563}, {"doc_source": "concepts/runnables.mdx", "score": 1.25238835811615}, {"doc_source": "how_to/index.mdx", "score": 1.2876183986663818}, {"doc_source": "concepts/runnables.mdx", "score": 1.3005759716033936}, {"doc_source": "how_to/index.mdx", "score": 1.3015141487121582}, {"doc_source": "how_to/index.mdx", "score": 1.3058210611343384}, {"doc_source": "introduction.mdx", "score": 1.3089969158172607}, {"doc_source": "how_to/index.mdx", "score": 1.3107244968414307}, {"doc_source": "concepts/streaming.mdx", "score": 1.3145726919174194}, {"doc_source": "introduction.mdx", "score": 1.3161388635635376}, {"doc_source": "concepts/tracing.mdx", "score": 1.3178154230117798}, {"doc_source": "how_to/index.mdx", "score": 1.322198748588562}]}
{"ts": 1747974799.575975, "query_id": "Q076", "question": "Which selector uses Maximal Marginal Relevance (MMR) to diversify examples?", "retrieved_items": [{"doc_source": "concepts/vectorstores.mdx", "score": 0.7102353572845459}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8871579170227051}, {"doc_source": "how_to/index.mdx", "score": 0.9365390539169312}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.0197370052337646}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.162663459777832}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1941781044006348}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2188851833343506}, {"doc_source": "how_to/index.mdx", "score": 1.2220258712768555}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2280943393707275}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.232224941253662}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2452598810195923}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2603269815444946}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2618411779403687}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2637079954147339}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2684409618377686}]}
{"ts": 1747974800.8550148, "query_id": "Q077", "question": "What is the recommended way to expose tool schemas to chat models that use OpenAI function calling?", "retrieved_items": [{"doc_source": "concepts/structured_outputs.mdx", "score": 0.6850448250770569}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.698444664478302}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7261625528335571}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8094802498817444}, {"doc_source": "concepts/chat_models.mdx", "score": 0.849743127822876}, {"doc_source": "concepts/tools.mdx", "score": 0.8521592020988464}, {"doc_source": "how_to/index.mdx", "score": 0.8920257091522217}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.8966454267501831}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8994669914245605}, {"doc_source": "concepts/index.mdx", "score": 0.9177568554878235}, {"doc_source": "concepts/tools.mdx", "score": 0.938710629940033}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.9534720182418823}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.9601718187332153}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.9619742035865784}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9715697765350342}]}
{"ts": 1747974802.757926, "query_id": "Q078", "question": "Explain the advantage of using `batch_as_completed` for long running tasks.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8249557018280029}, {"doc_source": "concepts/runnables.mdx", "score": 0.9212173223495483}, {"doc_source": "concepts/runnables.mdx", "score": 1.0497219562530518}, {"doc_source": "concepts/runnables.mdx", "score": 1.13790762424469}, {"doc_source": "concepts/lcel.mdx", "score": 1.1975584030151367}, {"doc_source": "concepts/runnables.mdx", "score": 1.2183756828308105}, {"doc_source": "concepts/lcel.mdx", "score": 1.309704065322876}, {"doc_source": "concepts/async.mdx", "score": 1.3255655765533447}, {"doc_source": "concepts/chat_models.mdx", "score": 1.328876256942749}, {"doc_source": "concepts/async.mdx", "score": 1.3401280641555786}, {"doc_source": "concepts/streaming.mdx", "score": 1.3442565202713013}, {"doc_source": "concepts/runnables.mdx", "score": 1.351682186126709}, {"doc_source": "concepts/async.mdx", "score": 1.3658310174942017}, {"doc_source": "concepts/async.mdx", "score": 1.3686457872390747}, {"doc_source": "concepts/streaming.mdx", "score": 1.3821702003479004}]}
{"ts": 1747974804.183891, "query_id": "Q079", "question": "Which two files outline strategies to trim messages to fit the context window?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 0.913860559463501}, {"doc_source": "concepts/chat_history.mdx", "score": 1.0845845937728882}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1432743072509766}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1763439178466797}, {"doc_source": "how_to/index.mdx", "score": 1.193688154220581}, {"doc_source": "concepts/index.mdx", "score": 1.2073917388916016}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2154500484466553}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2177214622497559}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2744289636611938}, {"doc_source": "concepts/index.mdx", "score": 1.28046715259552}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2915184497833252}, {"doc_source": "concepts/rag.mdx", "score": 1.2934471368789673}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2958765029907227}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3059769868850708}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3158063888549805}]}
{"ts": 1747974805.066262, "query_id": "Q080", "question": "What does the `max_concurrency` field not control when using a ChatModel?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 1.1156283617019653}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1210353374481201}, {"doc_source": "concepts/runnables.mdx", "score": 1.1427820920944214}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1628758907318115}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1745082139968872}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1941992044448853}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2042582035064697}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2172431945800781}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2192624807357788}, {"doc_source": "concepts/streaming.mdx", "score": 1.2207618951797485}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2464035749435425}, {"doc_source": "concepts/index.mdx", "score": 1.2531622648239136}, {"doc_source": "concepts/messages.mdx", "score": 1.2632336616516113}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2758305072784424}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2832262516021729}]}
{"ts": 1747974806.469536, "query_id": "Q081", "question": "Which document explains how to call tools in parallel with OpenAI?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9733020067214966}, {"doc_source": "how_to/index.mdx", "score": 1.0295323133468628}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.036679744720459}, {"doc_source": "how_to/index.mdx", "score": 1.0414315462112427}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0446522235870361}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.100960373878479}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1159029006958008}, {"doc_source": "how_to/index.mdx", "score": 1.1249816417694092}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.137969970703125}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1442979574203491}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1494214534759521}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1501390933990479}, {"doc_source": "how_to/index.mdx", "score": 1.1634527444839478}, {"doc_source": "concepts/chat_models.mdx", "score": 1.164323091506958}, {"doc_source": "concepts/rag.mdx", "score": 1.1689369678497314}]}
{"ts": 1747974808.133594, "query_id": "Q082", "question": "Describe how `RunnableGenerator` differs from `RunnableLambda`.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6835772395133972}, {"doc_source": "concepts/runnables.mdx", "score": 0.9616150856018066}, {"doc_source": "concepts/lcel.mdx", "score": 0.983863353729248}, {"doc_source": "concepts/runnables.mdx", "score": 1.0026142597198486}, {"doc_source": "concepts/runnables.mdx", "score": 1.034669041633606}, {"doc_source": "concepts/runnables.mdx", "score": 1.0363562107086182}, {"doc_source": "concepts/lcel.mdx", "score": 1.052700161933899}, {"doc_source": "concepts/lcel.mdx", "score": 1.0553299188613892}, {"doc_source": "concepts/lcel.mdx", "score": 1.0570900440216064}, {"doc_source": "concepts/runnables.mdx", "score": 1.0615851879119873}, {"doc_source": "concepts/runnables.mdx", "score": 1.0736385583877563}, {"doc_source": "concepts/lcel.mdx", "score": 1.107618808746338}, {"doc_source": "concepts/lcel.mdx", "score": 1.1096677780151367}, {"doc_source": "concepts/lcel.mdx", "score": 1.1183624267578125}, {"doc_source": "concepts/runnables.mdx", "score": 1.1211521625518799}]}
{"ts": 1747974810.5888581, "query_id": "Q083", "question": "Which vector store integration supports time weighted relevance?", "retrieved_items": [{"doc_source": "concepts/vectorstores.mdx", "score": 0.8826102018356323}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9357544183731079}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9397262334823608}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.9585659503936768}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9684504270553589}, {"doc_source": "concepts/retrieval.mdx", "score": 0.995840847492218}, {"doc_source": "how_to/index.mdx", "score": 1.0102925300598145}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0258868932724}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0401456356048584}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.0477368831634521}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0540452003479004}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0792102813720703}, {"doc_source": "how_to/index.mdx", "score": 1.1047005653381348}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1079487800598145}, {"doc_source": "how_to/index.mdx", "score": 1.1222803592681885}]}
{"ts": 1747974811.5002549, "query_id": "Q084", "question": "Explain how to use `run_name` for easier debugging in LangSmith.", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.9923598766326904}, {"doc_source": "concepts/runnables.mdx", "score": 1.0108919143676758}, {"doc_source": "concepts/tracing.mdx", "score": 1.0633167028427124}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.085711121559143}, {"doc_source": "how_to/index.mdx", "score": 1.1239879131317139}, {"doc_source": "concepts/runnables.mdx", "score": 1.1500142812728882}, {"doc_source": "how_to/index.mdx", "score": 1.1597473621368408}, {"doc_source": "how_to/index.mdx", "score": 1.1658848524093628}, {"doc_source": "concepts/architecture.mdx", "score": 1.1747628450393677}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.181648850440979}, {"doc_source": "concepts/runnables.mdx", "score": 1.186213731765747}, {"doc_source": "concepts/async.mdx", "score": 1.2064330577850342}, {"doc_source": "concepts/lcel.mdx", "score": 1.208608627319336}, {"doc_source": "concepts/index.mdx", "score": 1.2215217351913452}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2332525253295898}]}
{"ts": 1747974813.580565, "query_id": "Q085", "question": "Which two parameters must be provided to initialize `AzureAIDocumentIntelligenceLoader`?", "retrieved_items": [{"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.735744059085846}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.736417293548584}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.9849369525909424}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2548105716705322}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.259559988975525}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.2811298370361328}, {"doc_source": "how_to/index.mdx", "score": 1.289015769958496}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.3017258644104004}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3121771812438965}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.318652629852295}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.386300802230835}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3886783123016357}, {"doc_source": "how_to/index.mdx", "score": 1.3905794620513916}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.403723120689392}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4083105325698853}]}
{"ts": 1747974814.917753, "query_id": "Q086", "question": "What is the main benefit of `asam_stream` events over `astream`?", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.9555594325065613}, {"doc_source": "concepts/streaming.mdx", "score": 1.062842845916748}, {"doc_source": "concepts/streaming.mdx", "score": 1.0772480964660645}, {"doc_source": "concepts/streaming.mdx", "score": 1.1196714639663696}, {"doc_source": "concepts/streaming.mdx", "score": 1.156930685043335}, {"doc_source": "concepts/runnables.mdx", "score": 1.1888248920440674}, {"doc_source": "concepts/streaming.mdx", "score": 1.205086588859558}, {"doc_source": "concepts/streaming.mdx", "score": 1.2184903621673584}, {"doc_source": "concepts/streaming.mdx", "score": 1.2508530616760254}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2774133682250977}, {"doc_source": "concepts/streaming.mdx", "score": 1.2971076965332031}, {"doc_source": "concepts/streaming.mdx", "score": 1.3099783658981323}, {"doc_source": "concepts/streaming.mdx", "score": 1.314057469367981}, {"doc_source": "concepts/streaming.mdx", "score": 1.32149076461792}, {"doc_source": "concepts/streaming.mdx", "score": 1.3378397226333618}]}
{"ts": 1747974816.7198, "query_id": "Q087", "question": "How do you specify hidden arguments in a Tool so the model cannot set them?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.9853960275650024}, {"doc_source": "concepts/tools.mdx", "score": 0.9980179071426392}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.202382206916809}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2199089527130127}, {"doc_source": "how_to/index.mdx", "score": 1.2297667264938354}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2649283409118652}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2763835191726685}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.29307222366333}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.297879934310913}, {"doc_source": "concepts/tools.mdx", "score": 1.313031554222107}, {"doc_source": "concepts/tools.mdx", "score": 1.3139877319335938}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3191089630126953}, {"doc_source": "how_to/index.mdx", "score": 1.3260595798492432}, {"doc_source": "concepts/tools.mdx", "score": 1.3356359004974365}, {"doc_source": "concepts/tools.mdx", "score": 1.3416564464569092}]}
{"ts": 1747974818.796067, "query_id": "Q088", "question": "Which guide demonstrates building a SQL retriever over a large database?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.9645313024520874}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0184630155563354}, {"doc_source": "how_to/index.mdx", "score": 1.0300008058547974}, {"doc_source": "concepts/retrieval.mdx", "score": 1.045639991760254}, {"doc_source": "how_to/index.mdx", "score": 1.064558506011963}, {"doc_source": "how_to/index.mdx", "score": 1.0888372659683228}, {"doc_source": "how_to/index.mdx", "score": 1.0961384773254395}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1267627477645874}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1477768421173096}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1500608921051025}, {"doc_source": "how_to/index.mdx", "score": 1.1612540483474731}, {"doc_source": "concepts/retrieval.mdx", "score": 1.163454294204712}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1645228862762451}, {"doc_source": "how_to/index.mdx", "score": 1.1710789203643799}, {"doc_source": "concepts/retrieval.mdx", "score": 1.187834620475769}]}
{"ts": 1747974820.4775748, "query_id": "Q089", "question": "Why is semantic caching unlikely to hit after the first chat turn?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.708650529384613}, {"doc_source": "concepts/chat_models.mdx", "score": 0.875054121017456}, {"doc_source": "concepts/chat_models.mdx", "score": 1.245154857635498}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2828397750854492}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2855494022369385}, {"doc_source": "concepts/chat_history.mdx", "score": 1.307441234588623}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3185694217681885}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3202643394470215}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3410816192626953}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3492114543914795}, {"doc_source": "concepts/messages.mdx", "score": 1.3566855192184448}, {"doc_source": "concepts/lcel.mdx", "score": 1.380613088607788}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.389265775680542}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3918583393096924}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3925224542617798}]}
{"ts": 1747974821.830983, "query_id": "Q090", "question": "Describe a use case for `RecursiveCharacterTextSplitter`.", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.8234882950782776}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9785364270210266}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9962007403373718}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0289008617401123}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0948752164840698}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.128408432006836}, {"doc_source": "how_to/index.mdx", "score": 1.172791600227356}, {"doc_source": "how_to/index.mdx", "score": 1.2021992206573486}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2223774194717407}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2507487535476685}, {"doc_source": "concepts/retrievers.mdx", "score": 1.329925537109375}, {"doc_source": "concepts/tokens.mdx", "score": 1.352319598197937}, {"doc_source": "concepts/retrieval.mdx", "score": 1.3956105709075928}, {"doc_source": "concepts/tokens.mdx", "score": 1.398669958114624}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.4065728187561035}]}
{"ts": 1747974823.6879208, "query_id": "Q091", "question": "Give two methods to stream output from a ChatModel.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.8138336539268494}, {"doc_source": "concepts/streaming.mdx", "score": 0.841260552406311}, {"doc_source": "concepts/streaming.mdx", "score": 0.8450125455856323}, {"doc_source": "concepts/streaming.mdx", "score": 0.8512389063835144}, {"doc_source": "concepts/streaming.mdx", "score": 0.8527231216430664}, {"doc_source": "concepts/streaming.mdx", "score": 0.9393490552902222}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9669780135154724}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9841190576553345}, {"doc_source": "concepts/streaming.mdx", "score": 1.0099329948425293}, {"doc_source": "concepts/streaming.mdx", "score": 1.0141570568084717}, {"doc_source": "concepts/streaming.mdx", "score": 1.0317668914794922}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0569325685501099}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0589330196380615}, {"doc_source": "concepts/streaming.mdx", "score": 1.0633881092071533}, {"doc_source": "concepts/streaming.mdx", "score": 1.0653235912322998}]}
{"ts": 1747974825.47529, "query_id": "Q092", "question": "Which document teaches you to build a hybrid search retriever combining keyword and vector?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.7178983688354492}, {"doc_source": "concepts/retrieval.mdx", "score": 0.7420520186424255}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.7710949778556824}, {"doc_source": "concepts/retrievers.mdx", "score": 0.7863771915435791}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8333187699317932}, {"doc_source": "how_to/index.mdx", "score": 0.8712435960769653}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8728208541870117}, {"doc_source": "concepts/retrieval.mdx", "score": 0.8886033296585083}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9032837748527527}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9189973473548889}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.9291847944259644}, {"doc_source": "concepts/retrieval.mdx", "score": 0.961682915687561}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9675183892250061}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9734079241752625}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9780018329620361}]}
{"ts": 1747974826.6048682, "query_id": "Q093", "question": "What is the purpose of `annotated` type hints in tool schemas?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8371490836143494}, {"doc_source": "concepts/tools.mdx", "score": 0.916273295879364}, {"doc_source": "concepts/tools.mdx", "score": 1.0173289775848389}, {"doc_source": "concepts/tools.mdx", "score": 1.071599006652832}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.1164402961730957}, {"doc_source": "concepts/runnables.mdx", "score": 1.1681712865829468}, {"doc_source": "concepts/tools.mdx", "score": 1.2022013664245605}, {"doc_source": "concepts/tools.mdx", "score": 1.2135677337646484}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2294617891311646}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2331680059432983}, {"doc_source": "concepts/tools.mdx", "score": 1.2553740739822388}, {"doc_source": "concepts/runnables.mdx", "score": 1.2741104364395142}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.276098370552063}, {"doc_source": "concepts/tools.mdx", "score": 1.2924189567565918}, {"doc_source": "concepts/tools.mdx", "score": 1.2999508380889893}]}
{"ts": 1747974828.6372302, "query_id": "Q094", "question": "Explain why caching chat model responses is challenging.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5506515502929688}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7896692752838135}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0335804224014282}, {"doc_source": "concepts/streaming.mdx", "score": 1.089475154876709}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0898255109786987}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1052684783935547}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1398513317108154}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1816279888153076}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1856756210327148}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1868174076080322}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1891241073608398}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1912211179733276}, {"doc_source": "concepts/streaming.mdx", "score": 1.19907808303833}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2129855155944824}, {"doc_source": "concepts/index.mdx", "score": 1.2161725759506226}]}
{"ts": 1747974830.274693, "query_id": "Q095", "question": "Which two docs together explain trimming long documents before embedding?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.0125395059585571}, {"doc_source": "how_to/index.mdx", "score": 1.0350139141082764}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.05762779712677}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0660539865493774}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0733592510223389}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.098794937133789}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1393059492111206}, {"doc_source": "concepts/retrievers.mdx", "score": 1.148611307144165}, {"doc_source": "how_to/index.mdx", "score": 1.16908597946167}, {"doc_source": "how_to/index.mdx", "score": 1.19902503490448}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.204514503479004}, {"doc_source": "how_to/index.mdx", "score": 1.2303524017333984}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.237031102180481}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.2391890287399292}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.2593708038330078}]}
{"ts": 1747974831.285543, "query_id": "Q096", "question": "What class enables you to get structured JSON output directly from LLMS without a parser?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 1.2034281492233276}, {"doc_source": "how_to/index.mdx", "score": 1.2093915939331055}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2099642753601074}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2267794609069824}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2294461727142334}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.27760648727417}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.297562837600708}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2997373342514038}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.3054113388061523}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.3208720684051514}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.3372445106506348}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3387374877929688}, {"doc_source": "concepts/streaming.mdx", "score": 1.3597309589385986}, {"doc_source": "concepts/index.mdx", "score": 1.374760627746582}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3840692043304443}]}
{"ts": 1747974832.686389, "query_id": "Q097", "question": "Name three built in message field keys aside from `content`.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 1.0425972938537598}, {"doc_source": "concepts/messages.mdx", "score": 1.04299795627594}, {"doc_source": "concepts/messages.mdx", "score": 1.1517558097839355}, {"doc_source": "concepts/messages.mdx", "score": 1.1785087585449219}, {"doc_source": "concepts/messages.mdx", "score": 1.2024670839309692}, {"doc_source": "concepts/messages.mdx", "score": 1.2408519983291626}, {"doc_source": "concepts/messages.mdx", "score": 1.2423763275146484}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2603498697280884}, {"doc_source": "concepts/messages.mdx", "score": 1.273859977722168}, {"doc_source": "concepts/messages.mdx", "score": 1.3240563869476318}, {"doc_source": "concepts/index.mdx", "score": 1.3343313932418823}, {"doc_source": "concepts/messages.mdx", "score": 1.3381454944610596}, {"doc_source": "how_to/index.mdx", "score": 1.338607907295227}, {"doc_source": "concepts/messages.mdx", "score": 1.339224100112915}, {"doc_source": "concepts/multimodality.mdx", "score": 1.3422913551330566}]}
{"ts": 1747974833.753749, "query_id": "Q098", "question": "What is the first step when creating a custom Runnable for streaming transformations?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8269925117492676}, {"doc_source": "concepts/runnables.mdx", "score": 0.8744055032730103}, {"doc_source": "concepts/runnables.mdx", "score": 0.9603228569030762}, {"doc_source": "concepts/streaming.mdx", "score": 0.9715161919593811}, {"doc_source": "concepts/runnables.mdx", "score": 1.011565089225769}, {"doc_source": "concepts/streaming.mdx", "score": 1.062029480934143}, {"doc_source": "concepts/runnables.mdx", "score": 1.0718426704406738}, {"doc_source": "concepts/streaming.mdx", "score": 1.0781832933425903}, {"doc_source": "concepts/runnables.mdx", "score": 1.0874831676483154}, {"doc_source": "concepts/streaming.mdx", "score": 1.0997004508972168}, {"doc_source": "concepts/runnables.mdx", "score": 1.1032938957214355}, {"doc_source": "concepts/streaming.mdx", "score": 1.1199111938476562}, {"doc_source": "concepts/runnables.mdx", "score": 1.1215200424194336}, {"doc_source": "concepts/streaming.mdx", "score": 1.1279171705245972}, {"doc_source": "concepts/streaming.mdx", "score": 1.1354037523269653}]}
{"ts": 1747974835.60849, "query_id": "Q099", "question": "How does LangChain differentiate official vs community chat model integrations?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5819493532180786}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7878804206848145}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7971385717391968}, {"doc_source": "how_to/index.mdx", "score": 0.8113199472427368}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8398417234420776}, {"doc_source": "concepts/architecture.mdx", "score": 0.8510037660598755}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8524792194366455}, {"doc_source": "concepts/messages.mdx", "score": 0.8693563342094421}, {"doc_source": "how_to/index.mdx", "score": 0.8958495855331421}, {"doc_source": "concepts/architecture.mdx", "score": 0.9032067060470581}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9358336329460144}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9359391927719116}, {"doc_source": "how_to/installation.mdx", "score": 0.9377875924110413}, {"doc_source": "introduction.mdx", "score": 0.9504354596138}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9531245231628418}]}
{"ts": 1747974837.979444, "query_id": "Q100", "question": "Which two sources explain creating example selectors based on semantic similarity?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9905131459236145}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.0084058046340942}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0918560028076172}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1134625673294067}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1526708602905273}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1697940826416016}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1756577491760254}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1821441650390625}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2273885011672974}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2343604564666748}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2363139390945435}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2375123500823975}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2407219409942627}, {"doc_source": "how_to/index.mdx", "score": 1.2455099821090698}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2587909698486328}]}
{"ts": 1747974839.19047, "query_id": "Q101", "question": "What is the impact of not propagating `RunnableConfig` in Python 3.10 async code?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6224480867385864}, {"doc_source": "concepts/callbacks.mdx", "score": 0.9561946392059326}, {"doc_source": "concepts/runnables.mdx", "score": 0.9745022654533386}, {"doc_source": "concepts/runnables.mdx", "score": 0.9800995588302612}, {"doc_source": "concepts/runnables.mdx", "score": 0.9939375519752502}, {"doc_source": "concepts/runnables.mdx", "score": 1.05778968334198}, {"doc_source": "concepts/async.mdx", "score": 1.0845422744750977}, {"doc_source": "concepts/runnables.mdx", "score": 1.1024236679077148}, {"doc_source": "concepts/async.mdx", "score": 1.113790512084961}, {"doc_source": "concepts/runnables.mdx", "score": 1.123300552368164}, {"doc_source": "concepts/runnables.mdx", "score": 1.145546317100525}, {"doc_source": "concepts/tools.mdx", "score": 1.1513805389404297}, {"doc_source": "concepts/runnables.mdx", "score": 1.173658847808838}, {"doc_source": "concepts/runnables.mdx", "score": 1.188768982887268}, {"doc_source": "concepts/async.mdx", "score": 1.2118910551071167}]}
{"ts": 1747974841.261492, "query_id": "Q102", "question": "Describe the `@tool` option to hide a parameter while still injecting it at runtime.", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8322579264640808}, {"doc_source": "concepts/tools.mdx", "score": 0.8832777738571167}, {"doc_source": "concepts/tools.mdx", "score": 1.0314174890518188}, {"doc_source": "concepts/tools.mdx", "score": 1.082690715789795}, {"doc_source": "concepts/tools.mdx", "score": 1.0921907424926758}, {"doc_source": "how_to/index.mdx", "score": 1.097083330154419}, {"doc_source": "concepts/tools.mdx", "score": 1.101093053817749}, {"doc_source": "concepts/tools.mdx", "score": 1.1306755542755127}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1609480381011963}, {"doc_source": "concepts/tools.mdx", "score": 1.1792004108428955}, {"doc_source": "concepts/tools.mdx", "score": 1.190513014793396}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1923422813415527}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2232096195220947}, {"doc_source": "concepts/tools.mdx", "score": 1.235243320465088}, {"doc_source": "concepts/tools.mdx", "score": 1.2501554489135742}]}
{"ts": 1747974842.75084, "query_id": "Q103", "question": "Which notebook illustrates using an ensemble retriever strategy?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.8428757190704346}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9554160237312317}, {"doc_source": "concepts/retrievers.mdx", "score": 1.034416913986206}, {"doc_source": "how_to/index.mdx", "score": 1.0521678924560547}, {"doc_source": "concepts/retrievers.mdx", "score": 1.12274169921875}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1300461292266846}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1383275985717773}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1464548110961914}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1483328342437744}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1704511642456055}, {"doc_source": "how_to/index.mdx", "score": 1.1886812448501587}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1961408853530884}, {"doc_source": "concepts/retrieval.mdx", "score": 1.197824478149414}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2063908576965332}, {"doc_source": "concepts/rag.mdx", "score": 1.2147248983383179}]}
{"ts": 1747974845.068206, "query_id": "Q104", "question": "Give two benefits of the LangChain Expression Language (LCEL).", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.654735803604126}, {"doc_source": "concepts/lcel.mdx", "score": 0.7978137135505676}, {"doc_source": "concepts/lcel.mdx", "score": 0.7985596656799316}, {"doc_source": "concepts/lcel.mdx", "score": 0.8418781161308289}, {"doc_source": "concepts/lcel.mdx", "score": 0.8667476773262024}, {"doc_source": "concepts/lcel.mdx", "score": 0.877845287322998}, {"doc_source": "how_to/index.mdx", "score": 0.889362096786499}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9455296993255615}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9867230653762817}, {"doc_source": "concepts/lcel.mdx", "score": 0.9975361824035645}, {"doc_source": "concepts/lcel.mdx", "score": 1.014701008796692}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0167889595031738}, {"doc_source": "how_to/index.mdx", "score": 1.024009108543396}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0486652851104736}, {"doc_source": "concepts/index.mdx", "score": 1.0488210916519165}]}
{"ts": 1747974847.837586, "query_id": "Q105", "question": "What is the main purpose of the `rate_limiter` argument on ChatModels?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7162210941314697}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9837468862533569}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9852232933044434}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0046823024749756}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1289587020874023}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1574941873550415}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1808991432189941}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1929550170898438}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2080633640289307}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2084342241287231}, {"doc_source": "concepts/runnables.mdx", "score": 1.2110073566436768}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2242311239242554}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2281334400177002}, {"doc_source": "concepts/runnables.mdx", "score": 1.2281633615493774}, {"doc_source": "concepts/index.mdx", "score": 1.2284517288208008}]}
{"ts": 1747974849.2860641, "query_id": "Q106", "question": "How many tokens is 100 English words approximately equal to?", "retrieved_items": [{"doc_source": "concepts/tokens.mdx", "score": 0.7300253510475159}, {"doc_source": "concepts/tokens.mdx", "score": 1.0408051013946533}, {"doc_source": "concepts/tokens.mdx", "score": 1.0993263721466064}, {"doc_source": "concepts/tokens.mdx", "score": 1.1049894094467163}, {"doc_source": "concepts/tokens.mdx", "score": 1.1921958923339844}, {"doc_source": "concepts/tokens.mdx", "score": 1.318519949913025}, {"doc_source": "concepts/index.mdx", "score": 1.335258960723877}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3486576080322266}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3925007581710815}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4089988470077515}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.4448885917663574}, {"doc_source": "concepts/retrievers.mdx", "score": 1.4545087814331055}, {"doc_source": "concepts/retrieval.mdx", "score": 1.4580717086791992}, {"doc_source": "concepts/retrieval.mdx", "score": 1.469346046447754}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4745664596557617}]}
{"ts": 1747974851.188181, "query_id": "Q107", "question": "Explain the difference between `invoke` and `ainvoke`.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.9344920516014099}, {"doc_source": "concepts/async.mdx", "score": 1.1374543905258179}, {"doc_source": "concepts/async.mdx", "score": 1.2069815397262573}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.212014079093933}, {"doc_source": "concepts/streaming.mdx", "score": 1.2388455867767334}, {"doc_source": "concepts/runnables.mdx", "score": 1.266967535018921}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2930909395217896}, {"doc_source": "concepts/tools.mdx", "score": 1.3333708047866821}, {"doc_source": "concepts/async.mdx", "score": 1.3371084928512573}, {"doc_source": "concepts/messages.mdx", "score": 1.3991920948028564}, {"doc_source": "concepts/async.mdx", "score": 1.400232195854187}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.4016433954238892}, {"doc_source": "concepts/tools.mdx", "score": 1.4109489917755127}, {"doc_source": "concepts/retrieval.mdx", "score": 1.4119601249694824}, {"doc_source": "concepts/tools.mdx", "score": 1.4124888181686401}]}
{"ts": 1747974852.7995331, "query_id": "Q108", "question": "What does `batch_as_completed` return with each result to help match inputs?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9440418481826782}, {"doc_source": "concepts/runnables.mdx", "score": 0.9989973306655884}, {"doc_source": "concepts/runnables.mdx", "score": 1.1420438289642334}, {"doc_source": "concepts/runnables.mdx", "score": 1.2197997570037842}, {"doc_source": "concepts/runnables.mdx", "score": 1.333064317703247}, {"doc_source": "concepts/lcel.mdx", "score": 1.3625636100769043}, {"doc_source": "concepts/streaming.mdx", "score": 1.3666044473648071}, {"doc_source": "concepts/index.mdx", "score": 1.3682470321655273}, {"doc_source": "concepts/runnables.mdx", "score": 1.3810688257217407}, {"doc_source": "concepts/streaming.mdx", "score": 1.3926225900650024}, {"doc_source": "concepts/async.mdx", "score": 1.4001233577728271}, {"doc_source": "concepts/runnables.mdx", "score": 1.4073666334152222}, {"doc_source": "concepts/streaming.mdx", "score": 1.4097381830215454}, {"doc_source": "concepts/runnables.mdx", "score": 1.4186362028121948}, {"doc_source": "concepts/streaming.mdx", "score": 1.420689344406128}]}
{"ts": 1747974853.899284, "query_id": "Q109", "question": "Why might you choose Chroma over FAISS for prototyping?", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 0.9985740184783936}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3060898780822754}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3282228708267212}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3426599502563477}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3775732517242432}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3814351558685303}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.4028398990631104}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.4066612720489502}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.4098128080368042}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.4128831624984741}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.4198282957077026}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4267890453338623}, {"doc_source": "concepts/multimodality.mdx", "score": 1.4297932386398315}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.435242772102356}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.4381399154663086}]}
