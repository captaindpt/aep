{"ts": 1747929118.0805361, "query_id": "Q000", "question": "What is LangChain and why was it created?", "retrieved_items": [{"doc_source": "concepts/why_langchain.mdx", "score": 0.6254452466964722}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7753644585609436}, {"doc_source": "introduction.mdx", "score": 0.8178881406784058}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8628154993057251}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8780077695846558}]}
{"ts": 1747929121.041425, "query_id": "Q001", "question": "Explain the role of an Agent in LangChain.", "retrieved_items": [{"doc_source": "concepts/agents.mdx", "score": 0.7613274455070496}, {"doc_source": "concepts/agents.mdx", "score": 0.8567066192626953}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9450191855430603}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.01006281375885}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0102696418762207}]}
{"ts": 1747929123.7496471, "query_id": "Q002", "question": "How do I install LangChain with extras for tracing?", "retrieved_items": [{"doc_source": "how_to/installation.mdx", "score": 0.8146010041236877}, {"doc_source": "concepts/async.mdx", "score": 0.8546172380447388}, {"doc_source": "how_to/installation.mdx", "score": 0.8706044554710388}, {"doc_source": "how_to/installation.mdx", "score": 0.8820451498031616}, {"doc_source": "how_to/installation.mdx", "score": 0.9091845154762268}]}
{"ts": 1747929127.0541599, "query_id": "Q003", "question": "Show the minimal LCEL chain that streams token-by-token.", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.539919376373291}, {"doc_source": "concepts/streaming.mdx", "score": 0.7820814847946167}, {"doc_source": "concepts/lcel.mdx", "score": 0.9582014083862305}, {"doc_source": "how_to/index.mdx", "score": 0.9816372394561768}, {"doc_source": "concepts/lcel.mdx", "score": 0.9824016094207764}]}
{"ts": 1747929128.6785111, "query_id": "Q004", "question": "Compare FAISS and Chroma vector stores in LangChain.", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 0.8032916188240051}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.038285255432129}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.0537904500961304}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0544977188110352}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0763309001922607}]}
{"ts": 1747929131.690965, "query_id": "Q005", "question": "How can I load a Microsoft Word document into a LangChain pipeline?", "retrieved_items": [{"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.7629108428955078}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.8709366321563721}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.056767225265503}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0568058490753174}, {"doc_source": "concepts/index.mdx", "score": 1.0583122968673706}]}
{"ts": 1747929133.518173, "query_id": "Q006", "question": "Describe the Runnable protocol and its advantages.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.7726720571517944}, {"doc_source": "concepts/runnables.mdx", "score": 0.845411479473114}, {"doc_source": "concepts/runnables.mdx", "score": 0.8832782506942749}, {"doc_source": "concepts/runnables.mdx", "score": 0.9488406181335449}, {"doc_source": "concepts/runnables.mdx", "score": 0.9549243450164795}]}
{"ts": 1747929135.399523, "query_id": "Q007", "question": "What is Few-Shot prompting and how is it supported?", "retrieved_items": [{"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.6546075344085693}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.8982607126235962}, {"doc_source": "how_to/index.mdx", "score": 1.0817112922668457}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.0861105918884277}, {"doc_source": "concepts/index.mdx", "score": 1.0871243476867676}]}
{"ts": 1747929138.0460422, "query_id": "Q008", "question": "Give an example of a memory object that stores chat history.", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.0518193244934082}, {"doc_source": "concepts/chat_history.mdx", "score": 1.0831236839294434}, {"doc_source": "concepts/chat_history.mdx", "score": 1.09505033493042}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1070345640182495}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1656169891357422}]}
{"ts": 1747929139.997021, "query_id": "Q009", "question": "Outline the steps to trace a LangGraph flow in LangSmith.", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8308508396148682}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.838459849357605}, {"doc_source": "how_to/index.mdx", "score": 0.8499534130096436}, {"doc_source": "concepts/tracing.mdx", "score": 0.9168334007263184}, {"doc_source": "how_to/index.mdx", "score": 0.9685291051864624}]}
{"ts": 1747929141.729558, "query_id": "Q010", "question": "What packages must be pinned to use Pydantic v1 with LangChain?", "retrieved_items": [{"doc_source": "how_to/pydantic_compatibility.md", "score": 0.6671671867370605}, {"doc_source": "concepts/async.mdx", "score": 0.9412131309509277}, {"doc_source": "how_to/installation.mdx", "score": 0.944168210029602}, {"doc_source": "how_to/installation.mdx", "score": 0.9478291273117065}, {"doc_source": "how_to/installation.mdx", "score": 0.9517197608947754}]}
{"ts": 1747929143.654733, "query_id": "Q011", "question": "Explain tool calling via OpenAI function messages in LangChain.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.740321934223175}, {"doc_source": "concepts/messages.mdx", "score": 0.7671902179718018}, {"doc_source": "concepts/testing.mdx", "score": 0.8307634592056274}, {"doc_source": "concepts/messages.mdx", "score": 0.8540185689926147}, {"doc_source": "how_to/index.mdx", "score": 0.8562418222427368}]}
{"ts": 1747929146.5025358, "query_id": "Q012", "question": "How does Retrieval-Augmented Generation (RAG) work in LangChain?", "retrieved_items": [{"doc_source": "concepts/rag.mdx", "score": 0.6000490188598633}, {"doc_source": "concepts/rag.mdx", "score": 0.8095791339874268}, {"doc_source": "concepts/rag.mdx", "score": 0.9059208631515503}, {"doc_source": "how_to/index.mdx", "score": 0.9583843946456909}, {"doc_source": "how_to/index.mdx", "score": 0.9601413011550903}]}
{"ts": 1747929148.54981, "query_id": "Q013", "question": "Provide a code snippet to create embeddings with OpenAI via LangChain.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 0.8077185153961182}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8201206922531128}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8585704565048218}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8882757425308228}, {"doc_source": "concepts/chat_models.mdx", "score": 0.925346314907074}]}
{"ts": 1747929150.104703, "query_id": "Q014", "question": "List three callback events emitted during an LLM call.", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 0.8812906742095947}, {"doc_source": "concepts/callbacks.mdx", "score": 0.997167706489563}, {"doc_source": "how_to/index.mdx", "score": 1.0635215044021606}, {"doc_source": "concepts/streaming.mdx", "score": 1.163769245147705}, {"doc_source": "how_to/index.mdx", "score": 1.1762523651123047}]}
{"ts": 1747929151.774582, "query_id": "Q015", "question": "Why would you use the AsyncRunnable interface?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9026402235031128}, {"doc_source": "concepts/async.mdx", "score": 0.9287771582603455}, {"doc_source": "concepts/runnables.mdx", "score": 0.956597089767456}, {"doc_source": "concepts/lcel.mdx", "score": 0.9969758987426758}, {"doc_source": "concepts/runnables.mdx", "score": 1.0043002367019653}]}
{"ts": 1747929154.312989, "query_id": "Q016", "question": "What metrics does LangChain's evaluation module compute out-of-box?", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8653921484947205}, {"doc_source": "concepts/evaluation.mdx", "score": 0.9073658585548401}, {"doc_source": "how_to/index.mdx", "score": 0.9127697944641113}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9283957481384277}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9342676401138306}]}
{"ts": 1747929156.051739, "query_id": "Q017", "question": "Demonstrate how to chunk text using RecursiveCharacterTextSplitter.", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.812049925327301}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.981052041053772}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.000725269317627}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0025289058685303}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0186598300933838}]}
{"ts": 1747929157.978942, "query_id": "Q018", "question": "What environment variable controls OpenAI API retries in LangChain?", "retrieved_items": [{"doc_source": "concepts/architecture.mdx", "score": 1.1688098907470703}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1742488145828247}, {"doc_source": "concepts/async.mdx", "score": 1.1808713674545288}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1897053718566895}, {"doc_source": "concepts/async.mdx", "score": 1.2008657455444336}]}
{"ts": 1747929159.4848378, "query_id": "Q019", "question": "How can key-value stores be used to cache intermediate chain output?", "retrieved_items": [{"doc_source": "concepts/key_value_stores.mdx", "score": 0.9592975378036499}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.0025486946105957}, {"doc_source": "concepts/streaming.mdx", "score": 1.1441144943237305}, {"doc_source": "concepts/lcel.mdx", "score": 1.1675571203231812}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.2631535530090332}]}
{"ts": 1747929162.462912, "query_id": "Q020", "question": "Explain \"Multimodality\" support in LangChain with an example.", "retrieved_items": [{"doc_source": "concepts/multimodality.mdx", "score": 0.6336445808410645}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7260082960128784}, {"doc_source": "concepts/multimodality.mdx", "score": 0.8726955056190491}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9224695563316345}, {"doc_source": "concepts/tokens.mdx", "score": 0.9275288581848145}]}
{"ts": 1747929164.663998, "query_id": "Q021", "question": "What is the LangChain architecture diagram trying to convey?", "retrieved_items": [{"doc_source": "introduction.mdx", "score": 0.8010405898094177}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8222938776016235}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8366606831550598}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.846499502658844}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8662183284759521}]}
{"ts": 1747929166.634589, "query_id": "Q022", "question": "Provide two example selectors and when to use them.", "retrieved_items": [{"doc_source": "concepts/example_selectors.mdx", "score": 0.8731753826141357}, {"doc_source": "how_to/index.mdx", "score": 1.0530680418014526}, {"doc_source": "how_to/index.mdx", "score": 1.1752233505249023}, {"doc_source": "concepts/retrieval.mdx", "score": 1.212766408920288}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2197167873382568}]}
{"ts": 1747929168.9653351, "query_id": "Q023", "question": "How do OutputParsers simplify post-processing in LCEL?", "retrieved_items": [{"doc_source": "concepts/output_parsers.mdx", "score": 0.9454207420349121}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.9983141422271729}, {"doc_source": "concepts/lcel.mdx", "score": 1.0181138515472412}, {"doc_source": "concepts/lcel.mdx", "score": 1.0731346607208252}, {"doc_source": "how_to/index.mdx", "score": 1.0922462940216064}]}
{"ts": 1747929170.913902, "query_id": "Q024", "question": "Describe streaming chunk callbacks versus token callbacks.", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 1.0605249404907227}, {"doc_source": "concepts/streaming.mdx", "score": 1.064790964126587}, {"doc_source": "concepts/streaming.mdx", "score": 1.109907627105713}, {"doc_source": "concepts/streaming.mdx", "score": 1.1239831447601318}, {"doc_source": "concepts/streaming.mdx", "score": 1.133001685142517}]}
{"ts": 1747929172.935578, "query_id": "Q025", "question": "What is the purpose of the `ConversationBufferMemory` class and when should it be used?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.19468355178833}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2197318077087402}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2366690635681152}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2720564603805542}, {"doc_source": "concepts/chat_models.mdx", "score": 1.323288083076477}]}
{"ts": 1747929174.105526, "query_id": "Q026", "question": "Which LangChain text splitter lets you split documents by tokens and why would you choose it over a character splitter?", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.6629819273948669}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.772847592830658}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.8009275197982788}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.8801502585411072}, {"doc_source": "concepts/tokens.mdx", "score": 0.8919897079467773}]}
{"ts": 1747929175.776648, "query_id": "Q027", "question": "List two differences between `VectorStoreRetriever` and `EnsembleRetriever`.", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.7454788684844971}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8020661473274231}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8828622102737427}, {"doc_source": "how_to/index.mdx", "score": 1.0260703563690186}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0697033405303955}]}
{"ts": 1747929177.568906, "query_id": "Q028", "question": "How can you configure a chat model to automatically retry on ratelimiting errors?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5972589254379272}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7229748368263245}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9777226448059082}, {"doc_source": "concepts/chat_models.mdx", "score": 1.045217752456665}, {"doc_source": "concepts/chat_models.mdx", "score": 1.081334114074707}]}
{"ts": 1747929179.0760791, "query_id": "Q029", "question": "Explain how `InjectedToolArg` affects a tool schema and give one use case.", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.6153249740600586}, {"doc_source": "concepts/tools.mdx", "score": 0.6412188410758972}, {"doc_source": "concepts/tools.mdx", "score": 0.9971725344657898}, {"doc_source": "concepts/tools.mdx", "score": 1.0165512561798096}, {"doc_source": "concepts/tools.mdx", "score": 1.0167981386184692}]}
{"ts": 1747929180.992086, "query_id": "Q030", "question": "Which decorator is recommended for quickly turning a Python function into a LangChain Tool?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8534386157989502}, {"doc_source": "concepts/runnables.mdx", "score": 0.9625298976898193}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.969050943851471}, {"doc_source": "concepts/tools.mdx", "score": 1.0073217153549194}, {"doc_source": "concepts/tools.mdx", "score": 1.0272595882415771}]}
{"ts": 1747929182.655743, "query_id": "Q031", "question": "Describe how to batch inputs through a Runnable for better throughput.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6074963808059692}, {"doc_source": "concepts/runnables.mdx", "score": 0.7852352857589722}, {"doc_source": "concepts/runnables.mdx", "score": 0.9014800786972046}, {"doc_source": "concepts/runnables.mdx", "score": 0.9245012998580933}, {"doc_source": "concepts/lcel.mdx", "score": 0.9339675903320312}]}
{"ts": 1747929186.829071, "query_id": "Q032", "question": "What parameter in `RunnableConfig` controls the maximum number of parallel calls during batching?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8538581132888794}, {"doc_source": "concepts/runnables.mdx", "score": 0.8997510671615601}, {"doc_source": "concepts/runnables.mdx", "score": 0.9035041332244873}, {"doc_source": "concepts/runnables.mdx", "score": 0.9461213946342468}, {"doc_source": "concepts/runnables.mdx", "score": 0.9972647428512573}]}
{"ts": 1747929188.476589, "query_id": "Q033", "question": "Give an example of using `with_structured_output` to get JSON back from a chat model.", "retrieved_items": [{"doc_source": "concepts/structured_outputs.mdx", "score": 0.7788731455802917}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8990028500556946}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.9393729567527771}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9586504697799683}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9779047966003418}]}
{"ts": 1747929190.8893561, "query_id": "Q034", "question": "What callback would you use to stream intermediate events from a Runnable?", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 0.8936761021614075}, {"doc_source": "concepts/streaming.mdx", "score": 0.9890403747558594}, {"doc_source": "concepts/streaming.mdx", "score": 1.008561611175537}, {"doc_source": "concepts/streaming.mdx", "score": 1.011386513710022}, {"doc_source": "concepts/runnables.mdx", "score": 1.0387187004089355}]}
{"ts": 1747929192.671551, "query_id": "Q035", "question": "Which two standard parameters influence generation randomness and length in chat models?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.9568188786506653}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0022070407867432}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0072953701019287}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0455849170684814}, {"doc_source": "concepts/chat_models.mdx", "score": 1.065344214439392}]}
{"ts": 1747929194.234447, "query_id": "Q036", "question": "How do you persist a FAISS vector store to disk?", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 1.1065917015075684}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.1650935411453247}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1933917999267578}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.197279691696167}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.2159086465835571}]}
{"ts": 1747929195.137337, "query_id": "Q037", "question": "Explain the difference between `stream` and `astream` methods on a Runnable.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.7147121429443359}, {"doc_source": "concepts/streaming.mdx", "score": 0.7505459189414978}, {"doc_source": "concepts/streaming.mdx", "score": 0.7569414377212524}, {"doc_source": "concepts/runnables.mdx", "score": 0.7936859726905823}, {"doc_source": "concepts/streaming.mdx", "score": 0.8253673911094666}]}
{"ts": 1747929196.80612, "query_id": "Q038", "question": "What feature does LangChain provide to ensure prompts fit within an LLM's context window?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.8555485606193542}, {"doc_source": "how_to/index.mdx", "score": 0.9136518836021423}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9304401874542236}, {"doc_source": "how_to/index.mdx", "score": 0.9439922571182251}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9497433304786682}]}
{"ts": 1747929198.722068, "query_id": "Q039", "question": "Name three message roles supported by LangChain chat messages.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 0.7466216087341309}, {"doc_source": "concepts/messages.mdx", "score": 0.7780277729034424}, {"doc_source": "concepts/messages.mdx", "score": 0.887708306312561}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9011927843093872}, {"doc_source": "concepts/messages.mdx", "score": 0.907137930393219}]}
{"ts": 1747929200.410552, "query_id": "Q040", "question": "How can you combine docs returned by a Retriever before passing to an LLM?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.8409827947616577}, {"doc_source": "concepts/retrievers.mdx", "score": 0.847752034664154}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9799346923828125}, {"doc_source": "concepts/retrievers.mdx", "score": 1.003430724143982}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0319386720657349}]}
{"ts": 1747929201.894363, "query_id": "Q041", "question": "What does the `rate_limiter` parameter accept and why is it useful?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.9187660813331604}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9407780766487122}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0713908672332764}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1763097047805786}, {"doc_source": "concepts/runnables.mdx", "score": 1.2190871238708496}]}
{"ts": 1747929204.023978, "query_id": "Q042", "question": "Provide a code snippet to stream tokens from an OpenAI chat model using LCEL.", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.856160044670105}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9568954706192017}, {"doc_source": "concepts/streaming.mdx", "score": 0.9772512912750244}, {"doc_source": "concepts/streaming.mdx", "score": 0.9780283570289612}, {"doc_source": "concepts/streaming.mdx", "score": 0.993208646774292}]}
{"ts": 1747929207.660554, "query_id": "Q043", "question": "Which notebook demonstrates how to track token usage costs per request?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 1.275901436805725}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2869020700454712}, {"doc_source": "how_to/index.mdx", "score": 1.303704857826233}, {"doc_source": "concepts/tokens.mdx", "score": 1.3319091796875}, {"doc_source": "concepts/index.mdx", "score": 1.332702875137329}]}
{"ts": 1747929208.5271301, "query_id": "Q044", "question": "What are tool artifacts and how can they be returned from a tool?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.7272754907608032}, {"doc_source": "how_to/index.mdx", "score": 0.8022353053092957}, {"doc_source": "concepts/tools.mdx", "score": 0.854524552822113}, {"doc_source": "concepts/tools.mdx", "score": 0.9145180583000183}, {"doc_source": "concepts/tools.mdx", "score": 1.0443354845046997}]}
{"ts": 1747929210.233137, "query_id": "Q045", "question": "Explain how to asynchronously invoke a Runnable and await the result.", "retrieved_items": [{"doc_source": "concepts/async.mdx", "score": 0.8705847859382629}, {"doc_source": "concepts/runnables.mdx", "score": 0.9418070316314697}, {"doc_source": "concepts/async.mdx", "score": 1.0067267417907715}, {"doc_source": "concepts/runnables.mdx", "score": 1.0403969287872314}, {"doc_source": "concepts/runnables.mdx", "score": 1.0636825561523438}]}
{"ts": 1747929211.8818932, "query_id": "Q046", "question": "Which two documents should you read to understand LangGraph node construction?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.8435163497924805}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9646902084350586}, {"doc_source": "tutorials/index.mdx", "score": 0.9924482107162476}, {"doc_source": "how_to/index.mdx", "score": 1.0148169994354248}, {"doc_source": "concepts/streaming.mdx", "score": 1.0292083024978638}]}
{"ts": 1747929213.3594398, "query_id": "Q047", "question": "Describe the process of converting a Runnable into a Tool for agent use.", "retrieved_items": [{"doc_source": "concepts/tool_calling.mdx", "score": 0.8755333423614502}, {"doc_source": "how_to/index.mdx", "score": 0.9107277393341064}, {"doc_source": "concepts/runnables.mdx", "score": 1.0066193342208862}, {"doc_source": "concepts/runnables.mdx", "score": 1.0178004503250122}, {"doc_source": "concepts/runnables.mdx", "score": 1.0320122241973877}]}
{"ts": 1747929215.38001, "query_id": "Q048", "question": "What Python package contains community-contributed chat model integrations?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7933948040008545}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9213707447052002}, {"doc_source": "concepts/architecture.mdx", "score": 0.9721150398254395}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9874876141548157}, {"doc_source": "concepts/chat_models.mdx", "score": 1.016969919204712}]}
{"ts": 1747929216.868444, "query_id": "Q049", "question": "How can semantic caching improve LLM response times and what extra model does it rely on?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.744552493095398}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9082278609275818}, {"doc_source": "how_to/index.mdx", "score": 1.0322632789611816}, {"doc_source": "concepts/retrieval.mdx", "score": 1.097513198852539}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1060569286346436}]}
{"ts": 1747929218.407145, "query_id": "Q050", "question": "Which document loader would you use for processing JSON lines files?", "retrieved_items": [{"doc_source": "how_to/document_loader_json.mdx", "score": 0.6960840225219727}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.832432746887207}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.8498632907867432}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.9374403357505798}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.9548184871673584}]}
{"ts": 1747929220.186021, "query_id": "Q051", "question": "What is an `OutputParser` and why is it useful in LCEL chains?", "retrieved_items": [{"doc_source": "concepts/output_parsers.mdx", "score": 0.666566014289856}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.8718828558921814}, {"doc_source": "how_to/index.mdx", "score": 0.8773317337036133}, {"doc_source": "concepts/lcel.mdx", "score": 0.9573220014572144}, {"doc_source": "how_to/index.mdx", "score": 0.973764181137085}]}
{"ts": 1747929221.968441, "query_id": "Q052", "question": "Give an example selector that prioritizes shorter examples and describe its use case.", "retrieved_items": [{"doc_source": "concepts/example_selectors.mdx", "score": 0.7914028763771057}, {"doc_source": "how_to/index.mdx", "score": 0.8972762823104858}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0428736209869385}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0674412250518799}, {"doc_source": "how_to/index.mdx", "score": 1.0705254077911377}]}
{"ts": 1747929223.946869, "query_id": "Q053", "question": "Which environment variable can override OpenAI API base URL in LangChain?", "retrieved_items": [{"doc_source": "how_to/installation.mdx", "score": 1.117376446723938}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1246514320373535}, {"doc_source": "concepts/messages.mdx", "score": 1.146759271621704}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1508431434631348}, {"doc_source": "how_to/installation.mdx", "score": 1.1659457683563232}]}
{"ts": 1747929226.52366, "query_id": "Q054", "question": "Describe how to attach custom metadata to a Runnable run for later inspection in LangSmith.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8650420308113098}, {"doc_source": "concepts/runnables.mdx", "score": 1.0635780096054077}, {"doc_source": "concepts/runnables.mdx", "score": 1.0838408470153809}, {"doc_source": "concepts/runnables.mdx", "score": 1.0894790887832642}, {"doc_source": "concepts/runnables.mdx", "score": 1.0987040996551514}]}
{"ts": 1747929228.3960419, "query_id": "Q055", "question": "What does the `configurable_fields` helper enable for advanced chains?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9237531423568726}, {"doc_source": "concepts/runnables.mdx", "score": 1.1058437824249268}, {"doc_source": "concepts/runnables.mdx", "score": 1.1298916339874268}, {"doc_source": "concepts/runnables.mdx", "score": 1.2374111413955688}, {"doc_source": "concepts/lcel.mdx", "score": 1.2802116870880127}]}
{"ts": 1747929230.625088, "query_id": "Q056", "question": "Name two ChatModel parameters that help avoid exceeding rate limits.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5764163136482239}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7867007255554199}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8256187438964844}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9938455820083618}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9982819557189941}]}
{"ts": 1747929232.117014, "query_id": "Q057", "question": "How do you specify a custom UUID for a Runnable run?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9112035036087036}, {"doc_source": "concepts/runnables.mdx", "score": 1.0182586908340454}, {"doc_source": "concepts/runnables.mdx", "score": 1.0534567832946777}, {"doc_source": "concepts/runnables.mdx", "score": 1.0823373794555664}, {"doc_source": "concepts/runnables.mdx", "score": 1.1118052005767822}]}
{"ts": 1747929234.2651389, "query_id": "Q058", "question": "Compare `batch` and `batch_as_completed` when processing multiple inputs.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8131264448165894}, {"doc_source": "concepts/runnables.mdx", "score": 0.8468988537788391}, {"doc_source": "concepts/runnables.mdx", "score": 1.050938606262207}, {"doc_source": "concepts/runnables.mdx", "score": 1.110231876373291}, {"doc_source": "concepts/lcel.mdx", "score": 1.272564172744751}]}
{"ts": 1747929236.010709, "query_id": "Q059", "question": "How can LangChain assist with evaluating generated answers automatically?", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8659547567367554}, {"doc_source": "concepts/evaluation.mdx", "score": 0.9115371108055115}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9272716045379639}, {"doc_source": "how_to/index.mdx", "score": 0.9312191009521484}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9343278408050537}]}
{"ts": 1747929237.9015622, "query_id": "Q060", "question": "Provide two metrics supported by LangChain's built in evaluation module.", "retrieved_items": [{"doc_source": "concepts/evaluation.mdx", "score": 0.8831498026847839}, {"doc_source": "how_to/index.mdx", "score": 0.8951917290687561}, {"doc_source": "tutorials/index.mdx", "score": 0.9035339951515198}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9282285571098328}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9698907136917114}]}
{"ts": 1747929239.1746402, "query_id": "Q061", "question": "Which LangChain concept allows combining multiple Runnables with the `|` operator?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.662176251411438}, {"doc_source": "concepts/runnables.mdx", "score": 0.7448540925979614}, {"doc_source": "concepts/lcel.mdx", "score": 0.7530632019042969}, {"doc_source": "concepts/lcel.mdx", "score": 0.7633335590362549}, {"doc_source": "how_to/index.mdx", "score": 0.765129029750824}]}
{"ts": 1747929240.880244, "query_id": "Q062", "question": "Explain how to set up tool streaming so that tool results are incrementally passed back to the model.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.9531417489051819}, {"doc_source": "concepts/streaming.mdx", "score": 0.9633700847625732}, {"doc_source": "concepts/streaming.mdx", "score": 1.0008809566497803}, {"doc_source": "concepts/streaming.mdx", "score": 1.0244812965393066}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.049849510192871}]}
{"ts": 1747929243.9330509, "query_id": "Q063", "question": "What is the benefit of using `RunnableLambda` over subclassing a Runnable?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8221936225891113}, {"doc_source": "concepts/runnables.mdx", "score": 0.9719396233558655}, {"doc_source": "concepts/runnables.mdx", "score": 0.9908816814422607}, {"doc_source": "concepts/lcel.mdx", "score": 0.9929307699203491}, {"doc_source": "concepts/runnables.mdx", "score": 1.0357048511505127}]}
{"ts": 1747929245.6068668, "query_id": "Q064", "question": "Which two documents together explain how to build a streaming question answering chain with citations?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9879461526870728}, {"doc_source": "how_to/index.mdx", "score": 1.0481534004211426}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0582170486450195}, {"doc_source": "tutorials/index.mdx", "score": 1.0652120113372803}, {"doc_source": "how_to/index.mdx", "score": 1.0739481449127197}]}
{"ts": 1747929247.303746, "query_id": "Q065", "question": "How do you measure BLEU score in LangChain evaluations?", "retrieved_items": [{"doc_source": "concepts/embedding_models.mdx", "score": 0.9988754987716675}, {"doc_source": "tutorials/index.mdx", "score": 1.0152192115783691}, {"doc_source": "concepts/evaluation.mdx", "score": 1.0848697423934937}, {"doc_source": "how_to/index.mdx", "score": 1.0965179204940796}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.130312442779541}]}
{"ts": 1747929248.738645, "query_id": "Q066", "question": "What does the `ConversationBufferWindowMemory` keep in memory?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.1609647274017334}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1824369430541992}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2314629554748535}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2466609477996826}, {"doc_source": "concepts/chat_models.mdx", "score": 1.274240255355835}]}
{"ts": 1747929250.311868, "query_id": "Q067", "question": "Name a loader designed specifically for PDF files in LangChain.", "retrieved_items": [{"doc_source": "concepts/document_loaders.mdx", "score": 0.9647369980812073}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.0091040134429932}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.058826208114624}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.0941507816314697}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0969877243041992}]}
{"ts": 1747929251.805017, "query_id": "Q068", "question": "Which concept guide covers the standard streaming APIs exposed by Runnables?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.773382842540741}, {"doc_source": "concepts/runnables.mdx", "score": 0.844061017036438}, {"doc_source": "concepts/runnables.mdx", "score": 0.8500353693962097}, {"doc_source": "concepts/runnables.mdx", "score": 0.8660604357719421}, {"doc_source": "concepts/streaming.mdx", "score": 0.8675769567489624}]}
{"ts": 1747929253.8216372, "query_id": "Q069", "question": "What is the advantage of using `LangServe` with Runnables?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.821736752986908}, {"doc_source": "concepts/lcel.mdx", "score": 0.8782718181610107}, {"doc_source": "concepts/runnables.mdx", "score": 0.8922423124313354}, {"doc_source": "concepts/runnables.mdx", "score": 0.9144359827041626}, {"doc_source": "concepts/runnables.mdx", "score": 0.916905403137207}]}
{"ts": 1747929257.274509, "query_id": "Q070", "question": "Explain how ChatModels support multimodal inputs.", "retrieved_items": [{"doc_source": "concepts/multimodality.mdx", "score": 0.591445803642273}, {"doc_source": "concepts/multimodality.mdx", "score": 0.6075503826141357}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7238159775733948}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7808233499526978}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7816779613494873}]}
{"ts": 1747929258.988298, "query_id": "Q071", "question": "What two steps are recommended to recover from ratelimiting errors?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.8244627714157104}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9087772369384766}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1994655132293701}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2089636325836182}, {"doc_source": "concepts/retrieval.mdx", "score": 1.4205150604248047}]}
{"ts": 1747929260.528024, "query_id": "Q072", "question": "Which parameter allows a Runnable to stream events as JSON for UIs?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 1.053467869758606}, {"doc_source": "concepts/streaming.mdx", "score": 1.0660595893859863}, {"doc_source": "concepts/streaming.mdx", "score": 1.080946922302246}, {"doc_source": "concepts/streaming.mdx", "score": 1.0862126350402832}, {"doc_source": "concepts/runnables.mdx", "score": 1.0899157524108887}]}
{"ts": 1747929261.969293, "query_id": "Q073", "question": "Provide a code snippet to embed text with a local HuggingFace model.", "retrieved_items": [{"doc_source": "concepts/embedding_models.mdx", "score": 1.0072360038757324}, {"doc_source": "how_to/embed_text.mdx", "score": 1.0270473957061768}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0613993406295776}, {"doc_source": "how_to/embed_text.mdx", "score": 1.0636122226715088}, {"doc_source": "concepts/multimodality.mdx", "score": 1.071678638458252}]}
{"ts": 1747929264.485128, "query_id": "Q074", "question": "How does the `ContextualCompressionRetriever` reduce context windows?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 1.1378884315490723}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2814509868621826}, {"doc_source": "how_to/index.mdx", "score": 1.3003920316696167}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3162251710891724}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3543472290039062}]}
{"ts": 1747929266.5273652, "query_id": "Q075", "question": "What guide should you consult for configuring runtime chain internals like temperature?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 1.124155879020691}, {"doc_source": "concepts/runnables.mdx", "score": 1.1298439502716064}, {"doc_source": "how_to/index.mdx", "score": 1.1893037557601929}, {"doc_source": "tutorials/index.mdx", "score": 1.2221022844314575}, {"doc_source": "concepts/runnables.mdx", "score": 1.2521830797195435}]}
{"ts": 1747929268.467593, "query_id": "Q076", "question": "Which selector uses Maximal Marginal Relevance (MMR) to diversify examples?", "retrieved_items": [{"doc_source": "concepts/vectorstores.mdx", "score": 0.7091590762138367}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8868439197540283}, {"doc_source": "how_to/index.mdx", "score": 0.9365489482879639}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.0199460983276367}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.162663459777832}]}
{"ts": 1747929270.330086, "query_id": "Q077", "question": "What is the recommended way to expose tool schemas to chat models that use OpenAI function calling?", "retrieved_items": [{"doc_source": "concepts/structured_outputs.mdx", "score": 0.683066725730896}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.697941243648529}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7245759963989258}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8078393340110779}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8479928374290466}]}
{"ts": 1747929273.833506, "query_id": "Q078", "question": "Explain the advantage of using `batch_as_completed` for long running tasks.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8250684142112732}, {"doc_source": "concepts/runnables.mdx", "score": 0.9202640652656555}, {"doc_source": "concepts/runnables.mdx", "score": 1.0485730171203613}, {"doc_source": "concepts/runnables.mdx", "score": 1.138277292251587}, {"doc_source": "concepts/lcel.mdx", "score": 1.1977018117904663}]}
{"ts": 1747929276.3617191, "query_id": "Q079", "question": "Which two files outline strategies to trim messages to fit the context window?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 0.9143266677856445}, {"doc_source": "concepts/chat_history.mdx", "score": 1.0845847129821777}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1431303024291992}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1763439178466797}, {"doc_source": "how_to/index.mdx", "score": 1.194125771522522}]}
{"ts": 1747929277.8314612, "query_id": "Q080", "question": "What does the `max_concurrency` field not control when using a ChatModel?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 1.1156728267669678}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1200978755950928}, {"doc_source": "concepts/runnables.mdx", "score": 1.1442759037017822}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1625773906707764}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1747947931289673}]}
{"ts": 1747929279.419345, "query_id": "Q081", "question": "Which document explains how to call tools in parallel with OpenAI?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9734199047088623}, {"doc_source": "how_to/index.mdx", "score": 1.029984712600708}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.036679744720459}, {"doc_source": "how_to/index.mdx", "score": 1.0416762828826904}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.046389102935791}]}
{"ts": 1747929281.083821, "query_id": "Q082", "question": "Describe how `RunnableGenerator` differs from `RunnableLambda`.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6841990351676941}, {"doc_source": "concepts/runnables.mdx", "score": 0.9617829322814941}, {"doc_source": "concepts/lcel.mdx", "score": 0.9835832118988037}, {"doc_source": "concepts/runnables.mdx", "score": 1.0025906562805176}, {"doc_source": "concepts/runnables.mdx", "score": 1.034561276435852}]}
{"ts": 1747929283.342516, "query_id": "Q083", "question": "Which vector store integration supports time weighted relevance?", "retrieved_items": [{"doc_source": "concepts/vectorstores.mdx", "score": 0.8835893869400024}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9358210563659668}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9397540092468262}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.9587025046348572}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9685268402099609}]}
{"ts": 1747929284.485146, "query_id": "Q084", "question": "Explain how to use `run_name` for easier debugging in LangSmith.", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.9917372465133667}, {"doc_source": "concepts/runnables.mdx", "score": 1.0121091604232788}, {"doc_source": "concepts/tracing.mdx", "score": 1.0626484155654907}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0842201709747314}, {"doc_source": "how_to/index.mdx", "score": 1.1226170063018799}]}
{"ts": 1747929286.498973, "query_id": "Q085", "question": "Which two parameters must be provided to initialize `AzureAIDocumentIntelligenceLoader`?", "retrieved_items": [{"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.7358885407447815}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.7364019155502319}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.9850661158561707}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2547235488891602}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.2597793340682983}]}
{"ts": 1747929289.4352798, "query_id": "Q086", "question": "What is the main benefit of `asam_stream` events over `astream`?", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.9555391073226929}, {"doc_source": "concepts/streaming.mdx", "score": 1.0629931688308716}, {"doc_source": "concepts/streaming.mdx", "score": 1.0771819353103638}, {"doc_source": "concepts/streaming.mdx", "score": 1.1204309463500977}, {"doc_source": "concepts/streaming.mdx", "score": 1.1569164991378784}]}
{"ts": 1747929292.3360372, "query_id": "Q087", "question": "How do you specify hidden arguments in a Tool so the model cannot set them?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.985096275806427}, {"doc_source": "concepts/tools.mdx", "score": 0.9980179071426392}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.202382206916809}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2199090719223022}, {"doc_source": "how_to/index.mdx", "score": 1.2291779518127441}]}
{"ts": 1747929294.4044092, "query_id": "Q088", "question": "Which guide demonstrates building a SQL retriever over a large database?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.9645313024520874}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0184630155563354}, {"doc_source": "how_to/index.mdx", "score": 1.0291208028793335}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0456767082214355}, {"doc_source": "how_to/index.mdx", "score": 1.0644599199295044}]}
{"ts": 1747929296.316565, "query_id": "Q089", "question": "Why is semantic caching unlikely to hit after the first chat turn?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7096902132034302}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8752152919769287}, {"doc_source": "concepts/chat_models.mdx", "score": 1.245618462562561}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2828788757324219}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2855594158172607}]}
{"ts": 1747929298.41385, "query_id": "Q090", "question": "Describe a use case for `RecursiveCharacterTextSplitter`.", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.8241062164306641}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9780513644218445}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.995972752571106}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0288385152816772}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0912806987762451}]}
{"ts": 1747929300.4201448, "query_id": "Q091", "question": "Give two methods to stream output from a ChatModel.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.8135871887207031}, {"doc_source": "concepts/streaming.mdx", "score": 0.8409720659255981}, {"doc_source": "concepts/streaming.mdx", "score": 0.8448833227157593}, {"doc_source": "concepts/streaming.mdx", "score": 0.8515967726707458}, {"doc_source": "concepts/streaming.mdx", "score": 0.8556699752807617}]}
{"ts": 1747929302.1385422, "query_id": "Q092", "question": "Which document teaches you to build a hybrid search retriever combining keyword and vector?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.7178097367286682}, {"doc_source": "concepts/retrieval.mdx", "score": 0.7419501543045044}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.7707005739212036}, {"doc_source": "concepts/retrievers.mdx", "score": 0.7870883941650391}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8332999348640442}]}
{"ts": 1747929303.553479, "query_id": "Q093", "question": "What is the purpose of `annotated` type hints in tool schemas?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8371490836143494}, {"doc_source": "concepts/tools.mdx", "score": 0.9160736799240112}, {"doc_source": "concepts/tools.mdx", "score": 1.017225980758667}, {"doc_source": "concepts/tools.mdx", "score": 1.0715221166610718}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.1163747310638428}]}
{"ts": 1747929305.25314, "query_id": "Q094", "question": "Explain why caching chat model responses is challenging.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5507737398147583}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7897071242332458}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0343356132507324}, {"doc_source": "concepts/chat_models.mdx", "score": 1.089930772781372}, {"doc_source": "concepts/streaming.mdx", "score": 1.0905020236968994}]}
{"ts": 1747929309.10271, "query_id": "Q095", "question": "Which two docs together explain trimming long documents before embedding?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.0130244493484497}, {"doc_source": "how_to/index.mdx", "score": 1.0349889993667603}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0579864978790283}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0663399696350098}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0733367204666138}]}
{"ts": 1747929310.810234, "query_id": "Q096", "question": "What class enables you to get structured JSON output directly from LLMS without a parser?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 1.2029314041137695}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2095158100128174}, {"doc_source": "how_to/index.mdx", "score": 1.2106225490570068}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2266240119934082}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2288415431976318}]}
{"ts": 1747929312.4053729, "query_id": "Q097", "question": "Name three built in message field keys aside from `content`.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 1.0433361530303955}, {"doc_source": "concepts/messages.mdx", "score": 1.0436062812805176}, {"doc_source": "concepts/messages.mdx", "score": 1.151723861694336}, {"doc_source": "concepts/messages.mdx", "score": 1.1790271997451782}, {"doc_source": "concepts/messages.mdx", "score": 1.2028889656066895}]}
{"ts": 1747929313.69287, "query_id": "Q098", "question": "What is the first step when creating a custom Runnable for streaming transformations?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.826852560043335}, {"doc_source": "concepts/runnables.mdx", "score": 0.8749679923057556}, {"doc_source": "concepts/runnables.mdx", "score": 0.9593954086303711}, {"doc_source": "concepts/streaming.mdx", "score": 0.9715161323547363}, {"doc_source": "concepts/runnables.mdx", "score": 1.011547565460205}]}
{"ts": 1747929315.723382, "query_id": "Q099", "question": "How does LangChain differentiate official vs community chat model integrations?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5860792994499207}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.788343071937561}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7954086065292358}, {"doc_source": "how_to/index.mdx", "score": 0.8096698522567749}, {"doc_source": "concepts/chat_models.mdx", "score": 0.844828188419342}]}
{"ts": 1747929317.8595421, "query_id": "Q100", "question": "Which two sources explain creating example selectors based on semantic similarity?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9878307580947876}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.0077632665634155}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.091620683670044}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.113412618637085}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1519348621368408}]}
{"ts": 1747929319.3821828, "query_id": "Q101", "question": "What is the impact of not propagating `RunnableConfig` in Python 3.10 async code?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6224480867385864}, {"doc_source": "concepts/callbacks.mdx", "score": 0.9561946392059326}, {"doc_source": "concepts/runnables.mdx", "score": 0.9741816520690918}, {"doc_source": "concepts/runnables.mdx", "score": 0.9807288646697998}, {"doc_source": "concepts/runnables.mdx", "score": 0.9939502477645874}]}
{"ts": 1747929321.614419, "query_id": "Q102", "question": "Describe the `@tool` option to hide a parameter while still injecting it at runtime.", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8318654894828796}, {"doc_source": "concepts/tools.mdx", "score": 0.8834856152534485}, {"doc_source": "concepts/tools.mdx", "score": 1.0319503545761108}, {"doc_source": "concepts/tools.mdx", "score": 1.0825581550598145}, {"doc_source": "concepts/tools.mdx", "score": 1.0929360389709473}]}
{"ts": 1747929323.589374, "query_id": "Q103", "question": "Which notebook illustrates using an ensemble retriever strategy?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.8431100845336914}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9552979469299316}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0340551137924194}, {"doc_source": "how_to/index.mdx", "score": 1.051804542541504}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1225907802581787}]}
{"ts": 1747929325.5082428, "query_id": "Q104", "question": "Give two benefits of the LangChain Expression Language (LCEL).", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.6549415588378906}, {"doc_source": "concepts/lcel.mdx", "score": 0.7985216379165649}, {"doc_source": "concepts/lcel.mdx", "score": 0.7985752820968628}, {"doc_source": "concepts/lcel.mdx", "score": 0.8413019776344299}, {"doc_source": "concepts/lcel.mdx", "score": 0.8677247762680054}]}
{"ts": 1747929327.19661, "query_id": "Q105", "question": "What is the main purpose of the `rate_limiter` argument on ChatModels?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7172451615333557}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9859250783920288}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9862062335014343}, {"doc_source": "concepts/chat_models.mdx", "score": 1.006317138671875}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1295033693313599}]}
{"ts": 1747929329.430919, "query_id": "Q106", "question": "How many tokens is 100 English words approximately equal to?", "retrieved_items": [{"doc_source": "concepts/tokens.mdx", "score": 0.7300253510475159}, {"doc_source": "concepts/tokens.mdx", "score": 1.0408051013946533}, {"doc_source": "concepts/tokens.mdx", "score": 1.0993263721466064}, {"doc_source": "concepts/tokens.mdx", "score": 1.1049892902374268}, {"doc_source": "concepts/tokens.mdx", "score": 1.1935017108917236}]}
{"ts": 1747929331.053646, "query_id": "Q107", "question": "Explain the difference between `invoke` and `ainvoke`.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.934457540512085}, {"doc_source": "concepts/async.mdx", "score": 1.1377217769622803}, {"doc_source": "concepts/async.mdx", "score": 1.2116649150848389}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2120275497436523}, {"doc_source": "concepts/streaming.mdx", "score": 1.2388408184051514}]}
{"ts": 1747929333.0513961, "query_id": "Q108", "question": "What does `batch_as_completed` return with each result to help match inputs?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9435193538665771}, {"doc_source": "concepts/runnables.mdx", "score": 0.9990241527557373}, {"doc_source": "concepts/runnables.mdx", "score": 1.1418817043304443}, {"doc_source": "concepts/runnables.mdx", "score": 1.21968674659729}, {"doc_source": "concepts/runnables.mdx", "score": 1.3332716226577759}]}
