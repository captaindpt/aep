{"ts": 1747974856.538489, "query_id": "Q000", "question": "What is LangChain and why was it created?", "retrieved_items": [{"doc_source": "concepts/why_langchain.mdx", "score": 0.625643253326416}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7753477096557617}, {"doc_source": "introduction.mdx", "score": 0.8179041743278503}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8625105619430542}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.87779301404953}, {"doc_source": "how_to/index.mdx", "score": 0.8905119895935059}, {"doc_source": "concepts/architecture.mdx", "score": 0.8992112874984741}, {"doc_source": "concepts/lcel.mdx", "score": 0.9276219606399536}, {"doc_source": "introduction.mdx", "score": 0.9306679964065552}, {"doc_source": "concepts/lcel.mdx", "score": 0.9347690343856812}, {"doc_source": "concepts/architecture.mdx", "score": 0.9380263090133667}, {"doc_source": "tutorials/index.mdx", "score": 0.9413373470306396}, {"doc_source": "concepts/async.mdx", "score": 0.9587326049804688}, {"doc_source": "introduction.mdx", "score": 0.964128315448761}, {"doc_source": "how_to/installation.mdx", "score": 0.9663698673248291}]}
{"ts": 1747974858.10399, "query_id": "Q001", "question": "Explain the role of an Agent in LangChain.", "retrieved_items": [{"doc_source": "concepts/agents.mdx", "score": 0.7621573805809021}, {"doc_source": "concepts/agents.mdx", "score": 0.8566672801971436}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.944981575012207}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0098613500595093}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0101948976516724}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.019925832748413}, {"doc_source": "how_to/index.mdx", "score": 1.0214070081710815}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0418291091918945}, {"doc_source": "introduction.mdx", "score": 1.0460832118988037}, {"doc_source": "concepts/architecture.mdx", "score": 1.086291790008545}, {"doc_source": "concepts/async.mdx", "score": 1.0927711725234985}, {"doc_source": "concepts/index.mdx", "score": 1.097562313079834}, {"doc_source": "concepts/lcel.mdx", "score": 1.1027915477752686}, {"doc_source": "introduction.mdx", "score": 1.1086357831954956}, {"doc_source": "concepts/lcel.mdx", "score": 1.1192439794540405}]}
{"ts": 1747974860.206667, "query_id": "Q002", "question": "How do I install LangChain with extras for tracing?", "retrieved_items": [{"doc_source": "how_to/installation.mdx", "score": 0.8146346211433411}, {"doc_source": "concepts/async.mdx", "score": 0.854292094707489}, {"doc_source": "how_to/installation.mdx", "score": 0.8707662224769592}, {"doc_source": "how_to/installation.mdx", "score": 0.881988525390625}, {"doc_source": "how_to/installation.mdx", "score": 0.9093832969665527}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9152989387512207}, {"doc_source": "how_to/installation.mdx", "score": 0.9299374222755432}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9447572231292725}, {"doc_source": "tutorials/index.mdx", "score": 0.9480108022689819}, {"doc_source": "tutorials/index.mdx", "score": 0.9626230001449585}, {"doc_source": "how_to/index.mdx", "score": 0.9628430604934692}, {"doc_source": "how_to/index.mdx", "score": 0.9877479672431946}, {"doc_source": "concepts/tracing.mdx", "score": 0.9976584911346436}, {"doc_source": "concepts/architecture.mdx", "score": 0.9982905387878418}, {"doc_source": "introduction.mdx", "score": 1.0017629861831665}]}
{"ts": 1747974861.480812, "query_id": "Q003", "question": "Show the minimal LCEL chain that streams token-by-token.", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.5404735207557678}, {"doc_source": "concepts/streaming.mdx", "score": 0.7818113565444946}, {"doc_source": "concepts/lcel.mdx", "score": 0.9595978856086731}, {"doc_source": "how_to/index.mdx", "score": 0.981630802154541}, {"doc_source": "concepts/lcel.mdx", "score": 0.9824298620223999}, {"doc_source": "concepts/lcel.mdx", "score": 0.9857434034347534}, {"doc_source": "concepts/lcel.mdx", "score": 1.011744499206543}, {"doc_source": "concepts/lcel.mdx", "score": 1.0125576257705688}, {"doc_source": "how_to/index.mdx", "score": 1.0156954526901245}, {"doc_source": "concepts/streaming.mdx", "score": 1.0306841135025024}, {"doc_source": "concepts/lcel.mdx", "score": 1.0463629961013794}, {"doc_source": "concepts/streaming.mdx", "score": 1.0568232536315918}, {"doc_source": "concepts/streaming.mdx", "score": 1.0825427770614624}, {"doc_source": "concepts/lcel.mdx", "score": 1.132508397102356}, {"doc_source": "concepts/streaming.mdx", "score": 1.1389080286026}]}
{"ts": 1747974864.2902138, "query_id": "Q004", "question": "Compare FAISS and Chroma vector stores in LangChain.", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 0.8060839176177979}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0375652313232422}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.052844524383545}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.054166316986084}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0762150287628174}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0955020189285278}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1124827861785889}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1494396924972534}, {"doc_source": "concepts/multimodality.mdx", "score": 1.1515767574310303}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1746214628219604}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1773450374603271}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.197355031967163}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.241682767868042}, {"doc_source": "concepts/lcel.mdx", "score": 1.2421202659606934}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.2600233554840088}]}
{"ts": 1747974865.966701, "query_id": "Q005", "question": "How can I load a Microsoft Word document into a LangChain pipeline?", "retrieved_items": [{"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.7631633877754211}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.8706797361373901}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.0561035871505737}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0572795867919922}, {"doc_source": "concepts/index.mdx", "score": 1.0583124160766602}, {"doc_source": "introduction.mdx", "score": 1.0638165473937988}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0753902196884155}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.0757791996002197}, {"doc_source": "tutorials/index.mdx", "score": 1.0889952182769775}, {"doc_source": "how_to/index.mdx", "score": 1.0924112796783447}, {"doc_source": "tutorials/index.mdx", "score": 1.1295245885849}, {"doc_source": "tutorials/index.mdx", "score": 1.138889193534851}, {"doc_source": "concepts/lcel.mdx", "score": 1.143539309501648}, {"doc_source": "how_to/index.mdx", "score": 1.1449975967407227}, {"doc_source": "how_to/index.mdx", "score": 1.1471879482269287}]}
{"ts": 1747974867.565262, "query_id": "Q006", "question": "Describe the Runnable protocol and its advantages.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.7724380493164062}, {"doc_source": "concepts/runnables.mdx", "score": 0.8454801440238953}, {"doc_source": "concepts/runnables.mdx", "score": 0.8833574056625366}, {"doc_source": "concepts/runnables.mdx", "score": 0.9488957524299622}, {"doc_source": "concepts/runnables.mdx", "score": 0.9549585580825806}, {"doc_source": "concepts/lcel.mdx", "score": 0.9706860780715942}, {"doc_source": "concepts/runnables.mdx", "score": 1.0510280132293701}, {"doc_source": "concepts/lcel.mdx", "score": 1.0747597217559814}, {"doc_source": "concepts/runnables.mdx", "score": 1.076511263847351}, {"doc_source": "concepts/runnables.mdx", "score": 1.0801517963409424}, {"doc_source": "concepts/runnables.mdx", "score": 1.1130659580230713}, {"doc_source": "concepts/lcel.mdx", "score": 1.127894401550293}, {"doc_source": "concepts/lcel.mdx", "score": 1.1316051483154297}, {"doc_source": "concepts/runnables.mdx", "score": 1.135709285736084}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1439424753189087}]}
{"ts": 1747974868.8544052, "query_id": "Q007", "question": "What is Few-Shot prompting and how is it supported?", "retrieved_items": [{"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.6539869904518127}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.8982607126235962}, {"doc_source": "how_to/index.mdx", "score": 1.0801829099655151}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.08315908908844}, {"doc_source": "concepts/index.mdx", "score": 1.0852382183074951}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0978506803512573}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1167761087417603}, {"doc_source": "how_to/index.mdx", "score": 1.1301103830337524}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1818643808364868}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2026060819625854}, {"doc_source": "how_to/index.mdx", "score": 1.2045385837554932}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2113205194473267}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2270116806030273}, {"doc_source": "how_to/index.mdx", "score": 1.2281379699707031}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2336554527282715}]}
{"ts": 1747974870.559135, "query_id": "Q008", "question": "Give an example of a memory object that stores chat history.", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.0546066761016846}, {"doc_source": "concepts/chat_history.mdx", "score": 1.0839262008666992}, {"doc_source": "concepts/chat_history.mdx", "score": 1.0980626344680786}, {"doc_source": "concepts/chat_history.mdx", "score": 1.109199047088623}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1665425300598145}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2181367874145508}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.229980230331421}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2313518524169922}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2409284114837646}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2494919300079346}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2563244104385376}, {"doc_source": "concepts/messages.mdx", "score": 1.2579582929611206}, {"doc_source": "concepts/streaming.mdx", "score": 1.2614431381225586}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2699137926101685}, {"doc_source": "concepts/index.mdx", "score": 1.2709739208221436}]}
{"ts": 1747974872.00123, "query_id": "Q009", "question": "Outline the steps to trace a LangGraph flow in LangSmith.", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8301337957382202}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8385686278343201}, {"doc_source": "how_to/index.mdx", "score": 0.8489181399345398}, {"doc_source": "concepts/tracing.mdx", "score": 0.9181725382804871}, {"doc_source": "how_to/index.mdx", "score": 0.9691957235336304}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9921094179153442}, {"doc_source": "introduction.mdx", "score": 1.0403468608856201}, {"doc_source": "concepts/streaming.mdx", "score": 1.0460822582244873}, {"doc_source": "tutorials/index.mdx", "score": 1.0698779821395874}, {"doc_source": "how_to/index.mdx", "score": 1.0905511379241943}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0946513414382935}, {"doc_source": "concepts/streaming.mdx", "score": 1.106436014175415}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1459667682647705}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1461951732635498}, {"doc_source": "how_to/installation.mdx", "score": 1.1682076454162598}]}
{"ts": 1747974873.645466, "query_id": "Q010", "question": "What packages must be pinned to use Pydantic v1 with LangChain?", "retrieved_items": [{"doc_source": "how_to/pydantic_compatibility.md", "score": 0.6679131388664246}, {"doc_source": "concepts/async.mdx", "score": 0.9406142830848694}, {"doc_source": "how_to/installation.mdx", "score": 0.9444892406463623}, {"doc_source": "how_to/installation.mdx", "score": 0.9477394819259644}, {"doc_source": "how_to/installation.mdx", "score": 0.951897382736206}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9903054237365723}, {"doc_source": "how_to/installation.mdx", "score": 0.9912256002426147}, {"doc_source": "how_to/installation.mdx", "score": 0.9954094886779785}, {"doc_source": "how_to/installation.mdx", "score": 1.0310624837875366}, {"doc_source": "concepts/architecture.mdx", "score": 1.063117504119873}, {"doc_source": "concepts/architecture.mdx", "score": 1.116708517074585}, {"doc_source": "concepts/runnables.mdx", "score": 1.1506597995758057}, {"doc_source": "concepts/async.mdx", "score": 1.1581151485443115}, {"doc_source": "introduction.mdx", "score": 1.1684556007385254}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1736489534378052}]}
{"ts": 1747974875.5788429, "query_id": "Q011", "question": "Explain tool calling via OpenAI function messages in LangChain.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.744651198387146}, {"doc_source": "concepts/messages.mdx", "score": 0.7672620415687561}, {"doc_source": "concepts/testing.mdx", "score": 0.8310785293579102}, {"doc_source": "concepts/messages.mdx", "score": 0.8540018200874329}, {"doc_source": "how_to/index.mdx", "score": 0.8576446771621704}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.8632838726043701}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8920662999153137}, {"doc_source": "concepts/testing.mdx", "score": 0.8942639231681824}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9014481902122498}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9068634510040283}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9132900238037109}, {"doc_source": "concepts/messages.mdx", "score": 0.9183686971664429}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9187717437744141}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9202892780303955}, {"doc_source": "concepts/tools.mdx", "score": 0.9247482419013977}]}
{"ts": 1747974877.4186711, "query_id": "Q012", "question": "How does Retrieval-Augmented Generation (RAG) work in LangChain?", "retrieved_items": [{"doc_source": "concepts/rag.mdx", "score": 0.5998701453208923}, {"doc_source": "concepts/rag.mdx", "score": 0.8095791339874268}, {"doc_source": "concepts/rag.mdx", "score": 0.9054796099662781}, {"doc_source": "how_to/index.mdx", "score": 0.9584709405899048}, {"doc_source": "how_to/index.mdx", "score": 0.9606523513793945}, {"doc_source": "concepts/rag.mdx", "score": 0.9874659776687622}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0007789134979248}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0547834634780884}, {"doc_source": "concepts/retrieval.mdx", "score": 1.066786289215088}, {"doc_source": "concepts/rag.mdx", "score": 1.0728881359100342}, {"doc_source": "concepts/retrievers.mdx", "score": 1.095107078552246}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1009864807128906}, {"doc_source": "concepts/index.mdx", "score": 1.10440993309021}, {"doc_source": "how_to/index.mdx", "score": 1.1051747798919678}, {"doc_source": "introduction.mdx", "score": 1.1093780994415283}]}
{"ts": 1747974878.7324328, "query_id": "Q013", "question": "Provide a code snippet to create embeddings with OpenAI via LangChain.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 0.8076019287109375}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8198946714401245}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8584244251251221}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.888568639755249}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9257166385650635}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9357831478118896}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9412014484405518}, {"doc_source": "introduction.mdx", "score": 0.9481393098831177}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9868519306182861}, {"doc_source": "introduction.mdx", "score": 0.9871377944946289}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0094295740127563}, {"doc_source": "how_to/index.mdx", "score": 1.024054765701294}, {"doc_source": "how_to/installation.mdx", "score": 1.0268232822418213}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0315070152282715}, {"doc_source": "how_to/installation.mdx", "score": 1.033084750175476}]}
{"ts": 1747974880.812247, "query_id": "Q014", "question": "List three callback events emitted during an LLM call.", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 0.8810662031173706}, {"doc_source": "concepts/callbacks.mdx", "score": 0.9979001879692078}, {"doc_source": "how_to/index.mdx", "score": 1.0631077289581299}, {"doc_source": "concepts/streaming.mdx", "score": 1.1637296676635742}, {"doc_source": "how_to/index.mdx", "score": 1.1788917779922485}, {"doc_source": "concepts/streaming.mdx", "score": 1.2180194854736328}, {"doc_source": "concepts/callbacks.mdx", "score": 1.2337355613708496}, {"doc_source": "how_to/index.mdx", "score": 1.2730216979980469}, {"doc_source": "concepts/chat_models.mdx", "score": 1.289602518081665}, {"doc_source": "concepts/lcel.mdx", "score": 1.2896761894226074}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2918943166732788}, {"doc_source": "concepts/callbacks.mdx", "score": 1.309502124786377}, {"doc_source": "concepts/streaming.mdx", "score": 1.3112183809280396}, {"doc_source": "concepts/streaming.mdx", "score": 1.3125832080841064}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3161125183105469}]}
{"ts": 1747974882.1710742, "query_id": "Q015", "question": "Why would you use the AsyncRunnable interface?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9024970531463623}, {"doc_source": "concepts/async.mdx", "score": 0.9286519289016724}, {"doc_source": "concepts/runnables.mdx", "score": 0.9566218256950378}, {"doc_source": "concepts/lcel.mdx", "score": 0.9969758987426758}, {"doc_source": "concepts/runnables.mdx", "score": 1.0043003559112549}, {"doc_source": "concepts/runnables.mdx", "score": 1.063917636871338}, {"doc_source": "concepts/async.mdx", "score": 1.0713164806365967}, {"doc_source": "concepts/runnables.mdx", "score": 1.08419930934906}, {"doc_source": "concepts/runnables.mdx", "score": 1.0921388864517212}, {"doc_source": "concepts/async.mdx", "score": 1.112335443496704}, {"doc_source": "concepts/runnables.mdx", "score": 1.1137404441833496}, {"doc_source": "concepts/callbacks.mdx", "score": 1.1396353244781494}, {"doc_source": "concepts/streaming.mdx", "score": 1.1413309574127197}, {"doc_source": "concepts/runnables.mdx", "score": 1.142803430557251}, {"doc_source": "concepts/runnables.mdx", "score": 1.145635724067688}]}
{"ts": 1747974883.8451478, "query_id": "Q016", "question": "What metrics does LangChain's evaluation module compute out-of-box?", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8663620948791504}, {"doc_source": "concepts/evaluation.mdx", "score": 0.9073658585548401}, {"doc_source": "how_to/index.mdx", "score": 0.9129538536071777}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9283957481384277}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9346678256988525}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9462305307388306}, {"doc_source": "how_to/index.mdx", "score": 0.9599090814590454}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9808598160743713}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9956388473510742}, {"doc_source": "how_to/index.mdx", "score": 1.0374228954315186}, {"doc_source": "concepts/lcel.mdx", "score": 1.0416944026947021}, {"doc_source": "concepts/streaming.mdx", "score": 1.0569852590560913}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0617809295654297}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0649542808532715}, {"doc_source": "how_to/index.mdx", "score": 1.065202236175537}]}
{"ts": 1747974884.96559, "query_id": "Q017", "question": "Demonstrate how to chunk text using RecursiveCharacterTextSplitter.", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.8124167919158936}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9811102151870728}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0012919902801514}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.002683162689209}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.018438458442688}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.055332064628601}, {"doc_source": "how_to/index.mdx", "score": 1.0790477991104126}, {"doc_source": "how_to/index.mdx", "score": 1.096566915512085}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1525919437408447}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1681221723556519}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2836463451385498}, {"doc_source": "concepts/streaming.mdx", "score": 1.3952429294586182}, {"doc_source": "concepts/lcel.mdx", "score": 1.411849021911621}, {"doc_source": "concepts/streaming.mdx", "score": 1.4183542728424072}, {"doc_source": "how_to/index.mdx", "score": 1.4398785829544067}]}
{"ts": 1747974887.044966, "query_id": "Q018", "question": "What environment variable controls OpenAI API retries in LangChain?", "retrieved_items": [{"doc_source": "concepts/architecture.mdx", "score": 1.1664249897003174}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1724159717559814}, {"doc_source": "concepts/async.mdx", "score": 1.1811460256576538}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1881568431854248}, {"doc_source": "concepts/async.mdx", "score": 1.200913429260254}, {"doc_source": "how_to/installation.mdx", "score": 1.2060489654541016}, {"doc_source": "concepts/async.mdx", "score": 1.2172553539276123}, {"doc_source": "concepts/messages.mdx", "score": 1.2186334133148193}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2190780639648438}, {"doc_source": "concepts/async.mdx", "score": 1.219377040863037}, {"doc_source": "concepts/async.mdx", "score": 1.2266381978988647}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2283799648284912}, {"doc_source": "how_to/installation.mdx", "score": 1.2287760972976685}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2382664680480957}, {"doc_source": "concepts/streaming.mdx", "score": 1.2398751974105835}]}
{"ts": 1747974888.3848078, "query_id": "Q019", "question": "How can key-value stores be used to cache intermediate chain output?", "retrieved_items": [{"doc_source": "concepts/key_value_stores.mdx", "score": 0.9602024555206299}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.0030235052108765}, {"doc_source": "concepts/streaming.mdx", "score": 1.1442111730575562}, {"doc_source": "concepts/lcel.mdx", "score": 1.1699233055114746}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.262619972229004}, {"doc_source": "concepts/lcel.mdx", "score": 1.302964210510254}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3147226572036743}, {"doc_source": "concepts/lcel.mdx", "score": 1.3161532878875732}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3171077966690063}, {"doc_source": "concepts/lcel.mdx", "score": 1.3233036994934082}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3438587188720703}, {"doc_source": "concepts/lcel.mdx", "score": 1.3458898067474365}, {"doc_source": "concepts/lcel.mdx", "score": 1.353567361831665}, {"doc_source": "concepts/streaming.mdx", "score": 1.3565329313278198}, {"doc_source": "how_to/index.mdx", "score": 1.3574204444885254}]}
{"ts": 1747974890.312196, "query_id": "Q020", "question": "Explain \"Multimodality\" support in LangChain with an example.", "retrieved_items": [{"doc_source": "concepts/multimodality.mdx", "score": 0.6339109539985657}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7265490293502808}, {"doc_source": "concepts/multimodality.mdx", "score": 0.8755182027816772}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9242032766342163}, {"doc_source": "concepts/tokens.mdx", "score": 0.9289199709892273}, {"doc_source": "concepts/index.mdx", "score": 0.9448490142822266}, {"doc_source": "concepts/multimodality.mdx", "score": 0.947632908821106}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9589797258377075}, {"doc_source": "how_to/index.mdx", "score": 0.9748779535293579}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9858150482177734}, {"doc_source": "concepts/lcel.mdx", "score": 0.9895062446594238}, {"doc_source": "how_to/index.mdx", "score": 0.9911352396011353}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9984537363052368}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0071992874145508}, {"doc_source": "concepts/streaming.mdx", "score": 1.012650966644287}]}
{"ts": 1747974891.990338, "query_id": "Q021", "question": "What is the LangChain architecture diagram trying to convey?", "retrieved_items": [{"doc_source": "introduction.mdx", "score": 0.8011260032653809}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8224393129348755}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8367571830749512}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8460158705711365}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8661633729934692}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8832542896270752}, {"doc_source": "introduction.mdx", "score": 0.8887532949447632}, {"doc_source": "introduction.mdx", "score": 0.8937346935272217}, {"doc_source": "how_to/index.mdx", "score": 0.9176084995269775}, {"doc_source": "concepts/lcel.mdx", "score": 0.9284917712211609}, {"doc_source": "concepts/architecture.mdx", "score": 0.9333944320678711}, {"doc_source": "concepts/index.mdx", "score": 0.9401848912239075}, {"doc_source": "tutorials/index.mdx", "score": 0.9528964757919312}, {"doc_source": "concepts/architecture.mdx", "score": 0.9784926772117615}, {"doc_source": "concepts/index.mdx", "score": 0.9862108826637268}]}
{"ts": 1747974893.5366359, "query_id": "Q022", "question": "Provide two example selectors and when to use them.", "retrieved_items": [{"doc_source": "concepts/example_selectors.mdx", "score": 0.8741787075996399}, {"doc_source": "how_to/index.mdx", "score": 1.0563435554504395}, {"doc_source": "how_to/index.mdx", "score": 1.176438808441162}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2142183780670166}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2195253372192383}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.321842908859253}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3478730916976929}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.348643183708191}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3767101764678955}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3856935501098633}, {"doc_source": "how_to/index.mdx", "score": 1.4001023769378662}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.4052684307098389}, {"doc_source": "concepts/index.mdx", "score": 1.4129656553268433}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.4284651279449463}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.440999984741211}]}
{"ts": 1747974895.9722948, "query_id": "Q023", "question": "How do OutputParsers simplify post-processing in LCEL?", "retrieved_items": [{"doc_source": "concepts/output_parsers.mdx", "score": 0.9453783631324768}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.9980274438858032}, {"doc_source": "concepts/lcel.mdx", "score": 1.0187904834747314}, {"doc_source": "concepts/lcel.mdx", "score": 1.0716444253921509}, {"doc_source": "how_to/index.mdx", "score": 1.0895586013793945}, {"doc_source": "how_to/index.mdx", "score": 1.1126708984375}, {"doc_source": "concepts/index.mdx", "score": 1.188828468322754}, {"doc_source": "concepts/lcel.mdx", "score": 1.2002284526824951}, {"doc_source": "concepts/lcel.mdx", "score": 1.20735764503479}, {"doc_source": "how_to/index.mdx", "score": 1.2138540744781494}, {"doc_source": "concepts/lcel.mdx", "score": 1.2310343980789185}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2363011837005615}, {"doc_source": "concepts/streaming.mdx", "score": 1.2389776706695557}, {"doc_source": "concepts/streaming.mdx", "score": 1.2614870071411133}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.266556739807129}]}
{"ts": 1747974897.837515, "query_id": "Q024", "question": "Describe streaming chunk callbacks versus token callbacks.", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 1.0602614879608154}, {"doc_source": "concepts/streaming.mdx", "score": 1.0649443864822388}, {"doc_source": "concepts/streaming.mdx", "score": 1.1103088855743408}, {"doc_source": "concepts/streaming.mdx", "score": 1.1240227222442627}, {"doc_source": "concepts/streaming.mdx", "score": 1.1327488422393799}, {"doc_source": "concepts/streaming.mdx", "score": 1.1367862224578857}, {"doc_source": "concepts/streaming.mdx", "score": 1.1375436782836914}, {"doc_source": "concepts/streaming.mdx", "score": 1.1507196426391602}, {"doc_source": "concepts/lcel.mdx", "score": 1.1534249782562256}, {"doc_source": "concepts/streaming.mdx", "score": 1.1697911024093628}, {"doc_source": "concepts/streaming.mdx", "score": 1.1782793998718262}, {"doc_source": "concepts/runnables.mdx", "score": 1.1834486722946167}, {"doc_source": "concepts/callbacks.mdx", "score": 1.1852988004684448}, {"doc_source": "concepts/streaming.mdx", "score": 1.1944291591644287}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2007118463516235}]}
{"ts": 1747974899.141289, "query_id": "Q025", "question": "What is the purpose of the `ConversationBufferMemory` class and when should it be used?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.1946804523468018}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2197487354278564}, {"doc_source": "concepts/chat_models.mdx", "score": 1.236838936805725}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2724217176437378}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3232784271240234}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3516807556152344}, {"doc_source": "concepts/messages.mdx", "score": 1.3558132648468018}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3740025758743286}, {"doc_source": "concepts/chat_models.mdx", "score": 1.375034213066101}, {"doc_source": "concepts/chat_models.mdx", "score": 1.380035638809204}, {"doc_source": "how_to/index.mdx", "score": 1.3867992162704468}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3900904655456543}, {"doc_source": "concepts/streaming.mdx", "score": 1.392336130142212}, {"doc_source": "concepts/messages.mdx", "score": 1.4017438888549805}, {"doc_source": "concepts/index.mdx", "score": 1.4033839702606201}]}
{"ts": 1747974900.537382, "query_id": "Q026", "question": "Which LangChain text splitter lets you split documents by tokens and why would you choose it over a character splitter?", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.6639919281005859}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.7719472646713257}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.8010584115982056}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.8800575733184814}, {"doc_source": "concepts/tokens.mdx", "score": 0.8921389579772949}, {"doc_source": "concepts/lcel.mdx", "score": 0.9248173832893372}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9463869333267212}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9688575863838196}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9791551828384399}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.027786135673523}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0380240678787231}, {"doc_source": "concepts/index.mdx", "score": 1.0397882461547852}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0477733612060547}, {"doc_source": "introduction.mdx", "score": 1.0492931604385376}, {"doc_source": "how_to/index.mdx", "score": 1.0559037923812866}]}
{"ts": 1747974902.423483, "query_id": "Q027", "question": "List two differences between `VectorStoreRetriever` and `EnsembleRetriever`.", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.7439137697219849}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8023593425750732}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8832237720489502}, {"doc_source": "how_to/index.mdx", "score": 1.0252223014831543}, {"doc_source": "concepts/retrievers.mdx", "score": 1.069690465927124}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1026475429534912}, {"doc_source": "concepts/retrievers.mdx", "score": 1.117879033088684}, {"doc_source": "how_to/index.mdx", "score": 1.1326682567596436}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1536221504211426}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.156407356262207}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1670334339141846}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2473747730255127}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2545826435089111}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2578637599945068}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2652268409729004}]}
{"ts": 1747974904.049011, "query_id": "Q028", "question": "How can you configure a chat model to automatically retry on ratelimiting errors?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5974044799804688}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7214343547821045}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9778425693511963}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0452858209609985}, {"doc_source": "concepts/chat_models.mdx", "score": 1.081045150756836}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1291680335998535}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1523717641830444}, {"doc_source": "concepts/streaming.mdx", "score": 1.1567195653915405}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1602773666381836}, {"doc_source": "concepts/chat_models.mdx", "score": 1.174750566482544}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.198230266571045}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2025090456008911}, {"doc_source": "concepts/runnables.mdx", "score": 1.206661581993103}, {"doc_source": "concepts/chat_models.mdx", "score": 1.21598219871521}, {"doc_source": "concepts/rag.mdx", "score": 1.2159833908081055}]}
{"ts": 1747974905.371458, "query_id": "Q029", "question": "Explain how `InjectedToolArg` affects a tool schema and give one use case.", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.6152375936508179}, {"doc_source": "concepts/tools.mdx", "score": 0.641218900680542}, {"doc_source": "concepts/tools.mdx", "score": 0.9970071315765381}, {"doc_source": "concepts/tools.mdx", "score": 1.0166616439819336}, {"doc_source": "concepts/tools.mdx", "score": 1.0180271863937378}, {"doc_source": "concepts/tools.mdx", "score": 1.0186538696289062}, {"doc_source": "concepts/tools.mdx", "score": 1.0458378791809082}, {"doc_source": "concepts/tools.mdx", "score": 1.077510118484497}, {"doc_source": "concepts/tools.mdx", "score": 1.0919139385223389}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0986005067825317}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1134402751922607}, {"doc_source": "concepts/tools.mdx", "score": 1.1138689517974854}, {"doc_source": "concepts/tools.mdx", "score": 1.1253142356872559}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.1268203258514404}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.150414228439331}]}
{"ts": 1747974907.5937479, "query_id": "Q030", "question": "Which decorator is recommended for quickly turning a Python function into a LangChain Tool?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8510563373565674}, {"doc_source": "concepts/runnables.mdx", "score": 0.9630204439163208}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.9698821306228638}, {"doc_source": "concepts/tools.mdx", "score": 1.0076475143432617}, {"doc_source": "concepts/tools.mdx", "score": 1.0290484428405762}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0301283597946167}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0336294174194336}, {"doc_source": "concepts/async.mdx", "score": 1.0534178018569946}, {"doc_source": "concepts/tools.mdx", "score": 1.0609604120254517}, {"doc_source": "concepts/async.mdx", "score": 1.0753121376037598}, {"doc_source": "concepts/lcel.mdx", "score": 1.0760799646377563}, {"doc_source": "concepts/tools.mdx", "score": 1.0763249397277832}, {"doc_source": "concepts/async.mdx", "score": 1.0828388929367065}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1050995588302612}, {"doc_source": "concepts/testing.mdx", "score": 1.1078861951828003}]}
{"ts": 1747974909.302219, "query_id": "Q031", "question": "Describe how to batch inputs through a Runnable for better throughput.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6073238849639893}, {"doc_source": "concepts/runnables.mdx", "score": 0.7850158214569092}, {"doc_source": "concepts/runnables.mdx", "score": 0.901479959487915}, {"doc_source": "concepts/runnables.mdx", "score": 0.9244027137756348}, {"doc_source": "concepts/lcel.mdx", "score": 0.9339675903320312}, {"doc_source": "concepts/runnables.mdx", "score": 1.0409948825836182}, {"doc_source": "concepts/runnables.mdx", "score": 1.0664386749267578}, {"doc_source": "concepts/async.mdx", "score": 1.1181254386901855}, {"doc_source": "concepts/runnables.mdx", "score": 1.1306023597717285}, {"doc_source": "concepts/runnables.mdx", "score": 1.137177586555481}, {"doc_source": "concepts/runnables.mdx", "score": 1.1380716562271118}, {"doc_source": "concepts/runnables.mdx", "score": 1.165820837020874}, {"doc_source": "concepts/runnables.mdx", "score": 1.1681181192398071}, {"doc_source": "concepts/runnables.mdx", "score": 1.1727588176727295}, {"doc_source": "concepts/runnables.mdx", "score": 1.1837694644927979}]}
{"ts": 1747974910.8487759, "query_id": "Q032", "question": "What parameter in `RunnableConfig` controls the maximum number of parallel calls during batching?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8541860580444336}, {"doc_source": "concepts/runnables.mdx", "score": 0.8997519016265869}, {"doc_source": "concepts/runnables.mdx", "score": 0.9034135341644287}, {"doc_source": "concepts/runnables.mdx", "score": 0.9463394284248352}, {"doc_source": "concepts/runnables.mdx", "score": 0.996875524520874}, {"doc_source": "concepts/lcel.mdx", "score": 1.0377821922302246}, {"doc_source": "concepts/runnables.mdx", "score": 1.069813847541809}, {"doc_source": "concepts/runnables.mdx", "score": 1.0937137603759766}, {"doc_source": "concepts/runnables.mdx", "score": 1.1012307405471802}, {"doc_source": "concepts/runnables.mdx", "score": 1.134948492050171}, {"doc_source": "concepts/runnables.mdx", "score": 1.1446621417999268}, {"doc_source": "concepts/runnables.mdx", "score": 1.1581703424453735}, {"doc_source": "concepts/runnables.mdx", "score": 1.1750080585479736}, {"doc_source": "concepts/runnables.mdx", "score": 1.2127158641815186}, {"doc_source": "concepts/lcel.mdx", "score": 1.2188820838928223}]}
{"ts": 1747974911.7428749, "query_id": "Q033", "question": "Give an example of using `with_structured_output` to get JSON back from a chat model.", "retrieved_items": [{"doc_source": "concepts/structured_outputs.mdx", "score": 0.7809972167015076}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8987329006195068}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.9393086433410645}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9613921642303467}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9788405895233154}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.9875810742378235}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9997315406799316}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.0025910139083862}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0106785297393799}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0279011726379395}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0480084419250488}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.0823688507080078}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.116142988204956}, {"doc_source": "concepts/index.mdx", "score": 1.11763334274292}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.1182012557983398}]}
{"ts": 1747974915.8272471, "query_id": "Q034", "question": "What callback would you use to stream intermediate events from a Runnable?", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 0.8934335708618164}, {"doc_source": "concepts/streaming.mdx", "score": 0.9890404343605042}, {"doc_source": "concepts/streaming.mdx", "score": 1.008561611175537}, {"doc_source": "concepts/streaming.mdx", "score": 1.0113866329193115}, {"doc_source": "concepts/runnables.mdx", "score": 1.0397053956985474}, {"doc_source": "concepts/runnables.mdx", "score": 1.0663822889328003}, {"doc_source": "concepts/runnables.mdx", "score": 1.095582365989685}, {"doc_source": "concepts/callbacks.mdx", "score": 1.097243309020996}, {"doc_source": "concepts/streaming.mdx", "score": 1.0973433256149292}, {"doc_source": "concepts/streaming.mdx", "score": 1.1348118782043457}, {"doc_source": "concepts/runnables.mdx", "score": 1.1392388343811035}, {"doc_source": "concepts/streaming.mdx", "score": 1.1488618850708008}, {"doc_source": "concepts/streaming.mdx", "score": 1.1525089740753174}, {"doc_source": "concepts/streaming.mdx", "score": 1.1597548723220825}, {"doc_source": "concepts/runnables.mdx", "score": 1.1612067222595215}]}
{"ts": 1747974917.213463, "query_id": "Q035", "question": "Which two standard parameters influence generation randomness and length in chat models?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.9568188190460205}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0018939971923828}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0070934295654297}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0433233976364136}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0653163194656372}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1297802925109863}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1473336219787598}, {"doc_source": "concepts/chat_models.mdx", "score": 1.150551199913025}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1539814472198486}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1684939861297607}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1787173748016357}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1839427947998047}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1947615146636963}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2134618759155273}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2171735763549805}]}
{"ts": 1747974918.903862, "query_id": "Q036", "question": "How do you persist a FAISS vector store to disk?", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 1.1071107387542725}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.165313959121704}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.192087173461914}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.1964092254638672}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.2159513235092163}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2597808837890625}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.264936923980713}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2795114517211914}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2846730947494507}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2858422994613647}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2998005151748657}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.3221385478973389}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3378283977508545}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3469476699829102}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.3684356212615967}]}
{"ts": 1747974920.3206651, "query_id": "Q037", "question": "Explain the difference between `stream` and `astream` methods on a Runnable.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.7147161960601807}, {"doc_source": "concepts/streaming.mdx", "score": 0.7504462003707886}, {"doc_source": "concepts/streaming.mdx", "score": 0.7568306922912598}, {"doc_source": "concepts/runnables.mdx", "score": 0.7943024635314941}, {"doc_source": "concepts/streaming.mdx", "score": 0.8236364722251892}, {"doc_source": "concepts/streaming.mdx", "score": 0.8398924469947815}, {"doc_source": "concepts/runnables.mdx", "score": 0.8805309534072876}, {"doc_source": "concepts/runnables.mdx", "score": 1.0105451345443726}, {"doc_source": "concepts/streaming.mdx", "score": 1.0276488065719604}, {"doc_source": "concepts/runnables.mdx", "score": 1.055930256843567}, {"doc_source": "concepts/runnables.mdx", "score": 1.1137446165084839}, {"doc_source": "concepts/async.mdx", "score": 1.1150352954864502}, {"doc_source": "concepts/streaming.mdx", "score": 1.1173763275146484}, {"doc_source": "concepts/streaming.mdx", "score": 1.1288654804229736}, {"doc_source": "concepts/runnables.mdx", "score": 1.1471221446990967}]}
{"ts": 1747974922.414896, "query_id": "Q038", "question": "What feature does LangChain provide to ensure prompts fit within an LLM's context window?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.8541079163551331}, {"doc_source": "how_to/index.mdx", "score": 0.9134668707847595}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9302371740341187}, {"doc_source": "how_to/index.mdx", "score": 0.9465134739875793}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9496245980262756}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9637824296951294}, {"doc_source": "concepts/lcel.mdx", "score": 0.9696225523948669}, {"doc_source": "how_to/index.mdx", "score": 0.9765156507492065}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9804434776306152}, {"doc_source": "concepts/streaming.mdx", "score": 0.9844945073127747}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9858736991882324}, {"doc_source": "concepts/index.mdx", "score": 0.99067223072052}, {"doc_source": "concepts/chat_models.mdx", "score": 0.996686577796936}, {"doc_source": "tutorials/index.mdx", "score": 0.9972435235977173}, {"doc_source": "concepts/streaming.mdx", "score": 1.004397988319397}]}
{"ts": 1747974924.179656, "query_id": "Q039", "question": "Name three message roles supported by LangChain chat messages.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 0.7468531727790833}, {"doc_source": "concepts/messages.mdx", "score": 0.7780396342277527}, {"doc_source": "concepts/messages.mdx", "score": 0.8878283500671387}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9012303948402405}, {"doc_source": "concepts/messages.mdx", "score": 0.9069714546203613}, {"doc_source": "concepts/messages.mdx", "score": 0.9362462759017944}, {"doc_source": "concepts/messages.mdx", "score": 0.9395574331283569}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0092933177947998}, {"doc_source": "concepts/multimodality.mdx", "score": 1.016806960105896}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0168626308441162}, {"doc_source": "how_to/index.mdx", "score": 1.0289459228515625}, {"doc_source": "concepts/messages.mdx", "score": 1.041947603225708}, {"doc_source": "concepts/messages.mdx", "score": 1.053396224975586}, {"doc_source": "concepts/streaming.mdx", "score": 1.0550904273986816}, {"doc_source": "concepts/index.mdx", "score": 1.0617893934249878}]}
{"ts": 1747974925.305355, "query_id": "Q040", "question": "How can you combine docs returned by a Retriever before passing to an LLM?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.8409014940261841}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8478938341140747}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9801164865493774}, {"doc_source": "concepts/retrievers.mdx", "score": 1.003244400024414}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0324559211730957}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0521926879882812}, {"doc_source": "how_to/index.mdx", "score": 1.0557212829589844}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0585412979125977}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0690066814422607}, {"doc_source": "concepts/rag.mdx", "score": 1.124057412147522}, {"doc_source": "concepts/rag.mdx", "score": 1.1355570554733276}, {"doc_source": "concepts/retrieval.mdx", "score": 1.138745903968811}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1419570446014404}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1451324224472046}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1566014289855957}]}
{"ts": 1747974927.9899, "query_id": "Q041", "question": "What does the `rate_limiter` parameter accept and why is it useful?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.9186906814575195}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9408151507377625}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0711807012557983}, {"doc_source": "concepts/chat_models.mdx", "score": 1.176403284072876}, {"doc_source": "concepts/runnables.mdx", "score": 1.219515323638916}, {"doc_source": "concepts/runnables.mdx", "score": 1.270452857017517}, {"doc_source": "concepts/runnables.mdx", "score": 1.325438380241394}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3378229141235352}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3580830097198486}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3740849494934082}, {"doc_source": "concepts/chat_models.mdx", "score": 1.398261308670044}, {"doc_source": "concepts/chat_models.mdx", "score": 1.409281849861145}, {"doc_source": "concepts/runnables.mdx", "score": 1.4138648509979248}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4161173105239868}, {"doc_source": "concepts/runnables.mdx", "score": 1.432873010635376}]}
{"ts": 1747974929.950419, "query_id": "Q042", "question": "Provide a code snippet to stream tokens from an OpenAI chat model using LCEL.", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.8580672740936279}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9569725394248962}, {"doc_source": "concepts/streaming.mdx", "score": 0.9773334264755249}, {"doc_source": "concepts/streaming.mdx", "score": 0.9781379699707031}, {"doc_source": "concepts/streaming.mdx", "score": 0.9933494329452515}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0009372234344482}, {"doc_source": "concepts/streaming.mdx", "score": 1.0149884223937988}, {"doc_source": "concepts/streaming.mdx", "score": 1.015478491783142}, {"doc_source": "concepts/streaming.mdx", "score": 1.0285007953643799}, {"doc_source": "concepts/lcel.mdx", "score": 1.0335556268692017}, {"doc_source": "concepts/messages.mdx", "score": 1.034729242324829}, {"doc_source": "concepts/streaming.mdx", "score": 1.0400614738464355}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0528016090393066}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0541335344314575}, {"doc_source": "concepts/rag.mdx", "score": 1.0626581907272339}]}
{"ts": 1747974931.8053582, "query_id": "Q043", "question": "Which notebook demonstrates how to track token usage costs per request?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 1.2758994102478027}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.286210298538208}, {"doc_source": "how_to/index.mdx", "score": 1.3035249710083008}, {"doc_source": "concepts/tokens.mdx", "score": 1.3319091796875}, {"doc_source": "concepts/index.mdx", "score": 1.332702875137329}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3333897590637207}, {"doc_source": "concepts/tokens.mdx", "score": 1.3349742889404297}, {"doc_source": "introduction.mdx", "score": 1.3474395275115967}, {"doc_source": "how_to/index.mdx", "score": 1.350388526916504}, {"doc_source": "tutorials/index.mdx", "score": 1.3521878719329834}, {"doc_source": "how_to/index.mdx", "score": 1.3534271717071533}, {"doc_source": "concepts/tokens.mdx", "score": 1.3602854013442993}, {"doc_source": "concepts/index.mdx", "score": 1.3629999160766602}, {"doc_source": "tutorials/index.mdx", "score": 1.365742564201355}, {"doc_source": "how_to/index.mdx", "score": 1.366734504699707}]}
{"ts": 1747974932.722463, "query_id": "Q044", "question": "What are tool artifacts and how can they be returned from a tool?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.7273582220077515}, {"doc_source": "how_to/index.mdx", "score": 0.8020932078361511}, {"doc_source": "concepts/tools.mdx", "score": 0.8544261455535889}, {"doc_source": "concepts/tools.mdx", "score": 0.9141151309013367}, {"doc_source": "concepts/tools.mdx", "score": 1.0447263717651367}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1082346439361572}, {"doc_source": "how_to/index.mdx", "score": 1.1097279787063599}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.131062626838684}, {"doc_source": "concepts/messages.mdx", "score": 1.1470674276351929}, {"doc_source": "concepts/tools.mdx", "score": 1.1604117155075073}, {"doc_source": "concepts/tools.mdx", "score": 1.1638593673706055}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1663814783096313}, {"doc_source": "concepts/tools.mdx", "score": 1.1773717403411865}, {"doc_source": "concepts/tools.mdx", "score": 1.1785392761230469}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1901845932006836}]}
{"ts": 1747974934.0850902, "query_id": "Q045", "question": "Explain how to asynchronously invoke a Runnable and await the result.", "retrieved_items": [{"doc_source": "concepts/async.mdx", "score": 0.8696734309196472}, {"doc_source": "concepts/runnables.mdx", "score": 0.9418741464614868}, {"doc_source": "concepts/async.mdx", "score": 1.0064888000488281}, {"doc_source": "concepts/runnables.mdx", "score": 1.0402475595474243}, {"doc_source": "concepts/runnables.mdx", "score": 1.0634011030197144}, {"doc_source": "concepts/runnables.mdx", "score": 1.0665918588638306}, {"doc_source": "concepts/runnables.mdx", "score": 1.1048753261566162}, {"doc_source": "concepts/lcel.mdx", "score": 1.1444354057312012}, {"doc_source": "concepts/lcel.mdx", "score": 1.154728889465332}, {"doc_source": "concepts/runnables.mdx", "score": 1.1652884483337402}, {"doc_source": "concepts/runnables.mdx", "score": 1.1714683771133423}, {"doc_source": "concepts/runnables.mdx", "score": 1.174389123916626}, {"doc_source": "concepts/runnables.mdx", "score": 1.1981037855148315}, {"doc_source": "concepts/lcel.mdx", "score": 1.2030327320098877}, {"doc_source": "concepts/runnables.mdx", "score": 1.2038241624832153}]}
{"ts": 1747974935.837456, "query_id": "Q046", "question": "Which two documents should you read to understand LangGraph node construction?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.8433270454406738}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9650079011917114}, {"doc_source": "tutorials/index.mdx", "score": 0.9924482107162476}, {"doc_source": "how_to/index.mdx", "score": 1.0161128044128418}, {"doc_source": "concepts/streaming.mdx", "score": 1.0292083024978638}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0589890480041504}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0643954277038574}, {"doc_source": "introduction.mdx", "score": 1.0982979536056519}, {"doc_source": "introduction.mdx", "score": 1.100066900253296}, {"doc_source": "concepts/index.mdx", "score": 1.1131185293197632}, {"doc_source": "introduction.mdx", "score": 1.1280392408370972}, {"doc_source": "how_to/index.mdx", "score": 1.1306922435760498}, {"doc_source": "tutorials/index.mdx", "score": 1.134656310081482}, {"doc_source": "concepts/index.mdx", "score": 1.1410754919052124}, {"doc_source": "concepts/retrieval.mdx", "score": 1.142289638519287}]}
{"ts": 1747974936.8515131, "query_id": "Q047", "question": "Describe the process of converting a Runnable into a Tool for agent use.", "retrieved_items": [{"doc_source": "concepts/tool_calling.mdx", "score": 0.8755334615707397}, {"doc_source": "how_to/index.mdx", "score": 0.9113866090774536}, {"doc_source": "concepts/runnables.mdx", "score": 1.0067635774612427}, {"doc_source": "concepts/runnables.mdx", "score": 1.0176581144332886}, {"doc_source": "concepts/runnables.mdx", "score": 1.0320122241973877}, {"doc_source": "concepts/runnables.mdx", "score": 1.0383824110031128}, {"doc_source": "how_to/index.mdx", "score": 1.0550715923309326}, {"doc_source": "concepts/tools.mdx", "score": 1.0552451610565186}, {"doc_source": "concepts/runnables.mdx", "score": 1.0682663917541504}, {"doc_source": "concepts/tools.mdx", "score": 1.0735183954238892}, {"doc_source": "concepts/tools.mdx", "score": 1.1146529912948608}, {"doc_source": "concepts/runnables.mdx", "score": 1.122819423675537}, {"doc_source": "concepts/runnables.mdx", "score": 1.1274170875549316}, {"doc_source": "concepts/tools.mdx", "score": 1.1285423040390015}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1600502729415894}]}
{"ts": 1747974938.442484, "query_id": "Q048", "question": "What Python package contains community-contributed chat model integrations?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7932825684547424}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9217169880867004}, {"doc_source": "concepts/architecture.mdx", "score": 0.9720602035522461}, {"doc_source": "concepts/chat_models.mdx", "score": 0.991050660610199}, {"doc_source": "concepts/chat_models.mdx", "score": 1.017716407775879}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0466560125350952}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0605427026748657}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0620002746582031}, {"doc_source": "concepts/architecture.mdx", "score": 1.0700914859771729}, {"doc_source": "concepts/index.mdx", "score": 1.076059341430664}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0799974203109741}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0933806896209717}, {"doc_source": "how_to/installation.mdx", "score": 1.0948734283447266}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0949597358703613}, {"doc_source": "how_to/installation.mdx", "score": 1.1038950681686401}]}
{"ts": 1747974941.578604, "query_id": "Q049", "question": "How can semantic caching improve LLM response times and what extra model does it rely on?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7451983690261841}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9085593223571777}, {"doc_source": "how_to/index.mdx", "score": 1.0320791006088257}, {"doc_source": "concepts/retrieval.mdx", "score": 1.097856879234314}, {"doc_source": "concepts/retrieval.mdx", "score": 1.108012318611145}, {"doc_source": "how_to/index.mdx", "score": 1.1667096614837646}, {"doc_source": "how_to/index.mdx", "score": 1.171334981918335}, {"doc_source": "concepts/chat_models.mdx", "score": 1.181129813194275}, {"doc_source": "concepts/rag.mdx", "score": 1.1874606609344482}, {"doc_source": "concepts/lcel.mdx", "score": 1.1928619146347046}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.196015477180481}, {"doc_source": "concepts/rag.mdx", "score": 1.1987911462783813}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2026046514511108}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2224340438842773}, {"doc_source": "concepts/streaming.mdx", "score": 1.2235573530197144}]}
{"ts": 1747974943.271041, "query_id": "Q050", "question": "Which document loader would you use for processing JSON lines files?", "retrieved_items": [{"doc_source": "how_to/document_loader_json.mdx", "score": 0.6945728063583374}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.8332122564315796}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.8511616587638855}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.9374269247055054}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.9546530246734619}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.0603286027908325}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0620708465576172}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.0653338432312012}, {"doc_source": "how_to/index.mdx", "score": 1.0682519674301147}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.07631516456604}, {"doc_source": "how_to/index.mdx", "score": 1.0867478847503662}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.1033943891525269}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.1606346368789673}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.1673837900161743}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2411608695983887}]}
{"ts": 1747974944.633984, "query_id": "Q051", "question": "What is an `OutputParser` and why is it useful in LCEL chains?", "retrieved_items": [{"doc_source": "concepts/output_parsers.mdx", "score": 0.666566014289856}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.8717368245124817}, {"doc_source": "how_to/index.mdx", "score": 0.8749825358390808}, {"doc_source": "concepts/lcel.mdx", "score": 0.9573459625244141}, {"doc_source": "concepts/index.mdx", "score": 0.9734790325164795}, {"doc_source": "how_to/index.mdx", "score": 0.9745916724205017}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.9897770285606384}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.9972451329231262}, {"doc_source": "concepts/lcel.mdx", "score": 1.0475847721099854}, {"doc_source": "concepts/streaming.mdx", "score": 1.050242304801941}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.0783095359802246}, {"doc_source": "concepts/lcel.mdx", "score": 1.087519645690918}, {"doc_source": "concepts/streaming.mdx", "score": 1.0896022319793701}, {"doc_source": "concepts/lcel.mdx", "score": 1.093246340751648}, {"doc_source": "concepts/lcel.mdx", "score": 1.1054003238677979}]}
{"ts": 1747974945.8813949, "query_id": "Q052", "question": "Give an example selector that prioritizes shorter examples and describe its use case.", "retrieved_items": [{"doc_source": "concepts/example_selectors.mdx", "score": 0.7915581464767456}, {"doc_source": "how_to/index.mdx", "score": 0.8989303112030029}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0415642261505127}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0673329830169678}, {"doc_source": "how_to/index.mdx", "score": 1.0715770721435547}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.114398717880249}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2263935804367065}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2372627258300781}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2496864795684814}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.287245750427246}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2899835109710693}, {"doc_source": "concepts/index.mdx", "score": 1.292864441871643}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2983250617980957}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.359721302986145}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.370227336883545}]}
{"ts": 1747974947.730915, "query_id": "Q053", "question": "Which environment variable can override OpenAI API base URL in LangChain?", "retrieved_items": [{"doc_source": "how_to/installation.mdx", "score": 1.117238163948059}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1247845888137817}, {"doc_source": "concepts/messages.mdx", "score": 1.1472233533859253}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1501657962799072}, {"doc_source": "how_to/installation.mdx", "score": 1.1661661863327026}, {"doc_source": "concepts/architecture.mdx", "score": 1.1807626485824585}, {"doc_source": "introduction.mdx", "score": 1.1884618997573853}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1907936334609985}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1981210708618164}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1984342336654663}, {"doc_source": "introduction.mdx", "score": 1.2037458419799805}, {"doc_source": "concepts/architecture.mdx", "score": 1.214874505996704}, {"doc_source": "concepts/async.mdx", "score": 1.2155683040618896}, {"doc_source": "concepts/async.mdx", "score": 1.2264128923416138}, {"doc_source": "concepts/async.mdx", "score": 1.2305322885513306}]}
{"ts": 1747974948.75664, "query_id": "Q054", "question": "Describe how to attach custom metadata to a Runnable run for later inspection in LangSmith.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8650907874107361}, {"doc_source": "concepts/runnables.mdx", "score": 1.063724398612976}, {"doc_source": "concepts/runnables.mdx", "score": 1.0840266942977905}, {"doc_source": "concepts/runnables.mdx", "score": 1.0896906852722168}, {"doc_source": "concepts/runnables.mdx", "score": 1.0987436771392822}, {"doc_source": "concepts/runnables.mdx", "score": 1.108872652053833}, {"doc_source": "how_to/index.mdx", "score": 1.1295170783996582}, {"doc_source": "concepts/tools.mdx", "score": 1.1362147331237793}, {"doc_source": "concepts/runnables.mdx", "score": 1.151036262512207}, {"doc_source": "how_to/index.mdx", "score": 1.1560078859329224}, {"doc_source": "concepts/runnables.mdx", "score": 1.156150460243225}, {"doc_source": "tutorials/index.mdx", "score": 1.1598234176635742}, {"doc_source": "concepts/runnables.mdx", "score": 1.1604278087615967}, {"doc_source": "concepts/runnables.mdx", "score": 1.1727242469787598}, {"doc_source": "concepts/runnables.mdx", "score": 1.1823261976242065}]}
{"ts": 1747974950.3077052, "query_id": "Q055", "question": "What does the `configurable_fields` helper enable for advanced chains?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9237215518951416}, {"doc_source": "concepts/runnables.mdx", "score": 1.1059658527374268}, {"doc_source": "concepts/runnables.mdx", "score": 1.129848837852478}, {"doc_source": "concepts/runnables.mdx", "score": 1.2372381687164307}, {"doc_source": "concepts/lcel.mdx", "score": 1.2803070545196533}, {"doc_source": "concepts/runnables.mdx", "score": 1.2866963148117065}, {"doc_source": "concepts/lcel.mdx", "score": 1.291886806488037}, {"doc_source": "concepts/lcel.mdx", "score": 1.2932255268096924}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.301077127456665}, {"doc_source": "concepts/lcel.mdx", "score": 1.3022758960723877}, {"doc_source": "concepts/lcel.mdx", "score": 1.3103312253952026}, {"doc_source": "concepts/lcel.mdx", "score": 1.3288748264312744}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3338690996170044}, {"doc_source": "concepts/runnables.mdx", "score": 1.335625171661377}, {"doc_source": "how_to/index.mdx", "score": 1.3409390449523926}]}
{"ts": 1747974952.701718, "query_id": "Q056", "question": "Name two ChatModel parameters that help avoid exceeding rate limits.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5763458013534546}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7863516807556152}, {"doc_source": "concepts/chat_models.mdx", "score": 0.825593113899231}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9939647316932678}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9982819557189941}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0520739555358887}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0629053115844727}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0733782052993774}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0872578620910645}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0920438766479492}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0973937511444092}, {"doc_source": "concepts/chat_models.mdx", "score": 1.101324439048767}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1098026037216187}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1320072412490845}, {"doc_source": "concepts/index.mdx", "score": 1.1390801668167114}]}
{"ts": 1747974955.023264, "query_id": "Q057", "question": "How do you specify a custom UUID for a Runnable run?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9111871123313904}, {"doc_source": "concepts/runnables.mdx", "score": 1.0184091329574585}, {"doc_source": "concepts/runnables.mdx", "score": 1.0532326698303223}, {"doc_source": "concepts/runnables.mdx", "score": 1.0823341608047485}, {"doc_source": "concepts/runnables.mdx", "score": 1.111817479133606}, {"doc_source": "concepts/runnables.mdx", "score": 1.124255895614624}, {"doc_source": "concepts/runnables.mdx", "score": 1.1606857776641846}, {"doc_source": "concepts/runnables.mdx", "score": 1.1609505414962769}, {"doc_source": "concepts/runnables.mdx", "score": 1.1710301637649536}, {"doc_source": "concepts/runnables.mdx", "score": 1.1771397590637207}, {"doc_source": "concepts/runnables.mdx", "score": 1.1943830251693726}, {"doc_source": "concepts/runnables.mdx", "score": 1.1971511840820312}, {"doc_source": "concepts/runnables.mdx", "score": 1.1978964805603027}, {"doc_source": "concepts/runnables.mdx", "score": 1.2038519382476807}, {"doc_source": "concepts/runnables.mdx", "score": 1.213449478149414}]}
{"ts": 1747974956.836232, "query_id": "Q058", "question": "Compare `batch` and `batch_as_completed` when processing multiple inputs.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.82097989320755}, {"doc_source": "concepts/runnables.mdx", "score": 0.8490561842918396}, {"doc_source": "concepts/runnables.mdx", "score": 1.0545458793640137}, {"doc_source": "concepts/runnables.mdx", "score": 1.115868091583252}, {"doc_source": "concepts/lcel.mdx", "score": 1.2787196636199951}, {"doc_source": "concepts/streaming.mdx", "score": 1.2967575788497925}, {"doc_source": "concepts/runnables.mdx", "score": 1.3136005401611328}, {"doc_source": "concepts/streaming.mdx", "score": 1.325455665588379}, {"doc_source": "concepts/runnables.mdx", "score": 1.345906138420105}, {"doc_source": "concepts/streaming.mdx", "score": 1.3510081768035889}, {"doc_source": "concepts/async.mdx", "score": 1.3605118989944458}, {"doc_source": "concepts/streaming.mdx", "score": 1.370046854019165}, {"doc_source": "concepts/runnables.mdx", "score": 1.3706135749816895}, {"doc_source": "concepts/lcel.mdx", "score": 1.3869820833206177}, {"doc_source": "concepts/streaming.mdx", "score": 1.3982813358306885}]}
{"ts": 1747974958.402126, "query_id": "Q059", "question": "How can LangChain assist with evaluating generated answers automatically?", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8681085109710693}, {"doc_source": "concepts/evaluation.mdx", "score": 0.9115369915962219}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9273431301116943}, {"doc_source": "how_to/index.mdx", "score": 0.9304296374320984}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9342418909072876}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.941401481628418}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9677556157112122}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9747974276542664}, {"doc_source": "how_to/index.mdx", "score": 0.993353545665741}, {"doc_source": "how_to/index.mdx", "score": 1.0075433254241943}, {"doc_source": "introduction.mdx", "score": 1.0214751958847046}, {"doc_source": "how_to/index.mdx", "score": 1.0237374305725098}, {"doc_source": "introduction.mdx", "score": 1.0372201204299927}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0457044839859009}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0484427213668823}]}
{"ts": 1747974960.113821, "query_id": "Q060", "question": "Provide two metrics supported by LangChain's built in evaluation module.", "retrieved_items": [{"doc_source": "concepts/evaluation.mdx", "score": 0.8831498026847839}, {"doc_source": "how_to/index.mdx", "score": 0.8950956463813782}, {"doc_source": "tutorials/index.mdx", "score": 0.9040983319282532}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9286414980888367}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9697948694229126}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9783912897109985}, {"doc_source": "how_to/index.mdx", "score": 0.9851574897766113}, {"doc_source": "concepts/streaming.mdx", "score": 1.0263952016830444}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0552783012390137}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0677473545074463}, {"doc_source": "concepts/index.mdx", "score": 1.0679476261138916}, {"doc_source": "how_to/index.mdx", "score": 1.0760440826416016}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0782742500305176}, {"doc_source": "concepts/architecture.mdx", "score": 1.0941060781478882}, {"doc_source": "concepts/lcel.mdx", "score": 1.1150919198989868}]}
{"ts": 1747974960.785889, "query_id": "Q061", "question": "Which LangChain concept allows combining multiple Runnables with the `|` operator?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.6602378487586975}, {"doc_source": "concepts/runnables.mdx", "score": 0.7449436783790588}, {"doc_source": "concepts/lcel.mdx", "score": 0.7530964612960815}, {"doc_source": "concepts/lcel.mdx", "score": 0.763640284538269}, {"doc_source": "how_to/index.mdx", "score": 0.7690602540969849}, {"doc_source": "concepts/runnables.mdx", "score": 0.7852416038513184}, {"doc_source": "concepts/lcel.mdx", "score": 0.7863801717758179}, {"doc_source": "concepts/lcel.mdx", "score": 0.7923846244812012}, {"doc_source": "concepts/async.mdx", "score": 0.8220717906951904}, {"doc_source": "concepts/lcel.mdx", "score": 0.844659686088562}, {"doc_source": "concepts/runnables.mdx", "score": 0.8499869108200073}, {"doc_source": "concepts/streaming.mdx", "score": 0.8546198606491089}, {"doc_source": "concepts/streaming.mdx", "score": 0.8547351360321045}, {"doc_source": "concepts/lcel.mdx", "score": 0.8575356006622314}, {"doc_source": "concepts/runnables.mdx", "score": 0.8635910749435425}]}
{"ts": 1747974962.4259148, "query_id": "Q062", "question": "Explain how to set up tool streaming so that tool results are incrementally passed back to the model.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.9547368884086609}, {"doc_source": "concepts/streaming.mdx", "score": 0.9633723497390747}, {"doc_source": "concepts/streaming.mdx", "score": 1.0007637739181519}, {"doc_source": "concepts/streaming.mdx", "score": 1.0245555639266968}, {"doc_source": "concepts/streaming.mdx", "score": 1.0504353046417236}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0511970520019531}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0673325061798096}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0933940410614014}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1050078868865967}, {"doc_source": "how_to/index.mdx", "score": 1.1092802286148071}, {"doc_source": "concepts/streaming.mdx", "score": 1.1165802478790283}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1197713613510132}, {"doc_source": "concepts/streaming.mdx", "score": 1.1251208782196045}, {"doc_source": "how_to/index.mdx", "score": 1.1289114952087402}, {"doc_source": "how_to/index.mdx", "score": 1.134570837020874}]}
{"ts": 1747974963.76135, "query_id": "Q063", "question": "What is the benefit of using `RunnableLambda` over subclassing a Runnable?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8218463659286499}, {"doc_source": "concepts/runnables.mdx", "score": 0.971828818321228}, {"doc_source": "concepts/runnables.mdx", "score": 0.9909914135932922}, {"doc_source": "concepts/lcel.mdx", "score": 0.9929336309432983}, {"doc_source": "concepts/runnables.mdx", "score": 1.035704493522644}, {"doc_source": "concepts/runnables.mdx", "score": 1.0421428680419922}, {"doc_source": "concepts/runnables.mdx", "score": 1.0544610023498535}, {"doc_source": "concepts/lcel.mdx", "score": 1.0581932067871094}, {"doc_source": "concepts/runnables.mdx", "score": 1.082933783531189}, {"doc_source": "concepts/lcel.mdx", "score": 1.1029776334762573}, {"doc_source": "concepts/lcel.mdx", "score": 1.1064671277999878}, {"doc_source": "concepts/lcel.mdx", "score": 1.1222094297409058}, {"doc_source": "concepts/lcel.mdx", "score": 1.1645139455795288}, {"doc_source": "concepts/runnables.mdx", "score": 1.1718106269836426}, {"doc_source": "concepts/lcel.mdx", "score": 1.194213628768921}]}
{"ts": 1747974965.057239, "query_id": "Q064", "question": "Which two documents together explain how to build a streaming question answering chain with citations?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9883008003234863}, {"doc_source": "how_to/index.mdx", "score": 1.0464465618133545}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0591962337493896}, {"doc_source": "tutorials/index.mdx", "score": 1.0655301809310913}, {"doc_source": "how_to/index.mdx", "score": 1.0730433464050293}, {"doc_source": "concepts/rag.mdx", "score": 1.0772706270217896}, {"doc_source": "concepts/streaming.mdx", "score": 1.0912648439407349}, {"doc_source": "concepts/retrieval.mdx", "score": 1.094118595123291}, {"doc_source": "concepts/streaming.mdx", "score": 1.0957013368606567}, {"doc_source": "how_to/index.mdx", "score": 1.1048098802566528}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1073362827301025}, {"doc_source": "how_to/index.mdx", "score": 1.1311227083206177}, {"doc_source": "tutorials/index.mdx", "score": 1.1387202739715576}, {"doc_source": "concepts/streaming.mdx", "score": 1.156180500984192}, {"doc_source": "how_to/index.mdx", "score": 1.1588430404663086}]}
{"ts": 1747974967.124991, "query_id": "Q065", "question": "How do you measure BLEU score in LangChain evaluations?", "retrieved_items": [{"doc_source": "concepts/embedding_models.mdx", "score": 0.9988753795623779}, {"doc_source": "tutorials/index.mdx", "score": 1.0170812606811523}, {"doc_source": "concepts/evaluation.mdx", "score": 1.0848697423934937}, {"doc_source": "how_to/index.mdx", "score": 1.0970184803009033}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1301233768463135}, {"doc_source": "concepts/messages.mdx", "score": 1.1464849710464478}, {"doc_source": "tutorials/index.mdx", "score": 1.1493760347366333}, {"doc_source": "how_to/index.mdx", "score": 1.1501708030700684}, {"doc_source": "how_to/index.mdx", "score": 1.164204478263855}, {"doc_source": "how_to/index.mdx", "score": 1.1771526336669922}, {"doc_source": "introduction.mdx", "score": 1.1810085773468018}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1813998222351074}, {"doc_source": "concepts/lcel.mdx", "score": 1.191497802734375}, {"doc_source": "how_to/index.mdx", "score": 1.1945518255233765}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1955022811889648}]}
{"ts": 1747974968.3621628, "query_id": "Q066", "question": "What does the `ConversationBufferWindowMemory` keep in memory?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.160813570022583}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1822974681854248}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2317322492599487}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2466661930084229}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2741059064865112}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2775177955627441}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2916362285614014}, {"doc_source": "concepts/messages.mdx", "score": 1.3124516010284424}, {"doc_source": "concepts/index.mdx", "score": 1.362526297569275}, {"doc_source": "concepts/index.mdx", "score": 1.38128662109375}, {"doc_source": "concepts/messages.mdx", "score": 1.4070820808410645}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4087013006210327}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4142076969146729}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.414412260055542}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4158304929733276}]}
{"ts": 1747974969.705237, "query_id": "Q067", "question": "Name a loader designed specifically for PDF files in LangChain.", "retrieved_items": [{"doc_source": "concepts/document_loaders.mdx", "score": 0.964363694190979}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.008927583694458}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0582906007766724}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.094031572341919}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0985198020935059}, {"doc_source": "how_to/index.mdx", "score": 1.1510967016220093}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.1550931930541992}, {"doc_source": "concepts/index.mdx", "score": 1.1572778224945068}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1970720291137695}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.2025426626205444}, {"doc_source": "concepts/retrievers.mdx", "score": 1.204717993736267}, {"doc_source": "concepts/lcel.mdx", "score": 1.2074899673461914}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2296643257141113}, {"doc_source": "how_to/index.mdx", "score": 1.2303788661956787}, {"doc_source": "introduction.mdx", "score": 1.2351627349853516}]}
{"ts": 1747974970.9907541, "query_id": "Q068", "question": "Which concept guide covers the standard streaming APIs exposed by Runnables?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.7736462354660034}, {"doc_source": "concepts/runnables.mdx", "score": 0.8441465497016907}, {"doc_source": "concepts/runnables.mdx", "score": 0.8498862981796265}, {"doc_source": "concepts/runnables.mdx", "score": 0.8666174411773682}, {"doc_source": "concepts/streaming.mdx", "score": 0.8680585622787476}, {"doc_source": "concepts/runnables.mdx", "score": 0.8796613812446594}, {"doc_source": "concepts/streaming.mdx", "score": 0.890842854976654}, {"doc_source": "concepts/runnables.mdx", "score": 0.9343417882919312}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9432806372642517}, {"doc_source": "concepts/index.mdx", "score": 0.9438942670822144}, {"doc_source": "concepts/streaming.mdx", "score": 0.9541480541229248}, {"doc_source": "concepts/streaming.mdx", "score": 0.9877237677574158}, {"doc_source": "concepts/lcel.mdx", "score": 1.003112554550171}, {"doc_source": "concepts/streaming.mdx", "score": 1.0087604522705078}, {"doc_source": "concepts/streaming.mdx", "score": 1.0172847509384155}]}
{"ts": 1747974972.198805, "query_id": "Q069", "question": "What is the advantage of using `LangServe` with Runnables?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8216073513031006}, {"doc_source": "concepts/lcel.mdx", "score": 0.8781896233558655}, {"doc_source": "concepts/runnables.mdx", "score": 0.892242431640625}, {"doc_source": "concepts/runnables.mdx", "score": 0.9144890904426575}, {"doc_source": "concepts/runnables.mdx", "score": 0.9168344736099243}, {"doc_source": "concepts/lcel.mdx", "score": 0.9368910789489746}, {"doc_source": "concepts/lcel.mdx", "score": 0.9406706094741821}, {"doc_source": "concepts/runnables.mdx", "score": 0.959851861000061}, {"doc_source": "concepts/async.mdx", "score": 0.9738438129425049}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9828202128410339}, {"doc_source": "concepts/architecture.mdx", "score": 0.997633695602417}, {"doc_source": "concepts/runnables.mdx", "score": 1.0009424686431885}, {"doc_source": "concepts/index.mdx", "score": 1.0071794986724854}, {"doc_source": "concepts/async.mdx", "score": 1.030613660812378}, {"doc_source": "concepts/runnables.mdx", "score": 1.0316195487976074}]}
{"ts": 1747974974.6898172, "query_id": "Q070", "question": "Explain how ChatModels support multimodal inputs.", "retrieved_items": [{"doc_source": "concepts/multimodality.mdx", "score": 0.591133713722229}, {"doc_source": "concepts/multimodality.mdx", "score": 0.6077048778533936}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7256899476051331}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7810487151145935}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7820441722869873}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7924947738647461}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8061254024505615}, {"doc_source": "concepts/chat_models.mdx", "score": 0.839788556098938}, {"doc_source": "concepts/tokens.mdx", "score": 0.8889898061752319}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8930299878120422}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8985527157783508}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9237212538719177}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9428342580795288}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9541950225830078}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9543265104293823}]}
{"ts": 1747974976.171158, "query_id": "Q071", "question": "What two steps are recommended to recover from ratelimiting errors?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.824700117111206}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9089843034744263}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1987733840942383}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2090424299240112}, {"doc_source": "concepts/retrieval.mdx", "score": 1.4205083847045898}, {"doc_source": "concepts/runnables.mdx", "score": 1.446563959121704}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4578031301498413}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4748451709747314}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.4796034097671509}, {"doc_source": "concepts/rag.mdx", "score": 1.5077643394470215}, {"doc_source": "concepts/retrieval.mdx", "score": 1.5166168212890625}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.525590181350708}, {"doc_source": "concepts/runnables.mdx", "score": 1.5352545976638794}, {"doc_source": "concepts/streaming.mdx", "score": 1.5451881885528564}, {"doc_source": "how_to/index.mdx", "score": 1.5489709377288818}]}
{"ts": 1747974977.252707, "query_id": "Q072", "question": "Which parameter allows a Runnable to stream events as JSON for UIs?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 1.0529756546020508}, {"doc_source": "concepts/streaming.mdx", "score": 1.066117286682129}, {"doc_source": "concepts/streaming.mdx", "score": 1.0809471607208252}, {"doc_source": "concepts/streaming.mdx", "score": 1.0862126350402832}, {"doc_source": "concepts/runnables.mdx", "score": 1.089738368988037}, {"doc_source": "concepts/runnables.mdx", "score": 1.1111892461776733}, {"doc_source": "concepts/runnables.mdx", "score": 1.125075101852417}, {"doc_source": "concepts/streaming.mdx", "score": 1.1306912899017334}, {"doc_source": "concepts/streaming.mdx", "score": 1.1317343711853027}, {"doc_source": "concepts/streaming.mdx", "score": 1.1325639486312866}, {"doc_source": "concepts/runnables.mdx", "score": 1.1376022100448608}, {"doc_source": "concepts/streaming.mdx", "score": 1.1416127681732178}, {"doc_source": "concepts/runnables.mdx", "score": 1.1419658660888672}, {"doc_source": "concepts/runnables.mdx", "score": 1.1532281637191772}, {"doc_source": "concepts/streaming.mdx", "score": 1.1546235084533691}]}
{"ts": 1747974978.895061, "query_id": "Q073", "question": "Provide a code snippet to embed text with a local HuggingFace model.", "retrieved_items": [{"doc_source": "concepts/embedding_models.mdx", "score": 1.0070794820785522}, {"doc_source": "how_to/embed_text.mdx", "score": 1.0275285243988037}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0612870454788208}, {"doc_source": "how_to/embed_text.mdx", "score": 1.0632615089416504}, {"doc_source": "concepts/multimodality.mdx", "score": 1.071363925933838}, {"doc_source": "concepts/multimodality.mdx", "score": 1.0833580493927002}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1117682456970215}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1272904872894287}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1460673809051514}, {"doc_source": "how_to/index.mdx", "score": 1.1546322107315063}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1575590372085571}, {"doc_source": "how_to/embed_text.mdx", "score": 1.163488507270813}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1648313999176025}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1771190166473389}, {"doc_source": "concepts/multimodality.mdx", "score": 1.180063247680664}]}
{"ts": 1747974980.460298, "query_id": "Q074", "question": "How does the `ContextualCompressionRetriever` reduce context windows?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 1.1378002166748047}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2801166772842407}, {"doc_source": "how_to/index.mdx", "score": 1.300673484802246}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3162928819656372}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3545430898666382}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.3702077865600586}, {"doc_source": "concepts/rag.mdx", "score": 1.4050337076187134}, {"doc_source": "concepts/rag.mdx", "score": 1.40959894657135}, {"doc_source": "concepts/rag.mdx", "score": 1.4117631912231445}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4130438566207886}, {"doc_source": "concepts/retrieval.mdx", "score": 1.423211932182312}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4324990510940552}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4369711875915527}, {"doc_source": "concepts/lcel.mdx", "score": 1.4374020099639893}, {"doc_source": "concepts/retrievers.mdx", "score": 1.4433979988098145}]}
{"ts": 1747974982.1317031, "query_id": "Q075", "question": "What guide should you consult for configuring runtime chain internals like temperature?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 1.1227996349334717}, {"doc_source": "concepts/runnables.mdx", "score": 1.1303682327270508}, {"doc_source": "how_to/index.mdx", "score": 1.1874008178710938}, {"doc_source": "tutorials/index.mdx", "score": 1.2215877771377563}, {"doc_source": "concepts/runnables.mdx", "score": 1.25238835811615}, {"doc_source": "how_to/index.mdx", "score": 1.2876183986663818}, {"doc_source": "concepts/runnables.mdx", "score": 1.3005759716033936}, {"doc_source": "how_to/index.mdx", "score": 1.3015141487121582}, {"doc_source": "how_to/index.mdx", "score": 1.3058210611343384}, {"doc_source": "introduction.mdx", "score": 1.3089969158172607}, {"doc_source": "how_to/index.mdx", "score": 1.3107244968414307}, {"doc_source": "concepts/streaming.mdx", "score": 1.3145726919174194}, {"doc_source": "introduction.mdx", "score": 1.3161388635635376}, {"doc_source": "concepts/tracing.mdx", "score": 1.3178154230117798}, {"doc_source": "how_to/index.mdx", "score": 1.322198748588562}]}
{"ts": 1747974983.704963, "query_id": "Q076", "question": "Which selector uses Maximal Marginal Relevance (MMR) to diversify examples?", "retrieved_items": [{"doc_source": "concepts/vectorstores.mdx", "score": 0.7102353572845459}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8871579170227051}, {"doc_source": "how_to/index.mdx", "score": 0.9365390539169312}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.0197370052337646}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.162663459777832}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1941781044006348}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2188851833343506}, {"doc_source": "how_to/index.mdx", "score": 1.2220258712768555}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2280943393707275}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.232224941253662}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2452598810195923}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2603269815444946}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2618411779403687}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2637079954147339}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2684409618377686}]}
{"ts": 1747974984.9659572, "query_id": "Q077", "question": "What is the recommended way to expose tool schemas to chat models that use OpenAI function calling?", "retrieved_items": [{"doc_source": "concepts/structured_outputs.mdx", "score": 0.6849903464317322}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.69837486743927}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7261172533035278}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8094043135643005}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8496934175491333}, {"doc_source": "concepts/tools.mdx", "score": 0.8521112203598022}, {"doc_source": "how_to/index.mdx", "score": 0.8918576836585999}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.8965787887573242}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8992609977722168}, {"doc_source": "concepts/index.mdx", "score": 0.9177855253219604}, {"doc_source": "concepts/tools.mdx", "score": 0.9386706948280334}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.953417956829071}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.9600339531898499}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.9619027972221375}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9715837240219116}]}
{"ts": 1747974986.9176362, "query_id": "Q078", "question": "Explain the advantage of using `batch_as_completed` for long running tasks.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8250684142112732}, {"doc_source": "concepts/runnables.mdx", "score": 0.9213829040527344}, {"doc_source": "concepts/runnables.mdx", "score": 1.0497732162475586}, {"doc_source": "concepts/runnables.mdx", "score": 1.1381616592407227}, {"doc_source": "concepts/lcel.mdx", "score": 1.1977016925811768}, {"doc_source": "concepts/runnables.mdx", "score": 1.2184693813323975}, {"doc_source": "concepts/lcel.mdx", "score": 1.3098376989364624}, {"doc_source": "concepts/async.mdx", "score": 1.3255894184112549}, {"doc_source": "concepts/chat_models.mdx", "score": 1.328920841217041}, {"doc_source": "concepts/async.mdx", "score": 1.3401374816894531}, {"doc_source": "concepts/streaming.mdx", "score": 1.344408392906189}, {"doc_source": "concepts/runnables.mdx", "score": 1.3516566753387451}, {"doc_source": "concepts/async.mdx", "score": 1.3658255338668823}, {"doc_source": "concepts/async.mdx", "score": 1.3686505556106567}, {"doc_source": "concepts/streaming.mdx", "score": 1.3821678161621094}]}
{"ts": 1747974989.2822342, "query_id": "Q079", "question": "Which two files outline strategies to trim messages to fit the context window?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 0.913860559463501}, {"doc_source": "concepts/chat_history.mdx", "score": 1.0845845937728882}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1432743072509766}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1763439178466797}, {"doc_source": "how_to/index.mdx", "score": 1.193688154220581}, {"doc_source": "concepts/index.mdx", "score": 1.2073917388916016}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2154500484466553}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2177214622497559}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2744289636611938}, {"doc_source": "concepts/index.mdx", "score": 1.28046715259552}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2915184497833252}, {"doc_source": "concepts/rag.mdx", "score": 1.2934471368789673}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2958765029907227}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3059769868850708}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3158063888549805}]}
{"ts": 1747974990.483092, "query_id": "Q080", "question": "What does the `max_concurrency` field not control when using a ChatModel?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 1.115612268447876}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1199140548706055}, {"doc_source": "concepts/runnables.mdx", "score": 1.1439998149871826}, {"doc_source": "concepts/chat_models.mdx", "score": 1.162825107574463}, {"doc_source": "concepts/chat_models.mdx", "score": 1.174802541732788}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1938117742538452}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2033753395080566}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2166136503219604}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2186038494110107}, {"doc_source": "concepts/streaming.mdx", "score": 1.2202670574188232}, {"doc_source": "concepts/chat_models.mdx", "score": 1.245578408241272}, {"doc_source": "concepts/index.mdx", "score": 1.25266432762146}, {"doc_source": "concepts/messages.mdx", "score": 1.2626831531524658}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2752513885498047}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2826340198516846}]}
{"ts": 1747974992.394871, "query_id": "Q081", "question": "Which document explains how to call tools in parallel with OpenAI?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9732374548912048}, {"doc_source": "how_to/index.mdx", "score": 1.0294321775436401}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.036596655845642}, {"doc_source": "how_to/index.mdx", "score": 1.041416883468628}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0445315837860107}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1009397506713867}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.11583411693573}, {"doc_source": "how_to/index.mdx", "score": 1.1249699592590332}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.137894630432129}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1441597938537598}, {"doc_source": "concepts/chat_models.mdx", "score": 1.149307131767273}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1500037908554077}, {"doc_source": "how_to/index.mdx", "score": 1.1634807586669922}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1642918586730957}, {"doc_source": "concepts/rag.mdx", "score": 1.168946623802185}]}
{"ts": 1747974993.776385, "query_id": "Q082", "question": "Describe how `RunnableGenerator` differs from `RunnableLambda`.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6835772395133972}, {"doc_source": "concepts/runnables.mdx", "score": 0.9616150856018066}, {"doc_source": "concepts/lcel.mdx", "score": 0.983863353729248}, {"doc_source": "concepts/runnables.mdx", "score": 1.0026142597198486}, {"doc_source": "concepts/runnables.mdx", "score": 1.034669041633606}, {"doc_source": "concepts/runnables.mdx", "score": 1.0363562107086182}, {"doc_source": "concepts/lcel.mdx", "score": 1.052700161933899}, {"doc_source": "concepts/lcel.mdx", "score": 1.0553299188613892}, {"doc_source": "concepts/lcel.mdx", "score": 1.0570900440216064}, {"doc_source": "concepts/runnables.mdx", "score": 1.0615851879119873}, {"doc_source": "concepts/runnables.mdx", "score": 1.0736385583877563}, {"doc_source": "concepts/lcel.mdx", "score": 1.107618808746338}, {"doc_source": "concepts/lcel.mdx", "score": 1.1096677780151367}, {"doc_source": "concepts/lcel.mdx", "score": 1.1183624267578125}, {"doc_source": "concepts/runnables.mdx", "score": 1.1211521625518799}]}
{"ts": 1747974995.8582542, "query_id": "Q083", "question": "Which vector store integration supports time weighted relevance?", "retrieved_items": [{"doc_source": "concepts/vectorstores.mdx", "score": 0.8826102018356323}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9357544183731079}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9397262334823608}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.9585659503936768}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9684504270553589}, {"doc_source": "concepts/retrieval.mdx", "score": 0.995840847492218}, {"doc_source": "how_to/index.mdx", "score": 1.0102925300598145}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0258868932724}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0401456356048584}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.0477368831634521}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0540452003479004}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0792102813720703}, {"doc_source": "how_to/index.mdx", "score": 1.1047005653381348}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1079487800598145}, {"doc_source": "how_to/index.mdx", "score": 1.1222803592681885}]}
{"ts": 1747974996.827751, "query_id": "Q084", "question": "Explain how to use `run_name` for easier debugging in LangSmith.", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.9924205541610718}, {"doc_source": "concepts/runnables.mdx", "score": 1.0108774900436401}, {"doc_source": "concepts/tracing.mdx", "score": 1.0632951259613037}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0857864618301392}, {"doc_source": "how_to/index.mdx", "score": 1.123980164527893}, {"doc_source": "concepts/runnables.mdx", "score": 1.150014877319336}, {"doc_source": "how_to/index.mdx", "score": 1.1598033905029297}, {"doc_source": "how_to/index.mdx", "score": 1.165876030921936}, {"doc_source": "concepts/architecture.mdx", "score": 1.1748327016830444}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1816563606262207}, {"doc_source": "concepts/runnables.mdx", "score": 1.1862512826919556}, {"doc_source": "concepts/async.mdx", "score": 1.2065379619598389}, {"doc_source": "concepts/lcel.mdx", "score": 1.2086435556411743}, {"doc_source": "concepts/index.mdx", "score": 1.2215471267700195}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2332805395126343}]}
{"ts": 1747974998.694422, "query_id": "Q085", "question": "Which two parameters must be provided to initialize `AzureAIDocumentIntelligenceLoader`?", "retrieved_items": [{"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.7358884215354919}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.7365229725837708}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.9850349426269531}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2548326253890991}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.2595810890197754}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.2811603546142578}, {"doc_source": "how_to/index.mdx", "score": 1.288942813873291}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.3017373085021973}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3121638298034668}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3186588287353516}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3863654136657715}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3887436389923096}, {"doc_source": "how_to/index.mdx", "score": 1.3905596733093262}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4037199020385742}, {"doc_source": "concepts/chat_models.mdx", "score": 1.408204436302185}]}
{"ts": 1747975000.176299, "query_id": "Q086", "question": "What is the main benefit of `asam_stream` events over `astream`?", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.955558717250824}, {"doc_source": "concepts/streaming.mdx", "score": 1.0628275871276855}, {"doc_source": "concepts/streaming.mdx", "score": 1.0772496461868286}, {"doc_source": "concepts/streaming.mdx", "score": 1.1196436882019043}, {"doc_source": "concepts/streaming.mdx", "score": 1.1569956541061401}, {"doc_source": "concepts/runnables.mdx", "score": 1.1887763738632202}, {"doc_source": "concepts/streaming.mdx", "score": 1.2051007747650146}, {"doc_source": "concepts/streaming.mdx", "score": 1.218480110168457}, {"doc_source": "concepts/streaming.mdx", "score": 1.2509089708328247}, {"doc_source": "concepts/chat_models.mdx", "score": 1.277359962463379}, {"doc_source": "concepts/streaming.mdx", "score": 1.297076940536499}, {"doc_source": "concepts/streaming.mdx", "score": 1.3099709749221802}, {"doc_source": "concepts/streaming.mdx", "score": 1.3140840530395508}, {"doc_source": "concepts/streaming.mdx", "score": 1.3215230703353882}, {"doc_source": "concepts/streaming.mdx", "score": 1.3378874063491821}]}
{"ts": 1747975002.7235892, "query_id": "Q087", "question": "How do you specify hidden arguments in a Tool so the model cannot set them?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.985377311706543}, {"doc_source": "concepts/tools.mdx", "score": 0.9980116486549377}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2024836540222168}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2199339866638184}, {"doc_source": "how_to/index.mdx", "score": 1.229870319366455}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.264965295791626}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.276423692703247}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2930829524993896}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2978734970092773}, {"doc_source": "concepts/tools.mdx", "score": 1.3129603862762451}, {"doc_source": "concepts/tools.mdx", "score": 1.3139734268188477}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3191628456115723}, {"doc_source": "how_to/index.mdx", "score": 1.326146125793457}, {"doc_source": "concepts/tools.mdx", "score": 1.3357146978378296}, {"doc_source": "concepts/tools.mdx", "score": 1.341621994972229}]}
{"ts": 1747975004.399035, "query_id": "Q088", "question": "Which guide demonstrates building a SQL retriever over a large database?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.9645313024520874}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0184630155563354}, {"doc_source": "how_to/index.mdx", "score": 1.0300008058547974}, {"doc_source": "concepts/retrieval.mdx", "score": 1.045639991760254}, {"doc_source": "how_to/index.mdx", "score": 1.064558506011963}, {"doc_source": "how_to/index.mdx", "score": 1.0888372659683228}, {"doc_source": "how_to/index.mdx", "score": 1.0961384773254395}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1267627477645874}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1477768421173096}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1500608921051025}, {"doc_source": "how_to/index.mdx", "score": 1.1612540483474731}, {"doc_source": "concepts/retrieval.mdx", "score": 1.163454294204712}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1645228862762451}, {"doc_source": "how_to/index.mdx", "score": 1.1710789203643799}, {"doc_source": "concepts/retrieval.mdx", "score": 1.187834620475769}]}
{"ts": 1747975005.863362, "query_id": "Q089", "question": "Why is semantic caching unlikely to hit after the first chat turn?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7086412310600281}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8751028180122375}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2451294660568237}, {"doc_source": "concepts/chat_models.mdx", "score": 1.282818078994751}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2855405807495117}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3074755668640137}, {"doc_source": "concepts/chat_history.mdx", "score": 1.318603754043579}, {"doc_source": "concepts/chat_models.mdx", "score": 1.32029390335083}, {"doc_source": "concepts/chat_models.mdx", "score": 1.341087818145752}, {"doc_source": "concepts/chat_history.mdx", "score": 1.349226951599121}, {"doc_source": "concepts/messages.mdx", "score": 1.3566924333572388}, {"doc_source": "concepts/lcel.mdx", "score": 1.380624532699585}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.38926362991333}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3919273614883423}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3925552368164062}]}
{"ts": 1747975007.320276, "query_id": "Q090", "question": "Describe a use case for `RecursiveCharacterTextSplitter`.", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.8234882950782776}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9785364270210266}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9962007403373718}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0289008617401123}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0948752164840698}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.128408432006836}, {"doc_source": "how_to/index.mdx", "score": 1.172791600227356}, {"doc_source": "how_to/index.mdx", "score": 1.2021992206573486}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2223774194717407}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2507487535476685}, {"doc_source": "concepts/retrievers.mdx", "score": 1.329925537109375}, {"doc_source": "concepts/tokens.mdx", "score": 1.352319598197937}, {"doc_source": "concepts/retrieval.mdx", "score": 1.3956105709075928}, {"doc_source": "concepts/tokens.mdx", "score": 1.398669958114624}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.4065728187561035}]}
{"ts": 1747975009.0578392, "query_id": "Q091", "question": "Give two methods to stream output from a ChatModel.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.8136933445930481}, {"doc_source": "concepts/streaming.mdx", "score": 0.8411746025085449}, {"doc_source": "concepts/streaming.mdx", "score": 0.8452506065368652}, {"doc_source": "concepts/streaming.mdx", "score": 0.8515913486480713}, {"doc_source": "concepts/streaming.mdx", "score": 0.852590799331665}, {"doc_source": "concepts/streaming.mdx", "score": 0.9392237663269043}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9671313762664795}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9843171834945679}, {"doc_source": "concepts/streaming.mdx", "score": 1.0099973678588867}, {"doc_source": "concepts/streaming.mdx", "score": 1.014281988143921}, {"doc_source": "concepts/streaming.mdx", "score": 1.0316846370697021}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0570170879364014}, {"doc_source": "concepts/chat_models.mdx", "score": 1.059035062789917}, {"doc_source": "concepts/streaming.mdx", "score": 1.0634422302246094}, {"doc_source": "concepts/streaming.mdx", "score": 1.0653234720230103}]}
{"ts": 1747975010.633436, "query_id": "Q092", "question": "Which document teaches you to build a hybrid search retriever combining keyword and vector?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.7179340124130249}, {"doc_source": "concepts/retrieval.mdx", "score": 0.7419501543045044}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.7708753347396851}, {"doc_source": "concepts/retrievers.mdx", "score": 0.7862471342086792}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8332114815711975}, {"doc_source": "how_to/index.mdx", "score": 0.8712801933288574}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8726967573165894}, {"doc_source": "concepts/retrieval.mdx", "score": 0.8885312080383301}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9033358097076416}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9189666509628296}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.9291468858718872}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9617823362350464}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9674727916717529}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9733364582061768}, {"doc_source": "concepts/retrievers.mdx", "score": 0.977925717830658}]}
{"ts": 1747975011.8363059, "query_id": "Q093", "question": "What is the purpose of `annotated` type hints in tool schemas?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8371490836143494}, {"doc_source": "concepts/tools.mdx", "score": 0.916273295879364}, {"doc_source": "concepts/tools.mdx", "score": 1.0173289775848389}, {"doc_source": "concepts/tools.mdx", "score": 1.071599006652832}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.1164402961730957}, {"doc_source": "concepts/runnables.mdx", "score": 1.1681712865829468}, {"doc_source": "concepts/tools.mdx", "score": 1.2022013664245605}, {"doc_source": "concepts/tools.mdx", "score": 1.2135677337646484}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2294617891311646}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2331680059432983}, {"doc_source": "concepts/tools.mdx", "score": 1.2553740739822388}, {"doc_source": "concepts/runnables.mdx", "score": 1.2741104364395142}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.276098370552063}, {"doc_source": "concepts/tools.mdx", "score": 1.2924189567565918}, {"doc_source": "concepts/tools.mdx", "score": 1.2999508380889893}]}
{"ts": 1747975014.173148, "query_id": "Q094", "question": "Explain why caching chat model responses is challenging.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5506515502929688}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7896692752838135}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0335804224014282}, {"doc_source": "concepts/streaming.mdx", "score": 1.089475154876709}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0898255109786987}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1052684783935547}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1398513317108154}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1816279888153076}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1856756210327148}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1868174076080322}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1891241073608398}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1912211179733276}, {"doc_source": "concepts/streaming.mdx", "score": 1.19907808303833}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2129855155944824}, {"doc_source": "concepts/index.mdx", "score": 1.2161725759506226}]}
{"ts": 1747975015.9503748, "query_id": "Q095", "question": "Which two docs together explain trimming long documents before embedding?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.0125395059585571}, {"doc_source": "how_to/index.mdx", "score": 1.0350139141082764}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.05762779712677}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0660539865493774}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0733592510223389}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.098794937133789}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1393059492111206}, {"doc_source": "concepts/retrievers.mdx", "score": 1.148611307144165}, {"doc_source": "how_to/index.mdx", "score": 1.16908597946167}, {"doc_source": "how_to/index.mdx", "score": 1.19902503490448}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.204514503479004}, {"doc_source": "how_to/index.mdx", "score": 1.2303524017333984}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.237031102180481}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.2391890287399292}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.2593708038330078}]}
{"ts": 1747975016.9503849, "query_id": "Q096", "question": "What class enables you to get structured JSON output directly from LLMS without a parser?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 1.203226089477539}, {"doc_source": "how_to/index.mdx", "score": 1.2092795372009277}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2099170684814453}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2266242504119873}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2292051315307617}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2773706912994385}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2972767353057861}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2995624542236328}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.3052818775177002}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.320579171180725}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.3370251655578613}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3384498357772827}, {"doc_source": "concepts/streaming.mdx", "score": 1.3596287965774536}, {"doc_source": "concepts/index.mdx", "score": 1.374590277671814}, {"doc_source": "concepts/chat_models.mdx", "score": 1.383988857269287}]}
{"ts": 1747975018.5614238, "query_id": "Q097", "question": "Name three built in message field keys aside from `content`.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 1.0429661273956299}, {"doc_source": "concepts/messages.mdx", "score": 1.043351173400879}, {"doc_source": "concepts/messages.mdx", "score": 1.1519321203231812}, {"doc_source": "concepts/messages.mdx", "score": 1.1787586212158203}, {"doc_source": "concepts/messages.mdx", "score": 1.202880620956421}, {"doc_source": "concepts/messages.mdx", "score": 1.240966796875}, {"doc_source": "concepts/messages.mdx", "score": 1.2426143884658813}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2606168985366821}, {"doc_source": "concepts/messages.mdx", "score": 1.2740371227264404}, {"doc_source": "concepts/messages.mdx", "score": 1.3241559267044067}, {"doc_source": "concepts/index.mdx", "score": 1.334504246711731}, {"doc_source": "concepts/messages.mdx", "score": 1.3383617401123047}, {"doc_source": "how_to/index.mdx", "score": 1.3388519287109375}, {"doc_source": "concepts/messages.mdx", "score": 1.339458703994751}, {"doc_source": "concepts/multimodality.mdx", "score": 1.3423969745635986}]}
{"ts": 1747975020.698621, "query_id": "Q098", "question": "What is the first step when creating a custom Runnable for streaming transformations?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8271245956420898}, {"doc_source": "concepts/runnables.mdx", "score": 0.8743887543678284}, {"doc_source": "concepts/runnables.mdx", "score": 0.9603124856948853}, {"doc_source": "concepts/streaming.mdx", "score": 0.9715011715888977}, {"doc_source": "concepts/runnables.mdx", "score": 1.0115373134613037}, {"doc_source": "concepts/streaming.mdx", "score": 1.061959147453308}, {"doc_source": "concepts/runnables.mdx", "score": 1.071929693222046}, {"doc_source": "concepts/streaming.mdx", "score": 1.0781320333480835}, {"doc_source": "concepts/runnables.mdx", "score": 1.0874477624893188}, {"doc_source": "concepts/streaming.mdx", "score": 1.099658727645874}, {"doc_source": "concepts/runnables.mdx", "score": 1.103398323059082}, {"doc_source": "concepts/streaming.mdx", "score": 1.1198455095291138}, {"doc_source": "concepts/runnables.mdx", "score": 1.1215707063674927}, {"doc_source": "concepts/streaming.mdx", "score": 1.1279311180114746}, {"doc_source": "concepts/streaming.mdx", "score": 1.135395884513855}]}
{"ts": 1747975022.277627, "query_id": "Q099", "question": "How does LangChain differentiate official vs community chat model integrations?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5819493532180786}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7878804206848145}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7971385717391968}, {"doc_source": "how_to/index.mdx", "score": 0.8113199472427368}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8398417234420776}, {"doc_source": "concepts/architecture.mdx", "score": 0.8510037660598755}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8524792194366455}, {"doc_source": "concepts/messages.mdx", "score": 0.8693563342094421}, {"doc_source": "how_to/index.mdx", "score": 0.8958495855331421}, {"doc_source": "concepts/architecture.mdx", "score": 0.9032067060470581}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9358336329460144}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9359391927719116}, {"doc_source": "how_to/installation.mdx", "score": 0.9377875924110413}, {"doc_source": "introduction.mdx", "score": 0.9504354596138}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9531245231628418}]}
{"ts": 1747975025.178863, "query_id": "Q100", "question": "Which two sources explain creating example selectors based on semantic similarity?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9904781579971313}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.0083988904953003}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0918245315551758}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1134653091430664}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1526477336883545}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1698050498962402}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1756787300109863}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1820921897888184}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2273938655853271}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2343169450759888}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2362961769104004}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2375138998031616}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2407399415969849}, {"doc_source": "how_to/index.mdx", "score": 1.2455041408538818}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2587742805480957}]}
{"ts": 1747975026.512851, "query_id": "Q101", "question": "What is the impact of not propagating `RunnableConfig` in Python 3.10 async code?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6224480867385864}, {"doc_source": "concepts/callbacks.mdx", "score": 0.9561946392059326}, {"doc_source": "concepts/runnables.mdx", "score": 0.9745022654533386}, {"doc_source": "concepts/runnables.mdx", "score": 0.9800995588302612}, {"doc_source": "concepts/runnables.mdx", "score": 0.9939375519752502}, {"doc_source": "concepts/runnables.mdx", "score": 1.05778968334198}, {"doc_source": "concepts/async.mdx", "score": 1.0845422744750977}, {"doc_source": "concepts/runnables.mdx", "score": 1.1024236679077148}, {"doc_source": "concepts/async.mdx", "score": 1.113790512084961}, {"doc_source": "concepts/runnables.mdx", "score": 1.123300552368164}, {"doc_source": "concepts/runnables.mdx", "score": 1.145546317100525}, {"doc_source": "concepts/tools.mdx", "score": 1.1513805389404297}, {"doc_source": "concepts/runnables.mdx", "score": 1.173658847808838}, {"doc_source": "concepts/runnables.mdx", "score": 1.188768982887268}, {"doc_source": "concepts/async.mdx", "score": 1.2118910551071167}]}
{"ts": 1747975028.284987, "query_id": "Q102", "question": "Describe the `@tool` option to hide a parameter while still injecting it at runtime.", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.832332193851471}, {"doc_source": "concepts/tools.mdx", "score": 0.883343517780304}, {"doc_source": "concepts/tools.mdx", "score": 1.03164803981781}, {"doc_source": "concepts/tools.mdx", "score": 1.0827676057815552}, {"doc_source": "concepts/tools.mdx", "score": 1.0923337936401367}, {"doc_source": "how_to/index.mdx", "score": 1.0972644090652466}, {"doc_source": "concepts/tools.mdx", "score": 1.1012494564056396}, {"doc_source": "concepts/tools.mdx", "score": 1.1308573484420776}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1611100435256958}, {"doc_source": "concepts/tools.mdx", "score": 1.1794072389602661}, {"doc_source": "concepts/tools.mdx", "score": 1.1906399726867676}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1924761533737183}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.223408579826355}, {"doc_source": "concepts/tools.mdx", "score": 1.2354609966278076}, {"doc_source": "concepts/tools.mdx", "score": 1.2503437995910645}]}
{"ts": 1747975030.3813891, "query_id": "Q103", "question": "Which notebook illustrates using an ensemble retriever strategy?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.8428757190704346}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9554160237312317}, {"doc_source": "concepts/retrievers.mdx", "score": 1.034416913986206}, {"doc_source": "how_to/index.mdx", "score": 1.0521678924560547}, {"doc_source": "concepts/retrievers.mdx", "score": 1.12274169921875}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1300461292266846}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1383275985717773}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1464548110961914}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1483328342437744}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1704511642456055}, {"doc_source": "how_to/index.mdx", "score": 1.1886812448501587}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1961408853530884}, {"doc_source": "concepts/retrieval.mdx", "score": 1.197824478149414}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2063908576965332}, {"doc_source": "concepts/rag.mdx", "score": 1.2147248983383179}]}
{"ts": 1747975033.952596, "query_id": "Q104", "question": "Give two benefits of the LangChain Expression Language (LCEL).", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.654735803604126}, {"doc_source": "concepts/lcel.mdx", "score": 0.7978137135505676}, {"doc_source": "concepts/lcel.mdx", "score": 0.7985596656799316}, {"doc_source": "concepts/lcel.mdx", "score": 0.8418781161308289}, {"doc_source": "concepts/lcel.mdx", "score": 0.8667476773262024}, {"doc_source": "concepts/lcel.mdx", "score": 0.877845287322998}, {"doc_source": "how_to/index.mdx", "score": 0.889362096786499}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9455296993255615}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9867230653762817}, {"doc_source": "concepts/lcel.mdx", "score": 0.9975361824035645}, {"doc_source": "concepts/lcel.mdx", "score": 1.014701008796692}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0167889595031738}, {"doc_source": "how_to/index.mdx", "score": 1.024009108543396}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0486652851104736}, {"doc_source": "concepts/index.mdx", "score": 1.0488210916519165}]}
{"ts": 1747975035.812258, "query_id": "Q105", "question": "What is the main purpose of the `rate_limiter` argument on ChatModels?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7170464396476746}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9848133325576782}, {"doc_source": "concepts/chat_models.mdx", "score": 0.986174464225769}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0061413049697876}, {"doc_source": "concepts/chat_models.mdx", "score": 1.129436731338501}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1587672233581543}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1820961236953735}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1940412521362305}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2085832357406616}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2093812227249146}, {"doc_source": "concepts/runnables.mdx", "score": 1.2114512920379639}, {"doc_source": "concepts/chat_models.mdx", "score": 1.22560715675354}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2279824018478394}, {"doc_source": "concepts/index.mdx", "score": 1.2291250228881836}, {"doc_source": "concepts/runnables.mdx", "score": 1.2291982173919678}]}
{"ts": 1747975037.491883, "query_id": "Q106", "question": "How many tokens is 100 English words approximately equal to?", "retrieved_items": [{"doc_source": "concepts/tokens.mdx", "score": 0.7300253510475159}, {"doc_source": "concepts/tokens.mdx", "score": 1.0408051013946533}, {"doc_source": "concepts/tokens.mdx", "score": 1.0993263721466064}, {"doc_source": "concepts/tokens.mdx", "score": 1.1049894094467163}, {"doc_source": "concepts/tokens.mdx", "score": 1.1921958923339844}, {"doc_source": "concepts/tokens.mdx", "score": 1.318519949913025}, {"doc_source": "concepts/index.mdx", "score": 1.335258960723877}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3486576080322266}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3925007581710815}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4089988470077515}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.4448885917663574}, {"doc_source": "concepts/retrievers.mdx", "score": 1.4545087814331055}, {"doc_source": "concepts/retrieval.mdx", "score": 1.4580717086791992}, {"doc_source": "concepts/retrieval.mdx", "score": 1.469346046447754}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4745664596557617}]}
{"ts": 1747975038.921356, "query_id": "Q107", "question": "Explain the difference between `invoke` and `ainvoke`.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.9344920516014099}, {"doc_source": "concepts/async.mdx", "score": 1.1374543905258179}, {"doc_source": "concepts/async.mdx", "score": 1.2069815397262573}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.212014079093933}, {"doc_source": "concepts/streaming.mdx", "score": 1.2388455867767334}, {"doc_source": "concepts/runnables.mdx", "score": 1.266967535018921}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2930909395217896}, {"doc_source": "concepts/tools.mdx", "score": 1.3333708047866821}, {"doc_source": "concepts/async.mdx", "score": 1.3371084928512573}, {"doc_source": "concepts/messages.mdx", "score": 1.3991920948028564}, {"doc_source": "concepts/async.mdx", "score": 1.400232195854187}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.4016433954238892}, {"doc_source": "concepts/tools.mdx", "score": 1.4109489917755127}, {"doc_source": "concepts/retrieval.mdx", "score": 1.4119601249694824}, {"doc_source": "concepts/tools.mdx", "score": 1.4124888181686401}]}
{"ts": 1747975040.4341838, "query_id": "Q108", "question": "What does `batch_as_completed` return with each result to help match inputs?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9440418481826782}, {"doc_source": "concepts/runnables.mdx", "score": 0.9989973306655884}, {"doc_source": "concepts/runnables.mdx", "score": 1.1420438289642334}, {"doc_source": "concepts/runnables.mdx", "score": 1.2197997570037842}, {"doc_source": "concepts/runnables.mdx", "score": 1.333064317703247}, {"doc_source": "concepts/lcel.mdx", "score": 1.3625636100769043}, {"doc_source": "concepts/streaming.mdx", "score": 1.3666044473648071}, {"doc_source": "concepts/index.mdx", "score": 1.3682470321655273}, {"doc_source": "concepts/runnables.mdx", "score": 1.3810688257217407}, {"doc_source": "concepts/streaming.mdx", "score": 1.3926225900650024}, {"doc_source": "concepts/async.mdx", "score": 1.4001233577728271}, {"doc_source": "concepts/runnables.mdx", "score": 1.4073666334152222}, {"doc_source": "concepts/streaming.mdx", "score": 1.4097381830215454}, {"doc_source": "concepts/runnables.mdx", "score": 1.4186362028121948}, {"doc_source": "concepts/streaming.mdx", "score": 1.420689344406128}]}
{"ts": 1747975042.7988591, "query_id": "Q109", "question": "Why might you choose Chroma over FAISS for prototyping?", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 0.9985740184783936}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3060898780822754}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3282228708267212}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3426599502563477}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3775732517242432}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3814351558685303}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.4028398990631104}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.4066612720489502}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.4098128080368042}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.4128831624984741}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.4198282957077026}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4267890453338623}, {"doc_source": "concepts/multimodality.mdx", "score": 1.4297932386398315}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.435242772102356}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.4381399154663086}]}
