{"ts": 1747974133.878081, "query_id": "Q000", "question": "What is LangChain and why was it created?", "retrieved_items": [{"doc_source": "concepts/why_langchain.mdx", "score": 0.6255886554718018}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7719470262527466}, {"doc_source": "introduction.mdx", "score": 0.8177490234375}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8628194332122803}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8777493834495544}, {"doc_source": "how_to/index.mdx", "score": 0.8919028639793396}, {"doc_source": "concepts/architecture.mdx", "score": 0.9002081751823425}, {"doc_source": "introduction.mdx", "score": 0.9269158840179443}, {"doc_source": "concepts/lcel.mdx", "score": 0.9294493198394775}, {"doc_source": "concepts/lcel.mdx", "score": 0.9354511499404907}, {"doc_source": "concepts/architecture.mdx", "score": 0.9384127259254456}, {"doc_source": "tutorials/index.mdx", "score": 0.9416078925132751}, {"doc_source": "concepts/async.mdx", "score": 0.959408164024353}, {"doc_source": "introduction.mdx", "score": 0.9649421572685242}, {"doc_source": "how_to/installation.mdx", "score": 0.9656673669815063}]}
{"ts": 1747974135.860291, "query_id": "Q001", "question": "Explain the role of an Agent in LangChain.", "retrieved_items": [{"doc_source": "concepts/agents.mdx", "score": 0.7623122930526733}, {"doc_source": "concepts/agents.mdx", "score": 0.857016921043396}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9448797702789307}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0088632106781006}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0099601745605469}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0204702615737915}, {"doc_source": "how_to/index.mdx", "score": 1.0226364135742188}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0419998168945312}, {"doc_source": "introduction.mdx", "score": 1.0462841987609863}, {"doc_source": "concepts/architecture.mdx", "score": 1.0866960287094116}, {"doc_source": "concepts/async.mdx", "score": 1.0927362442016602}, {"doc_source": "concepts/index.mdx", "score": 1.0976709127426147}, {"doc_source": "concepts/lcel.mdx", "score": 1.1029802560806274}, {"doc_source": "introduction.mdx", "score": 1.1087360382080078}, {"doc_source": "introduction.mdx", "score": 1.1181899309158325}]}
{"ts": 1747974137.9138849, "query_id": "Q002", "question": "How do I install LangChain with extras for tracing?", "retrieved_items": [{"doc_source": "how_to/installation.mdx", "score": 0.814586877822876}, {"doc_source": "concepts/async.mdx", "score": 0.8542864322662354}, {"doc_source": "how_to/installation.mdx", "score": 0.8703579902648926}, {"doc_source": "how_to/installation.mdx", "score": 0.8818719387054443}, {"doc_source": "how_to/installation.mdx", "score": 0.9088873267173767}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9152572154998779}, {"doc_source": "how_to/installation.mdx", "score": 0.9297798275947571}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9446476697921753}, {"doc_source": "tutorials/index.mdx", "score": 0.9472787976264954}, {"doc_source": "tutorials/index.mdx", "score": 0.9621576070785522}, {"doc_source": "how_to/index.mdx", "score": 0.9622718691825867}, {"doc_source": "how_to/index.mdx", "score": 0.9875659942626953}, {"doc_source": "concepts/tracing.mdx", "score": 0.9972443580627441}, {"doc_source": "concepts/architecture.mdx", "score": 0.9983574748039246}, {"doc_source": "introduction.mdx", "score": 1.0015761852264404}]}
{"ts": 1747974140.912297, "query_id": "Q003", "question": "Show the minimal LCEL chain that streams token-by-token.", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.5398178100585938}, {"doc_source": "concepts/streaming.mdx", "score": 0.7820838689804077}, {"doc_source": "concepts/lcel.mdx", "score": 0.9582654237747192}, {"doc_source": "how_to/index.mdx", "score": 0.9816068410873413}, {"doc_source": "concepts/lcel.mdx", "score": 0.9824652075767517}, {"doc_source": "concepts/lcel.mdx", "score": 0.9867833256721497}, {"doc_source": "concepts/lcel.mdx", "score": 1.012133002281189}, {"doc_source": "concepts/lcel.mdx", "score": 1.0125445127487183}, {"doc_source": "how_to/index.mdx", "score": 1.0161371231079102}, {"doc_source": "concepts/streaming.mdx", "score": 1.0313630104064941}, {"doc_source": "concepts/lcel.mdx", "score": 1.0473300218582153}, {"doc_source": "concepts/streaming.mdx", "score": 1.0570454597473145}, {"doc_source": "concepts/streaming.mdx", "score": 1.0826560258865356}, {"doc_source": "concepts/lcel.mdx", "score": 1.131955623626709}, {"doc_source": "concepts/streaming.mdx", "score": 1.139676570892334}]}
{"ts": 1747974142.26989, "query_id": "Q004", "question": "Compare FAISS and Chroma vector stores in LangChain.", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 0.8064562082290649}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0380516052246094}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.0540862083435059}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.054433822631836}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0763309001922607}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0955021381378174}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1127716302871704}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1493860483169556}, {"doc_source": "concepts/multimodality.mdx", "score": 1.15178644657135}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1759387254714966}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.178620457649231}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.197440505027771}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.2420120239257812}, {"doc_source": "concepts/lcel.mdx", "score": 1.2432668209075928}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.2593061923980713}]}
{"ts": 1747974143.776337, "query_id": "Q005", "question": "How can I load a Microsoft Word document into a LangChain pipeline?", "retrieved_items": [{"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.7628761529922485}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.8708082437515259}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.056182861328125}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0562770366668701}, {"doc_source": "concepts/index.mdx", "score": 1.057929277420044}, {"doc_source": "introduction.mdx", "score": 1.0636441707611084}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.0752602815628052}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0753333568572998}, {"doc_source": "tutorials/index.mdx", "score": 1.088841438293457}, {"doc_source": "how_to/index.mdx", "score": 1.0933682918548584}, {"doc_source": "tutorials/index.mdx", "score": 1.129634141921997}, {"doc_source": "tutorials/index.mdx", "score": 1.1384129524230957}, {"doc_source": "how_to/index.mdx", "score": 1.1432363986968994}, {"doc_source": "how_to/index.mdx", "score": 1.1451671123504639}, {"doc_source": "concepts/lcel.mdx", "score": 1.1454722881317139}]}
{"ts": 1747974145.362468, "query_id": "Q006", "question": "Describe the Runnable protocol and its advantages.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.7731239795684814}, {"doc_source": "concepts/runnables.mdx", "score": 0.8452415466308594}, {"doc_source": "concepts/runnables.mdx", "score": 0.8842644095420837}, {"doc_source": "concepts/runnables.mdx", "score": 0.9478476643562317}, {"doc_source": "concepts/runnables.mdx", "score": 0.9550008177757263}, {"doc_source": "concepts/lcel.mdx", "score": 0.9721733331680298}, {"doc_source": "concepts/runnables.mdx", "score": 1.0511798858642578}, {"doc_source": "concepts/lcel.mdx", "score": 1.0748101472854614}, {"doc_source": "concepts/runnables.mdx", "score": 1.076511263847351}, {"doc_source": "concepts/runnables.mdx", "score": 1.0790627002716064}, {"doc_source": "concepts/runnables.mdx", "score": 1.112937092781067}, {"doc_source": "concepts/lcel.mdx", "score": 1.1278302669525146}, {"doc_source": "concepts/lcel.mdx", "score": 1.1307921409606934}, {"doc_source": "concepts/runnables.mdx", "score": 1.1359281539916992}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1438543796539307}]}
{"ts": 1747974147.525944, "query_id": "Q007", "question": "What is Few-Shot prompting and how is it supported?", "retrieved_items": [{"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.6560158133506775}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.8974824547767639}, {"doc_source": "how_to/index.mdx", "score": 1.0831446647644043}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.0848637819290161}, {"doc_source": "concepts/index.mdx", "score": 1.088640570640564}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0988576412200928}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1179701089859009}, {"doc_source": "how_to/index.mdx", "score": 1.1325939893722534}, {"doc_source": "concepts/retrieval.mdx", "score": 1.182403802871704}, {"doc_source": "how_to/index.mdx", "score": 1.2043898105621338}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2044847011566162}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2127572298049927}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2261569499969482}, {"doc_source": "how_to/index.mdx", "score": 1.2301995754241943}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2342830896377563}]}
{"ts": 1747974148.948475, "query_id": "Q008", "question": "Give an example of a memory object that stores chat history.", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.0517735481262207}, {"doc_source": "concepts/chat_history.mdx", "score": 1.082876205444336}, {"doc_source": "concepts/chat_history.mdx", "score": 1.0965147018432617}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1069953441619873}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1656996011734009}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2141824960708618}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2293341159820557}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2297366857528687}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2390540838241577}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2466309070587158}, {"doc_source": "concepts/messages.mdx", "score": 1.254551887512207}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2550827264785767}, {"doc_source": "concepts/streaming.mdx", "score": 1.2615671157836914}, {"doc_source": "concepts/index.mdx", "score": 1.2663601636886597}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2691627740859985}]}
{"ts": 1747974150.8204858, "query_id": "Q009", "question": "Outline the steps to trace a LangGraph flow in LangSmith.", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8302746415138245}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8385640382766724}, {"doc_source": "how_to/index.mdx", "score": 0.8492603302001953}, {"doc_source": "concepts/tracing.mdx", "score": 0.9169788956642151}, {"doc_source": "how_to/index.mdx", "score": 0.9687942266464233}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9922584295272827}, {"doc_source": "introduction.mdx", "score": 1.040656566619873}, {"doc_source": "concepts/streaming.mdx", "score": 1.0460822582244873}, {"doc_source": "tutorials/index.mdx", "score": 1.069827675819397}, {"doc_source": "how_to/index.mdx", "score": 1.0903525352478027}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0954984426498413}, {"doc_source": "concepts/streaming.mdx", "score": 1.1068718433380127}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1459362506866455}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1465338468551636}, {"doc_source": "how_to/installation.mdx", "score": 1.168305516242981}]}
{"ts": 1747974152.18668, "query_id": "Q010", "question": "What packages must be pinned to use Pydantic v1 with LangChain?", "retrieved_items": [{"doc_source": "how_to/pydantic_compatibility.md", "score": 0.6664679646492004}, {"doc_source": "concepts/async.mdx", "score": 0.9409855604171753}, {"doc_source": "how_to/installation.mdx", "score": 0.9448832273483276}, {"doc_source": "how_to/installation.mdx", "score": 0.9478612542152405}, {"doc_source": "how_to/installation.mdx", "score": 0.951988935470581}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9901312589645386}, {"doc_source": "how_to/installation.mdx", "score": 0.9912832379341125}, {"doc_source": "how_to/installation.mdx", "score": 0.9953155517578125}, {"doc_source": "how_to/installation.mdx", "score": 1.0306077003479004}, {"doc_source": "concepts/architecture.mdx", "score": 1.0631096363067627}, {"doc_source": "concepts/architecture.mdx", "score": 1.1166257858276367}, {"doc_source": "concepts/runnables.mdx", "score": 1.150731086730957}, {"doc_source": "concepts/async.mdx", "score": 1.158193588256836}, {"doc_source": "introduction.mdx", "score": 1.166212797164917}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1730281114578247}]}
{"ts": 1747974153.8949928, "query_id": "Q011", "question": "Explain tool calling via OpenAI function messages in LangChain.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7406128644943237}, {"doc_source": "concepts/messages.mdx", "score": 0.7672620415687561}, {"doc_source": "concepts/testing.mdx", "score": 0.8310785293579102}, {"doc_source": "concepts/messages.mdx", "score": 0.854005753993988}, {"doc_source": "how_to/index.mdx", "score": 0.8562871217727661}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.864314079284668}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8911005258560181}, {"doc_source": "concepts/testing.mdx", "score": 0.8943495154380798}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9009040594100952}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9068634510040283}, {"doc_source": "concepts/messages.mdx", "score": 0.910158097743988}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9134863018989563}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9174975752830505}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9203602075576782}, {"doc_source": "concepts/tools.mdx", "score": 0.9251934289932251}]}
{"ts": 1747974157.8704078, "query_id": "Q012", "question": "How does Retrieval-Augmented Generation (RAG) work in LangChain?", "retrieved_items": [{"doc_source": "concepts/rag.mdx", "score": 0.5998701453208923}, {"doc_source": "concepts/rag.mdx", "score": 0.8095791339874268}, {"doc_source": "concepts/rag.mdx", "score": 0.9054796099662781}, {"doc_source": "how_to/index.mdx", "score": 0.9593743085861206}, {"doc_source": "how_to/index.mdx", "score": 0.961368978023529}, {"doc_source": "concepts/rag.mdx", "score": 0.9878777265548706}, {"doc_source": "concepts/retrieval.mdx", "score": 1.000842809677124}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0547475814819336}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0669705867767334}, {"doc_source": "concepts/rag.mdx", "score": 1.0728881359100342}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0946245193481445}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1005637645721436}, {"doc_source": "concepts/index.mdx", "score": 1.1044740676879883}, {"doc_source": "how_to/index.mdx", "score": 1.1064239740371704}, {"doc_source": "introduction.mdx", "score": 1.1092777252197266}]}
{"ts": 1747974159.787135, "query_id": "Q013", "question": "Provide a code snippet to create embeddings with OpenAI via LangChain.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 0.8076019287109375}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8200914859771729}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8585704565048218}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8882757425308228}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9253103733062744}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9357831478118896}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9413596391677856}, {"doc_source": "introduction.mdx", "score": 0.9481597542762756}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9868519306182861}, {"doc_source": "introduction.mdx", "score": 0.98773592710495}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0075700283050537}, {"doc_source": "how_to/index.mdx", "score": 1.0213619470596313}, {"doc_source": "how_to/installation.mdx", "score": 1.0268982648849487}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0315173864364624}, {"doc_source": "how_to/installation.mdx", "score": 1.032348394393921}]}
{"ts": 1747974161.904568, "query_id": "Q014", "question": "List three callback events emitted during an LLM call.", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 0.881537914276123}, {"doc_source": "concepts/callbacks.mdx", "score": 0.9973568916320801}, {"doc_source": "how_to/index.mdx", "score": 1.0638720989227295}, {"doc_source": "concepts/streaming.mdx", "score": 1.1637694835662842}, {"doc_source": "how_to/index.mdx", "score": 1.1762391328811646}, {"doc_source": "concepts/streaming.mdx", "score": 1.2179234027862549}, {"doc_source": "concepts/callbacks.mdx", "score": 1.2335350513458252}, {"doc_source": "how_to/index.mdx", "score": 1.2724765539169312}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2893606424331665}, {"doc_source": "concepts/lcel.mdx", "score": 1.2911416292190552}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2918074131011963}, {"doc_source": "concepts/callbacks.mdx", "score": 1.3093136548995972}, {"doc_source": "concepts/streaming.mdx", "score": 1.3112527132034302}, {"doc_source": "concepts/streaming.mdx", "score": 1.3123754262924194}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3158711194992065}]}
{"ts": 1747974164.019677, "query_id": "Q015", "question": "Why would you use the AsyncRunnable interface?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9023818969726562}, {"doc_source": "concepts/async.mdx", "score": 0.9287770986557007}, {"doc_source": "concepts/runnables.mdx", "score": 0.9564101696014404}, {"doc_source": "concepts/lcel.mdx", "score": 0.9973033666610718}, {"doc_source": "concepts/runnables.mdx", "score": 1.0056053400039673}, {"doc_source": "concepts/runnables.mdx", "score": 1.0640469789505005}, {"doc_source": "concepts/async.mdx", "score": 1.0716168880462646}, {"doc_source": "concepts/runnables.mdx", "score": 1.084078073501587}, {"doc_source": "concepts/runnables.mdx", "score": 1.0924113988876343}, {"doc_source": "concepts/async.mdx", "score": 1.1123627424240112}, {"doc_source": "concepts/runnables.mdx", "score": 1.112804651260376}, {"doc_source": "concepts/callbacks.mdx", "score": 1.1396353244781494}, {"doc_source": "concepts/streaming.mdx", "score": 1.1413309574127197}, {"doc_source": "concepts/runnables.mdx", "score": 1.1429204940795898}, {"doc_source": "concepts/runnables.mdx", "score": 1.1455062627792358}]}
{"ts": 1747974165.945929, "query_id": "Q016", "question": "What metrics does LangChain's evaluation module compute out-of-box?", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8662213087081909}, {"doc_source": "concepts/evaluation.mdx", "score": 0.9073658585548401}, {"doc_source": "how_to/index.mdx", "score": 0.9127787947654724}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9283957481384277}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9340823292732239}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9461000561714172}, {"doc_source": "how_to/index.mdx", "score": 0.9595882892608643}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9807580709457397}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9954663515090942}, {"doc_source": "how_to/index.mdx", "score": 1.0331342220306396}, {"doc_source": "concepts/lcel.mdx", "score": 1.0415709018707275}, {"doc_source": "concepts/streaming.mdx", "score": 1.0571372509002686}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0619819164276123}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0649542808532715}, {"doc_source": "how_to/index.mdx", "score": 1.0652847290039062}]}
{"ts": 1747974167.232102, "query_id": "Q017", "question": "Demonstrate how to chunk text using RecursiveCharacterTextSplitter.", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.8115847706794739}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9808012247085571}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.001142144203186}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0025250911712646}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0183744430541992}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0549973249435425}, {"doc_source": "how_to/index.mdx", "score": 1.079284429550171}, {"doc_source": "how_to/index.mdx", "score": 1.0962448120117188}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.151304006576538}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.168376088142395}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2846084833145142}, {"doc_source": "concepts/streaming.mdx", "score": 1.3952429294586182}, {"doc_source": "concepts/lcel.mdx", "score": 1.4133775234222412}, {"doc_source": "concepts/streaming.mdx", "score": 1.4186959266662598}, {"doc_source": "how_to/index.mdx", "score": 1.4395052194595337}]}
{"ts": 1747974169.305469, "query_id": "Q018", "question": "What environment variable controls OpenAI API retries in LangChain?", "retrieved_items": [{"doc_source": "concepts/architecture.mdx", "score": 1.1670194864273071}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1727871894836426}, {"doc_source": "concepts/async.mdx", "score": 1.1808714866638184}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1895170211791992}, {"doc_source": "concepts/async.mdx", "score": 1.2008659839630127}, {"doc_source": "how_to/installation.mdx", "score": 1.2059146165847778}, {"doc_source": "concepts/async.mdx", "score": 1.2173888683319092}, {"doc_source": "concepts/messages.mdx", "score": 1.2186334133148193}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2190780639648438}, {"doc_source": "concepts/async.mdx", "score": 1.2197818756103516}, {"doc_source": "concepts/async.mdx", "score": 1.2266381978988647}, {"doc_source": "how_to/installation.mdx", "score": 1.2281590700149536}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2283285856246948}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2381393909454346}, {"doc_source": "concepts/streaming.mdx", "score": 1.2400835752487183}]}
{"ts": 1747974172.455056, "query_id": "Q019", "question": "How can key-value stores be used to cache intermediate chain output?", "retrieved_items": [{"doc_source": "concepts/key_value_stores.mdx", "score": 0.9591304063796997}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.0031025409698486}, {"doc_source": "concepts/streaming.mdx", "score": 1.144041895866394}, {"doc_source": "concepts/lcel.mdx", "score": 1.1675883531570435}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.2633285522460938}, {"doc_source": "concepts/lcel.mdx", "score": 1.3031084537506104}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3144984245300293}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.31681489944458}, {"doc_source": "concepts/lcel.mdx", "score": 1.317382574081421}, {"doc_source": "concepts/lcel.mdx", "score": 1.3239097595214844}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.343010663986206}, {"doc_source": "concepts/lcel.mdx", "score": 1.3475465774536133}, {"doc_source": "concepts/lcel.mdx", "score": 1.3536181449890137}, {"doc_source": "concepts/streaming.mdx", "score": 1.3562471866607666}, {"doc_source": "how_to/index.mdx", "score": 1.3574310541152954}]}
{"ts": 1747974174.024553, "query_id": "Q020", "question": "Explain \"Multimodality\" support in LangChain with an example.", "retrieved_items": [{"doc_source": "concepts/multimodality.mdx", "score": 0.6335165500640869}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7257100343704224}, {"doc_source": "concepts/multimodality.mdx", "score": 0.8759865760803223}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9223558306694031}, {"doc_source": "concepts/tokens.mdx", "score": 0.9275288581848145}, {"doc_source": "concepts/index.mdx", "score": 0.9437615871429443}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9461684823036194}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9549509286880493}, {"doc_source": "how_to/index.mdx", "score": 0.9723312854766846}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.982714831829071}, {"doc_source": "concepts/lcel.mdx", "score": 0.9893240928649902}, {"doc_source": "how_to/index.mdx", "score": 0.9902994632720947}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9967695474624634}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.005495548248291}, {"doc_source": "concepts/streaming.mdx", "score": 1.0113331079483032}]}
{"ts": 1747974176.261452, "query_id": "Q021", "question": "What is the LangChain architecture diagram trying to convey?", "retrieved_items": [{"doc_source": "introduction.mdx", "score": 0.7984373569488525}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8224698305130005}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.836712658405304}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8461186289787292}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.865486741065979}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8841321468353271}, {"doc_source": "introduction.mdx", "score": 0.8892748355865479}, {"doc_source": "introduction.mdx", "score": 0.8935341835021973}, {"doc_source": "how_to/index.mdx", "score": 0.919353723526001}, {"doc_source": "concepts/lcel.mdx", "score": 0.929567277431488}, {"doc_source": "concepts/architecture.mdx", "score": 0.9335117340087891}, {"doc_source": "concepts/index.mdx", "score": 0.9397701025009155}, {"doc_source": "tutorials/index.mdx", "score": 0.9522937536239624}, {"doc_source": "concepts/architecture.mdx", "score": 0.9796682000160217}, {"doc_source": "concepts/index.mdx", "score": 0.9863183498382568}]}
{"ts": 1747974177.753537, "query_id": "Q022", "question": "Provide two example selectors and when to use them.", "retrieved_items": [{"doc_source": "concepts/example_selectors.mdx", "score": 0.8727792501449585}, {"doc_source": "how_to/index.mdx", "score": 1.054222583770752}, {"doc_source": "how_to/index.mdx", "score": 1.1764665842056274}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2127736806869507}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2196248769760132}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3216681480407715}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3476994037628174}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3491770029067993}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3768370151519775}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3860269784927368}, {"doc_source": "how_to/index.mdx", "score": 1.3998876810073853}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.4052965641021729}, {"doc_source": "concepts/index.mdx", "score": 1.413228154182434}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.4284143447875977}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.44089937210083}]}
{"ts": 1747974180.057884, "query_id": "Q023", "question": "How do OutputParsers simplify post-processing in LCEL?", "retrieved_items": [{"doc_source": "concepts/output_parsers.mdx", "score": 0.9453810453414917}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.9981114268302917}, {"doc_source": "concepts/lcel.mdx", "score": 1.018088698387146}, {"doc_source": "concepts/lcel.mdx", "score": 1.073350191116333}, {"doc_source": "how_to/index.mdx", "score": 1.090490698814392}, {"doc_source": "how_to/index.mdx", "score": 1.1125707626342773}, {"doc_source": "concepts/index.mdx", "score": 1.1886208057403564}, {"doc_source": "concepts/lcel.mdx", "score": 1.2001725435256958}, {"doc_source": "concepts/lcel.mdx", "score": 1.2074143886566162}, {"doc_source": "how_to/index.mdx", "score": 1.2140491008758545}, {"doc_source": "concepts/lcel.mdx", "score": 1.2309906482696533}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2362818717956543}, {"doc_source": "concepts/streaming.mdx", "score": 1.2390079498291016}, {"doc_source": "concepts/streaming.mdx", "score": 1.2601666450500488}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2674719095230103}]}
{"ts": 1747974181.747149, "query_id": "Q024", "question": "Describe streaming chunk callbacks versus token callbacks.", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 1.060326337814331}, {"doc_source": "concepts/streaming.mdx", "score": 1.064970850944519}, {"doc_source": "concepts/streaming.mdx", "score": 1.109907627105713}, {"doc_source": "concepts/streaming.mdx", "score": 1.1237585544586182}, {"doc_source": "concepts/streaming.mdx", "score": 1.133001685142517}, {"doc_source": "concepts/streaming.mdx", "score": 1.1373907327651978}, {"doc_source": "concepts/streaming.mdx", "score": 1.1375508308410645}, {"doc_source": "concepts/streaming.mdx", "score": 1.1506047248840332}, {"doc_source": "concepts/lcel.mdx", "score": 1.1528308391571045}, {"doc_source": "concepts/streaming.mdx", "score": 1.169683575630188}, {"doc_source": "concepts/streaming.mdx", "score": 1.1782993078231812}, {"doc_source": "concepts/runnables.mdx", "score": 1.1834464073181152}, {"doc_source": "concepts/callbacks.mdx", "score": 1.1853896379470825}, {"doc_source": "concepts/streaming.mdx", "score": 1.1943708658218384}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2007179260253906}]}
{"ts": 1747974183.310978, "query_id": "Q025", "question": "What is the purpose of the `ConversationBufferMemory` class and when should it be used?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.19468355178833}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2201353311538696}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2367615699768066}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2724394798278809}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3232970237731934}, {"doc_source": "concepts/chat_models.mdx", "score": 1.351617693901062}, {"doc_source": "concepts/messages.mdx", "score": 1.3561862707138062}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3739843368530273}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3750181198120117}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3799524307250977}, {"doc_source": "how_to/index.mdx", "score": 1.3878865242004395}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3899154663085938}, {"doc_source": "concepts/streaming.mdx", "score": 1.3923389911651611}, {"doc_source": "concepts/messages.mdx", "score": 1.4017157554626465}, {"doc_source": "concepts/index.mdx", "score": 1.403372049331665}]}
{"ts": 1747974184.90549, "query_id": "Q026", "question": "Which LangChain text splitter lets you split documents by tokens and why would you choose it over a character splitter?", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.661493182182312}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.7730017900466919}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.8015855550765991}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.8804944753646851}, {"doc_source": "concepts/tokens.mdx", "score": 0.8909444212913513}, {"doc_source": "concepts/lcel.mdx", "score": 0.9270370006561279}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9462966918945312}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9713072776794434}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9812499284744263}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.026100993156433}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0369690656661987}, {"doc_source": "concepts/index.mdx", "score": 1.0397673845291138}, {"doc_source": "introduction.mdx", "score": 1.0476574897766113}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.049213171005249}, {"doc_source": "how_to/index.mdx", "score": 1.057349443435669}]}
{"ts": 1747974187.15871, "query_id": "Q027", "question": "List two differences between `VectorStoreRetriever` and `EnsembleRetriever`.", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.7453950643539429}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8021279573440552}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8828607201576233}, {"doc_source": "how_to/index.mdx", "score": 1.0256764888763428}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0694477558135986}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.102748155593872}, {"doc_source": "concepts/retrievers.mdx", "score": 1.117579698562622}, {"doc_source": "how_to/index.mdx", "score": 1.135270118713379}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1537195444107056}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1575567722320557}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1668651103973389}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2473558187484741}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2545320987701416}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2574810981750488}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2666493654251099}]}
{"ts": 1747974189.494144, "query_id": "Q028", "question": "How can you configure a chat model to automatically retry on ratelimiting errors?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5972297191619873}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7230130434036255}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9777129888534546}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0452178716659546}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0813199281692505}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1291544437408447}, {"doc_source": "concepts/chat_models.mdx", "score": 1.151963233947754}, {"doc_source": "concepts/streaming.mdx", "score": 1.1571592092514038}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1601706743240356}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1742055416107178}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1980221271514893}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2024019956588745}, {"doc_source": "concepts/runnables.mdx", "score": 1.2071409225463867}, {"doc_source": "concepts/chat_models.mdx", "score": 1.215909481048584}, {"doc_source": "concepts/rag.mdx", "score": 1.21603262424469}]}
{"ts": 1747974192.0259268, "query_id": "Q029", "question": "Explain how `InjectedToolArg` affects a tool schema and give one use case.", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.6152375936508179}, {"doc_source": "concepts/tools.mdx", "score": 0.641218900680542}, {"doc_source": "concepts/tools.mdx", "score": 0.9970071315765381}, {"doc_source": "concepts/tools.mdx", "score": 1.0159955024719238}, {"doc_source": "concepts/tools.mdx", "score": 1.0168166160583496}, {"doc_source": "concepts/tools.mdx", "score": 1.0184942483901978}, {"doc_source": "concepts/tools.mdx", "score": 1.0452795028686523}, {"doc_source": "concepts/tools.mdx", "score": 1.077510118484497}, {"doc_source": "concepts/tools.mdx", "score": 1.0917478799819946}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0985794067382812}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1134402751922607}, {"doc_source": "concepts/tools.mdx", "score": 1.1141332387924194}, {"doc_source": "concepts/tools.mdx", "score": 1.1253142356872559}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.1267939805984497}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1518738269805908}]}
{"ts": 1747974193.7751281, "query_id": "Q030", "question": "Which decorator is recommended for quickly turning a Python function into a LangChain Tool?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8487316966056824}, {"doc_source": "concepts/runnables.mdx", "score": 0.9630016684532166}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.9700244665145874}, {"doc_source": "concepts/tools.mdx", "score": 1.00749671459198}, {"doc_source": "concepts/tools.mdx", "score": 1.0290484428405762}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0300828218460083}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0336222648620605}, {"doc_source": "concepts/async.mdx", "score": 1.0536881685256958}, {"doc_source": "concepts/tools.mdx", "score": 1.0611387491226196}, {"doc_source": "concepts/async.mdx", "score": 1.0753121376037598}, {"doc_source": "concepts/lcel.mdx", "score": 1.0765608549118042}, {"doc_source": "concepts/tools.mdx", "score": 1.0787968635559082}, {"doc_source": "concepts/async.mdx", "score": 1.0833714008331299}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1049656867980957}, {"doc_source": "concepts/testing.mdx", "score": 1.1094768047332764}]}
{"ts": 1747974195.1205618, "query_id": "Q031", "question": "Describe how to batch inputs through a Runnable for better throughput.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6071752905845642}, {"doc_source": "concepts/runnables.mdx", "score": 0.785365104675293}, {"doc_source": "concepts/runnables.mdx", "score": 0.9014456868171692}, {"doc_source": "concepts/runnables.mdx", "score": 0.9244521856307983}, {"doc_source": "concepts/lcel.mdx", "score": 0.9332213401794434}, {"doc_source": "concepts/runnables.mdx", "score": 1.0414299964904785}, {"doc_source": "concepts/runnables.mdx", "score": 1.0648247003555298}, {"doc_source": "concepts/async.mdx", "score": 1.117110013961792}, {"doc_source": "concepts/runnables.mdx", "score": 1.132145881652832}, {"doc_source": "concepts/runnables.mdx", "score": 1.137253761291504}, {"doc_source": "concepts/runnables.mdx", "score": 1.1382300853729248}, {"doc_source": "concepts/runnables.mdx", "score": 1.1658470630645752}, {"doc_source": "concepts/runnables.mdx", "score": 1.1671687364578247}, {"doc_source": "concepts/runnables.mdx", "score": 1.1727207899093628}, {"doc_source": "concepts/runnables.mdx", "score": 1.1839094161987305}]}
{"ts": 1747974196.612983, "query_id": "Q032", "question": "What parameter in `RunnableConfig` controls the maximum number of parallel calls during batching?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.854267954826355}, {"doc_source": "concepts/runnables.mdx", "score": 0.8996121287345886}, {"doc_source": "concepts/runnables.mdx", "score": 0.9033942222595215}, {"doc_source": "concepts/runnables.mdx", "score": 0.9461213946342468}, {"doc_source": "concepts/runnables.mdx", "score": 0.9967213273048401}, {"doc_source": "concepts/lcel.mdx", "score": 1.0374784469604492}, {"doc_source": "concepts/runnables.mdx", "score": 1.0695950984954834}, {"doc_source": "concepts/runnables.mdx", "score": 1.09369695186615}, {"doc_source": "concepts/runnables.mdx", "score": 1.100693702697754}, {"doc_source": "concepts/runnables.mdx", "score": 1.1349602937698364}, {"doc_source": "concepts/runnables.mdx", "score": 1.1439012289047241}, {"doc_source": "concepts/runnables.mdx", "score": 1.1581496000289917}, {"doc_source": "concepts/runnables.mdx", "score": 1.174177646636963}, {"doc_source": "concepts/runnables.mdx", "score": 1.2126373052597046}, {"doc_source": "concepts/lcel.mdx", "score": 1.2179193496704102}]}
{"ts": 1747974197.2877178, "query_id": "Q033", "question": "Give an example of using `with_structured_output` to get JSON back from a chat model.", "retrieved_items": [{"doc_source": "concepts/structured_outputs.mdx", "score": 0.7809569239616394}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8982127904891968}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.93953537940979}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9598015546798706}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9786237478256226}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.9876483678817749}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9995797276496887}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.0026147365570068}, {"doc_source": "concepts/retrieval.mdx", "score": 1.010894775390625}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0279632806777954}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0480142831802368}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.082299828529358}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.116348147392273}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.1182029247283936}, {"doc_source": "concepts/index.mdx", "score": 1.1231802701950073}]}
{"ts": 1747974199.496518, "query_id": "Q034", "question": "What callback would you use to stream intermediate events from a Runnable?", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 0.8932899832725525}, {"doc_source": "concepts/streaming.mdx", "score": 0.9889703989028931}, {"doc_source": "concepts/streaming.mdx", "score": 1.0083987712860107}, {"doc_source": "concepts/streaming.mdx", "score": 1.0112031698226929}, {"doc_source": "concepts/runnables.mdx", "score": 1.039158821105957}, {"doc_source": "concepts/runnables.mdx", "score": 1.0665335655212402}, {"doc_source": "concepts/runnables.mdx", "score": 1.0957553386688232}, {"doc_source": "concepts/callbacks.mdx", "score": 1.0972172021865845}, {"doc_source": "concepts/streaming.mdx", "score": 1.0972607135772705}, {"doc_source": "concepts/streaming.mdx", "score": 1.1346112489700317}, {"doc_source": "concepts/runnables.mdx", "score": 1.138902187347412}, {"doc_source": "concepts/streaming.mdx", "score": 1.1487555503845215}, {"doc_source": "concepts/streaming.mdx", "score": 1.1521456241607666}, {"doc_source": "concepts/runnables.mdx", "score": 1.1603279113769531}, {"doc_source": "concepts/streaming.mdx", "score": 1.1616007089614868}]}
{"ts": 1747974200.8134139, "query_id": "Q035", "question": "Which two standard parameters influence generation randomness and length in chat models?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.9570950269699097}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0023435354232788}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0075596570968628}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0435528755187988}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0654771327972412}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1300246715545654}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1481531858444214}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1514999866485596}, {"doc_source": "concepts/chat_models.mdx", "score": 1.154242992401123}, {"doc_source": "concepts/chat_models.mdx", "score": 1.168658971786499}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1790071725845337}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1845673322677612}, {"doc_source": "concepts/chat_history.mdx", "score": 1.194903016090393}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2135601043701172}, {"doc_source": "concepts/chat_models.mdx", "score": 1.217368245124817}]}
{"ts": 1747974201.877135, "query_id": "Q036", "question": "How do you persist a FAISS vector store to disk?", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 1.10737144947052}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.1657390594482422}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.193411111831665}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.1963579654693604}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.2157866954803467}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2577821016311646}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2647576332092285}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2794480323791504}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2846288681030273}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2864830493927002}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3001227378845215}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.320568323135376}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.337680697441101}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.346894383430481}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.369031548500061}]}
{"ts": 1747974202.7006028, "query_id": "Q037", "question": "Explain the difference between `stream` and `astream` methods on a Runnable.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.7146492004394531}, {"doc_source": "concepts/streaming.mdx", "score": 0.7505303621292114}, {"doc_source": "concepts/streaming.mdx", "score": 0.7567533254623413}, {"doc_source": "concepts/runnables.mdx", "score": 0.7942098379135132}, {"doc_source": "concepts/streaming.mdx", "score": 0.8235426545143127}, {"doc_source": "concepts/streaming.mdx", "score": 0.8398621082305908}, {"doc_source": "concepts/runnables.mdx", "score": 0.881005048751831}, {"doc_source": "concepts/runnables.mdx", "score": 1.0099560022354126}, {"doc_source": "concepts/streaming.mdx", "score": 1.0276134014129639}, {"doc_source": "concepts/runnables.mdx", "score": 1.0565654039382935}, {"doc_source": "concepts/runnables.mdx", "score": 1.114068865776062}, {"doc_source": "concepts/async.mdx", "score": 1.1148728132247925}, {"doc_source": "concepts/streaming.mdx", "score": 1.1172845363616943}, {"doc_source": "concepts/streaming.mdx", "score": 1.1280181407928467}, {"doc_source": "concepts/runnables.mdx", "score": 1.1470985412597656}]}
{"ts": 1747974204.0856519, "query_id": "Q038", "question": "What feature does LangChain provide to ensure prompts fit within an LLM's context window?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.8554450273513794}, {"doc_source": "how_to/index.mdx", "score": 0.9133279919624329}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9302371740341187}, {"doc_source": "how_to/index.mdx", "score": 0.9439353942871094}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9485678672790527}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9638192653656006}, {"doc_source": "concepts/lcel.mdx", "score": 0.9699181914329529}, {"doc_source": "how_to/index.mdx", "score": 0.9751831889152527}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9790245294570923}, {"doc_source": "concepts/streaming.mdx", "score": 0.9846986532211304}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9860131740570068}, {"doc_source": "concepts/index.mdx", "score": 0.9908493161201477}, {"doc_source": "tutorials/index.mdx", "score": 0.997306227684021}, {"doc_source": "concepts/chat_models.mdx", "score": 0.998253345489502}, {"doc_source": "concepts/streaming.mdx", "score": 1.004465103149414}]}
{"ts": 1747974205.461205, "query_id": "Q039", "question": "Name three message roles supported by LangChain chat messages.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 0.747031033039093}, {"doc_source": "concepts/messages.mdx", "score": 0.7780460119247437}, {"doc_source": "concepts/messages.mdx", "score": 0.8877400159835815}, {"doc_source": "concepts/chat_models.mdx", "score": 0.90141761302948}, {"doc_source": "concepts/messages.mdx", "score": 0.9076243042945862}, {"doc_source": "concepts/messages.mdx", "score": 0.9362462759017944}, {"doc_source": "concepts/messages.mdx", "score": 0.9409505724906921}, {"doc_source": "concepts/chat_models.mdx", "score": 1.006990671157837}, {"doc_source": "concepts/multimodality.mdx", "score": 1.016806960105896}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0168626308441162}, {"doc_source": "how_to/index.mdx", "score": 1.0272340774536133}, {"doc_source": "concepts/messages.mdx", "score": 1.0423829555511475}, {"doc_source": "concepts/streaming.mdx", "score": 1.0550471544265747}, {"doc_source": "concepts/messages.mdx", "score": 1.0594207048416138}, {"doc_source": "concepts/index.mdx", "score": 1.0617893934249878}]}
{"ts": 1747974206.6597672, "query_id": "Q040", "question": "How can you combine docs returned by a Retriever before passing to an LLM?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.8409594297409058}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8477333188056946}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9801145195960999}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0033735036849976}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0323984622955322}, {"doc_source": "concepts/retrievers.mdx", "score": 1.052318811416626}, {"doc_source": "how_to/index.mdx", "score": 1.0555713176727295}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0584992170333862}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0690417289733887}, {"doc_source": "concepts/rag.mdx", "score": 1.1239819526672363}, {"doc_source": "concepts/rag.mdx", "score": 1.1354403495788574}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1387860774993896}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1418030261993408}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1448073387145996}, {"doc_source": "concepts/retrieval.mdx", "score": 1.156416893005371}]}
{"ts": 1747974208.494329, "query_id": "Q041", "question": "What does the `rate_limiter` parameter accept and why is it useful?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.9189773201942444}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9408151507377625}, {"doc_source": "concepts/chat_models.mdx", "score": 1.071387529373169}, {"doc_source": "concepts/chat_models.mdx", "score": 1.176348328590393}, {"doc_source": "concepts/runnables.mdx", "score": 1.2191662788391113}, {"doc_source": "concepts/runnables.mdx", "score": 1.2674280405044556}, {"doc_source": "concepts/runnables.mdx", "score": 1.325438380241394}, {"doc_source": "concepts/chat_models.mdx", "score": 1.33738112449646}, {"doc_source": "concepts/chat_models.mdx", "score": 1.357758641242981}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3734252452850342}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3992327451705933}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4092170000076294}, {"doc_source": "concepts/runnables.mdx", "score": 1.4138648509979248}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4157195091247559}, {"doc_source": "concepts/runnables.mdx", "score": 1.4318615198135376}]}
{"ts": 1747974210.405779, "query_id": "Q042", "question": "Provide a code snippet to stream tokens from an OpenAI chat model using LCEL.", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.8564382791519165}, {"doc_source": "concepts/chat_models.mdx", "score": 0.956968367099762}, {"doc_source": "concepts/streaming.mdx", "score": 0.9773334264755249}, {"doc_source": "concepts/streaming.mdx", "score": 0.9781180024147034}, {"doc_source": "concepts/streaming.mdx", "score": 0.9932388663291931}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0012881755828857}, {"doc_source": "concepts/streaming.mdx", "score": 1.0151252746582031}, {"doc_source": "concepts/streaming.mdx", "score": 1.015478491783142}, {"doc_source": "concepts/streaming.mdx", "score": 1.030043363571167}, {"doc_source": "concepts/lcel.mdx", "score": 1.0327832698822021}, {"doc_source": "concepts/messages.mdx", "score": 1.034729242324829}, {"doc_source": "concepts/streaming.mdx", "score": 1.0400614738464355}, {"doc_source": "concepts/chat_models.mdx", "score": 1.051882266998291}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0532902479171753}, {"doc_source": "concepts/rag.mdx", "score": 1.0626581907272339}]}
{"ts": 1747974212.3468502, "query_id": "Q043", "question": "Which notebook demonstrates how to track token usage costs per request?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 1.2758543491363525}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.286621332168579}, {"doc_source": "how_to/index.mdx", "score": 1.3037256002426147}, {"doc_source": "concepts/tokens.mdx", "score": 1.3319091796875}, {"doc_source": "concepts/index.mdx", "score": 1.332702875137329}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3331736326217651}, {"doc_source": "concepts/tokens.mdx", "score": 1.3349742889404297}, {"doc_source": "introduction.mdx", "score": 1.3488354682922363}, {"doc_source": "how_to/index.mdx", "score": 1.3503950834274292}, {"doc_source": "tutorials/index.mdx", "score": 1.3513808250427246}, {"doc_source": "how_to/index.mdx", "score": 1.3527246713638306}, {"doc_source": "concepts/tokens.mdx", "score": 1.3604589700698853}, {"doc_source": "concepts/index.mdx", "score": 1.3635762929916382}, {"doc_source": "tutorials/index.mdx", "score": 1.36659574508667}, {"doc_source": "how_to/index.mdx", "score": 1.3667963743209839}]}
{"ts": 1747974213.069493, "query_id": "Q044", "question": "What are tool artifacts and how can they be returned from a tool?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.7273582220077515}, {"doc_source": "how_to/index.mdx", "score": 0.8022218346595764}, {"doc_source": "concepts/tools.mdx", "score": 0.8544261455535889}, {"doc_source": "concepts/tools.mdx", "score": 0.9141151309013367}, {"doc_source": "concepts/tools.mdx", "score": 1.0438547134399414}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1082346439361572}, {"doc_source": "how_to/index.mdx", "score": 1.1097557544708252}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1310503482818604}, {"doc_source": "concepts/messages.mdx", "score": 1.1417675018310547}, {"doc_source": "concepts/tools.mdx", "score": 1.1601356267929077}, {"doc_source": "concepts/tools.mdx", "score": 1.164244294166565}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1686551570892334}, {"doc_source": "concepts/tools.mdx", "score": 1.1773319244384766}, {"doc_source": "concepts/tools.mdx", "score": 1.1780436038970947}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1902446746826172}]}
{"ts": 1747974216.9679072, "query_id": "Q045", "question": "Explain how to asynchronously invoke a Runnable and await the result.", "retrieved_items": [{"doc_source": "concepts/async.mdx", "score": 0.8705847859382629}, {"doc_source": "concepts/runnables.mdx", "score": 0.9416512250900269}, {"doc_source": "concepts/async.mdx", "score": 1.0067267417907715}, {"doc_source": "concepts/runnables.mdx", "score": 1.0403454303741455}, {"doc_source": "concepts/runnables.mdx", "score": 1.0636825561523438}, {"doc_source": "concepts/runnables.mdx", "score": 1.0664417743682861}, {"doc_source": "concepts/runnables.mdx", "score": 1.105614423751831}, {"doc_source": "concepts/lcel.mdx", "score": 1.1440401077270508}, {"doc_source": "concepts/lcel.mdx", "score": 1.1547610759735107}, {"doc_source": "concepts/runnables.mdx", "score": 1.1654165983200073}, {"doc_source": "concepts/runnables.mdx", "score": 1.1720000505447388}, {"doc_source": "concepts/runnables.mdx", "score": 1.1744804382324219}, {"doc_source": "concepts/runnables.mdx", "score": 1.198018193244934}, {"doc_source": "concepts/lcel.mdx", "score": 1.2028217315673828}, {"doc_source": "concepts/runnables.mdx", "score": 1.2040263414382935}]}
{"ts": 1747974218.8381271, "query_id": "Q046", "question": "Which two documents should you read to understand LangGraph node construction?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.8442439436912537}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9646652936935425}, {"doc_source": "tutorials/index.mdx", "score": 0.9927448630332947}, {"doc_source": "how_to/index.mdx", "score": 1.015730857849121}, {"doc_source": "concepts/streaming.mdx", "score": 1.0292083024978638}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0584360361099243}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0665067434310913}, {"doc_source": "introduction.mdx", "score": 1.0958677530288696}, {"doc_source": "introduction.mdx", "score": 1.0983973741531372}, {"doc_source": "concepts/index.mdx", "score": 1.1131726503372192}, {"doc_source": "introduction.mdx", "score": 1.12821626663208}, {"doc_source": "how_to/index.mdx", "score": 1.130258321762085}, {"doc_source": "tutorials/index.mdx", "score": 1.1347427368164062}, {"doc_source": "concepts/index.mdx", "score": 1.1411209106445312}, {"doc_source": "concepts/retrieval.mdx", "score": 1.142289638519287}]}
{"ts": 1747974219.870199, "query_id": "Q047", "question": "Describe the process of converting a Runnable into a Tool for agent use.", "retrieved_items": [{"doc_source": "concepts/tool_calling.mdx", "score": 0.8755334615707397}, {"doc_source": "how_to/index.mdx", "score": 0.9108408689498901}, {"doc_source": "concepts/runnables.mdx", "score": 1.00651216506958}, {"doc_source": "concepts/runnables.mdx", "score": 1.0178650617599487}, {"doc_source": "concepts/runnables.mdx", "score": 1.0310521125793457}, {"doc_source": "concepts/runnables.mdx", "score": 1.0388060808181763}, {"doc_source": "how_to/index.mdx", "score": 1.0551937818527222}, {"doc_source": "concepts/tools.mdx", "score": 1.0552451610565186}, {"doc_source": "concepts/runnables.mdx", "score": 1.0686213970184326}, {"doc_source": "concepts/tools.mdx", "score": 1.0735183954238892}, {"doc_source": "concepts/tools.mdx", "score": 1.1142468452453613}, {"doc_source": "concepts/runnables.mdx", "score": 1.1228525638580322}, {"doc_source": "concepts/runnables.mdx", "score": 1.1284215450286865}, {"doc_source": "concepts/tools.mdx", "score": 1.1286561489105225}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1600738763809204}]}
{"ts": 1747974221.475384, "query_id": "Q048", "question": "What Python package contains community-contributed chat model integrations?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7932523488998413}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9213708639144897}, {"doc_source": "concepts/architecture.mdx", "score": 0.9719313383102417}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9882925748825073}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0169700384140015}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0465401411056519}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0607023239135742}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0621925592422485}, {"doc_source": "concepts/architecture.mdx", "score": 1.0702598094940186}, {"doc_source": "concepts/index.mdx", "score": 1.076059341430664}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0798521041870117}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0933806896209717}, {"doc_source": "how_to/installation.mdx", "score": 1.0946872234344482}, {"doc_source": "concepts/chat_models.mdx", "score": 1.094994306564331}, {"doc_source": "how_to/installation.mdx", "score": 1.1039150953292847}]}
{"ts": 1747974222.719095, "query_id": "Q049", "question": "How can semantic caching improve LLM response times and what extra model does it rely on?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7451748847961426}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9084821343421936}, {"doc_source": "how_to/index.mdx", "score": 1.0335711240768433}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0980552434921265}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1065105199813843}, {"doc_source": "how_to/index.mdx", "score": 1.1673729419708252}, {"doc_source": "how_to/index.mdx", "score": 1.1724309921264648}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1810581684112549}, {"doc_source": "concepts/rag.mdx", "score": 1.1874606609344482}, {"doc_source": "concepts/lcel.mdx", "score": 1.1922037601470947}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1961326599121094}, {"doc_source": "concepts/rag.mdx", "score": 1.1987911462783813}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2026046514511108}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2226396799087524}, {"doc_source": "concepts/streaming.mdx", "score": 1.222733736038208}]}
{"ts": 1747974224.419617, "query_id": "Q050", "question": "Which document loader would you use for processing JSON lines files?", "retrieved_items": [{"doc_source": "how_to/document_loader_json.mdx", "score": 0.6960445642471313}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.8321785926818848}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.8512913584709167}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.9375039339065552}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.9541094303131104}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0560908317565918}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.0603611469268799}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.0652978420257568}, {"doc_source": "how_to/index.mdx", "score": 1.0684109926223755}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.07631516456604}, {"doc_source": "how_to/index.mdx", "score": 1.0871188640594482}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.103253960609436}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.1605806350708008}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.167436957359314}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2414987087249756}]}
{"ts": 1747974225.648782, "query_id": "Q051", "question": "What is an `OutputParser` and why is it useful in LCEL chains?", "retrieved_items": [{"doc_source": "concepts/output_parsers.mdx", "score": 0.666566014289856}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.8718829154968262}, {"doc_source": "how_to/index.mdx", "score": 0.8753885626792908}, {"doc_source": "concepts/lcel.mdx", "score": 0.9574340581893921}, {"doc_source": "concepts/index.mdx", "score": 0.9735132455825806}, {"doc_source": "how_to/index.mdx", "score": 0.9743713140487671}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.9907498359680176}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.9972451329231262}, {"doc_source": "concepts/lcel.mdx", "score": 1.0475422143936157}, {"doc_source": "concepts/streaming.mdx", "score": 1.0501556396484375}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.0783095359802246}, {"doc_source": "concepts/streaming.mdx", "score": 1.0883365869522095}, {"doc_source": "concepts/lcel.mdx", "score": 1.0888357162475586}, {"doc_source": "concepts/lcel.mdx", "score": 1.0933077335357666}, {"doc_source": "concepts/lcel.mdx", "score": 1.105439305305481}]}
{"ts": 1747974227.864226, "query_id": "Q052", "question": "Give an example selector that prioritizes shorter examples and describe its use case.", "retrieved_items": [{"doc_source": "concepts/example_selectors.mdx", "score": 0.7906169891357422}, {"doc_source": "how_to/index.mdx", "score": 0.8976929783821106}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0417726039886475}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0674413442611694}, {"doc_source": "how_to/index.mdx", "score": 1.0718224048614502}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.11545991897583}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2266030311584473}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2373075485229492}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2497491836547852}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2874565124511719}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2900316715240479}, {"doc_source": "concepts/index.mdx", "score": 1.2930505275726318}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2983436584472656}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3611565828323364}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.3704984188079834}]}
{"ts": 1747974230.527971, "query_id": "Q053", "question": "Which environment variable can override OpenAI API base URL in LangChain?", "retrieved_items": [{"doc_source": "how_to/installation.mdx", "score": 1.11719810962677}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1243118047714233}, {"doc_source": "concepts/messages.mdx", "score": 1.1472233533859253}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1506541967391968}, {"doc_source": "how_to/installation.mdx", "score": 1.165562629699707}, {"doc_source": "concepts/architecture.mdx", "score": 1.1812148094177246}, {"doc_source": "introduction.mdx", "score": 1.1884878873825073}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1907782554626465}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.197434425354004}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1980059146881104}, {"doc_source": "introduction.mdx", "score": 1.2042648792266846}, {"doc_source": "concepts/architecture.mdx", "score": 1.2147096395492554}, {"doc_source": "concepts/async.mdx", "score": 1.215280532836914}, {"doc_source": "concepts/async.mdx", "score": 1.2263463735580444}, {"doc_source": "concepts/async.mdx", "score": 1.230574131011963}]}
{"ts": 1747974231.365964, "query_id": "Q054", "question": "Describe how to attach custom metadata to a Runnable run for later inspection in LangSmith.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8650342226028442}, {"doc_source": "concepts/runnables.mdx", "score": 1.0639784336090088}, {"doc_source": "concepts/runnables.mdx", "score": 1.0846139192581177}, {"doc_source": "concepts/runnables.mdx", "score": 1.088841438293457}, {"doc_source": "concepts/runnables.mdx", "score": 1.0989418029785156}, {"doc_source": "concepts/runnables.mdx", "score": 1.1090259552001953}, {"doc_source": "how_to/index.mdx", "score": 1.128206729888916}, {"doc_source": "concepts/tools.mdx", "score": 1.1362147331237793}, {"doc_source": "concepts/runnables.mdx", "score": 1.150998592376709}, {"doc_source": "how_to/index.mdx", "score": 1.1560053825378418}, {"doc_source": "concepts/runnables.mdx", "score": 1.156534194946289}, {"doc_source": "tutorials/index.mdx", "score": 1.159673810005188}, {"doc_source": "concepts/runnables.mdx", "score": 1.1606323719024658}, {"doc_source": "concepts/runnables.mdx", "score": 1.174673080444336}, {"doc_source": "concepts/runnables.mdx", "score": 1.1823261976242065}]}
{"ts": 1747974233.2696629, "query_id": "Q055", "question": "What does the `configurable_fields` helper enable for advanced chains?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9244881868362427}, {"doc_source": "concepts/runnables.mdx", "score": 1.1057524681091309}, {"doc_source": "concepts/runnables.mdx", "score": 1.129970669746399}, {"doc_source": "concepts/runnables.mdx", "score": 1.2372381687164307}, {"doc_source": "concepts/lcel.mdx", "score": 1.2811269760131836}, {"doc_source": "concepts/runnables.mdx", "score": 1.2867777347564697}, {"doc_source": "concepts/lcel.mdx", "score": 1.2921444177627563}, {"doc_source": "concepts/lcel.mdx", "score": 1.2939543724060059}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.3011118173599243}, {"doc_source": "concepts/lcel.mdx", "score": 1.302300214767456}, {"doc_source": "concepts/lcel.mdx", "score": 1.310983419418335}, {"doc_source": "concepts/lcel.mdx", "score": 1.328723430633545}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3337057828903198}, {"doc_source": "concepts/runnables.mdx", "score": 1.335965871810913}, {"doc_source": "concepts/chat_models.mdx", "score": 1.341069221496582}]}
{"ts": 1747974234.636884, "query_id": "Q056", "question": "Name two ChatModel parameters that help avoid exceeding rate limits.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5764228701591492}, {"doc_source": "concepts/chat_models.mdx", "score": 0.786797046661377}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8256374597549438}, {"doc_source": "concepts/chat_models.mdx", "score": 0.993874192237854}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9982858896255493}, {"doc_source": "concepts/chat_models.mdx", "score": 1.051943063735962}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0629923343658447}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0729771852493286}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0875115394592285}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0924327373504639}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0973167419433594}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1015684604644775}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1097288131713867}, {"doc_source": "concepts/chat_models.mdx", "score": 1.131658911705017}, {"doc_source": "concepts/index.mdx", "score": 1.1390085220336914}]}
{"ts": 1747974235.825143, "query_id": "Q057", "question": "How do you specify a custom UUID for a Runnable run?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9112035036087036}, {"doc_source": "concepts/runnables.mdx", "score": 1.018385887145996}, {"doc_source": "concepts/runnables.mdx", "score": 1.0536280870437622}, {"doc_source": "concepts/runnables.mdx", "score": 1.0814731121063232}, {"doc_source": "concepts/runnables.mdx", "score": 1.111826777458191}, {"doc_source": "concepts/runnables.mdx", "score": 1.1248376369476318}, {"doc_source": "concepts/runnables.mdx", "score": 1.1606018543243408}, {"doc_source": "concepts/runnables.mdx", "score": 1.1610076427459717}, {"doc_source": "concepts/runnables.mdx", "score": 1.1709966659545898}, {"doc_source": "concepts/runnables.mdx", "score": 1.1753994226455688}, {"doc_source": "concepts/runnables.mdx", "score": 1.194420337677002}, {"doc_source": "concepts/runnables.mdx", "score": 1.1969250440597534}, {"doc_source": "concepts/runnables.mdx", "score": 1.198728084564209}, {"doc_source": "concepts/runnables.mdx", "score": 1.2035760879516602}, {"doc_source": "concepts/runnables.mdx", "score": 1.2135136127471924}]}
{"ts": 1747974237.9765382, "query_id": "Q058", "question": "Compare `batch` and `batch_as_completed` when processing multiple inputs.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8206232786178589}, {"doc_source": "concepts/runnables.mdx", "score": 0.8489506244659424}, {"doc_source": "concepts/runnables.mdx", "score": 1.0541388988494873}, {"doc_source": "concepts/runnables.mdx", "score": 1.1159586906433105}, {"doc_source": "concepts/lcel.mdx", "score": 1.2779628038406372}, {"doc_source": "concepts/streaming.mdx", "score": 1.2969393730163574}, {"doc_source": "concepts/runnables.mdx", "score": 1.313962459564209}, {"doc_source": "concepts/streaming.mdx", "score": 1.325455665588379}, {"doc_source": "concepts/runnables.mdx", "score": 1.344900131225586}, {"doc_source": "concepts/streaming.mdx", "score": 1.3510081768035889}, {"doc_source": "concepts/async.mdx", "score": 1.3606773614883423}, {"doc_source": "concepts/streaming.mdx", "score": 1.370046854019165}, {"doc_source": "concepts/runnables.mdx", "score": 1.3710707426071167}, {"doc_source": "concepts/lcel.mdx", "score": 1.3867771625518799}, {"doc_source": "concepts/streaming.mdx", "score": 1.3982813358306885}]}
{"ts": 1747974239.713094, "query_id": "Q059", "question": "How can LangChain assist with evaluating generated answers automatically?", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8681861162185669}, {"doc_source": "concepts/evaluation.mdx", "score": 0.9115369915962219}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9272153377532959}, {"doc_source": "how_to/index.mdx", "score": 0.9319011569023132}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9333845376968384}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9414187073707581}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9671382904052734}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9749717116355896}, {"doc_source": "how_to/index.mdx", "score": 0.9882859587669373}, {"doc_source": "how_to/index.mdx", "score": 1.0079916715621948}, {"doc_source": "introduction.mdx", "score": 1.0212862491607666}, {"doc_source": "how_to/index.mdx", "score": 1.0229500532150269}, {"doc_source": "introduction.mdx", "score": 1.0375463962554932}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0457044839859009}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0472661256790161}]}
{"ts": 1747974241.362206, "query_id": "Q060", "question": "Provide two metrics supported by LangChain's built in evaluation module.", "retrieved_items": [{"doc_source": "concepts/evaluation.mdx", "score": 0.8834241628646851}, {"doc_source": "how_to/index.mdx", "score": 0.895149827003479}, {"doc_source": "tutorials/index.mdx", "score": 0.9036540985107422}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9274737238883972}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9687185287475586}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9775195717811584}, {"doc_source": "how_to/index.mdx", "score": 0.9827457666397095}, {"doc_source": "concepts/streaming.mdx", "score": 1.026128888130188}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0542540550231934}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0649406909942627}, {"doc_source": "concepts/index.mdx", "score": 1.0691990852355957}, {"doc_source": "how_to/index.mdx", "score": 1.0727943181991577}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0777595043182373}, {"doc_source": "concepts/architecture.mdx", "score": 1.0931299924850464}, {"doc_source": "concepts/lcel.mdx", "score": 1.113149642944336}]}
{"ts": 1747974242.273998, "query_id": "Q061", "question": "Which LangChain concept allows combining multiple Runnables with the `|` operator?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.6622096300125122}, {"doc_source": "concepts/runnables.mdx", "score": 0.745435357093811}, {"doc_source": "concepts/lcel.mdx", "score": 0.7543333172798157}, {"doc_source": "concepts/lcel.mdx", "score": 0.7627063989639282}, {"doc_source": "how_to/index.mdx", "score": 0.7676836252212524}, {"doc_source": "concepts/runnables.mdx", "score": 0.7853280901908875}, {"doc_source": "concepts/lcel.mdx", "score": 0.7858154773712158}, {"doc_source": "concepts/lcel.mdx", "score": 0.7925240993499756}, {"doc_source": "concepts/async.mdx", "score": 0.8218586444854736}, {"doc_source": "concepts/lcel.mdx", "score": 0.8447744846343994}, {"doc_source": "concepts/runnables.mdx", "score": 0.8520084023475647}, {"doc_source": "concepts/streaming.mdx", "score": 0.8547351360321045}, {"doc_source": "concepts/streaming.mdx", "score": 0.8548612594604492}, {"doc_source": "concepts/lcel.mdx", "score": 0.8594031929969788}, {"doc_source": "concepts/runnables.mdx", "score": 0.8636296987533569}]}
{"ts": 1747974243.654691, "query_id": "Q062", "question": "Explain how to set up tool streaming so that tool results are incrementally passed back to the model.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.9530894756317139}, {"doc_source": "concepts/streaming.mdx", "score": 0.9633723497390747}, {"doc_source": "concepts/streaming.mdx", "score": 1.0007063150405884}, {"doc_source": "concepts/streaming.mdx", "score": 1.0243852138519287}, {"doc_source": "concepts/streaming.mdx", "score": 1.0504353046417236}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0511990785598755}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0673325061798096}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.093457579612732}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1050078868865967}, {"doc_source": "how_to/index.mdx", "score": 1.108426570892334}, {"doc_source": "concepts/streaming.mdx", "score": 1.1165802478790283}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1199523210525513}, {"doc_source": "concepts/streaming.mdx", "score": 1.1251208782196045}, {"doc_source": "how_to/index.mdx", "score": 1.1283061504364014}, {"doc_source": "how_to/index.mdx", "score": 1.1344350576400757}]}
{"ts": 1747974245.355963, "query_id": "Q063", "question": "What is the benefit of using `RunnableLambda` over subclassing a Runnable?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8222899436950684}, {"doc_source": "concepts/runnables.mdx", "score": 0.97190260887146}, {"doc_source": "concepts/runnables.mdx", "score": 0.9911425113677979}, {"doc_source": "concepts/lcel.mdx", "score": 0.9946490526199341}, {"doc_source": "concepts/runnables.mdx", "score": 1.0356467962265015}, {"doc_source": "concepts/runnables.mdx", "score": 1.0414469242095947}, {"doc_source": "concepts/runnables.mdx", "score": 1.0558290481567383}, {"doc_source": "concepts/lcel.mdx", "score": 1.0581414699554443}, {"doc_source": "concepts/runnables.mdx", "score": 1.0831265449523926}, {"doc_source": "concepts/lcel.mdx", "score": 1.1030985116958618}, {"doc_source": "concepts/lcel.mdx", "score": 1.1071369647979736}, {"doc_source": "concepts/lcel.mdx", "score": 1.121413230895996}, {"doc_source": "concepts/lcel.mdx", "score": 1.164240837097168}, {"doc_source": "concepts/runnables.mdx", "score": 1.1722389459609985}, {"doc_source": "concepts/lcel.mdx", "score": 1.1942577362060547}]}
{"ts": 1747974247.078827, "query_id": "Q064", "question": "Which two documents together explain how to build a streaming question answering chain with citations?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9878460168838501}, {"doc_source": "how_to/index.mdx", "score": 1.048918604850769}, {"doc_source": "concepts/retrieval.mdx", "score": 1.059093713760376}, {"doc_source": "tutorials/index.mdx", "score": 1.0656181573867798}, {"doc_source": "how_to/index.mdx", "score": 1.0741608142852783}, {"doc_source": "concepts/rag.mdx", "score": 1.077179193496704}, {"doc_source": "concepts/streaming.mdx", "score": 1.0899779796600342}, {"doc_source": "concepts/retrieval.mdx", "score": 1.093954086303711}, {"doc_source": "concepts/streaming.mdx", "score": 1.0956562757492065}, {"doc_source": "how_to/index.mdx", "score": 1.1047030687332153}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1073529720306396}, {"doc_source": "how_to/index.mdx", "score": 1.1311637163162231}, {"doc_source": "tutorials/index.mdx", "score": 1.1383445262908936}, {"doc_source": "concepts/streaming.mdx", "score": 1.1559886932373047}, {"doc_source": "how_to/index.mdx", "score": 1.1581488847732544}]}
{"ts": 1747974254.157699, "query_id": "Q065", "question": "How do you measure BLEU score in LangChain evaluations?", "retrieved_items": [{"doc_source": "concepts/embedding_models.mdx", "score": 0.9988753795623779}, {"doc_source": "tutorials/index.mdx", "score": 1.016527771949768}, {"doc_source": "concepts/evaluation.mdx", "score": 1.0848697423934937}, {"doc_source": "how_to/index.mdx", "score": 1.0964648723602295}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.130068063735962}, {"doc_source": "concepts/messages.mdx", "score": 1.1464849710464478}, {"doc_source": "tutorials/index.mdx", "score": 1.1486425399780273}, {"doc_source": "how_to/index.mdx", "score": 1.1489417552947998}, {"doc_source": "how_to/index.mdx", "score": 1.164609432220459}, {"doc_source": "how_to/index.mdx", "score": 1.1769291162490845}, {"doc_source": "introduction.mdx", "score": 1.180908441543579}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1812695264816284}, {"doc_source": "how_to/index.mdx", "score": 1.1903209686279297}, {"doc_source": "concepts/lcel.mdx", "score": 1.1913890838623047}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1956989765167236}]}
{"ts": 1747974255.477996, "query_id": "Q066", "question": "What does the `ConversationBufferWindowMemory` keep in memory?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.1609814167022705}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1822974681854248}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2317322492599487}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2465821504592896}, {"doc_source": "concepts/chat_models.mdx", "score": 1.273543357849121}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2775177955627441}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2917218208312988}, {"doc_source": "concepts/messages.mdx", "score": 1.3130019903182983}, {"doc_source": "concepts/index.mdx", "score": 1.362526297569275}, {"doc_source": "concepts/index.mdx", "score": 1.38128662109375}, {"doc_source": "concepts/messages.mdx", "score": 1.4068348407745361}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4078221321105957}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4140498638153076}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4146268367767334}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.414838194847107}]}
{"ts": 1747974256.7801201, "query_id": "Q067", "question": "Name a loader designed specifically for PDF files in LangChain.", "retrieved_items": [{"doc_source": "concepts/document_loaders.mdx", "score": 0.9647963047027588}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.0090519189834595}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0585426092147827}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.093834638595581}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0982685089111328}, {"doc_source": "how_to/index.mdx", "score": 1.1512643098831177}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.1551878452301025}, {"doc_source": "concepts/index.mdx", "score": 1.157232403755188}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1970720291137695}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.2023850679397583}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2048368453979492}, {"doc_source": "concepts/lcel.mdx", "score": 1.2077836990356445}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2298574447631836}, {"doc_source": "how_to/index.mdx", "score": 1.230638861656189}, {"doc_source": "introduction.mdx", "score": 1.2330278158187866}]}
{"ts": 1747974257.925765, "query_id": "Q068", "question": "Which concept guide covers the standard streaming APIs exposed by Runnables?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.7737112641334534}, {"doc_source": "concepts/runnables.mdx", "score": 0.8448584079742432}, {"doc_source": "concepts/runnables.mdx", "score": 0.8511358499526978}, {"doc_source": "concepts/runnables.mdx", "score": 0.866132378578186}, {"doc_source": "concepts/streaming.mdx", "score": 0.8676927089691162}, {"doc_source": "concepts/runnables.mdx", "score": 0.8800975680351257}, {"doc_source": "concepts/streaming.mdx", "score": 0.8908606767654419}, {"doc_source": "concepts/runnables.mdx", "score": 0.9338169097900391}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9437613487243652}, {"doc_source": "concepts/index.mdx", "score": 0.9438942670822144}, {"doc_source": "concepts/streaming.mdx", "score": 0.9541480541229248}, {"doc_source": "concepts/streaming.mdx", "score": 0.9877237677574158}, {"doc_source": "concepts/lcel.mdx", "score": 1.0021892786026}, {"doc_source": "concepts/streaming.mdx", "score": 1.0088260173797607}, {"doc_source": "concepts/streaming.mdx", "score": 1.0172847509384155}]}
{"ts": 1747974259.08898, "query_id": "Q069", "question": "What is the advantage of using `LangServe` with Runnables?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8218550682067871}, {"doc_source": "concepts/lcel.mdx", "score": 0.8807365298271179}, {"doc_source": "concepts/runnables.mdx", "score": 0.8942659497261047}, {"doc_source": "concepts/runnables.mdx", "score": 0.914657711982727}, {"doc_source": "concepts/runnables.mdx", "score": 0.9169715642929077}, {"doc_source": "concepts/lcel.mdx", "score": 0.9371641874313354}, {"doc_source": "concepts/lcel.mdx", "score": 0.9404588937759399}, {"doc_source": "concepts/runnables.mdx", "score": 0.9596583843231201}, {"doc_source": "concepts/async.mdx", "score": 0.9740961790084839}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9826189279556274}, {"doc_source": "concepts/architecture.mdx", "score": 0.9951614141464233}, {"doc_source": "concepts/runnables.mdx", "score": 1.0010749101638794}, {"doc_source": "concepts/index.mdx", "score": 1.0071794986724854}, {"doc_source": "concepts/async.mdx", "score": 1.0290040969848633}, {"doc_source": "concepts/runnables.mdx", "score": 1.0312614440917969}]}
{"ts": 1747974261.2247198, "query_id": "Q070", "question": "Explain how ChatModels support multimodal inputs.", "retrieved_items": [{"doc_source": "concepts/multimodality.mdx", "score": 0.591133713722229}, {"doc_source": "concepts/multimodality.mdx", "score": 0.6077048778533936}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7255555391311646}, {"doc_source": "concepts/chat_models.mdx", "score": 0.780855119228363}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7817521691322327}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7921609878540039}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8061254024505615}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8399001359939575}, {"doc_source": "concepts/tokens.mdx", "score": 0.8889898061752319}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8937234282493591}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8991349339485168}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9237212538719177}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9428757429122925}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9542033076286316}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9543963670730591}]}
{"ts": 1747974262.870226, "query_id": "Q071", "question": "What two steps are recommended to recover from ratelimiting errors?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.8246297836303711}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9089843034744263}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1995959281921387}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2089513540267944}, {"doc_source": "concepts/retrieval.mdx", "score": 1.419960379600525}, {"doc_source": "concepts/runnables.mdx", "score": 1.4464714527130127}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4580895900726318}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4748291969299316}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.4796034097671509}, {"doc_source": "concepts/rag.mdx", "score": 1.5081822872161865}, {"doc_source": "concepts/retrieval.mdx", "score": 1.5164611339569092}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.525590181350708}, {"doc_source": "concepts/runnables.mdx", "score": 1.5352545976638794}, {"doc_source": "concepts/streaming.mdx", "score": 1.5452731847763062}, {"doc_source": "how_to/index.mdx", "score": 1.5485355854034424}]}
{"ts": 1747974264.0289621, "query_id": "Q072", "question": "Which parameter allows a Runnable to stream events as JSON for UIs?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 1.053411602973938}, {"doc_source": "concepts/streaming.mdx", "score": 1.0660966634750366}, {"doc_source": "concepts/streaming.mdx", "score": 1.080962896347046}, {"doc_source": "concepts/streaming.mdx", "score": 1.086254358291626}, {"doc_source": "concepts/runnables.mdx", "score": 1.0901336669921875}, {"doc_source": "concepts/runnables.mdx", "score": 1.1123993396759033}, {"doc_source": "concepts/runnables.mdx", "score": 1.1253749132156372}, {"doc_source": "concepts/streaming.mdx", "score": 1.1307424306869507}, {"doc_source": "concepts/streaming.mdx", "score": 1.1317486763000488}, {"doc_source": "concepts/streaming.mdx", "score": 1.1320847272872925}, {"doc_source": "concepts/runnables.mdx", "score": 1.1370493173599243}, {"doc_source": "concepts/streaming.mdx", "score": 1.1416332721710205}, {"doc_source": "concepts/runnables.mdx", "score": 1.1420352458953857}, {"doc_source": "concepts/runnables.mdx", "score": 1.1528651714324951}, {"doc_source": "concepts/streaming.mdx", "score": 1.154650330543518}]}
{"ts": 1747974265.970442, "query_id": "Q073", "question": "Provide a code snippet to embed text with a local HuggingFace model.", "retrieved_items": [{"doc_source": "concepts/embedding_models.mdx", "score": 1.0072360038757324}, {"doc_source": "how_to/embed_text.mdx", "score": 1.027862310409546}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0613993406295776}, {"doc_source": "how_to/embed_text.mdx", "score": 1.0615369081497192}, {"doc_source": "concepts/multimodality.mdx", "score": 1.0715380907058716}, {"doc_source": "concepts/multimodality.mdx", "score": 1.0837111473083496}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1069062948226929}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1281545162200928}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1458141803741455}, {"doc_source": "how_to/index.mdx", "score": 1.154518723487854}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1575590372085571}, {"doc_source": "how_to/embed_text.mdx", "score": 1.1633853912353516}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1648677587509155}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1771190166473389}, {"doc_source": "concepts/multimodality.mdx", "score": 1.1799732446670532}]}
{"ts": 1747974267.421509, "query_id": "Q074", "question": "How does the `ContextualCompressionRetriever` reduce context windows?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 1.1383445262908936}, {"doc_source": "concepts/chat_models.mdx", "score": 1.280341625213623}, {"doc_source": "how_to/index.mdx", "score": 1.3007456064224243}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3162517547607422}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3542683124542236}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.3699824810028076}, {"doc_source": "concepts/rag.mdx", "score": 1.4050337076187134}, {"doc_source": "concepts/rag.mdx", "score": 1.40959894657135}, {"doc_source": "concepts/rag.mdx", "score": 1.4117631912231445}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.413751482963562}, {"doc_source": "concepts/retrieval.mdx", "score": 1.4231635332107544}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4318535327911377}, {"doc_source": "concepts/lcel.mdx", "score": 1.4359848499298096}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.436851143836975}, {"doc_source": "concepts/retrievers.mdx", "score": 1.4433348178863525}]}
{"ts": 1747974269.076354, "query_id": "Q075", "question": "What guide should you consult for configuring runtime chain internals like temperature?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 1.1232409477233887}, {"doc_source": "concepts/runnables.mdx", "score": 1.130516529083252}, {"doc_source": "how_to/index.mdx", "score": 1.1875925064086914}, {"doc_source": "tutorials/index.mdx", "score": 1.2212636470794678}, {"doc_source": "concepts/runnables.mdx", "score": 1.252436637878418}, {"doc_source": "how_to/index.mdx", "score": 1.2877519130706787}, {"doc_source": "how_to/index.mdx", "score": 1.3006731271743774}, {"doc_source": "concepts/runnables.mdx", "score": 1.3008782863616943}, {"doc_source": "how_to/index.mdx", "score": 1.304787516593933}, {"doc_source": "introduction.mdx", "score": 1.30855131149292}, {"doc_source": "how_to/index.mdx", "score": 1.3105030059814453}, {"doc_source": "concepts/streaming.mdx", "score": 1.3140385150909424}, {"doc_source": "introduction.mdx", "score": 1.3155182600021362}, {"doc_source": "concepts/tracing.mdx", "score": 1.318131685256958}, {"doc_source": "how_to/index.mdx", "score": 1.3216148614883423}]}
{"ts": 1747974270.631077, "query_id": "Q076", "question": "Which selector uses Maximal Marginal Relevance (MMR) to diversify examples?", "retrieved_items": [{"doc_source": "concepts/vectorstores.mdx", "score": 0.7082768678665161}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8893823623657227}, {"doc_source": "how_to/index.mdx", "score": 0.9370960593223572}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.019716739654541}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1647734642028809}, {"doc_source": "concepts/retrieval.mdx", "score": 1.195101261138916}, {"doc_source": "concepts/retrievers.mdx", "score": 1.218024492263794}, {"doc_source": "how_to/index.mdx", "score": 1.223414421081543}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.228658676147461}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2334977388381958}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2464914321899414}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2610132694244385}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2610528469085693}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2646901607513428}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2677290439605713}]}
{"ts": 1747974272.443439, "query_id": "Q077", "question": "What is the recommended way to expose tool schemas to chat models that use OpenAI function calling?", "retrieved_items": [{"doc_source": "concepts/structured_outputs.mdx", "score": 0.6826080083847046}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.697941243648529}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7246154546737671}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8079149723052979}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8479928970336914}, {"doc_source": "concepts/tools.mdx", "score": 0.850881040096283}, {"doc_source": "how_to/index.mdx", "score": 0.891886293888092}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.8962244391441345}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8981042504310608}, {"doc_source": "concepts/index.mdx", "score": 0.9164720177650452}, {"doc_source": "concepts/tools.mdx", "score": 0.936228334903717}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.9527020454406738}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.959715723991394}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.9609017372131348}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9698219299316406}]}
{"ts": 1747974274.6667929, "query_id": "Q078", "question": "Explain the advantage of using `batch_as_completed` for long running tasks.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.824834942817688}, {"doc_source": "concepts/runnables.mdx", "score": 0.9204135537147522}, {"doc_source": "concepts/runnables.mdx", "score": 1.0485215187072754}, {"doc_source": "concepts/runnables.mdx", "score": 1.137923240661621}, {"doc_source": "concepts/lcel.mdx", "score": 1.1977558135986328}, {"doc_source": "concepts/runnables.mdx", "score": 1.2185401916503906}, {"doc_source": "concepts/lcel.mdx", "score": 1.311350703239441}, {"doc_source": "concepts/async.mdx", "score": 1.3250874280929565}, {"doc_source": "concepts/chat_models.mdx", "score": 1.327980399131775}, {"doc_source": "concepts/async.mdx", "score": 1.340092658996582}, {"doc_source": "concepts/streaming.mdx", "score": 1.3442565202713013}, {"doc_source": "concepts/runnables.mdx", "score": 1.3518644571304321}, {"doc_source": "concepts/async.mdx", "score": 1.3654407262802124}, {"doc_source": "concepts/async.mdx", "score": 1.3685309886932373}, {"doc_source": "concepts/streaming.mdx", "score": 1.3821702003479004}]}
{"ts": 1747974276.29423, "query_id": "Q079", "question": "Which two files outline strategies to trim messages to fit the context window?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 0.913860559463501}, {"doc_source": "concepts/chat_history.mdx", "score": 1.0843418836593628}, {"doc_source": "concepts/chat_models.mdx", "score": 1.143091082572937}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1763439178466797}, {"doc_source": "how_to/index.mdx", "score": 1.1932933330535889}, {"doc_source": "concepts/index.mdx", "score": 1.2073917388916016}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2154500484466553}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2177094221115112}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2745035886764526}, {"doc_source": "concepts/index.mdx", "score": 1.28046715259552}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2915983200073242}, {"doc_source": "concepts/rag.mdx", "score": 1.2934471368789673}, {"doc_source": "concepts/chat_models.mdx", "score": 1.295935869216919}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3059769868850708}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3158063888549805}]}
{"ts": 1747974277.9085739, "query_id": "Q080", "question": "What does the `max_concurrency` field not control when using a ChatModel?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 1.11580228805542}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1204005479812622}, {"doc_source": "concepts/runnables.mdx", "score": 1.1443986892700195}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1621142625808716}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1748194694519043}, {"doc_source": "concepts/chat_models.mdx", "score": 1.194480299949646}, {"doc_source": "concepts/chat_models.mdx", "score": 1.203214168548584}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2167277336120605}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2186797857284546}, {"doc_source": "concepts/streaming.mdx", "score": 1.220299482345581}, {"doc_source": "concepts/chat_models.mdx", "score": 1.245568037033081}, {"doc_source": "concepts/index.mdx", "score": 1.2526741027832031}, {"doc_source": "concepts/messages.mdx", "score": 1.262699007987976}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2753844261169434}, {"doc_source": "concepts/chat_history.mdx", "score": 1.282158613204956}]}
{"ts": 1747974279.488822, "query_id": "Q081", "question": "Which document explains how to call tools in parallel with OpenAI?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9734101295471191}, {"doc_source": "how_to/index.mdx", "score": 1.0296671390533447}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.036679744720459}, {"doc_source": "how_to/index.mdx", "score": 1.041528582572937}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.046389102935791}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.100960373878479}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1159029006958008}, {"doc_source": "how_to/index.mdx", "score": 1.124979019165039}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.1374777555465698}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1442821025848389}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1494145393371582}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1501597166061401}, {"doc_source": "how_to/index.mdx", "score": 1.1639477014541626}, {"doc_source": "concepts/chat_models.mdx", "score": 1.164583683013916}, {"doc_source": "concepts/rag.mdx", "score": 1.1689369678497314}]}
{"ts": 1747974280.758763, "query_id": "Q082", "question": "Describe how `RunnableGenerator` differs from `RunnableLambda`.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6841926574707031}, {"doc_source": "concepts/runnables.mdx", "score": 0.9616811275482178}, {"doc_source": "concepts/lcel.mdx", "score": 0.9837167263031006}, {"doc_source": "concepts/runnables.mdx", "score": 1.0025105476379395}, {"doc_source": "concepts/runnables.mdx", "score": 1.034605622291565}, {"doc_source": "concepts/runnables.mdx", "score": 1.036828875541687}, {"doc_source": "concepts/lcel.mdx", "score": 1.0530364513397217}, {"doc_source": "concepts/lcel.mdx", "score": 1.0560325384140015}, {"doc_source": "concepts/lcel.mdx", "score": 1.0561673641204834}, {"doc_source": "concepts/runnables.mdx", "score": 1.060743808746338}, {"doc_source": "concepts/runnables.mdx", "score": 1.073660969734192}, {"doc_source": "concepts/lcel.mdx", "score": 1.1077572107315063}, {"doc_source": "concepts/lcel.mdx", "score": 1.1096677780151367}, {"doc_source": "concepts/lcel.mdx", "score": 1.118467926979065}, {"doc_source": "concepts/runnables.mdx", "score": 1.1213338375091553}]}
{"ts": 1747974281.9679449, "query_id": "Q083", "question": "Which vector store integration supports time weighted relevance?", "retrieved_items": [{"doc_source": "concepts/vectorstores.mdx", "score": 0.8833379149436951}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9368526935577393}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9392023086547852}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.9585649967193604}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9684179425239563}, {"doc_source": "concepts/retrieval.mdx", "score": 0.995840847492218}, {"doc_source": "how_to/index.mdx", "score": 1.0101748704910278}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0259709358215332}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0411107540130615}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.0478541851043701}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0541436672210693}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.078991413116455}, {"doc_source": "how_to/index.mdx", "score": 1.1049082279205322}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1079626083374023}, {"doc_source": "how_to/index.mdx", "score": 1.122788429260254}]}
{"ts": 1747974282.97079, "query_id": "Q084", "question": "Explain how to use `run_name` for easier debugging in LangSmith.", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.9942529201507568}, {"doc_source": "concepts/runnables.mdx", "score": 1.0110976696014404}, {"doc_source": "concepts/tracing.mdx", "score": 1.0626718997955322}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0860579013824463}, {"doc_source": "how_to/index.mdx", "score": 1.1235222816467285}, {"doc_source": "concepts/runnables.mdx", "score": 1.1498334407806396}, {"doc_source": "how_to/index.mdx", "score": 1.1625165939331055}, {"doc_source": "how_to/index.mdx", "score": 1.1674062013626099}, {"doc_source": "concepts/architecture.mdx", "score": 1.172900915145874}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1814759969711304}, {"doc_source": "concepts/runnables.mdx", "score": 1.185954213142395}, {"doc_source": "concepts/async.mdx", "score": 1.2058812379837036}, {"doc_source": "concepts/lcel.mdx", "score": 1.207152009010315}, {"doc_source": "concepts/index.mdx", "score": 1.221964716911316}, {"doc_source": "how_to/index.mdx", "score": 1.2327266931533813}]}
{"ts": 1747974285.708426, "query_id": "Q085", "question": "Which two parameters must be provided to initialize `AzureAIDocumentIntelligenceLoader`?", "retrieved_items": [{"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.7358838319778442}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.7363237738609314}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.9851270914077759}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2546788454055786}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.2595438957214355}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.2810471057891846}, {"doc_source": "how_to/index.mdx", "score": 1.289158582687378}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.301668643951416}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.312044620513916}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3180160522460938}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3863654136657715}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3888587951660156}, {"doc_source": "how_to/index.mdx", "score": 1.390916347503662}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4037060737609863}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4084140062332153}]}
{"ts": 1747974287.034239, "query_id": "Q086", "question": "What is the main benefit of `asam_stream` events over `astream`?", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.955558717250824}, {"doc_source": "concepts/streaming.mdx", "score": 1.0628275871276855}, {"doc_source": "concepts/streaming.mdx", "score": 1.0772496461868286}, {"doc_source": "concepts/streaming.mdx", "score": 1.1196436882019043}, {"doc_source": "concepts/streaming.mdx", "score": 1.1569956541061401}, {"doc_source": "concepts/runnables.mdx", "score": 1.1884583234786987}, {"doc_source": "concepts/streaming.mdx", "score": 1.2053475379943848}, {"doc_source": "concepts/streaming.mdx", "score": 1.2172844409942627}, {"doc_source": "concepts/streaming.mdx", "score": 1.2531800270080566}, {"doc_source": "concepts/chat_models.mdx", "score": 1.27734375}, {"doc_source": "concepts/streaming.mdx", "score": 1.297076940536499}, {"doc_source": "concepts/streaming.mdx", "score": 1.3099709749221802}, {"doc_source": "concepts/streaming.mdx", "score": 1.3129611015319824}, {"doc_source": "concepts/streaming.mdx", "score": 1.321699619293213}, {"doc_source": "concepts/streaming.mdx", "score": 1.3375320434570312}]}
{"ts": 1747974289.825461, "query_id": "Q087", "question": "How do you specify hidden arguments in a Tool so the model cannot set them?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.9853256940841675}, {"doc_source": "concepts/tools.mdx", "score": 0.997951090335846}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2023496627807617}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2198395729064941}, {"doc_source": "how_to/index.mdx", "score": 1.2289962768554688}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2648427486419678}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2763252258300781}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2929222583770752}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2978508472442627}, {"doc_source": "concepts/tools.mdx", "score": 1.3120687007904053}, {"doc_source": "concepts/tools.mdx", "score": 1.3128738403320312}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3182668685913086}, {"doc_source": "how_to/index.mdx", "score": 1.3263905048370361}, {"doc_source": "concepts/tools.mdx", "score": 1.3354039192199707}, {"doc_source": "concepts/tools.mdx", "score": 1.3414688110351562}]}
{"ts": 1747974292.1990368, "query_id": "Q088", "question": "Which guide demonstrates building a SQL retriever over a large database?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.9640151262283325}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0184630155563354}, {"doc_source": "how_to/index.mdx", "score": 1.0298004150390625}, {"doc_source": "concepts/retrieval.mdx", "score": 1.045705795288086}, {"doc_source": "how_to/index.mdx", "score": 1.064396619796753}, {"doc_source": "how_to/index.mdx", "score": 1.0887192487716675}, {"doc_source": "how_to/index.mdx", "score": 1.0963537693023682}, {"doc_source": "concepts/retrieval.mdx", "score": 1.126770257949829}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1477290391921997}, {"doc_source": "concepts/retrievers.mdx", "score": 1.153114676475525}, {"doc_source": "how_to/index.mdx", "score": 1.161637306213379}, {"doc_source": "concepts/retrieval.mdx", "score": 1.163454294204712}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1647007465362549}, {"doc_source": "how_to/index.mdx", "score": 1.1706514358520508}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1877450942993164}]}
{"ts": 1747974293.808792, "query_id": "Q089", "question": "Why is semantic caching unlikely to hit after the first chat turn?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7085064649581909}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8751232624053955}, {"doc_source": "concepts/chat_models.mdx", "score": 1.245640516281128}, {"doc_source": "concepts/chat_models.mdx", "score": 1.282827615737915}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2855405807495117}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3074755668640137}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3183785676956177}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3201875686645508}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3409019708633423}, {"doc_source": "concepts/chat_history.mdx", "score": 1.349226951599121}, {"doc_source": "concepts/messages.mdx", "score": 1.3569996356964111}, {"doc_source": "concepts/lcel.mdx", "score": 1.3793096542358398}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3891592025756836}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3922781944274902}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3925552368164062}]}
{"ts": 1747974295.259517, "query_id": "Q090", "question": "Describe a use case for `RecursiveCharacterTextSplitter`.", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.8238505125045776}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.97772216796875}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9961610436439514}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0287396907806396}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0928171873092651}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1283893585205078}, {"doc_source": "how_to/index.mdx", "score": 1.1728707551956177}, {"doc_source": "how_to/index.mdx", "score": 1.2018749713897705}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2219791412353516}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2504268884658813}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3303723335266113}, {"doc_source": "concepts/tokens.mdx", "score": 1.3521983623504639}, {"doc_source": "concepts/retrieval.mdx", "score": 1.3956092596054077}, {"doc_source": "concepts/tokens.mdx", "score": 1.398541808128357}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.4065721035003662}]}
{"ts": 1747974297.0993931, "query_id": "Q091", "question": "Give two methods to stream output from a ChatModel.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.813692033290863}, {"doc_source": "concepts/streaming.mdx", "score": 0.8409720063209534}, {"doc_source": "concepts/streaming.mdx", "score": 0.8448833227157593}, {"doc_source": "concepts/streaming.mdx", "score": 0.8515967130661011}, {"doc_source": "concepts/streaming.mdx", "score": 0.8558679223060608}, {"doc_source": "concepts/streaming.mdx", "score": 0.9392857551574707}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9671568274497986}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9844830632209778}, {"doc_source": "concepts/streaming.mdx", "score": 1.01005220413208}, {"doc_source": "concepts/streaming.mdx", "score": 1.012641191482544}, {"doc_source": "concepts/streaming.mdx", "score": 1.0316481590270996}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0570168495178223}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0588486194610596}, {"doc_source": "concepts/streaming.mdx", "score": 1.0635240077972412}, {"doc_source": "concepts/streaming.mdx", "score": 1.0652880668640137}]}
{"ts": 1747974299.662368, "query_id": "Q092", "question": "Which document teaches you to build a hybrid search retriever combining keyword and vector?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.7176859378814697}, {"doc_source": "concepts/retrieval.mdx", "score": 0.7420231103897095}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.7706047296524048}, {"doc_source": "concepts/retrievers.mdx", "score": 0.7877494096755981}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8331260085105896}, {"doc_source": "how_to/index.mdx", "score": 0.8711493015289307}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8748965859413147}, {"doc_source": "concepts/retrieval.mdx", "score": 0.8885716795921326}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9037724733352661}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9182834625244141}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.9278995990753174}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9631999731063843}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9677978157997131}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9730082154273987}, {"doc_source": "concepts/retrievers.mdx", "score": 0.978100061416626}]}
{"ts": 1747974301.099225, "query_id": "Q093", "question": "What is the purpose of `annotated` type hints in tool schemas?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8373091816902161}, {"doc_source": "concepts/tools.mdx", "score": 0.9164195656776428}, {"doc_source": "concepts/tools.mdx", "score": 1.017409086227417}, {"doc_source": "concepts/tools.mdx", "score": 1.071670413017273}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.1164155006408691}, {"doc_source": "concepts/runnables.mdx", "score": 1.1682435274124146}, {"doc_source": "concepts/tools.mdx", "score": 1.2012579441070557}, {"doc_source": "concepts/tools.mdx", "score": 1.2137415409088135}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2296128273010254}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2332876920700073}, {"doc_source": "concepts/tools.mdx", "score": 1.2551058530807495}, {"doc_source": "concepts/runnables.mdx", "score": 1.2741718292236328}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2752597332000732}, {"doc_source": "concepts/tools.mdx", "score": 1.2926347255706787}, {"doc_source": "concepts/tools.mdx", "score": 1.2997902631759644}]}
{"ts": 1747974302.4823081, "query_id": "Q094", "question": "Explain why caching chat model responses is challenging.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5505865812301636}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7895950078964233}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0342262983322144}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0898255109786987}, {"doc_source": "concepts/streaming.mdx", "score": 1.090616226196289}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1050816774368286}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1393959522247314}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1816985607147217}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1859321594238281}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1868174076080322}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1886284351348877}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1912211179733276}, {"doc_source": "concepts/streaming.mdx", "score": 1.19907808303833}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.213365077972412}, {"doc_source": "concepts/index.mdx", "score": 1.214538335800171}]}
{"ts": 1747974304.253379, "query_id": "Q095", "question": "Which two docs together explain trimming long documents before embedding?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.0125395059585571}, {"doc_source": "how_to/index.mdx", "score": 1.0349562168121338}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0576825141906738}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0661780834197998}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0736777782440186}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0988661050796509}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1394691467285156}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1488080024719238}, {"doc_source": "how_to/index.mdx", "score": 1.1691153049468994}, {"doc_source": "how_to/index.mdx", "score": 1.1994526386260986}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.20473051071167}, {"doc_source": "how_to/index.mdx", "score": 1.2248973846435547}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2368866205215454}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.2391890287399292}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.25919508934021}]}
{"ts": 1747974305.259388, "query_id": "Q096", "question": "What class enables you to get structured JSON output directly from LLMS without a parser?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 1.2031898498535156}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2099170684814453}, {"doc_source": "how_to/index.mdx", "score": 1.2100777626037598}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2266242504119873}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2280499935150146}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2773706912994385}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2982163429260254}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2994334697723389}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.305161714553833}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.3205721378326416}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.3370251655578613}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3385083675384521}, {"doc_source": "concepts/streaming.mdx", "score": 1.3595550060272217}, {"doc_source": "concepts/index.mdx", "score": 1.374741554260254}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3846209049224854}]}
{"ts": 1747974307.447021, "query_id": "Q097", "question": "Name three built in message field keys aside from `content`.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 1.042978048324585}, {"doc_source": "concepts/messages.mdx", "score": 1.0433833599090576}, {"doc_source": "concepts/messages.mdx", "score": 1.15213143825531}, {"doc_source": "concepts/messages.mdx", "score": 1.178863525390625}, {"doc_source": "concepts/messages.mdx", "score": 1.2030270099639893}, {"doc_source": "concepts/messages.mdx", "score": 1.2408971786499023}, {"doc_source": "concepts/messages.mdx", "score": 1.243306279182434}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2607184648513794}, {"doc_source": "concepts/messages.mdx", "score": 1.2740477323532104}, {"doc_source": "concepts/messages.mdx", "score": 1.321795105934143}, {"doc_source": "concepts/index.mdx", "score": 1.334504246711731}, {"doc_source": "concepts/messages.mdx", "score": 1.3383458852767944}, {"doc_source": "how_to/index.mdx", "score": 1.3388173580169678}, {"doc_source": "concepts/messages.mdx", "score": 1.3393723964691162}, {"doc_source": "concepts/multimodality.mdx", "score": 1.342320203781128}]}
{"ts": 1747974308.800229, "query_id": "Q098", "question": "What is the first step when creating a custom Runnable for streaming transformations?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8271044492721558}, {"doc_source": "concepts/runnables.mdx", "score": 0.8748779296875}, {"doc_source": "concepts/runnables.mdx", "score": 0.9594393968582153}, {"doc_source": "concepts/streaming.mdx", "score": 0.9714803099632263}, {"doc_source": "concepts/runnables.mdx", "score": 1.0124541521072388}, {"doc_source": "concepts/streaming.mdx", "score": 1.0620286464691162}, {"doc_source": "concepts/runnables.mdx", "score": 1.0720059871673584}, {"doc_source": "concepts/streaming.mdx", "score": 1.0781365633010864}, {"doc_source": "concepts/runnables.mdx", "score": 1.0866339206695557}, {"doc_source": "concepts/streaming.mdx", "score": 1.0995770692825317}, {"doc_source": "concepts/runnables.mdx", "score": 1.1026315689086914}, {"doc_source": "concepts/streaming.mdx", "score": 1.1202977895736694}, {"doc_source": "concepts/runnables.mdx", "score": 1.1217155456542969}, {"doc_source": "concepts/streaming.mdx", "score": 1.1278563737869263}, {"doc_source": "concepts/streaming.mdx", "score": 1.1353209018707275}]}
{"ts": 1747974310.949686, "query_id": "Q099", "question": "How does LangChain differentiate official vs community chat model integrations?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5815395712852478}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7871180176734924}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7939127683639526}, {"doc_source": "how_to/index.mdx", "score": 0.8075361251831055}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8399544358253479}, {"doc_source": "concepts/architecture.mdx", "score": 0.8508797287940979}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8524792194366455}, {"doc_source": "concepts/messages.mdx", "score": 0.8693899512290955}, {"doc_source": "how_to/index.mdx", "score": 0.8973360657691956}, {"doc_source": "concepts/architecture.mdx", "score": 0.9034607410430908}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9358336329460144}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9359591007232666}, {"doc_source": "how_to/installation.mdx", "score": 0.9371069669723511}, {"doc_source": "introduction.mdx", "score": 0.9506454467773438}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9510951042175293}]}
{"ts": 1747974313.308264, "query_id": "Q100", "question": "Which two sources explain creating example selectors based on semantic similarity?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9879101514816284}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.0084675550460815}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0930001735687256}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.113565444946289}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1504074335098267}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1698050498962402}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1754688024520874}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.18146812915802}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.227590560913086}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2347385883331299}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2362747192382812}, {"doc_source": "concepts/retrieval.mdx", "score": 1.237623929977417}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2411400079727173}, {"doc_source": "how_to/index.mdx", "score": 1.2457594871520996}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2587742805480957}]}
{"ts": 1747974315.124943, "query_id": "Q101", "question": "What is the impact of not propagating `RunnableConfig` in Python 3.10 async code?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6224303245544434}, {"doc_source": "concepts/callbacks.mdx", "score": 0.9561946392059326}, {"doc_source": "concepts/runnables.mdx", "score": 0.9741815328598022}, {"doc_source": "concepts/runnables.mdx", "score": 0.9802408814430237}, {"doc_source": "concepts/runnables.mdx", "score": 0.9939375519752502}, {"doc_source": "concepts/runnables.mdx", "score": 1.056923508644104}, {"doc_source": "concepts/async.mdx", "score": 1.08447265625}, {"doc_source": "concepts/runnables.mdx", "score": 1.1026204824447632}, {"doc_source": "concepts/async.mdx", "score": 1.112349510192871}, {"doc_source": "concepts/runnables.mdx", "score": 1.1232359409332275}, {"doc_source": "concepts/runnables.mdx", "score": 1.145546317100525}, {"doc_source": "concepts/tools.mdx", "score": 1.1513805389404297}, {"doc_source": "concepts/runnables.mdx", "score": 1.1735879182815552}, {"doc_source": "concepts/runnables.mdx", "score": 1.1887950897216797}, {"doc_source": "concepts/async.mdx", "score": 1.2118910551071167}]}
{"ts": 1747974317.2752671, "query_id": "Q102", "question": "Describe the `@tool` option to hide a parameter while still injecting it at runtime.", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8323071002960205}, {"doc_source": "concepts/tools.mdx", "score": 0.8833290338516235}, {"doc_source": "concepts/tools.mdx", "score": 1.0316475629806519}, {"doc_source": "concepts/tools.mdx", "score": 1.0827802419662476}, {"doc_source": "concepts/tools.mdx", "score": 1.092280626296997}, {"doc_source": "how_to/index.mdx", "score": 1.096388339996338}, {"doc_source": "concepts/tools.mdx", "score": 1.1009502410888672}, {"doc_source": "concepts/tools.mdx", "score": 1.1291502714157104}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.16111421585083}, {"doc_source": "concepts/tools.mdx", "score": 1.178646445274353}, {"doc_source": "concepts/tools.mdx", "score": 1.1906005144119263}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1924166679382324}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2233836650848389}, {"doc_source": "concepts/tools.mdx", "score": 1.2346513271331787}, {"doc_source": "concepts/tools.mdx", "score": 1.2499943971633911}]}
{"ts": 1747974319.456987, "query_id": "Q103", "question": "Which notebook illustrates using an ensemble retriever strategy?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.8431239128112793}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9554126858711243}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0338448286056519}, {"doc_source": "how_to/index.mdx", "score": 1.0517991781234741}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1222755908966064}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1300511360168457}, {"doc_source": "concepts/retrievers.mdx", "score": 1.138392686843872}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1464223861694336}, {"doc_source": "concepts/retrievers.mdx", "score": 1.148524522781372}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1701980829238892}, {"doc_source": "how_to/index.mdx", "score": 1.1895989179611206}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1957892179489136}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1974852085113525}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2051587104797363}, {"doc_source": "concepts/rag.mdx", "score": 1.2147248983383179}]}
{"ts": 1747974320.907951, "query_id": "Q104", "question": "Give two benefits of the LangChain Expression Language (LCEL).", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.6550620794296265}, {"doc_source": "concepts/lcel.mdx", "score": 0.7985444068908691}, {"doc_source": "concepts/lcel.mdx", "score": 0.8025520443916321}, {"doc_source": "concepts/lcel.mdx", "score": 0.8407782316207886}, {"doc_source": "concepts/lcel.mdx", "score": 0.8667981624603271}, {"doc_source": "concepts/lcel.mdx", "score": 0.8784947395324707}, {"doc_source": "how_to/index.mdx", "score": 0.8897539973258972}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9456223845481873}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9856065511703491}, {"doc_source": "concepts/lcel.mdx", "score": 0.9990330934524536}, {"doc_source": "concepts/lcel.mdx", "score": 1.0152767896652222}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0167815685272217}, {"doc_source": "how_to/index.mdx", "score": 1.0240325927734375}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0489985942840576}, {"doc_source": "concepts/index.mdx", "score": 1.049255132675171}]}
{"ts": 1747974322.90672, "query_id": "Q105", "question": "What is the main purpose of the `rate_limiter` argument on ChatModels?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7170464396476746}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9857767820358276}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9863804578781128}, {"doc_source": "concepts/chat_models.mdx", "score": 1.006213903427124}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1292283535003662}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1587672233581543}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1813194751739502}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1942893266677856}, {"doc_source": "concepts/chat_models.mdx", "score": 1.208898901939392}, {"doc_source": "concepts/chat_models.mdx", "score": 1.209385871887207}, {"doc_source": "concepts/runnables.mdx", "score": 1.2115675210952759}, {"doc_source": "concepts/runnables.mdx", "score": 1.2256733179092407}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2258588075637817}, {"doc_source": "concepts/chat_models.mdx", "score": 1.227956771850586}, {"doc_source": "concepts/index.mdx", "score": 1.2291250228881836}]}
{"ts": 1747974324.476827, "query_id": "Q106", "question": "How many tokens is 100 English words approximately equal to?", "retrieved_items": [{"doc_source": "concepts/tokens.mdx", "score": 0.7300253510475159}, {"doc_source": "concepts/tokens.mdx", "score": 1.0408051013946533}, {"doc_source": "concepts/tokens.mdx", "score": 1.0993263721466064}, {"doc_source": "concepts/tokens.mdx", "score": 1.1050782203674316}, {"doc_source": "concepts/tokens.mdx", "score": 1.1934698820114136}, {"doc_source": "concepts/tokens.mdx", "score": 1.318519949913025}, {"doc_source": "concepts/index.mdx", "score": 1.335258960723877}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3486131429672241}, {"doc_source": "concepts/chat_models.mdx", "score": 1.391812801361084}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4089173078536987}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.4448885917663574}, {"doc_source": "concepts/retrievers.mdx", "score": 1.4527873992919922}, {"doc_source": "concepts/retrieval.mdx", "score": 1.4580717086791992}, {"doc_source": "concepts/retrieval.mdx", "score": 1.469346046447754}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4745930433273315}]}
{"ts": 1747974325.829482, "query_id": "Q107", "question": "Explain the difference between `invoke` and `ainvoke`.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.934457540512085}, {"doc_source": "concepts/async.mdx", "score": 1.1377217769622803}, {"doc_source": "concepts/async.mdx", "score": 1.2116649150848389}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2120275497436523}, {"doc_source": "concepts/streaming.mdx", "score": 1.2388408184051514}, {"doc_source": "concepts/runnables.mdx", "score": 1.266902208328247}, {"doc_source": "concepts/chat_models.mdx", "score": 1.293036937713623}, {"doc_source": "concepts/tools.mdx", "score": 1.333390235900879}, {"doc_source": "concepts/async.mdx", "score": 1.3368453979492188}, {"doc_source": "concepts/async.mdx", "score": 1.400228500366211}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.4016673564910889}, {"doc_source": "concepts/messages.mdx", "score": 1.4030615091323853}, {"doc_source": "concepts/tools.mdx", "score": 1.4112576246261597}, {"doc_source": "concepts/retrieval.mdx", "score": 1.4119783639907837}, {"doc_source": "concepts/tools.mdx", "score": 1.4129979610443115}]}
{"ts": 1747974327.469258, "query_id": "Q108", "question": "What does `batch_as_completed` return with each result to help match inputs?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9442788362503052}, {"doc_source": "concepts/runnables.mdx", "score": 0.998837411403656}, {"doc_source": "concepts/runnables.mdx", "score": 1.141977310180664}, {"doc_source": "concepts/runnables.mdx", "score": 1.2198410034179688}, {"doc_source": "concepts/runnables.mdx", "score": 1.3332490921020508}, {"doc_source": "concepts/lcel.mdx", "score": 1.361945629119873}, {"doc_source": "concepts/streaming.mdx", "score": 1.3664203882217407}, {"doc_source": "concepts/index.mdx", "score": 1.3679932355880737}, {"doc_source": "concepts/runnables.mdx", "score": 1.3807415962219238}, {"doc_source": "concepts/streaming.mdx", "score": 1.3926225900650024}, {"doc_source": "concepts/async.mdx", "score": 1.4003173112869263}, {"doc_source": "concepts/runnables.mdx", "score": 1.4074628353118896}, {"doc_source": "concepts/streaming.mdx", "score": 1.4097381830215454}, {"doc_source": "concepts/runnables.mdx", "score": 1.4185608625411987}, {"doc_source": "concepts/streaming.mdx", "score": 1.420689344406128}]}
{"ts": 1747974328.842675, "query_id": "Q109", "question": "Why might you choose Chroma over FAISS for prototyping?", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 0.9983886480331421}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3057998418807983}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3277068138122559}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.345019817352295}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3771840333938599}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3824838399887085}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.4036633968353271}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.4067103862762451}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.4098128080368042}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.4128397703170776}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.417332410812378}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4267890453338623}, {"doc_source": "concepts/multimodality.mdx", "score": 1.4297406673431396}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.4354984760284424}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.4381399154663086}]}
