{"ts": 1747974402.6486468, "query_id": "Q000", "question": "What is LangChain and why was it created?", "retrieved_items": [{"doc_source": "concepts/why_langchain.mdx", "score": 0.6254758834838867}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7717767953872681}, {"doc_source": "introduction.mdx", "score": 0.8172064423561096}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8637759685516357}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8777604103088379}, {"doc_source": "how_to/index.mdx", "score": 0.891899824142456}, {"doc_source": "concepts/architecture.mdx", "score": 0.9001404047012329}, {"doc_source": "concepts/lcel.mdx", "score": 0.9283114075660706}, {"doc_source": "introduction.mdx", "score": 0.9307405948638916}, {"doc_source": "concepts/lcel.mdx", "score": 0.9353309869766235}, {"doc_source": "concepts/architecture.mdx", "score": 0.9354451894760132}, {"doc_source": "tutorials/index.mdx", "score": 0.9404783248901367}, {"doc_source": "concepts/async.mdx", "score": 0.9587169289588928}, {"doc_source": "introduction.mdx", "score": 0.9641686677932739}, {"doc_source": "how_to/installation.mdx", "score": 0.965599775314331}]}
{"ts": 1747974405.444297, "query_id": "Q001", "question": "Explain the role of an Agent in LangChain.", "retrieved_items": [{"doc_source": "concepts/agents.mdx", "score": 0.7623252868652344}, {"doc_source": "concepts/agents.mdx", "score": 0.8576846122741699}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9453485608100891}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0085797309875488}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0098464488983154}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0205658674240112}, {"doc_source": "how_to/index.mdx", "score": 1.0227583646774292}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0418100357055664}, {"doc_source": "introduction.mdx", "score": 1.0460634231567383}, {"doc_source": "concepts/architecture.mdx", "score": 1.0851770639419556}, {"doc_source": "concepts/async.mdx", "score": 1.0926876068115234}, {"doc_source": "concepts/index.mdx", "score": 1.097399115562439}, {"doc_source": "concepts/lcel.mdx", "score": 1.102743148803711}, {"doc_source": "introduction.mdx", "score": 1.106758952140808}, {"doc_source": "concepts/lcel.mdx", "score": 1.1198902130126953}]}
{"ts": 1747974407.4182389, "query_id": "Q002", "question": "How do I install LangChain with extras for tracing?", "retrieved_items": [{"doc_source": "how_to/installation.mdx", "score": 0.814527153968811}, {"doc_source": "concepts/async.mdx", "score": 0.8556032776832581}, {"doc_source": "how_to/installation.mdx", "score": 0.8704622983932495}, {"doc_source": "how_to/installation.mdx", "score": 0.8810732364654541}, {"doc_source": "how_to/installation.mdx", "score": 0.9090052247047424}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9152833223342896}, {"doc_source": "how_to/installation.mdx", "score": 0.9302113652229309}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9448955655097961}, {"doc_source": "tutorials/index.mdx", "score": 0.9475994110107422}, {"doc_source": "tutorials/index.mdx", "score": 0.9607210159301758}, {"doc_source": "how_to/index.mdx", "score": 0.9626967310905457}, {"doc_source": "how_to/index.mdx", "score": 0.9876580238342285}, {"doc_source": "concepts/tracing.mdx", "score": 0.9975783228874207}, {"doc_source": "concepts/architecture.mdx", "score": 0.998455286026001}, {"doc_source": "introduction.mdx", "score": 1.0015681982040405}]}
{"ts": 1747974409.067681, "query_id": "Q003", "question": "Show the minimal LCEL chain that streams token-by-token.", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.5412482023239136}, {"doc_source": "concepts/streaming.mdx", "score": 0.7812732458114624}, {"doc_source": "concepts/lcel.mdx", "score": 0.9581571221351624}, {"doc_source": "how_to/index.mdx", "score": 0.9815590381622314}, {"doc_source": "concepts/lcel.mdx", "score": 0.982903778553009}, {"doc_source": "concepts/lcel.mdx", "score": 0.9863206148147583}, {"doc_source": "concepts/lcel.mdx", "score": 1.01215398311615}, {"doc_source": "concepts/lcel.mdx", "score": 1.012533187866211}, {"doc_source": "how_to/index.mdx", "score": 1.0161570310592651}, {"doc_source": "concepts/streaming.mdx", "score": 1.0324339866638184}, {"doc_source": "concepts/lcel.mdx", "score": 1.0460660457611084}, {"doc_source": "concepts/streaming.mdx", "score": 1.0574296712875366}, {"doc_source": "concepts/streaming.mdx", "score": 1.0826456546783447}, {"doc_source": "concepts/lcel.mdx", "score": 1.1324347257614136}, {"doc_source": "concepts/streaming.mdx", "score": 1.1393272876739502}]}
{"ts": 1747974410.686267, "query_id": "Q004", "question": "Compare FAISS and Chroma vector stores in LangChain.", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 0.806323766708374}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.037778377532959}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.0529485940933228}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0549564361572266}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0764188766479492}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0956001281738281}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1129543781280518}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1496317386627197}, {"doc_source": "concepts/multimodality.mdx", "score": 1.1514850854873657}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.175119400024414}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.178574800491333}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.1971969604492188}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.242172360420227}, {"doc_source": "concepts/lcel.mdx", "score": 1.2435327768325806}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.260191798210144}]}
{"ts": 1747974412.3305469, "query_id": "Q005", "question": "How can I load a Microsoft Word document into a LangChain pipeline?", "retrieved_items": [{"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.7632331848144531}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.8719604015350342}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.0564827919006348}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0573278665542603}, {"doc_source": "concepts/index.mdx", "score": 1.058683156967163}, {"doc_source": "introduction.mdx", "score": 1.0635589361190796}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0754883289337158}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.075537085533142}, {"doc_source": "tutorials/index.mdx", "score": 1.0889569520950317}, {"doc_source": "how_to/index.mdx", "score": 1.0932178497314453}, {"doc_source": "tutorials/index.mdx", "score": 1.128834843635559}, {"doc_source": "tutorials/index.mdx", "score": 1.137353777885437}, {"doc_source": "how_to/index.mdx", "score": 1.145056128501892}, {"doc_source": "concepts/lcel.mdx", "score": 1.1452560424804688}, {"doc_source": "how_to/index.mdx", "score": 1.1470720767974854}]}
{"ts": 1747974413.724447, "query_id": "Q006", "question": "Describe the Runnable protocol and its advantages.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.7724519371986389}, {"doc_source": "concepts/runnables.mdx", "score": 0.8454568982124329}, {"doc_source": "concepts/runnables.mdx", "score": 0.8861877918243408}, {"doc_source": "concepts/runnables.mdx", "score": 0.9481357336044312}, {"doc_source": "concepts/runnables.mdx", "score": 0.9551618695259094}, {"doc_source": "concepts/lcel.mdx", "score": 0.969807505607605}, {"doc_source": "concepts/runnables.mdx", "score": 1.0510046482086182}, {"doc_source": "concepts/lcel.mdx", "score": 1.0747355222702026}, {"doc_source": "concepts/runnables.mdx", "score": 1.0763626098632812}, {"doc_source": "concepts/runnables.mdx", "score": 1.0817084312438965}, {"doc_source": "concepts/runnables.mdx", "score": 1.1127334833145142}, {"doc_source": "concepts/lcel.mdx", "score": 1.126885175704956}, {"doc_source": "concepts/lcel.mdx", "score": 1.1312940120697021}, {"doc_source": "concepts/runnables.mdx", "score": 1.1360814571380615}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1468580961227417}]}
{"ts": 1747974415.195085, "query_id": "Q007", "question": "What is Few-Shot prompting and how is it supported?", "retrieved_items": [{"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.65395188331604}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.898457944393158}, {"doc_source": "how_to/index.mdx", "score": 1.0790667533874512}, {"doc_source": "concepts/index.mdx", "score": 1.0852378606796265}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.0853203535079956}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0976786613464355}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1166355609893799}, {"doc_source": "how_to/index.mdx", "score": 1.1294190883636475}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1815369129180908}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2032020092010498}, {"doc_source": "how_to/index.mdx", "score": 1.2052701711654663}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2127323150634766}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2271336317062378}, {"doc_source": "how_to/index.mdx", "score": 1.2285244464874268}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.233394742012024}]}
{"ts": 1747974416.8909922, "query_id": "Q008", "question": "Give an example of a memory object that stores chat history.", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.0520497560501099}, {"doc_source": "concepts/chat_history.mdx", "score": 1.08331298828125}, {"doc_source": "concepts/chat_history.mdx", "score": 1.0952194929122925}, {"doc_source": "concepts/chat_history.mdx", "score": 1.106571078300476}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1659780740737915}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2140761613845825}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.227189540863037}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.228520393371582}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2391600608825684}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2477774620056152}, {"doc_source": "concepts/messages.mdx", "score": 1.2542842626571655}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2549846172332764}, {"doc_source": "concepts/streaming.mdx", "score": 1.2613730430603027}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2689050436019897}, {"doc_source": "concepts/index.mdx", "score": 1.2690443992614746}]}
{"ts": 1747974418.362571, "query_id": "Q009", "question": "Outline the steps to trace a LangGraph flow in LangSmith.", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8306165337562561}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8386545181274414}, {"doc_source": "how_to/index.mdx", "score": 0.849303662776947}, {"doc_source": "concepts/tracing.mdx", "score": 0.9179869294166565}, {"doc_source": "how_to/index.mdx", "score": 0.969007134437561}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9924338459968567}, {"doc_source": "introduction.mdx", "score": 1.0402048826217651}, {"doc_source": "concepts/streaming.mdx", "score": 1.0460867881774902}, {"doc_source": "tutorials/index.mdx", "score": 1.0699455738067627}, {"doc_source": "how_to/index.mdx", "score": 1.089228868484497}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0955265760421753}, {"doc_source": "concepts/streaming.mdx", "score": 1.107004165649414}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1456140279769897}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.146463394165039}, {"doc_source": "how_to/installation.mdx", "score": 1.1680586338043213}]}
{"ts": 1747974420.680951, "query_id": "Q010", "question": "What packages must be pinned to use Pydantic v1 with LangChain?", "retrieved_items": [{"doc_source": "how_to/pydantic_compatibility.md", "score": 0.6664668321609497}, {"doc_source": "concepts/async.mdx", "score": 0.9395840764045715}, {"doc_source": "how_to/installation.mdx", "score": 0.9441332817077637}, {"doc_source": "how_to/installation.mdx", "score": 0.9453698396682739}, {"doc_source": "how_to/installation.mdx", "score": 0.9516873359680176}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.990216851234436}, {"doc_source": "how_to/installation.mdx", "score": 0.9911788702011108}, {"doc_source": "how_to/installation.mdx", "score": 0.9952425956726074}, {"doc_source": "how_to/installation.mdx", "score": 1.0311696529388428}, {"doc_source": "concepts/architecture.mdx", "score": 1.061868667602539}, {"doc_source": "concepts/architecture.mdx", "score": 1.1165894269943237}, {"doc_source": "concepts/runnables.mdx", "score": 1.1503750085830688}, {"doc_source": "concepts/async.mdx", "score": 1.1581392288208008}, {"doc_source": "introduction.mdx", "score": 1.1684225797653198}, {"doc_source": "concepts/chat_models.mdx", "score": 1.173921823501587}]}
{"ts": 1747974422.6510432, "query_id": "Q011", "question": "Explain tool calling via OpenAI function messages in LangChain.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7447327971458435}, {"doc_source": "concepts/messages.mdx", "score": 0.7672620415687561}, {"doc_source": "concepts/testing.mdx", "score": 0.8308311700820923}, {"doc_source": "concepts/messages.mdx", "score": 0.8539841771125793}, {"doc_source": "how_to/index.mdx", "score": 0.8576458692550659}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.8648943901062012}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8911707401275635}, {"doc_source": "concepts/testing.mdx", "score": 0.8943482041358948}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9010043740272522}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9070305824279785}, {"doc_source": "concepts/messages.mdx", "score": 0.9096877574920654}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9132900238037109}, {"doc_source": "concepts/chat_models.mdx", "score": 0.917262077331543}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9202529191970825}, {"doc_source": "concepts/tools.mdx", "score": 0.9243170022964478}]}
{"ts": 1747974424.724979, "query_id": "Q012", "question": "How does Retrieval-Augmented Generation (RAG) work in LangChain?", "retrieved_items": [{"doc_source": "concepts/rag.mdx", "score": 0.6001891493797302}, {"doc_source": "concepts/rag.mdx", "score": 0.8109723329544067}, {"doc_source": "concepts/rag.mdx", "score": 0.9092830419540405}, {"doc_source": "how_to/index.mdx", "score": 0.9584965705871582}, {"doc_source": "how_to/index.mdx", "score": 0.9609336853027344}, {"doc_source": "concepts/rag.mdx", "score": 0.9876400232315063}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0011532306671143}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.054542064666748}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0708664655685425}, {"doc_source": "concepts/rag.mdx", "score": 1.075296401977539}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0957509279251099}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1024479866027832}, {"doc_source": "concepts/index.mdx", "score": 1.1052881479263306}, {"doc_source": "how_to/index.mdx", "score": 1.1064003705978394}, {"doc_source": "introduction.mdx", "score": 1.1090004444122314}]}
{"ts": 1747974426.4555688, "query_id": "Q013", "question": "Provide a code snippet to create embeddings with OpenAI via LangChain.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 0.8076019287109375}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8199805021286011}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8551894426345825}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.8890330791473389}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9257434606552124}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9350428581237793}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9415718913078308}, {"doc_source": "introduction.mdx", "score": 0.9477457404136658}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9869521260261536}, {"doc_source": "introduction.mdx", "score": 0.9871881008148193}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0074729919433594}, {"doc_source": "how_to/index.mdx", "score": 1.0239622592926025}, {"doc_source": "how_to/installation.mdx", "score": 1.026763677597046}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0314662456512451}, {"doc_source": "how_to/installation.mdx", "score": 1.032660961151123}]}
{"ts": 1747974428.053807, "query_id": "Q014", "question": "List three callback events emitted during an LLM call.", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 0.8812132477760315}, {"doc_source": "concepts/callbacks.mdx", "score": 0.9978700280189514}, {"doc_source": "how_to/index.mdx", "score": 1.063828706741333}, {"doc_source": "concepts/streaming.mdx", "score": 1.1637150049209595}, {"doc_source": "how_to/index.mdx", "score": 1.1788103580474854}, {"doc_source": "concepts/streaming.mdx", "score": 1.2179994583129883}, {"doc_source": "concepts/callbacks.mdx", "score": 1.233553171157837}, {"doc_source": "how_to/index.mdx", "score": 1.2714643478393555}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2895344495773315}, {"doc_source": "concepts/lcel.mdx", "score": 1.2898828983306885}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2918610572814941}, {"doc_source": "concepts/callbacks.mdx", "score": 1.3097190856933594}, {"doc_source": "concepts/streaming.mdx", "score": 1.3113605976104736}, {"doc_source": "concepts/streaming.mdx", "score": 1.3121944665908813}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3160865306854248}]}
{"ts": 1747974429.2602282, "query_id": "Q015", "question": "Why would you use the AsyncRunnable interface?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9025844931602478}, {"doc_source": "concepts/async.mdx", "score": 0.9286672472953796}, {"doc_source": "concepts/runnables.mdx", "score": 0.9567350745201111}, {"doc_source": "concepts/lcel.mdx", "score": 0.9969164729118347}, {"doc_source": "concepts/runnables.mdx", "score": 1.0076544284820557}, {"doc_source": "concepts/runnables.mdx", "score": 1.0641136169433594}, {"doc_source": "concepts/async.mdx", "score": 1.0721104145050049}, {"doc_source": "concepts/runnables.mdx", "score": 1.084437370300293}, {"doc_source": "concepts/runnables.mdx", "score": 1.0924921035766602}, {"doc_source": "concepts/async.mdx", "score": 1.1125662326812744}, {"doc_source": "concepts/runnables.mdx", "score": 1.1129610538482666}, {"doc_source": "concepts/callbacks.mdx", "score": 1.1375898122787476}, {"doc_source": "concepts/streaming.mdx", "score": 1.1408095359802246}, {"doc_source": "concepts/runnables.mdx", "score": 1.1424119472503662}, {"doc_source": "concepts/runnables.mdx", "score": 1.1435871124267578}]}
{"ts": 1747974430.470222, "query_id": "Q016", "question": "What metrics does LangChain's evaluation module compute out-of-box?", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8660074472427368}, {"doc_source": "concepts/evaluation.mdx", "score": 0.9074095487594604}, {"doc_source": "how_to/index.mdx", "score": 0.9124665260314941}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9286249279975891}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9342960715293884}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9463154077529907}, {"doc_source": "how_to/index.mdx", "score": 0.9599213004112244}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9808132648468018}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9954172372817993}, {"doc_source": "how_to/index.mdx", "score": 1.0373879671096802}, {"doc_source": "concepts/lcel.mdx", "score": 1.0417304039001465}, {"doc_source": "concepts/streaming.mdx", "score": 1.0570991039276123}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0621938705444336}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0649007558822632}, {"doc_source": "how_to/index.mdx", "score": 1.0654006004333496}]}
{"ts": 1747974432.8164449, "query_id": "Q017", "question": "Demonstrate how to chunk text using RecursiveCharacterTextSplitter.", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.810623288154602}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9813220500946045}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0011482238769531}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0026135444641113}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0188729763031006}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0552606582641602}, {"doc_source": "how_to/index.mdx", "score": 1.0799036026000977}, {"doc_source": "how_to/index.mdx", "score": 1.096592664718628}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1518950462341309}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1683926582336426}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2844328880310059}, {"doc_source": "concepts/streaming.mdx", "score": 1.3953427076339722}, {"doc_source": "concepts/lcel.mdx", "score": 1.4134962558746338}, {"doc_source": "concepts/streaming.mdx", "score": 1.4184529781341553}, {"doc_source": "how_to/index.mdx", "score": 1.438891053199768}]}
{"ts": 1747974434.615454, "query_id": "Q018", "question": "What environment variable controls OpenAI API retries in LangChain?", "retrieved_items": [{"doc_source": "concepts/architecture.mdx", "score": 1.166106939315796}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1742563247680664}, {"doc_source": "concepts/async.mdx", "score": 1.1810634136199951}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1889452934265137}, {"doc_source": "concepts/async.mdx", "score": 1.2010347843170166}, {"doc_source": "how_to/installation.mdx", "score": 1.2060797214508057}, {"doc_source": "concepts/async.mdx", "score": 1.2172753810882568}, {"doc_source": "concepts/messages.mdx", "score": 1.2186334133148193}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2191307544708252}, {"doc_source": "concepts/async.mdx", "score": 1.2197930812835693}, {"doc_source": "concepts/async.mdx", "score": 1.2265474796295166}, {"doc_source": "how_to/installation.mdx", "score": 1.2282994985580444}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2286524772644043}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2381477355957031}, {"doc_source": "concepts/streaming.mdx", "score": 1.2397561073303223}]}
{"ts": 1747974435.844007, "query_id": "Q019", "question": "How can key-value stores be used to cache intermediate chain output?", "retrieved_items": [{"doc_source": "concepts/key_value_stores.mdx", "score": 0.9596561193466187}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.0025532245635986}, {"doc_source": "concepts/streaming.mdx", "score": 1.1447491645812988}, {"doc_source": "concepts/lcel.mdx", "score": 1.1683268547058105}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.262619972229004}, {"doc_source": "concepts/lcel.mdx", "score": 1.3031315803527832}, {"doc_source": "concepts/chat_models.mdx", "score": 1.314720630645752}, {"doc_source": "concepts/lcel.mdx", "score": 1.3152283430099487}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3171377182006836}, {"doc_source": "concepts/lcel.mdx", "score": 1.323241114616394}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.343809962272644}, {"doc_source": "concepts/lcel.mdx", "score": 1.3476201295852661}, {"doc_source": "concepts/lcel.mdx", "score": 1.3536109924316406}, {"doc_source": "concepts/streaming.mdx", "score": 1.3567309379577637}, {"doc_source": "how_to/index.mdx", "score": 1.3574750423431396}]}
{"ts": 1747974437.2073321, "query_id": "Q020", "question": "Explain \"Multimodality\" support in LangChain with an example.", "retrieved_items": [{"doc_source": "concepts/multimodality.mdx", "score": 0.6334351301193237}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7256749272346497}, {"doc_source": "concepts/multimodality.mdx", "score": 0.8758634328842163}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9222817420959473}, {"doc_source": "concepts/tokens.mdx", "score": 0.9279479384422302}, {"doc_source": "concepts/index.mdx", "score": 0.9438893795013428}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9458745718002319}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9548799991607666}, {"doc_source": "how_to/index.mdx", "score": 0.9740550518035889}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9827178120613098}, {"doc_source": "concepts/lcel.mdx", "score": 0.9875329732894897}, {"doc_source": "how_to/index.mdx", "score": 0.9898403286933899}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9970738291740417}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0054562091827393}, {"doc_source": "concepts/streaming.mdx", "score": 1.0112998485565186}]}
{"ts": 1747974438.845257, "query_id": "Q021", "question": "What is the LangChain architecture diagram trying to convey?", "retrieved_items": [{"doc_source": "introduction.mdx", "score": 0.8011831045150757}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8223511576652527}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8373987674713135}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8459807634353638}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8653424978256226}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8845939040184021}, {"doc_source": "introduction.mdx", "score": 0.8881816864013672}, {"doc_source": "introduction.mdx", "score": 0.8932554721832275}, {"doc_source": "how_to/index.mdx", "score": 0.9195669889450073}, {"doc_source": "concepts/lcel.mdx", "score": 0.9283061623573303}, {"doc_source": "concepts/architecture.mdx", "score": 0.9346671104431152}, {"doc_source": "concepts/index.mdx", "score": 0.9394577145576477}, {"doc_source": "tutorials/index.mdx", "score": 0.9528219699859619}, {"doc_source": "concepts/architecture.mdx", "score": 0.9791138172149658}, {"doc_source": "concepts/index.mdx", "score": 0.9890123605728149}]}
{"ts": 1747974440.465698, "query_id": "Q022", "question": "Provide two example selectors and when to use them.", "retrieved_items": [{"doc_source": "concepts/example_selectors.mdx", "score": 0.8706028461456299}, {"doc_source": "how_to/index.mdx", "score": 1.0569872856140137}, {"doc_source": "how_to/index.mdx", "score": 1.1754515171051025}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2144951820373535}, {"doc_source": "concepts/retrieval.mdx", "score": 1.219663143157959}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.321818470954895}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3460285663604736}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3490889072418213}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3766906261444092}, {"doc_source": "concepts/retrievers.mdx", "score": 1.388052225112915}, {"doc_source": "how_to/index.mdx", "score": 1.3985549211502075}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.4052791595458984}, {"doc_source": "concepts/index.mdx", "score": 1.4129005670547485}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.4287936687469482}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.4399715662002563}]}
{"ts": 1747974442.334404, "query_id": "Q023", "question": "How do OutputParsers simplify post-processing in LCEL?", "retrieved_items": [{"doc_source": "concepts/output_parsers.mdx", "score": 0.9455335140228271}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.9982435703277588}, {"doc_source": "concepts/lcel.mdx", "score": 1.0195881128311157}, {"doc_source": "concepts/lcel.mdx", "score": 1.0724183320999146}, {"doc_source": "how_to/index.mdx", "score": 1.0906825065612793}, {"doc_source": "how_to/index.mdx", "score": 1.1119800806045532}, {"doc_source": "concepts/index.mdx", "score": 1.1887753009796143}, {"doc_source": "concepts/lcel.mdx", "score": 1.200204849243164}, {"doc_source": "concepts/lcel.mdx", "score": 1.2074003219604492}, {"doc_source": "how_to/index.mdx", "score": 1.2140668630599976}, {"doc_source": "concepts/lcel.mdx", "score": 1.2310104370117188}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.236943006515503}, {"doc_source": "concepts/streaming.mdx", "score": 1.238986611366272}, {"doc_source": "concepts/streaming.mdx", "score": 1.260324478149414}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.266735315322876}]}
{"ts": 1747974443.781565, "query_id": "Q024", "question": "Describe streaming chunk callbacks versus token callbacks.", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 1.0604393482208252}, {"doc_source": "concepts/streaming.mdx", "score": 1.064861536026001}, {"doc_source": "concepts/streaming.mdx", "score": 1.1107678413391113}, {"doc_source": "concepts/streaming.mdx", "score": 1.1238956451416016}, {"doc_source": "concepts/streaming.mdx", "score": 1.1324293613433838}, {"doc_source": "concepts/streaming.mdx", "score": 1.1372489929199219}, {"doc_source": "concepts/streaming.mdx", "score": 1.137282371520996}, {"doc_source": "concepts/streaming.mdx", "score": 1.150165319442749}, {"doc_source": "concepts/lcel.mdx", "score": 1.1529057025909424}, {"doc_source": "concepts/streaming.mdx", "score": 1.1696388721466064}, {"doc_source": "concepts/streaming.mdx", "score": 1.1774295568466187}, {"doc_source": "concepts/runnables.mdx", "score": 1.1832433938980103}, {"doc_source": "concepts/callbacks.mdx", "score": 1.1855175495147705}, {"doc_source": "concepts/streaming.mdx", "score": 1.1957030296325684}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2010231018066406}]}
{"ts": 1747974445.1294742, "query_id": "Q025", "question": "What is the purpose of the `ConversationBufferMemory` class and when should it be used?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.194882869720459}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2198406457901}, {"doc_source": "concepts/chat_models.mdx", "score": 1.236889123916626}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2719743251800537}, {"doc_source": "concepts/chat_models.mdx", "score": 1.323313593864441}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3519634008407593}, {"doc_source": "concepts/messages.mdx", "score": 1.355742335319519}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3742501735687256}, {"doc_source": "concepts/chat_models.mdx", "score": 1.375274419784546}, {"doc_source": "concepts/chat_models.mdx", "score": 1.380083680152893}, {"doc_source": "how_to/index.mdx", "score": 1.387562870979309}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3897444009780884}, {"doc_source": "concepts/streaming.mdx", "score": 1.3922309875488281}, {"doc_source": "concepts/messages.mdx", "score": 1.4016337394714355}, {"doc_source": "concepts/index.mdx", "score": 1.404270052909851}]}
{"ts": 1747974447.280145, "query_id": "Q026", "question": "Which LangChain text splitter lets you split documents by tokens and why would you choose it over a character splitter?", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.6639671921730042}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.7723296880722046}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.8009575009346008}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.8805655837059021}, {"doc_source": "concepts/tokens.mdx", "score": 0.8922033905982971}, {"doc_source": "concepts/lcel.mdx", "score": 0.9271477460861206}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9462777376174927}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9688692092895508}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9796664714813232}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0263170003890991}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0373482704162598}, {"doc_source": "concepts/index.mdx", "score": 1.0415388345718384}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0477497577667236}, {"doc_source": "introduction.mdx", "score": 1.0486621856689453}, {"doc_source": "how_to/index.mdx", "score": 1.056288242340088}]}
{"ts": 1747974450.5902069, "query_id": "Q027", "question": "List two differences between `VectorStoreRetriever` and `EnsembleRetriever`.", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.7453963160514832}, {"doc_source": "concepts/retrievers.mdx", "score": 0.80172199010849}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8829959630966187}, {"doc_source": "how_to/index.mdx", "score": 1.0258440971374512}, {"doc_source": "concepts/retrievers.mdx", "score": 1.069481372833252}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1028382778167725}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1190979480743408}, {"doc_source": "how_to/index.mdx", "score": 1.1345804929733276}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1535923480987549}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1562185287475586}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1667084693908691}, {"doc_source": "concepts/retrieval.mdx", "score": 1.247297763824463}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2546985149383545}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2573353052139282}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2667609453201294}]}
{"ts": 1747974457.978448, "query_id": "Q028", "question": "How can you configure a chat model to automatically retry on ratelimiting errors?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5967234373092651}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7230130434036255}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9779880046844482}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0450061559677124}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0815215110778809}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1284717321395874}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1517705917358398}, {"doc_source": "concepts/streaming.mdx", "score": 1.1570521593093872}, {"doc_source": "concepts/chat_models.mdx", "score": 1.160644292831421}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1744003295898438}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1978473663330078}, {"doc_source": "concepts/chat_models.mdx", "score": 1.202340006828308}, {"doc_source": "concepts/runnables.mdx", "score": 1.207148790359497}, {"doc_source": "concepts/rag.mdx", "score": 1.2116070985794067}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2159106731414795}]}
{"ts": 1747974462.8368158, "query_id": "Q029", "question": "Explain how `InjectedToolArg` affects a tool schema and give one use case.", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.615127682685852}, {"doc_source": "concepts/tools.mdx", "score": 0.6404827237129211}, {"doc_source": "concepts/tools.mdx", "score": 0.9967231750488281}, {"doc_source": "concepts/tools.mdx", "score": 1.0166620016098022}, {"doc_source": "concepts/tools.mdx", "score": 1.0168594121932983}, {"doc_source": "concepts/tools.mdx", "score": 1.0186424255371094}, {"doc_source": "concepts/tools.mdx", "score": 1.045816421508789}, {"doc_source": "concepts/tools.mdx", "score": 1.076610803604126}, {"doc_source": "concepts/tools.mdx", "score": 1.0918231010437012}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0987728834152222}, {"doc_source": "concepts/tools.mdx", "score": 1.1141679286956787}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1170544624328613}, {"doc_source": "concepts/tools.mdx", "score": 1.1255568265914917}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.1269255876541138}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1521060466766357}]}
{"ts": 1747974465.3107631, "query_id": "Q030", "question": "Which decorator is recommended for quickly turning a Python function into a LangChain Tool?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8549029231071472}, {"doc_source": "concepts/runnables.mdx", "score": 0.9633462429046631}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.969854474067688}, {"doc_source": "concepts/tools.mdx", "score": 1.007396936416626}, {"doc_source": "concepts/tools.mdx", "score": 1.0290944576263428}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0300372838974}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0335427522659302}, {"doc_source": "concepts/async.mdx", "score": 1.053871512413025}, {"doc_source": "concepts/tools.mdx", "score": 1.0605661869049072}, {"doc_source": "concepts/async.mdx", "score": 1.0753388404846191}, {"doc_source": "concepts/lcel.mdx", "score": 1.0760059356689453}, {"doc_source": "concepts/tools.mdx", "score": 1.0790026187896729}, {"doc_source": "concepts/async.mdx", "score": 1.0833125114440918}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1050561666488647}, {"doc_source": "concepts/testing.mdx", "score": 1.1092727184295654}]}
{"ts": 1747974466.779829, "query_id": "Q031", "question": "Describe how to batch inputs through a Runnable for better throughput.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.60727459192276}, {"doc_source": "concepts/runnables.mdx", "score": 0.7842290997505188}, {"doc_source": "concepts/runnables.mdx", "score": 0.9015371203422546}, {"doc_source": "concepts/runnables.mdx", "score": 0.9195675253868103}, {"doc_source": "concepts/lcel.mdx", "score": 0.9342222213745117}, {"doc_source": "concepts/runnables.mdx", "score": 1.0410566329956055}, {"doc_source": "concepts/runnables.mdx", "score": 1.0663800239562988}, {"doc_source": "concepts/async.mdx", "score": 1.1186548471450806}, {"doc_source": "concepts/runnables.mdx", "score": 1.132509708404541}, {"doc_source": "concepts/runnables.mdx", "score": 1.1374541521072388}, {"doc_source": "concepts/runnables.mdx", "score": 1.1381354331970215}, {"doc_source": "concepts/runnables.mdx", "score": 1.1658589839935303}, {"doc_source": "concepts/runnables.mdx", "score": 1.167335867881775}, {"doc_source": "concepts/runnables.mdx", "score": 1.1727924346923828}, {"doc_source": "concepts/runnables.mdx", "score": 1.1834015846252441}]}
{"ts": 1747974468.713202, "query_id": "Q032", "question": "What parameter in `RunnableConfig` controls the maximum number of parallel calls during batching?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8542652726173401}, {"doc_source": "concepts/runnables.mdx", "score": 0.8995379209518433}, {"doc_source": "concepts/runnables.mdx", "score": 0.9033696055412292}, {"doc_source": "concepts/runnables.mdx", "score": 0.9460870027542114}, {"doc_source": "concepts/runnables.mdx", "score": 0.9960886240005493}, {"doc_source": "concepts/lcel.mdx", "score": 1.0376683473587036}, {"doc_source": "concepts/runnables.mdx", "score": 1.0697987079620361}, {"doc_source": "concepts/runnables.mdx", "score": 1.093762993812561}, {"doc_source": "concepts/runnables.mdx", "score": 1.096153736114502}, {"doc_source": "concepts/runnables.mdx", "score": 1.135257601737976}, {"doc_source": "concepts/runnables.mdx", "score": 1.1440680027008057}, {"doc_source": "concepts/runnables.mdx", "score": 1.1582269668579102}, {"doc_source": "concepts/runnables.mdx", "score": 1.1750770807266235}, {"doc_source": "concepts/runnables.mdx", "score": 1.213775873184204}, {"doc_source": "concepts/lcel.mdx", "score": 1.219487190246582}]}
{"ts": 1747974469.8683, "query_id": "Q033", "question": "Give an example of using `with_structured_output` to get JSON back from a chat model.", "retrieved_items": [{"doc_source": "concepts/structured_outputs.mdx", "score": 0.7788553833961487}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8992528319358826}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.9395872354507446}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9606637954711914}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9786237478256226}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.9876483678817749}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9997878074645996}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.002611756324768}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0111923217773438}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0279462337493896}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0541388988494873}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.082299828529358}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.116208553314209}, {"doc_source": "concepts/index.mdx", "score": 1.117616891860962}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.1191314458847046}]}
{"ts": 1747974472.376897, "query_id": "Q034", "question": "What callback would you use to stream intermediate events from a Runnable?", "retrieved_items": [{"doc_source": "concepts/callbacks.mdx", "score": 0.8936169147491455}, {"doc_source": "concepts/streaming.mdx", "score": 0.9887364506721497}, {"doc_source": "concepts/streaming.mdx", "score": 1.0082886219024658}, {"doc_source": "concepts/streaming.mdx", "score": 1.011270523071289}, {"doc_source": "concepts/runnables.mdx", "score": 1.0389477014541626}, {"doc_source": "concepts/runnables.mdx", "score": 1.0664324760437012}, {"doc_source": "concepts/callbacks.mdx", "score": 1.0955580472946167}, {"doc_source": "concepts/runnables.mdx", "score": 1.0956356525421143}, {"doc_source": "concepts/streaming.mdx", "score": 1.0969667434692383}, {"doc_source": "concepts/streaming.mdx", "score": 1.1347041130065918}, {"doc_source": "concepts/runnables.mdx", "score": 1.1395747661590576}, {"doc_source": "concepts/streaming.mdx", "score": 1.1487879753112793}, {"doc_source": "concepts/streaming.mdx", "score": 1.152193546295166}, {"doc_source": "concepts/runnables.mdx", "score": 1.1572115421295166}, {"doc_source": "concepts/streaming.mdx", "score": 1.1613118648529053}]}
{"ts": 1747974473.988204, "query_id": "Q035", "question": "Which two standard parameters influence generation randomness and length in chat models?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.9557917714118958}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0020349025726318}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0073881149291992}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0433661937713623}, {"doc_source": "concepts/chat_models.mdx", "score": 1.065783977508545}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1297993659973145}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1472969055175781}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1511142253875732}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1538963317871094}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1684739589691162}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1784284114837646}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1825515031814575}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1945102214813232}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2134039402008057}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2174088954925537}]}
{"ts": 1747974475.45362, "query_id": "Q036", "question": "How do you persist a FAISS vector store to disk?", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 1.1071107387542725}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.165313959121704}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1924409866333008}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.1964092254638672}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.2159513235092163}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2579549551010132}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2647576332092285}, {"doc_source": "concepts/multimodality.mdx", "score": 1.279651165008545}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2844500541687012}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2856274843215942}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3002053499221802}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.3207850456237793}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3381010293960571}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3469091653823853}, {"doc_source": "concepts/key_value_stores.mdx", "score": 1.3684356212615967}]}
{"ts": 1747974476.512198, "query_id": "Q037", "question": "Explain the difference between `stream` and `astream` methods on a Runnable.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.713911235332489}, {"doc_source": "concepts/streaming.mdx", "score": 0.7504515051841736}, {"doc_source": "concepts/streaming.mdx", "score": 0.7569279670715332}, {"doc_source": "concepts/runnables.mdx", "score": 0.7937798500061035}, {"doc_source": "concepts/streaming.mdx", "score": 0.822458028793335}, {"doc_source": "concepts/streaming.mdx", "score": 0.840156078338623}, {"doc_source": "concepts/runnables.mdx", "score": 0.8814693689346313}, {"doc_source": "concepts/runnables.mdx", "score": 1.0070042610168457}, {"doc_source": "concepts/streaming.mdx", "score": 1.0271800756454468}, {"doc_source": "concepts/runnables.mdx", "score": 1.0559834241867065}, {"doc_source": "concepts/runnables.mdx", "score": 1.1138025522232056}, {"doc_source": "concepts/async.mdx", "score": 1.1155859231948853}, {"doc_source": "concepts/streaming.mdx", "score": 1.1172935962677002}, {"doc_source": "concepts/streaming.mdx", "score": 1.129162311553955}, {"doc_source": "concepts/runnables.mdx", "score": 1.1471195220947266}]}
{"ts": 1747974478.333653, "query_id": "Q038", "question": "What feature does LangChain provide to ensure prompts fit within an LLM's context window?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.854735255241394}, {"doc_source": "how_to/index.mdx", "score": 0.9119795560836792}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9303054213523865}, {"doc_source": "how_to/index.mdx", "score": 0.946584165096283}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9485878944396973}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.964057445526123}, {"doc_source": "concepts/lcel.mdx", "score": 0.9698225855827332}, {"doc_source": "how_to/index.mdx", "score": 0.9735656380653381}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9806835651397705}, {"doc_source": "concepts/streaming.mdx", "score": 0.9843946099281311}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9859236478805542}, {"doc_source": "concepts/index.mdx", "score": 0.9908485412597656}, {"doc_source": "tutorials/index.mdx", "score": 0.9972068071365356}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9977144598960876}, {"doc_source": "concepts/streaming.mdx", "score": 1.0044549703598022}]}
{"ts": 1747974480.397734, "query_id": "Q039", "question": "Name three message roles supported by LangChain chat messages.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 0.7483441233634949}, {"doc_source": "concepts/messages.mdx", "score": 0.7781182527542114}, {"doc_source": "concepts/messages.mdx", "score": 0.8879894018173218}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9013852477073669}, {"doc_source": "concepts/messages.mdx", "score": 0.9076243042945862}, {"doc_source": "concepts/messages.mdx", "score": 0.9360724687576294}, {"doc_source": "concepts/messages.mdx", "score": 0.9397140741348267}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0096782445907593}, {"doc_source": "concepts/multimodality.mdx", "score": 1.0166736841201782}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0169320106506348}, {"doc_source": "how_to/index.mdx", "score": 1.0291216373443604}, {"doc_source": "concepts/messages.mdx", "score": 1.0419305562973022}, {"doc_source": "concepts/streaming.mdx", "score": 1.0545978546142578}, {"doc_source": "concepts/messages.mdx", "score": 1.0594207048416138}, {"doc_source": "concepts/index.mdx", "score": 1.0652755498886108}]}
{"ts": 1747974481.657749, "query_id": "Q040", "question": "How can you combine docs returned by a Retriever before passing to an LLM?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.8401700258255005}, {"doc_source": "concepts/retrievers.mdx", "score": 0.8477263450622559}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9800206422805786}, {"doc_source": "concepts/retrievers.mdx", "score": 1.001344084739685}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0328223705291748}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0523030757904053}, {"doc_source": "how_to/index.mdx", "score": 1.0554286241531372}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0583717823028564}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0684733390808105}, {"doc_source": "concepts/rag.mdx", "score": 1.1213154792785645}, {"doc_source": "concepts/rag.mdx", "score": 1.1352429389953613}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1411254405975342}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1424391269683838}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1461389064788818}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1568372249603271}]}
{"ts": 1747974483.290176, "query_id": "Q041", "question": "What does the `rate_limiter` parameter accept and why is it useful?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.9187310934066772}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9422239661216736}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0711052417755127}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1763826608657837}, {"doc_source": "concepts/runnables.mdx", "score": 1.2191627025604248}, {"doc_source": "concepts/runnables.mdx", "score": 1.2705492973327637}, {"doc_source": "concepts/runnables.mdx", "score": 1.3260612487792969}, {"doc_source": "concepts/chat_models.mdx", "score": 1.337694764137268}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3555595874786377}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3742976188659668}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3992327451705933}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4093446731567383}, {"doc_source": "concepts/runnables.mdx", "score": 1.413640022277832}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4155991077423096}, {"doc_source": "concepts/runnables.mdx", "score": 1.428739070892334}]}
{"ts": 1747974484.54795, "query_id": "Q042", "question": "Provide a code snippet to stream tokens from an OpenAI chat model using LCEL.", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.8582490682601929}, {"doc_source": "concepts/chat_models.mdx", "score": 0.956898033618927}, {"doc_source": "concepts/streaming.mdx", "score": 0.9768795967102051}, {"doc_source": "concepts/streaming.mdx", "score": 0.978650689125061}, {"doc_source": "concepts/streaming.mdx", "score": 0.9934386014938354}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0012500286102295}, {"doc_source": "concepts/streaming.mdx", "score": 1.0133063793182373}, {"doc_source": "concepts/streaming.mdx", "score": 1.015396237373352}, {"doc_source": "concepts/streaming.mdx", "score": 1.0284532308578491}, {"doc_source": "concepts/lcel.mdx", "score": 1.0327088832855225}, {"doc_source": "concepts/messages.mdx", "score": 1.034729242324829}, {"doc_source": "concepts/streaming.mdx", "score": 1.040419340133667}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0514034032821655}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0535870790481567}, {"doc_source": "concepts/rag.mdx", "score": 1.059841513633728}]}
{"ts": 1747974486.548484, "query_id": "Q043", "question": "Which notebook demonstrates how to track token usage costs per request?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 1.27592933177948}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2869781255722046}, {"doc_source": "how_to/index.mdx", "score": 1.3038727045059204}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3272309303283691}, {"doc_source": "concepts/tokens.mdx", "score": 1.331723690032959}, {"doc_source": "concepts/index.mdx", "score": 1.3327587842941284}, {"doc_source": "concepts/tokens.mdx", "score": 1.3361790180206299}, {"doc_source": "introduction.mdx", "score": 1.347623348236084}, {"doc_source": "how_to/index.mdx", "score": 1.3510795831680298}, {"doc_source": "tutorials/index.mdx", "score": 1.3523105382919312}, {"doc_source": "how_to/index.mdx", "score": 1.3527593612670898}, {"doc_source": "concepts/tokens.mdx", "score": 1.360180377960205}, {"doc_source": "concepts/index.mdx", "score": 1.3631162643432617}, {"doc_source": "tutorials/index.mdx", "score": 1.366808533668518}, {"doc_source": "how_to/index.mdx", "score": 1.3668568134307861}]}
{"ts": 1747974487.3631501, "query_id": "Q044", "question": "What are tool artifacts and how can they be returned from a tool?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.727272093296051}, {"doc_source": "how_to/index.mdx", "score": 0.8021578192710876}, {"doc_source": "concepts/tools.mdx", "score": 0.854106068611145}, {"doc_source": "concepts/tools.mdx", "score": 0.9144396781921387}, {"doc_source": "concepts/tools.mdx", "score": 1.0441399812698364}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1078284978866577}, {"doc_source": "how_to/index.mdx", "score": 1.10982084274292}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1312429904937744}, {"doc_source": "concepts/messages.mdx", "score": 1.146448016166687}, {"doc_source": "concepts/tools.mdx", "score": 1.160258412361145}, {"doc_source": "concepts/tools.mdx", "score": 1.1642193794250488}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1687871217727661}, {"doc_source": "concepts/tools.mdx", "score": 1.1772897243499756}, {"doc_source": "concepts/tools.mdx", "score": 1.1785367727279663}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1902446746826172}]}
{"ts": 1747974489.7326121, "query_id": "Q045", "question": "Explain how to asynchronously invoke a Runnable and await the result.", "retrieved_items": [{"doc_source": "concepts/async.mdx", "score": 0.8700994253158569}, {"doc_source": "concepts/runnables.mdx", "score": 0.941871702671051}, {"doc_source": "concepts/async.mdx", "score": 1.006682276725769}, {"doc_source": "concepts/runnables.mdx", "score": 1.0403202772140503}, {"doc_source": "concepts/runnables.mdx", "score": 1.0637942552566528}, {"doc_source": "concepts/runnables.mdx", "score": 1.0663615465164185}, {"doc_source": "concepts/runnables.mdx", "score": 1.1072101593017578}, {"doc_source": "concepts/lcel.mdx", "score": 1.1443973779678345}, {"doc_source": "concepts/lcel.mdx", "score": 1.1533472537994385}, {"doc_source": "concepts/runnables.mdx", "score": 1.1655141115188599}, {"doc_source": "concepts/runnables.mdx", "score": 1.171178936958313}, {"doc_source": "concepts/runnables.mdx", "score": 1.1744595766067505}, {"doc_source": "concepts/runnables.mdx", "score": 1.1982861757278442}, {"doc_source": "concepts/runnables.mdx", "score": 1.2001253366470337}, {"doc_source": "concepts/lcel.mdx", "score": 1.2035157680511475}]}
{"ts": 1747974493.065845, "query_id": "Q046", "question": "Which two documents should you read to understand LangGraph node construction?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.8442721962928772}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9655553102493286}, {"doc_source": "tutorials/index.mdx", "score": 0.9927608966827393}, {"doc_source": "how_to/index.mdx", "score": 1.0147225856781006}, {"doc_source": "concepts/streaming.mdx", "score": 1.029054880142212}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0588780641555786}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0668433904647827}, {"doc_source": "introduction.mdx", "score": 1.095724105834961}, {"doc_source": "introduction.mdx", "score": 1.100203037261963}, {"doc_source": "concepts/index.mdx", "score": 1.1154401302337646}, {"doc_source": "introduction.mdx", "score": 1.1279008388519287}, {"doc_source": "how_to/index.mdx", "score": 1.130448579788208}, {"doc_source": "tutorials/index.mdx", "score": 1.135604739189148}, {"doc_source": "concepts/index.mdx", "score": 1.1411046981811523}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1425261497497559}]}
{"ts": 1747974494.5350552, "query_id": "Q047", "question": "Describe the process of converting a Runnable into a Tool for agent use.", "retrieved_items": [{"doc_source": "concepts/tool_calling.mdx", "score": 0.8793895244598389}, {"doc_source": "how_to/index.mdx", "score": 0.9111272096633911}, {"doc_source": "concepts/runnables.mdx", "score": 1.0066287517547607}, {"doc_source": "concepts/runnables.mdx", "score": 1.0178998708724976}, {"doc_source": "concepts/runnables.mdx", "score": 1.0312137603759766}, {"doc_source": "concepts/runnables.mdx", "score": 1.0385303497314453}, {"doc_source": "concepts/tools.mdx", "score": 1.0550682544708252}, {"doc_source": "how_to/index.mdx", "score": 1.055663824081421}, {"doc_source": "concepts/runnables.mdx", "score": 1.0685560703277588}, {"doc_source": "concepts/tools.mdx", "score": 1.073490858078003}, {"doc_source": "concepts/tools.mdx", "score": 1.1141221523284912}, {"doc_source": "concepts/runnables.mdx", "score": 1.123765468597412}, {"doc_source": "concepts/tools.mdx", "score": 1.128460168838501}, {"doc_source": "concepts/runnables.mdx", "score": 1.1299278736114502}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1603796482086182}]}
{"ts": 1747974495.866327, "query_id": "Q048", "question": "What Python package contains community-contributed chat model integrations?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7925833463668823}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9204559922218323}, {"doc_source": "concepts/architecture.mdx", "score": 0.9720733761787415}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9905232191085815}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0169777870178223}, {"doc_source": "concepts/chat_models.mdx", "score": 1.046437382698059}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0605427026748657}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0620002746582031}, {"doc_source": "concepts/architecture.mdx", "score": 1.068495512008667}, {"doc_source": "concepts/index.mdx", "score": 1.0761137008666992}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0797456502914429}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0934478044509888}, {"doc_source": "concepts/chat_models.mdx", "score": 1.094994306564331}, {"doc_source": "how_to/installation.mdx", "score": 1.09523606300354}, {"doc_source": "how_to/installation.mdx", "score": 1.1014119386672974}]}
{"ts": 1747974497.1693702, "query_id": "Q049", "question": "How can semantic caching improve LLM response times and what extra model does it rely on?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7440979480743408}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9080697298049927}, {"doc_source": "how_to/index.mdx", "score": 1.0342024564743042}, {"doc_source": "concepts/retrieval.mdx", "score": 1.09732985496521}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1074485778808594}, {"doc_source": "how_to/index.mdx", "score": 1.1665376424789429}, {"doc_source": "how_to/index.mdx", "score": 1.171736478805542}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1818196773529053}, {"doc_source": "concepts/rag.mdx", "score": 1.1864943504333496}, {"doc_source": "concepts/lcel.mdx", "score": 1.1926460266113281}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1956417560577393}, {"doc_source": "concepts/rag.mdx", "score": 1.1985766887664795}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2018204927444458}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2224962711334229}, {"doc_source": "concepts/streaming.mdx", "score": 1.223001480102539}]}
{"ts": 1747974498.9999092, "query_id": "Q050", "question": "Which document loader would you use for processing JSON lines files?", "retrieved_items": [{"doc_source": "how_to/document_loader_json.mdx", "score": 0.6945672035217285}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.8326038718223572}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.8513107299804688}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.9373772144317627}, {"doc_source": "how_to/document_loader_json.mdx", "score": 0.9552669525146484}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.056041955947876}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.0615594387054443}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.0654962062835693}, {"doc_source": "how_to/index.mdx", "score": 1.067831039428711}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0744415521621704}, {"doc_source": "how_to/index.mdx", "score": 1.0878868103027344}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.1030635833740234}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.1608220338821411}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.1694434881210327}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2417174577713013}]}
{"ts": 1747974500.368768, "query_id": "Q051", "question": "What is an `OutputParser` and why is it useful in LCEL chains?", "retrieved_items": [{"doc_source": "concepts/output_parsers.mdx", "score": 0.6667928695678711}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.8721698522567749}, {"doc_source": "how_to/index.mdx", "score": 0.8758141994476318}, {"doc_source": "concepts/lcel.mdx", "score": 0.9584242105484009}, {"doc_source": "concepts/index.mdx", "score": 0.9734724164009094}, {"doc_source": "how_to/index.mdx", "score": 0.9738172292709351}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.990077793598175}, {"doc_source": "concepts/output_parsers.mdx", "score": 0.9982463121414185}, {"doc_source": "concepts/lcel.mdx", "score": 1.0474107265472412}, {"doc_source": "concepts/streaming.mdx", "score": 1.0504469871520996}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.0792319774627686}, {"doc_source": "concepts/streaming.mdx", "score": 1.0876919031143188}, {"doc_source": "concepts/lcel.mdx", "score": 1.0882914066314697}, {"doc_source": "concepts/lcel.mdx", "score": 1.093258261680603}, {"doc_source": "concepts/lcel.mdx", "score": 1.105423092842102}]}
{"ts": 1747974501.88247, "query_id": "Q052", "question": "Give an example selector that prioritizes shorter examples and describe its use case.", "retrieved_items": [{"doc_source": "concepts/example_selectors.mdx", "score": 0.7887049317359924}, {"doc_source": "how_to/index.mdx", "score": 0.8991820812225342}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0411700010299683}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0678659677505493}, {"doc_source": "how_to/index.mdx", "score": 1.0706192255020142}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1151505708694458}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2267123460769653}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2374786138534546}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2498239278793335}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2877274751663208}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.289049744606018}, {"doc_source": "concepts/index.mdx", "score": 1.2927141189575195}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2982432842254639}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3596129417419434}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.370265007019043}]}
{"ts": 1747974503.461459, "query_id": "Q053", "question": "Which environment variable can override OpenAI API base URL in LangChain?", "retrieved_items": [{"doc_source": "how_to/installation.mdx", "score": 1.1172690391540527}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1245015859603882}, {"doc_source": "concepts/messages.mdx", "score": 1.1472233533859253}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1504149436950684}, {"doc_source": "how_to/installation.mdx", "score": 1.1658716201782227}, {"doc_source": "concepts/architecture.mdx", "score": 1.180288553237915}, {"doc_source": "introduction.mdx", "score": 1.1882423162460327}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1907625198364258}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1974210739135742}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1980302333831787}, {"doc_source": "introduction.mdx", "score": 1.2038071155548096}, {"doc_source": "concepts/architecture.mdx", "score": 1.2150046825408936}, {"doc_source": "concepts/async.mdx", "score": 1.2154819965362549}, {"doc_source": "concepts/async.mdx", "score": 1.2265604734420776}, {"doc_source": "concepts/async.mdx", "score": 1.2304750680923462}]}
{"ts": 1747974504.212254, "query_id": "Q054", "question": "Describe how to attach custom metadata to a Runnable run for later inspection in LangSmith.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8648906946182251}, {"doc_source": "concepts/runnables.mdx", "score": 1.063988447189331}, {"doc_source": "concepts/runnables.mdx", "score": 1.0841691493988037}, {"doc_source": "concepts/runnables.mdx", "score": 1.088952660560608}, {"doc_source": "concepts/runnables.mdx", "score": 1.0988863706588745}, {"doc_source": "concepts/runnables.mdx", "score": 1.1089909076690674}, {"doc_source": "how_to/index.mdx", "score": 1.126436710357666}, {"doc_source": "concepts/tools.mdx", "score": 1.1369460821151733}, {"doc_source": "concepts/runnables.mdx", "score": 1.1510531902313232}, {"doc_source": "how_to/index.mdx", "score": 1.1556286811828613}, {"doc_source": "concepts/runnables.mdx", "score": 1.1561923027038574}, {"doc_source": "tutorials/index.mdx", "score": 1.160003662109375}, {"doc_source": "concepts/runnables.mdx", "score": 1.1605188846588135}, {"doc_source": "concepts/runnables.mdx", "score": 1.1743700504302979}, {"doc_source": "concepts/runnables.mdx", "score": 1.182107925415039}]}
{"ts": 1747974506.217386, "query_id": "Q055", "question": "What does the `configurable_fields` helper enable for advanced chains?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9245166182518005}, {"doc_source": "concepts/runnables.mdx", "score": 1.1059552431106567}, {"doc_source": "concepts/runnables.mdx", "score": 1.129764437675476}, {"doc_source": "concepts/runnables.mdx", "score": 1.2373719215393066}, {"doc_source": "concepts/lcel.mdx", "score": 1.2801016569137573}, {"doc_source": "concepts/runnables.mdx", "score": 1.2865967750549316}, {"doc_source": "concepts/lcel.mdx", "score": 1.2918304204940796}, {"doc_source": "concepts/lcel.mdx", "score": 1.2931445837020874}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.3011258840560913}, {"doc_source": "concepts/lcel.mdx", "score": 1.3021317720413208}, {"doc_source": "concepts/lcel.mdx", "score": 1.3106954097747803}, {"doc_source": "concepts/lcel.mdx", "score": 1.3286364078521729}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3336657285690308}, {"doc_source": "concepts/runnables.mdx", "score": 1.3356646299362183}, {"doc_source": "how_to/index.mdx", "score": 1.3399765491485596}]}
{"ts": 1747974507.8289, "query_id": "Q056", "question": "Name two ChatModel parameters that help avoid exceeding rate limits.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5761818289756775}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7867628335952759}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8257307410240173}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9939004182815552}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9975638389587402}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0522077083587646}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0627987384796143}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0729273557662964}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0871338844299316}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0921616554260254}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0972299575805664}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1012747287750244}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1097888946533203}, {"doc_source": "concepts/chat_models.mdx", "score": 1.131824254989624}, {"doc_source": "concepts/chat_models.mdx", "score": 1.139465570449829}]}
{"ts": 1747974508.8991501, "query_id": "Q057", "question": "How do you specify a custom UUID for a Runnable run?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9093594551086426}, {"doc_source": "concepts/runnables.mdx", "score": 1.0184352397918701}, {"doc_source": "concepts/runnables.mdx", "score": 1.0533887147903442}, {"doc_source": "concepts/runnables.mdx", "score": 1.0815205574035645}, {"doc_source": "concepts/runnables.mdx", "score": 1.1119786500930786}, {"doc_source": "concepts/runnables.mdx", "score": 1.1247038841247559}, {"doc_source": "concepts/runnables.mdx", "score": 1.1607615947723389}, {"doc_source": "concepts/runnables.mdx", "score": 1.1610331535339355}, {"doc_source": "concepts/runnables.mdx", "score": 1.1710928678512573}, {"doc_source": "concepts/runnables.mdx", "score": 1.1776912212371826}, {"doc_source": "concepts/runnables.mdx", "score": 1.1944401264190674}, {"doc_source": "concepts/runnables.mdx", "score": 1.1969860792160034}, {"doc_source": "concepts/runnables.mdx", "score": 1.1987062692642212}, {"doc_source": "concepts/runnables.mdx", "score": 1.2008357048034668}, {"doc_source": "concepts/runnables.mdx", "score": 1.2136021852493286}]}
{"ts": 1747974510.768237, "query_id": "Q058", "question": "Compare `batch` and `batch_as_completed` when processing multiple inputs.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8200660347938538}, {"doc_source": "concepts/runnables.mdx", "score": 0.8494798541069031}, {"doc_source": "concepts/runnables.mdx", "score": 1.0473711490631104}, {"doc_source": "concepts/runnables.mdx", "score": 1.1157987117767334}, {"doc_source": "concepts/lcel.mdx", "score": 1.2788989543914795}, {"doc_source": "concepts/streaming.mdx", "score": 1.2967993021011353}, {"doc_source": "concepts/runnables.mdx", "score": 1.3137811422348022}, {"doc_source": "concepts/streaming.mdx", "score": 1.3271781206130981}, {"doc_source": "concepts/runnables.mdx", "score": 1.3460314273834229}, {"doc_source": "concepts/streaming.mdx", "score": 1.3510231971740723}, {"doc_source": "concepts/async.mdx", "score": 1.361152172088623}, {"doc_source": "concepts/streaming.mdx", "score": 1.3701081275939941}, {"doc_source": "concepts/runnables.mdx", "score": 1.3703453540802002}, {"doc_source": "concepts/lcel.mdx", "score": 1.3869268894195557}, {"doc_source": "concepts/streaming.mdx", "score": 1.3983385562896729}]}
{"ts": 1747974512.317544, "query_id": "Q059", "question": "How can LangChain assist with evaluating generated answers automatically?", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.8676735162734985}, {"doc_source": "concepts/evaluation.mdx", "score": 0.9116879105567932}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9272871017456055}, {"doc_source": "how_to/index.mdx", "score": 0.931599497795105}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9333962798118591}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9414273500442505}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9670411348342896}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9751759171485901}, {"doc_source": "how_to/index.mdx", "score": 0.9933944344520569}, {"doc_source": "how_to/index.mdx", "score": 1.0083061456680298}, {"doc_source": "introduction.mdx", "score": 1.0208687782287598}, {"doc_source": "how_to/index.mdx", "score": 1.0232571363449097}, {"doc_source": "introduction.mdx", "score": 1.0371451377868652}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0456856489181519}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0484874248504639}]}
{"ts": 1747974513.995166, "query_id": "Q060", "question": "Provide two metrics supported by LangChain's built in evaluation module.", "retrieved_items": [{"doc_source": "concepts/evaluation.mdx", "score": 0.882729709148407}, {"doc_source": "how_to/index.mdx", "score": 0.8946409821510315}, {"doc_source": "tutorials/index.mdx", "score": 0.9040490388870239}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9283467531204224}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.969840943813324}, {"doc_source": "concepts/embedding_models.mdx", "score": 0.9785069227218628}, {"doc_source": "how_to/index.mdx", "score": 0.983342170715332}, {"doc_source": "concepts/streaming.mdx", "score": 1.0263751745224}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0552425384521484}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.065946102142334}, {"doc_source": "concepts/index.mdx", "score": 1.0680656433105469}, {"doc_source": "how_to/index.mdx", "score": 1.0758962631225586}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0782783031463623}, {"doc_source": "concepts/architecture.mdx", "score": 1.09422767162323}, {"doc_source": "concepts/lcel.mdx", "score": 1.115015983581543}]}
{"ts": 1747974514.867476, "query_id": "Q061", "question": "Which LangChain concept allows combining multiple Runnables with the `|` operator?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.6619524955749512}, {"doc_source": "concepts/runnables.mdx", "score": 0.7449985146522522}, {"doc_source": "concepts/lcel.mdx", "score": 0.7531129121780396}, {"doc_source": "concepts/lcel.mdx", "score": 0.7628594636917114}, {"doc_source": "how_to/index.mdx", "score": 0.7645400166511536}, {"doc_source": "concepts/runnables.mdx", "score": 0.7845576405525208}, {"doc_source": "concepts/lcel.mdx", "score": 0.7863311171531677}, {"doc_source": "concepts/lcel.mdx", "score": 0.7924821376800537}, {"doc_source": "concepts/async.mdx", "score": 0.822674036026001}, {"doc_source": "concepts/lcel.mdx", "score": 0.8444672226905823}, {"doc_source": "concepts/runnables.mdx", "score": 0.8508225679397583}, {"doc_source": "concepts/streaming.mdx", "score": 0.8542447686195374}, {"doc_source": "concepts/streaming.mdx", "score": 0.8547438383102417}, {"doc_source": "concepts/lcel.mdx", "score": 0.8584628701210022}, {"doc_source": "concepts/runnables.mdx", "score": 0.863335371017456}]}
{"ts": 1747974516.433772, "query_id": "Q062", "question": "Explain how to set up tool streaming so that tool results are incrementally passed back to the model.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.9548559188842773}, {"doc_source": "concepts/streaming.mdx", "score": 0.9635952711105347}, {"doc_source": "concepts/streaming.mdx", "score": 1.0011006593704224}, {"doc_source": "concepts/streaming.mdx", "score": 1.024632453918457}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0512418746948242}, {"doc_source": "concepts/streaming.mdx", "score": 1.051426887512207}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.0672231912612915}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0934622287750244}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1049416065216064}, {"doc_source": "how_to/index.mdx", "score": 1.1088159084320068}, {"doc_source": "concepts/streaming.mdx", "score": 1.1156015396118164}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1197659969329834}, {"doc_source": "concepts/streaming.mdx", "score": 1.1252328157424927}, {"doc_source": "how_to/index.mdx", "score": 1.128438949584961}, {"doc_source": "how_to/index.mdx", "score": 1.134683609008789}]}
{"ts": 1747974517.819407, "query_id": "Q063", "question": "What is the benefit of using `RunnableLambda` over subclassing a Runnable?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8221232891082764}, {"doc_source": "concepts/runnables.mdx", "score": 0.9718983769416809}, {"doc_source": "concepts/runnables.mdx", "score": 0.9910827875137329}, {"doc_source": "concepts/lcel.mdx", "score": 0.9928208589553833}, {"doc_source": "concepts/runnables.mdx", "score": 1.0357584953308105}, {"doc_source": "concepts/runnables.mdx", "score": 1.041549563407898}, {"doc_source": "concepts/runnables.mdx", "score": 1.0562787055969238}, {"doc_source": "concepts/lcel.mdx", "score": 1.0576661825180054}, {"doc_source": "concepts/runnables.mdx", "score": 1.0832369327545166}, {"doc_source": "concepts/lcel.mdx", "score": 1.1029603481292725}, {"doc_source": "concepts/lcel.mdx", "score": 1.1066434383392334}, {"doc_source": "concepts/lcel.mdx", "score": 1.1217803955078125}, {"doc_source": "concepts/lcel.mdx", "score": 1.1639609336853027}, {"doc_source": "concepts/runnables.mdx", "score": 1.1717863082885742}, {"doc_source": "concepts/runnables.mdx", "score": 1.1909104585647583}]}
{"ts": 1747974519.350816, "query_id": "Q064", "question": "Which two documents together explain how to build a streaming question answering chain with citations?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9878866672515869}, {"doc_source": "how_to/index.mdx", "score": 1.046334981918335}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0589706897735596}, {"doc_source": "tutorials/index.mdx", "score": 1.065586805343628}, {"doc_source": "how_to/index.mdx", "score": 1.0740373134613037}, {"doc_source": "concepts/rag.mdx", "score": 1.0781641006469727}, {"doc_source": "concepts/streaming.mdx", "score": 1.092083215713501}, {"doc_source": "concepts/streaming.mdx", "score": 1.0956504344940186}, {"doc_source": "concepts/retrieval.mdx", "score": 1.098290205001831}, {"doc_source": "how_to/index.mdx", "score": 1.1049220561981201}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1073155403137207}, {"doc_source": "how_to/index.mdx", "score": 1.1305477619171143}, {"doc_source": "tutorials/index.mdx", "score": 1.1399085521697998}, {"doc_source": "concepts/streaming.mdx", "score": 1.1564042568206787}, {"doc_source": "how_to/index.mdx", "score": 1.1583555936813354}]}
{"ts": 1747974520.735759, "query_id": "Q065", "question": "How do you measure BLEU score in LangChain evaluations?", "retrieved_items": [{"doc_source": "concepts/embedding_models.mdx", "score": 0.9989585876464844}, {"doc_source": "tutorials/index.mdx", "score": 1.0159780979156494}, {"doc_source": "concepts/evaluation.mdx", "score": 1.0848612785339355}, {"doc_source": "how_to/index.mdx", "score": 1.0967556238174438}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1303505897521973}, {"doc_source": "concepts/messages.mdx", "score": 1.1464849710464478}, {"doc_source": "tutorials/index.mdx", "score": 1.1490092277526855}, {"doc_source": "how_to/index.mdx", "score": 1.149566411972046}, {"doc_source": "how_to/index.mdx", "score": 1.1644765138626099}, {"doc_source": "how_to/index.mdx", "score": 1.177036166191101}, {"doc_source": "introduction.mdx", "score": 1.1807559728622437}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1813280582427979}, {"doc_source": "concepts/lcel.mdx", "score": 1.191955327987671}, {"doc_source": "how_to/index.mdx", "score": 1.1944268941879272}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1958832740783691}]}
{"ts": 1747974524.132159, "query_id": "Q066", "question": "What does the `ConversationBufferWindowMemory` keep in memory?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.1610500812530518}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1826257705688477}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2314040660858154}, {"doc_source": "concepts/chat_models.mdx", "score": 1.246814489364624}, {"doc_source": "concepts/chat_models.mdx", "score": 1.274022102355957}, {"doc_source": "concepts/chat_history.mdx", "score": 1.2776298522949219}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2917147874832153}, {"doc_source": "concepts/messages.mdx", "score": 1.3128268718719482}, {"doc_source": "concepts/index.mdx", "score": 1.362764596939087}, {"doc_source": "concepts/index.mdx", "score": 1.3825510740280151}, {"doc_source": "concepts/messages.mdx", "score": 1.4069018363952637}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4086127281188965}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4107457399368286}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.414424180984497}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4150896072387695}]}
{"ts": 1747974525.439056, "query_id": "Q067", "question": "Name a loader designed specifically for PDF files in LangChain.", "retrieved_items": [{"doc_source": "concepts/document_loaders.mdx", "score": 0.9651826620101929}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.00960111618042}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0584146976470947}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.0941755771636963}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.0985047817230225}, {"doc_source": "how_to/index.mdx", "score": 1.1507281064987183}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.1556894779205322}, {"doc_source": "concepts/index.mdx", "score": 1.1575894355773926}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1972167491912842}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 1.2027297019958496}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2047603130340576}, {"doc_source": "concepts/lcel.mdx", "score": 1.207625389099121}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2297759056091309}, {"doc_source": "how_to/index.mdx", "score": 1.2316646575927734}, {"doc_source": "introduction.mdx", "score": 1.2353248596191406}]}
{"ts": 1747974526.6425579, "query_id": "Q068", "question": "Which concept guide covers the standard streaming APIs exposed by Runnables?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.7733585238456726}, {"doc_source": "concepts/runnables.mdx", "score": 0.846548318862915}, {"doc_source": "concepts/runnables.mdx", "score": 0.8499844074249268}, {"doc_source": "concepts/runnables.mdx", "score": 0.8666893243789673}, {"doc_source": "concepts/streaming.mdx", "score": 0.868037223815918}, {"doc_source": "concepts/runnables.mdx", "score": 0.8796707391738892}, {"doc_source": "concepts/streaming.mdx", "score": 0.8901397585868835}, {"doc_source": "concepts/runnables.mdx", "score": 0.9326713681221008}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9453091621398926}, {"doc_source": "concepts/index.mdx", "score": 0.9460058212280273}, {"doc_source": "concepts/streaming.mdx", "score": 0.9540077447891235}, {"doc_source": "concepts/streaming.mdx", "score": 0.9876226186752319}, {"doc_source": "concepts/lcel.mdx", "score": 1.0030391216278076}, {"doc_source": "concepts/streaming.mdx", "score": 1.0083638429641724}, {"doc_source": "concepts/streaming.mdx", "score": 1.017276406288147}]}
{"ts": 1747974528.087564, "query_id": "Q069", "question": "What is the advantage of using `LangServe` with Runnables?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8217673301696777}, {"doc_source": "concepts/lcel.mdx", "score": 0.8784109354019165}, {"doc_source": "concepts/runnables.mdx", "score": 0.893774688243866}, {"doc_source": "concepts/runnables.mdx", "score": 0.9146988391876221}, {"doc_source": "concepts/runnables.mdx", "score": 0.916937530040741}, {"doc_source": "concepts/lcel.mdx", "score": 0.9368709325790405}, {"doc_source": "concepts/lcel.mdx", "score": 0.9403908252716064}, {"doc_source": "concepts/runnables.mdx", "score": 0.9598751068115234}, {"doc_source": "concepts/async.mdx", "score": 0.9743815660476685}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9846447706222534}, {"doc_source": "concepts/architecture.mdx", "score": 0.9954168796539307}, {"doc_source": "concepts/runnables.mdx", "score": 1.0010335445404053}, {"doc_source": "concepts/index.mdx", "score": 1.0063585042953491}, {"doc_source": "concepts/async.mdx", "score": 1.0305331945419312}, {"doc_source": "concepts/runnables.mdx", "score": 1.0331995487213135}]}
{"ts": 1747974529.944003, "query_id": "Q070", "question": "Explain how ChatModels support multimodal inputs.", "retrieved_items": [{"doc_source": "concepts/multimodality.mdx", "score": 0.5912604331970215}, {"doc_source": "concepts/multimodality.mdx", "score": 0.6074106097221375}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7254828214645386}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7807154655456543}, {"doc_source": "concepts/multimodality.mdx", "score": 0.7819159030914307}, {"doc_source": "concepts/chat_models.mdx", "score": 0.792104184627533}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8060004711151123}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8390437364578247}, {"doc_source": "concepts/tokens.mdx", "score": 0.8893482089042664}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8936097621917725}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8989677429199219}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9251059293746948}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9457724690437317}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9542033076286316}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9543265104293823}]}
{"ts": 1747974532.1795661, "query_id": "Q071", "question": "What two steps are recommended to recover from ratelimiting errors?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.8242883086204529}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9093111157417297}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1995959281921387}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2091940641403198}, {"doc_source": "concepts/retrieval.mdx", "score": 1.4161889553070068}, {"doc_source": "concepts/runnables.mdx", "score": 1.4464462995529175}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4578070640563965}, {"doc_source": "concepts/chat_models.mdx", "score": 1.4748070240020752}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.479519248008728}, {"doc_source": "concepts/rag.mdx", "score": 1.5080429315567017}, {"doc_source": "concepts/retrieval.mdx", "score": 1.5166122913360596}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.5228855609893799}, {"doc_source": "concepts/runnables.mdx", "score": 1.5351816415786743}, {"doc_source": "concepts/streaming.mdx", "score": 1.5454583168029785}, {"doc_source": "how_to/index.mdx", "score": 1.5494223833084106}]}
{"ts": 1747974533.158276, "query_id": "Q072", "question": "Which parameter allows a Runnable to stream events as JSON for UIs?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 1.0530344247817993}, {"doc_source": "concepts/streaming.mdx", "score": 1.0659735202789307}, {"doc_source": "concepts/streaming.mdx", "score": 1.0805227756500244}, {"doc_source": "concepts/streaming.mdx", "score": 1.0858633518218994}, {"doc_source": "concepts/runnables.mdx", "score": 1.0899498462677002}, {"doc_source": "concepts/runnables.mdx", "score": 1.1112364530563354}, {"doc_source": "concepts/runnables.mdx", "score": 1.1250706911087036}, {"doc_source": "concepts/streaming.mdx", "score": 1.1308112144470215}, {"doc_source": "concepts/streaming.mdx", "score": 1.1311652660369873}, {"doc_source": "concepts/streaming.mdx", "score": 1.1324262619018555}, {"doc_source": "concepts/runnables.mdx", "score": 1.137572169303894}, {"doc_source": "concepts/streaming.mdx", "score": 1.1416499614715576}, {"doc_source": "concepts/runnables.mdx", "score": 1.1423659324645996}, {"doc_source": "concepts/runnables.mdx", "score": 1.1527018547058105}, {"doc_source": "concepts/streaming.mdx", "score": 1.154660940170288}]}
{"ts": 1747974534.464874, "query_id": "Q073", "question": "Provide a code snippet to embed text with a local HuggingFace model.", "retrieved_items": [{"doc_source": "concepts/embedding_models.mdx", "score": 1.0066365003585815}, {"doc_source": "how_to/embed_text.mdx", "score": 1.0270702838897705}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.0596823692321777}, {"doc_source": "how_to/embed_text.mdx", "score": 1.0635778903961182}, {"doc_source": "concepts/multimodality.mdx", "score": 1.0717648267745972}, {"doc_source": "concepts/multimodality.mdx", "score": 1.0836189985275269}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1077531576156616}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.127073884010315}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1460490226745605}, {"doc_source": "how_to/index.mdx", "score": 1.1541293859481812}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1572504043579102}, {"doc_source": "how_to/embed_text.mdx", "score": 1.1612759828567505}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1650911569595337}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.177020788192749}, {"doc_source": "concepts/multimodality.mdx", "score": 1.1803061962127686}]}
{"ts": 1747974536.0431309, "query_id": "Q074", "question": "How does the `ContextualCompressionRetriever` reduce context windows?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 1.138444423675537}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2801432609558105}, {"doc_source": "how_to/index.mdx", "score": 1.3006089925765991}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3162164688110352}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3545007705688477}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.3706351518630981}, {"doc_source": "concepts/rag.mdx", "score": 1.4051086902618408}, {"doc_source": "concepts/rag.mdx", "score": 1.409595251083374}, {"doc_source": "concepts/rag.mdx", "score": 1.4125803709030151}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4137392044067383}, {"doc_source": "concepts/retrieval.mdx", "score": 1.423210859298706}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4316335916519165}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4366742372512817}, {"doc_source": "concepts/lcel.mdx", "score": 1.4372661113739014}, {"doc_source": "concepts/retrievers.mdx", "score": 1.4432822465896606}]}
{"ts": 1747974538.0952802, "query_id": "Q075", "question": "What guide should you consult for configuring runtime chain internals like temperature?", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 1.124815821647644}, {"doc_source": "concepts/runnables.mdx", "score": 1.1300218105316162}, {"doc_source": "how_to/index.mdx", "score": 1.1895829439163208}, {"doc_source": "tutorials/index.mdx", "score": 1.2220138311386108}, {"doc_source": "concepts/runnables.mdx", "score": 1.2521029710769653}, {"doc_source": "how_to/index.mdx", "score": 1.2883386611938477}, {"doc_source": "concepts/runnables.mdx", "score": 1.3020563125610352}, {"doc_source": "how_to/index.mdx", "score": 1.3022387027740479}, {"doc_source": "how_to/index.mdx", "score": 1.3056435585021973}, {"doc_source": "introduction.mdx", "score": 1.3080811500549316}, {"doc_source": "how_to/index.mdx", "score": 1.3104841709136963}, {"doc_source": "concepts/streaming.mdx", "score": 1.31498384475708}, {"doc_source": "introduction.mdx", "score": 1.3152050971984863}, {"doc_source": "concepts/tracing.mdx", "score": 1.3193345069885254}, {"doc_source": "how_to/index.mdx", "score": 1.3216063976287842}]}
{"ts": 1747974539.117861, "query_id": "Q076", "question": "Which selector uses Maximal Marginal Relevance (MMR) to diversify examples?", "retrieved_items": [{"doc_source": "concepts/vectorstores.mdx", "score": 0.7068984508514404}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8869249224662781}, {"doc_source": "how_to/index.mdx", "score": 0.9368320107460022}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.0181915760040283}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1620793342590332}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1941536664962769}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2192630767822266}, {"doc_source": "how_to/index.mdx", "score": 1.2217292785644531}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.227527141571045}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2320713996887207}, {"doc_source": "concepts/retrievers.mdx", "score": 1.245355486869812}, {"doc_source": "concepts/retrieval.mdx", "score": 1.260202407836914}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.261643886566162}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2639776468276978}, {"doc_source": "concepts/retrievers.mdx", "score": 1.2678759098052979}]}
{"ts": 1747974540.645473, "query_id": "Q077", "question": "What is the recommended way to expose tool schemas to chat models that use OpenAI function calling?", "retrieved_items": [{"doc_source": "concepts/structured_outputs.mdx", "score": 0.6839545369148254}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.69944167137146}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7261172533035278}, {"doc_source": "concepts/structured_outputs.mdx", "score": 0.8094886541366577}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8495033979415894}, {"doc_source": "concepts/tools.mdx", "score": 0.8519311547279358}, {"doc_source": "how_to/index.mdx", "score": 0.8925682902336121}, {"doc_source": "concepts/tool_calling.mdx", "score": 0.8967816233634949}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.8992266654968262}, {"doc_source": "concepts/index.mdx", "score": 0.9179431200027466}, {"doc_source": "concepts/tools.mdx", "score": 0.93720942735672}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.9533958435058594}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.9596738815307617}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 0.9617875814437866}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9715374708175659}]}
{"ts": 1747974543.16584, "query_id": "Q078", "question": "Explain the advantage of using `batch_as_completed` for long running tasks.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8248700499534607}, {"doc_source": "concepts/runnables.mdx", "score": 0.9200083017349243}, {"doc_source": "concepts/runnables.mdx", "score": 1.0427577495574951}, {"doc_source": "concepts/runnables.mdx", "score": 1.1378562450408936}, {"doc_source": "concepts/lcel.mdx", "score": 1.1975274085998535}, {"doc_source": "concepts/runnables.mdx", "score": 1.2184984683990479}, {"doc_source": "concepts/lcel.mdx", "score": 1.309138536453247}, {"doc_source": "concepts/async.mdx", "score": 1.3258593082427979}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3289153575897217}, {"doc_source": "concepts/async.mdx", "score": 1.3402255773544312}, {"doc_source": "concepts/streaming.mdx", "score": 1.3441308736801147}, {"doc_source": "concepts/runnables.mdx", "score": 1.3511642217636108}, {"doc_source": "concepts/async.mdx", "score": 1.3665167093276978}, {"doc_source": "concepts/async.mdx", "score": 1.368828296661377}, {"doc_source": "concepts/streaming.mdx", "score": 1.3830136060714722}]}
{"ts": 1747974545.193892, "query_id": "Q079", "question": "Which two files outline strategies to trim messages to fit the context window?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 0.9143385887145996}, {"doc_source": "concepts/chat_history.mdx", "score": 1.08457612991333}, {"doc_source": "concepts/chat_models.mdx", "score": 1.143282413482666}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1761504411697388}, {"doc_source": "how_to/index.mdx", "score": 1.1938716173171997}, {"doc_source": "concepts/index.mdx", "score": 1.2091017961502075}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2153418064117432}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2174098491668701}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2752289772033691}, {"doc_source": "concepts/index.mdx", "score": 1.2807074785232544}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2915983200073242}, {"doc_source": "concepts/rag.mdx", "score": 1.2920501232147217}, {"doc_source": "concepts/chat_models.mdx", "score": 1.295925259590149}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3060719966888428}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3158260583877563}]}
{"ts": 1747974546.493739, "query_id": "Q080", "question": "What does the `max_concurrency` field not control when using a ChatModel?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 1.1154112815856934}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1204005479812622}, {"doc_source": "concepts/runnables.mdx", "score": 1.1446199417114258}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1621845960617065}, {"doc_source": "concepts/chat_models.mdx", "score": 1.174848198890686}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1945078372955322}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2030328512191772}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2168546915054321}, {"doc_source": "concepts/chat_models.mdx", "score": 1.219204068183899}, {"doc_source": "concepts/streaming.mdx", "score": 1.2202725410461426}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2455284595489502}, {"doc_source": "concepts/index.mdx", "score": 1.2545242309570312}, {"doc_source": "concepts/messages.mdx", "score": 1.2623050212860107}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2745370864868164}, {"doc_source": "concepts/chat_history.mdx", "score": 1.282762050628662}]}
{"ts": 1747974548.268497, "query_id": "Q081", "question": "Which document explains how to call tools in parallel with OpenAI?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9732943177223206}, {"doc_source": "how_to/index.mdx", "score": 1.0294963121414185}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0356497764587402}, {"doc_source": "how_to/index.mdx", "score": 1.041914701461792}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.0464969873428345}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.100308895111084}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.115471601486206}, {"doc_source": "how_to/index.mdx", "score": 1.124219298362732}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.1374797821044922}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1445221900939941}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1492102146148682}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.150212049484253}, {"doc_source": "how_to/index.mdx", "score": 1.1632888317108154}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1641819477081299}, {"doc_source": "concepts/rag.mdx", "score": 1.1692724227905273}]}
{"ts": 1747974549.468765, "query_id": "Q082", "question": "Describe how `RunnableGenerator` differs from `RunnableLambda`.", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6842465996742249}, {"doc_source": "concepts/runnables.mdx", "score": 0.9617108702659607}, {"doc_source": "concepts/lcel.mdx", "score": 0.9837562441825867}, {"doc_source": "concepts/runnables.mdx", "score": 1.0025997161865234}, {"doc_source": "concepts/runnables.mdx", "score": 1.0347131490707397}, {"doc_source": "concepts/runnables.mdx", "score": 1.038126826286316}, {"doc_source": "concepts/lcel.mdx", "score": 1.0523731708526611}, {"doc_source": "concepts/lcel.mdx", "score": 1.0554639101028442}, {"doc_source": "concepts/lcel.mdx", "score": 1.056566596031189}, {"doc_source": "concepts/runnables.mdx", "score": 1.0609230995178223}, {"doc_source": "concepts/runnables.mdx", "score": 1.0736973285675049}, {"doc_source": "concepts/lcel.mdx", "score": 1.107395887374878}, {"doc_source": "concepts/lcel.mdx", "score": 1.1096526384353638}, {"doc_source": "concepts/runnables.mdx", "score": 1.1177915334701538}, {"doc_source": "concepts/lcel.mdx", "score": 1.1184874773025513}]}
{"ts": 1747974550.9077172, "query_id": "Q083", "question": "Which vector store integration supports time weighted relevance?", "retrieved_items": [{"doc_source": "concepts/vectorstores.mdx", "score": 0.8827235698699951}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9366466999053955}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9391461610794067}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.9585649967193604}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9685896039009094}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9957469701766968}, {"doc_source": "how_to/index.mdx", "score": 1.0111651420593262}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0220847129821777}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0403485298156738}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.0477368831634521}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.05352783203125}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.0788472890853882}, {"doc_source": "how_to/index.mdx", "score": 1.10494065284729}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1080209016799927}, {"doc_source": "how_to/index.mdx", "score": 1.1220836639404297}]}
{"ts": 1747974551.960216, "query_id": "Q084", "question": "Explain how to use `run_name` for easier debugging in LangSmith.", "retrieved_items": [{"doc_source": "tutorials/index.mdx", "score": 0.9928377866744995}, {"doc_source": "concepts/runnables.mdx", "score": 1.0111323595046997}, {"doc_source": "concepts/tracing.mdx", "score": 1.0632253885269165}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.086233139038086}, {"doc_source": "how_to/index.mdx", "score": 1.1224148273468018}, {"doc_source": "concepts/runnables.mdx", "score": 1.150101900100708}, {"doc_source": "how_to/index.mdx", "score": 1.1612894535064697}, {"doc_source": "how_to/index.mdx", "score": 1.166468858718872}, {"doc_source": "concepts/architecture.mdx", "score": 1.1725082397460938}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.1814242601394653}, {"doc_source": "concepts/runnables.mdx", "score": 1.1863436698913574}, {"doc_source": "concepts/async.mdx", "score": 1.2083866596221924}, {"doc_source": "concepts/lcel.mdx", "score": 1.208693027496338}, {"doc_source": "concepts/index.mdx", "score": 1.2221781015396118}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2336030006408691}]}
{"ts": 1747974554.0268748, "query_id": "Q085", "question": "Which two parameters must be provided to initialize `AzureAIDocumentIntelligenceLoader`?", "retrieved_items": [{"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.7355740666389465}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.7382179498672485}, {"doc_source": "how_to/document_loader_office_file.mdx", "score": 0.9853199124336243}, {"doc_source": "concepts/chat_models.mdx", "score": 1.254820466041565}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.2595510482788086}, {"doc_source": "concepts/document_loaders.mdx", "score": 1.2828747034072876}, {"doc_source": "how_to/index.mdx", "score": 1.2886799573898315}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.301715612411499}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.312117338180542}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.318564772605896}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.385900616645813}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3886665105819702}, {"doc_source": "how_to/index.mdx", "score": 1.3917732238769531}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4040577411651611}, {"doc_source": "concepts/chat_models.mdx", "score": 1.409309983253479}]}
{"ts": 1747974555.842653, "query_id": "Q086", "question": "What is the main benefit of `asam_stream` events over `astream`?", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.9552755355834961}, {"doc_source": "concepts/streaming.mdx", "score": 1.0626449584960938}, {"doc_source": "concepts/streaming.mdx", "score": 1.0766979455947876}, {"doc_source": "concepts/streaming.mdx", "score": 1.1203203201293945}, {"doc_source": "concepts/streaming.mdx", "score": 1.1565361022949219}, {"doc_source": "concepts/runnables.mdx", "score": 1.1882538795471191}, {"doc_source": "concepts/streaming.mdx", "score": 1.205546498298645}, {"doc_source": "concepts/streaming.mdx", "score": 1.2185463905334473}, {"doc_source": "concepts/streaming.mdx", "score": 1.2514526844024658}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2782061100006104}, {"doc_source": "concepts/streaming.mdx", "score": 1.297114610671997}, {"doc_source": "concepts/streaming.mdx", "score": 1.309992790222168}, {"doc_source": "concepts/streaming.mdx", "score": 1.314072847366333}, {"doc_source": "concepts/streaming.mdx", "score": 1.3214149475097656}, {"doc_source": "concepts/streaming.mdx", "score": 1.337416648864746}]}
{"ts": 1747974557.8159318, "query_id": "Q087", "question": "How do you specify hidden arguments in a Tool so the model cannot set them?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.9848969578742981}, {"doc_source": "concepts/tools.mdx", "score": 0.9980555772781372}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2051539421081543}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2200138568878174}, {"doc_source": "how_to/index.mdx", "score": 1.229272484779358}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.266995906829834}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.276363492012024}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2930560111999512}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2980937957763672}, {"doc_source": "concepts/tools.mdx", "score": 1.3123301267623901}, {"doc_source": "concepts/tools.mdx", "score": 1.313123106956482}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3181912899017334}, {"doc_source": "how_to/index.mdx", "score": 1.3263784646987915}, {"doc_source": "concepts/tools.mdx", "score": 1.3357560634613037}, {"doc_source": "concepts/tools.mdx", "score": 1.3415215015411377}]}
{"ts": 1747974559.6885061, "query_id": "Q088", "question": "Which guide demonstrates building a SQL retriever over a large database?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.965441107749939}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0186799764633179}, {"doc_source": "how_to/index.mdx", "score": 1.0300374031066895}, {"doc_source": "concepts/retrieval.mdx", "score": 1.0454614162445068}, {"doc_source": "how_to/index.mdx", "score": 1.0644941329956055}, {"doc_source": "how_to/index.mdx", "score": 1.0892114639282227}, {"doc_source": "how_to/index.mdx", "score": 1.0970289707183838}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1260111331939697}, {"doc_source": "concepts/retrievers.mdx", "score": 1.147639274597168}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1515958309173584}, {"doc_source": "how_to/index.mdx", "score": 1.163494348526001}, {"doc_source": "concepts/retrieval.mdx", "score": 1.16355299949646}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1643468141555786}, {"doc_source": "how_to/index.mdx", "score": 1.1706827878952026}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1874701976776123}]}
{"ts": 1747974560.954706, "query_id": "Q089", "question": "Why is semantic caching unlikely to hit after the first chat turn?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.708777904510498}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8750511407852173}, {"doc_source": "concepts/chat_models.mdx", "score": 1.245640516281128}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2831244468688965}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2858912944793701}, {"doc_source": "concepts/chat_history.mdx", "score": 1.3079935312271118}, {"doc_source": "concepts/chat_history.mdx", "score": 1.31861412525177}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3203045129776}, {"doc_source": "concepts/chat_models.mdx", "score": 1.340956449508667}, {"doc_source": "concepts/chat_history.mdx", "score": 1.348594069480896}, {"doc_source": "concepts/messages.mdx", "score": 1.3570618629455566}, {"doc_source": "concepts/lcel.mdx", "score": 1.3799017667770386}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.389176607131958}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.3922117948532104}, {"doc_source": "concepts/chat_history.mdx", "score": 1.392723560333252}]}
{"ts": 1747974562.489221, "query_id": "Q090", "question": "Describe a use case for `RecursiveCharacterTextSplitter`.", "retrieved_items": [{"doc_source": "concepts/text_splitters.mdx", "score": 0.8221566677093506}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9785577058792114}, {"doc_source": "concepts/text_splitters.mdx", "score": 0.9962900876998901}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0288711786270142}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.093240737915039}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1284321546554565}, {"doc_source": "how_to/index.mdx", "score": 1.1729884147644043}, {"doc_source": "how_to/index.mdx", "score": 1.2018511295318604}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.222290277481079}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2510898113250732}, {"doc_source": "concepts/retrievers.mdx", "score": 1.3304141759872437}, {"doc_source": "concepts/tokens.mdx", "score": 1.3529222011566162}, {"doc_source": "concepts/retrieval.mdx", "score": 1.3956043720245361}, {"doc_source": "concepts/tokens.mdx", "score": 1.398803949356079}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.4061627388000488}]}
{"ts": 1747974564.784378, "query_id": "Q091", "question": "Give two methods to stream output from a ChatModel.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.8138102293014526}, {"doc_source": "concepts/streaming.mdx", "score": 0.8409689664840698}, {"doc_source": "concepts/streaming.mdx", "score": 0.8452734351158142}, {"doc_source": "concepts/streaming.mdx", "score": 0.8522126078605652}, {"doc_source": "concepts/streaming.mdx", "score": 0.8526111841201782}, {"doc_source": "concepts/streaming.mdx", "score": 0.9392991065979004}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9727135896682739}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9842149615287781}, {"doc_source": "concepts/streaming.mdx", "score": 1.0104668140411377}, {"doc_source": "concepts/streaming.mdx", "score": 1.0148916244506836}, {"doc_source": "concepts/streaming.mdx", "score": 1.031814694404602}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0567359924316406}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0587255954742432}, {"doc_source": "concepts/streaming.mdx", "score": 1.0638238191604614}, {"doc_source": "concepts/streaming.mdx", "score": 1.0649222135543823}]}
{"ts": 1747974566.4505239, "query_id": "Q092", "question": "Which document teaches you to build a hybrid search retriever combining keyword and vector?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.7182904481887817}, {"doc_source": "concepts/retrieval.mdx", "score": 0.7420840263366699}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.7709743976593018}, {"doc_source": "concepts/retrievers.mdx", "score": 0.786737322807312}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.8332136273384094}, {"doc_source": "how_to/index.mdx", "score": 0.8713193535804749}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.873077392578125}, {"doc_source": "concepts/retrieval.mdx", "score": 0.888505220413208}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9028327465057373}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9184011220932007}, {"doc_source": "concepts/vectorstores.mdx", "score": 0.9290446043014526}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9632231593132019}, {"doc_source": "concepts/retrieval.mdx", "score": 0.9686093926429749}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9729511737823486}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9782532453536987}]}
{"ts": 1747974567.6641972, "query_id": "Q093", "question": "What is the purpose of `annotated` type hints in tool schemas?", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8381932973861694}, {"doc_source": "concepts/tools.mdx", "score": 0.9158917665481567}, {"doc_source": "concepts/tools.mdx", "score": 1.0163748264312744}, {"doc_source": "concepts/tools.mdx", "score": 1.071291446685791}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.1164422035217285}, {"doc_source": "concepts/runnables.mdx", "score": 1.166992425918579}, {"doc_source": "concepts/tools.mdx", "score": 1.2020418643951416}, {"doc_source": "concepts/tools.mdx", "score": 1.2137564420700073}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2296028137207031}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2334264516830444}, {"doc_source": "concepts/tools.mdx", "score": 1.2550171613693237}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.275193691253662}, {"doc_source": "concepts/runnables.mdx", "score": 1.2758440971374512}, {"doc_source": "concepts/tools.mdx", "score": 1.2925127744674683}, {"doc_source": "concepts/tools.mdx", "score": 1.2996699810028076}]}
{"ts": 1747974569.452446, "query_id": "Q094", "question": "Explain why caching chat model responses is challenging.", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5507657527923584}, {"doc_source": "concepts/chat_models.mdx", "score": 0.7899144887924194}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0342262983322144}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0889759063720703}, {"doc_source": "concepts/streaming.mdx", "score": 1.0895308256149292}, {"doc_source": "concepts/chat_models.mdx", "score": 1.105066180229187}, {"doc_source": "concepts/chat_history.mdx", "score": 1.1399636268615723}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1818137168884277}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1852924823760986}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1866739988327026}, {"doc_source": "concepts/chat_models.mdx", "score": 1.188708782196045}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1922723054885864}, {"doc_source": "concepts/streaming.mdx", "score": 1.1988402605056763}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.2129855155944824}, {"doc_source": "concepts/index.mdx", "score": 1.2162585258483887}]}
{"ts": 1747974570.92859, "query_id": "Q095", "question": "Which two docs together explain trimming long documents before embedding?", "retrieved_items": [{"doc_source": "concepts/chat_history.mdx", "score": 1.0128487348556519}, {"doc_source": "how_to/index.mdx", "score": 1.0352662801742554}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0577418804168701}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.0668108463287354}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0724895000457764}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.09827721118927}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.1429303884506226}, {"doc_source": "concepts/retrievers.mdx", "score": 1.148833990097046}, {"doc_source": "how_to/index.mdx", "score": 1.1683566570281982}, {"doc_source": "how_to/index.mdx", "score": 1.2007496356964111}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.2037601470947266}, {"doc_source": "how_to/index.mdx", "score": 1.2253055572509766}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.2367942333221436}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.2385375499725342}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.2595019340515137}]}
{"ts": 1747974571.867757, "query_id": "Q096", "question": "What class enables you to get structured JSON output directly from LLMS without a parser?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 1.2031793594360352}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2091846466064453}, {"doc_source": "how_to/index.mdx", "score": 1.2110381126403809}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2278110980987549}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2289191484451294}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.2775062322616577}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.2974629402160645}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.2994256019592285}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.3051637411117554}, {"doc_source": "concepts/output_parsers.mdx", "score": 1.3208140134811401}, {"doc_source": "concepts/structured_outputs.mdx", "score": 1.3370476961135864}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.3386317491531372}, {"doc_source": "concepts/streaming.mdx", "score": 1.3600431680679321}, {"doc_source": "concepts/index.mdx", "score": 1.3745918273925781}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3847289085388184}]}
{"ts": 1747974573.301482, "query_id": "Q097", "question": "Name three built in message field keys aside from `content`.", "retrieved_items": [{"doc_source": "concepts/messages.mdx", "score": 1.042458176612854}, {"doc_source": "concepts/messages.mdx", "score": 1.0430659055709839}, {"doc_source": "concepts/messages.mdx", "score": 1.1518919467926025}, {"doc_source": "concepts/messages.mdx", "score": 1.1774113178253174}, {"doc_source": "concepts/messages.mdx", "score": 1.2023333311080933}, {"doc_source": "concepts/messages.mdx", "score": 1.2408009767532349}, {"doc_source": "concepts/messages.mdx", "score": 1.2419166564941406}, {"doc_source": "concepts/multimodality.mdx", "score": 1.2602442502975464}, {"doc_source": "concepts/messages.mdx", "score": 1.274167537689209}, {"doc_source": "concepts/messages.mdx", "score": 1.321706771850586}, {"doc_source": "concepts/index.mdx", "score": 1.3373733758926392}, {"doc_source": "concepts/messages.mdx", "score": 1.3381032943725586}, {"doc_source": "how_to/index.mdx", "score": 1.3389719724655151}, {"doc_source": "concepts/messages.mdx", "score": 1.3391928672790527}, {"doc_source": "concepts/multimodality.mdx", "score": 1.3420100212097168}]}
{"ts": 1747974574.315566, "query_id": "Q098", "question": "What is the first step when creating a custom Runnable for streaming transformations?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.8268914818763733}, {"doc_source": "concepts/runnables.mdx", "score": 0.8744097352027893}, {"doc_source": "concepts/runnables.mdx", "score": 0.9601890444755554}, {"doc_source": "concepts/streaming.mdx", "score": 0.9715527296066284}, {"doc_source": "concepts/runnables.mdx", "score": 1.0116742849349976}, {"doc_source": "concepts/streaming.mdx", "score": 1.0618860721588135}, {"doc_source": "concepts/runnables.mdx", "score": 1.0719717741012573}, {"doc_source": "concepts/streaming.mdx", "score": 1.076568365097046}, {"doc_source": "concepts/runnables.mdx", "score": 1.0867959260940552}, {"doc_source": "concepts/streaming.mdx", "score": 1.099592924118042}, {"doc_source": "concepts/runnables.mdx", "score": 1.102736234664917}, {"doc_source": "concepts/streaming.mdx", "score": 1.1200764179229736}, {"doc_source": "concepts/runnables.mdx", "score": 1.1217560768127441}, {"doc_source": "concepts/streaming.mdx", "score": 1.1279449462890625}, {"doc_source": "concepts/streaming.mdx", "score": 1.1351699829101562}]}
{"ts": 1747974576.0206552, "query_id": "Q099", "question": "How does LangChain differentiate official vs community chat model integrations?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.5824944972991943}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.7871352434158325}, {"doc_source": "concepts/chat_models.mdx", "score": 0.79694664478302}, {"doc_source": "how_to/index.mdx", "score": 0.8112407922744751}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8392316699028015}, {"doc_source": "concepts/architecture.mdx", "score": 0.8509944677352905}, {"doc_source": "concepts/chat_models.mdx", "score": 0.8525364995002747}, {"doc_source": "concepts/messages.mdx", "score": 0.8693147301673889}, {"doc_source": "how_to/index.mdx", "score": 0.8970538377761841}, {"doc_source": "concepts/architecture.mdx", "score": 0.901557981967926}, {"doc_source": "concepts/multimodality.mdx", "score": 0.9357731342315674}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9359391927719116}, {"doc_source": "how_to/installation.mdx", "score": 0.9375085234642029}, {"doc_source": "introduction.mdx", "score": 0.9504719972610474}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9513298273086548}]}
{"ts": 1747974578.02351, "query_id": "Q100", "question": "Which two sources explain creating example selectors based on semantic similarity?", "retrieved_items": [{"doc_source": "how_to/index.mdx", "score": 0.9909029006958008}, {"doc_source": "concepts/example_selectors.mdx", "score": 1.006283164024353}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.091930866241455}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.1133822202682495}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1534223556518555}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.1697380542755127}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1754310131072998}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.1814050674438477}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2275516986846924}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.2345815896987915}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.2359466552734375}, {"doc_source": "concepts/retrieval.mdx", "score": 1.237579345703125}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2411596775054932}, {"doc_source": "how_to/index.mdx", "score": 1.2454160451889038}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2587569952011108}]}
{"ts": 1747974579.21036, "query_id": "Q101", "question": "What is the impact of not propagating `RunnableConfig` in Python 3.10 async code?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.6225790977478027}, {"doc_source": "concepts/callbacks.mdx", "score": 0.9566248655319214}, {"doc_source": "concepts/runnables.mdx", "score": 0.9745354652404785}, {"doc_source": "concepts/runnables.mdx", "score": 0.9801317453384399}, {"doc_source": "concepts/runnables.mdx", "score": 0.9940198659896851}, {"doc_source": "concepts/runnables.mdx", "score": 1.0587245225906372}, {"doc_source": "concepts/async.mdx", "score": 1.085208535194397}, {"doc_source": "concepts/runnables.mdx", "score": 1.1024963855743408}, {"doc_source": "concepts/async.mdx", "score": 1.1134986877441406}, {"doc_source": "concepts/runnables.mdx", "score": 1.1231186389923096}, {"doc_source": "concepts/runnables.mdx", "score": 1.1455885171890259}, {"doc_source": "concepts/tools.mdx", "score": 1.1553400754928589}, {"doc_source": "concepts/runnables.mdx", "score": 1.1744250059127808}, {"doc_source": "concepts/runnables.mdx", "score": 1.189570426940918}, {"doc_source": "concepts/async.mdx", "score": 1.2117208242416382}]}
{"ts": 1747974581.157705, "query_id": "Q102", "question": "Describe the `@tool` option to hide a parameter while still injecting it at runtime.", "retrieved_items": [{"doc_source": "concepts/tools.mdx", "score": 0.8319226503372192}, {"doc_source": "concepts/tools.mdx", "score": 0.8824236392974854}, {"doc_source": "concepts/tools.mdx", "score": 1.0317213535308838}, {"doc_source": "concepts/tools.mdx", "score": 1.081040382385254}, {"doc_source": "concepts/tools.mdx", "score": 1.0918346643447876}, {"doc_source": "how_to/index.mdx", "score": 1.0966284275054932}, {"doc_source": "concepts/tools.mdx", "score": 1.101292610168457}, {"doc_source": "concepts/tools.mdx", "score": 1.13003408908844}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1631686687469482}, {"doc_source": "concepts/tools.mdx", "score": 1.1794569492340088}, {"doc_source": "concepts/tools.mdx", "score": 1.190786600112915}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.1920857429504395}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2235742807388306}, {"doc_source": "concepts/tools.mdx", "score": 1.235705852508545}, {"doc_source": "concepts/tools.mdx", "score": 1.24996817111969}]}
{"ts": 1747974582.760121, "query_id": "Q103", "question": "Which notebook illustrates using an ensemble retriever strategy?", "retrieved_items": [{"doc_source": "concepts/retrievers.mdx", "score": 0.8426902890205383}, {"doc_source": "concepts/retrievers.mdx", "score": 0.9529894590377808}, {"doc_source": "concepts/retrievers.mdx", "score": 1.0341742038726807}, {"doc_source": "how_to/index.mdx", "score": 1.0524252653121948}, {"doc_source": "concepts/retrievers.mdx", "score": 1.122220516204834}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1299257278442383}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1389737129211426}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1463559865951538}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1495258808135986}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1707154512405396}, {"doc_source": "how_to/index.mdx", "score": 1.1904109716415405}, {"doc_source": "concepts/retrievers.mdx", "score": 1.1962287425994873}, {"doc_source": "concepts/retrieval.mdx", "score": 1.1980725526809692}, {"doc_source": "concepts/retrieval.mdx", "score": 1.2106939554214478}, {"doc_source": "concepts/rag.mdx", "score": 1.21422278881073}]}
{"ts": 1747974584.547735, "query_id": "Q104", "question": "Give two benefits of the LangChain Expression Language (LCEL).", "retrieved_items": [{"doc_source": "concepts/lcel.mdx", "score": 0.6549113988876343}, {"doc_source": "concepts/lcel.mdx", "score": 0.7974624633789062}, {"doc_source": "concepts/lcel.mdx", "score": 0.799213707447052}, {"doc_source": "concepts/lcel.mdx", "score": 0.8415046334266663}, {"doc_source": "concepts/lcel.mdx", "score": 0.8660224676132202}, {"doc_source": "concepts/lcel.mdx", "score": 0.8771257996559143}, {"doc_source": "how_to/index.mdx", "score": 0.8891428709030151}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9463222026824951}, {"doc_source": "concepts/why_langchain.mdx", "score": 0.9860144853591919}, {"doc_source": "concepts/lcel.mdx", "score": 0.9972375631332397}, {"doc_source": "concepts/lcel.mdx", "score": 1.015416145324707}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0171955823898315}, {"doc_source": "how_to/index.mdx", "score": 1.023201823234558}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.0494506359100342}, {"doc_source": "concepts/index.mdx", "score": 1.0505870580673218}]}
{"ts": 1747974586.053192, "query_id": "Q105", "question": "What is the main purpose of the `rate_limiter` argument on ChatModels?", "retrieved_items": [{"doc_source": "concepts/chat_models.mdx", "score": 0.7163459062576294}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9844474196434021}, {"doc_source": "concepts/chat_models.mdx", "score": 0.9850702285766602}, {"doc_source": "concepts/chat_models.mdx", "score": 1.0045276880264282}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1289957761764526}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1580770015716553}, {"doc_source": "concepts/chat_models.mdx", "score": 1.180097222328186}, {"doc_source": "concepts/chat_models.mdx", "score": 1.1922576427459717}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2081583738327026}, {"doc_source": "concepts/chat_models.mdx", "score": 1.208539605140686}, {"doc_source": "concepts/runnables.mdx", "score": 1.2108354568481445}, {"doc_source": "concepts/chat_models.mdx", "score": 1.224280595779419}, {"doc_source": "concepts/runnables.mdx", "score": 1.2280248403549194}, {"doc_source": "concepts/chat_models.mdx", "score": 1.2285503149032593}, {"doc_source": "concepts/index.mdx", "score": 1.2297008037567139}]}
{"ts": 1747974587.738672, "query_id": "Q106", "question": "How many tokens is 100 English words approximately equal to?", "retrieved_items": [{"doc_source": "concepts/tokens.mdx", "score": 0.7296366691589355}, {"doc_source": "concepts/tokens.mdx", "score": 1.0422085523605347}, {"doc_source": "concepts/tokens.mdx", "score": 1.0989744663238525}, {"doc_source": "concepts/tokens.mdx", "score": 1.1050974130630493}, {"doc_source": "concepts/tokens.mdx", "score": 1.1922731399536133}, {"doc_source": "concepts/tokens.mdx", "score": 1.31990647315979}, {"doc_source": "concepts/index.mdx", "score": 1.3359782695770264}, {"doc_source": "concepts/chat_models.mdx", "score": 1.348307728767395}, {"doc_source": "concepts/chat_models.mdx", "score": 1.3920774459838867}, {"doc_source": "concepts/text_splitters.mdx", "score": 1.4086834192276}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.443727731704712}, {"doc_source": "concepts/retrievers.mdx", "score": 1.4541678428649902}, {"doc_source": "concepts/retrieval.mdx", "score": 1.4581822156906128}, {"doc_source": "concepts/retrieval.mdx", "score": 1.46946382522583}, {"doc_source": "how_to/document_loader_json.mdx", "score": 1.4750642776489258}]}
{"ts": 1747974590.002157, "query_id": "Q107", "question": "Explain the difference between `invoke` and `ainvoke`.", "retrieved_items": [{"doc_source": "concepts/streaming.mdx", "score": 0.9343318939208984}, {"doc_source": "concepts/async.mdx", "score": 1.1376290321350098}, {"doc_source": "concepts/async.mdx", "score": 1.2079036235809326}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.2120720148086548}, {"doc_source": "concepts/streaming.mdx", "score": 1.2382277250289917}, {"doc_source": "concepts/runnables.mdx", "score": 1.2671971321105957}, {"doc_source": "concepts/chat_models.mdx", "score": 1.286611557006836}, {"doc_source": "concepts/tools.mdx", "score": 1.3331979513168335}, {"doc_source": "concepts/async.mdx", "score": 1.3368849754333496}, {"doc_source": "concepts/async.mdx", "score": 1.400622844696045}, {"doc_source": "concepts/tool_calling.mdx", "score": 1.4019262790679932}, {"doc_source": "concepts/messages.mdx", "score": 1.4027173519134521}, {"doc_source": "concepts/tools.mdx", "score": 1.4106988906860352}, {"doc_source": "concepts/retrieval.mdx", "score": 1.412031888961792}, {"doc_source": "concepts/tools.mdx", "score": 1.4130761623382568}]}
{"ts": 1747974591.278837, "query_id": "Q108", "question": "What does `batch_as_completed` return with each result to help match inputs?", "retrieved_items": [{"doc_source": "concepts/runnables.mdx", "score": 0.9437086582183838}, {"doc_source": "concepts/runnables.mdx", "score": 0.9991036653518677}, {"doc_source": "concepts/runnables.mdx", "score": 1.1365041732788086}, {"doc_source": "concepts/runnables.mdx", "score": 1.219710111618042}, {"doc_source": "concepts/runnables.mdx", "score": 1.3332375288009644}, {"doc_source": "concepts/lcel.mdx", "score": 1.3625571727752686}, {"doc_source": "concepts/streaming.mdx", "score": 1.3667690753936768}, {"doc_source": "concepts/index.mdx", "score": 1.3682972192764282}, {"doc_source": "concepts/runnables.mdx", "score": 1.3812754154205322}, {"doc_source": "concepts/streaming.mdx", "score": 1.3924541473388672}, {"doc_source": "concepts/async.mdx", "score": 1.400485873222351}, {"doc_source": "concepts/runnables.mdx", "score": 1.406741738319397}, {"doc_source": "concepts/streaming.mdx", "score": 1.409852147102356}, {"doc_source": "concepts/runnables.mdx", "score": 1.4185580015182495}, {"doc_source": "concepts/streaming.mdx", "score": 1.420438289642334}]}
{"ts": 1747974592.588032, "query_id": "Q109", "question": "Why might you choose Chroma over FAISS for prototyping?", "retrieved_items": [{"doc_source": "how_to/vectorstores.mdx", "score": 0.9985740184783936}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3059722185134888}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.328582763671875}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3428386449813843}, {"doc_source": "concepts/vectorstores.mdx", "score": 1.3775885105133057}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.3823306560516357}, {"doc_source": "how_to/vectorstores.mdx", "score": 1.4028398990631104}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.4060779809951782}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.4097108840942383}, {"doc_source": "concepts/embedding_models.mdx", "score": 1.4127839803695679}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.4173530340194702}, {"doc_source": "concepts/chat_models.mdx", "score": 1.426792860031128}, {"doc_source": "concepts/multimodality.mdx", "score": 1.4293193817138672}, {"doc_source": "concepts/few_shot_prompting.mdx", "score": 1.4349693059921265}, {"doc_source": "concepts/why_langchain.mdx", "score": 1.4381399154663086}]}
